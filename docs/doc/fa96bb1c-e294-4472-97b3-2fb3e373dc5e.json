{
    "summary": "The code utilizes PaddleVideo for video inference, including preprocessing steps and various action recognition techniques. It also offers classes for human detection and pose estimation which can be used for classification or object detection tasks in videos with NMS and label/probability display.",
    "details": [
        {
            "comment": "This code block is an import and error handling section for various Python libraries such as imageio, matplotlib, and json. It also contains license information and warning messages for required packages.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":0-33",
            "content": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport json\nimport os\nimport shutil\nimport sys\nfrom typing import List\nimport pickle\nimport cv2\ntry:\n    import imageio\nexcept ImportError as e:\n    print(\n        f\"Warning! {e}, [imageio] package and it's dependencies is required for VideoSwin.\"\n    )\ntry:\n    import matplotlib as mpl\n    import matplotlib.cm as cm\nexcept ImportError as e:\n    print(\n        f\"Warning! {e}, [matplotlib] package and it's dependencies is required for ADDS.\""
        },
        {
            "comment": "This code imports necessary libraries, defines a directory path, appends the directory to the system path, and imports classes and functions from various modules within the PaddleVideo framework. It also includes abstract methods for building pipelines and metrics, as well as utility functions for model segmentation and post-processing.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":34-57",
            "content": "    )\nimport numpy as np\nimport paddle\nimport paddle.nn.functional as F\nimport pandas\nfrom PIL import Image\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.abspath(os.path.join(__dir__, '../')))\nfrom abc import abstractmethod\nfrom paddlevideo.loader.builder import build_pipeline\nfrom paddlevideo.loader.pipelines import (\n    AutoPadding, CenterCrop, DecodeSampler, FeatureDecoder, FrameDecoder,\n    GroupResize, Image2Array, ImageDecoder, JitterScale, MultiCrop,\n    Normalization, PackOutput, Sampler, SamplerPkl, Scale, SkeletonNorm,\n    TenCrop, ToArray, UniformCrop, VideoDecoder, SegmentationSampler,\n    SketeonCropSample, MultiCenterCrop, SketeonCropSample, UniformSampleFrames,\n    PoseDecode, PoseCompact, Resize, CenterCrop_V2, GeneratePoseTarget,\n    FormatShape, Collect)\nfrom paddlevideo.metrics.ava_utils import read_labelmap\nfrom paddlevideo.metrics.bmn_metric import boundary_choose, soft_nms\nfrom paddlevideo.utils import Registry, build, get_config\nfrom paddlevideo.modeling.framework.segmenters.utils import ASRFPostProcessing"
        },
        {
            "comment": "This code imports functions from the \"ava_predict\" and \"yowo_utils\" modules. It defines a function called \"build_inference_helper\" which uses the \"Registry\" class to build an inference helper object. The base class for this object is defined as \"Base_Inference_helper\". This class has an initializer that takes arguments for number of segmentations, length of each segmentation, short size, target size, and top k.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":59-85",
            "content": "from tools.ava_predict import (detection_inference, frame_extraction,\n                               get_detection_result, get_timestep_result,\n                               pack_result, visualize)\nfrom paddlevideo.modeling.framework.localizers.yowo_utils import nms, get_region_boxes\nINFERENCE = Registry('inference')\ndef build_inference_helper(cfg):\n    return build(cfg, INFERENCE)\nclass Base_Inference_helper():\n    def __init__(self,\n                 num_seg=8,\n                 seg_len=1,\n                 short_size=256,\n                 target_size=224,\n                 top_k=1):\n        \"\"\"Base_Inference_helper\n        Args:\n            num_seg (int, optional): number of segmentations of an sliced input video. Defaults to 8.\n            seg_len (int, optional): length of each segmentation. Defaults to 1.\n            short_size (int, optional): short size of input video. Defaults to 256.\n            target_size (int, optional): size of cropped video. Defaults to 224.\n            top_k (int, optional): select topk result in outputs. Defaults to 1."
        },
        {
            "comment": "This code defines an abstract class with a preprocess method and a concrete implementation of the preprocess_batch method. The class has attributes for the number of segments, segment length, short size, target size, and top k. The preprocess_batch method processes each input file in a list of file paths and concatenates the processed data into batches. The input files are stored in the self.input\\_file attribute.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":86-120",
            "content": "        \"\"\"\n        self.num_seg = num_seg\n        self.seg_len = seg_len\n        self.short_size = short_size\n        self.target_size = target_size\n        self.top_k = top_k\n    @abstractmethod\n    def preprocess(self, input_file: str):\n        \"\"\"preprocess abstractmethod\n        Args:\n            input_file (str): input file path.\n        \"\"\"\n        pass\n    def preprocess_batch(self, file_list: List[str]) -> List[np.ndarray]:\n        \"\"\"preprocess for file list\n        Args:\n            file_list (List[str]): file pathes in an list, [path1, path2, ...].\n        Returns:\n            List[np.ndarray]: batched inputs data, [data_batch[0], data_batch[1], ...].\n        \"\"\"\n        batched_inputs = []\n        for file in file_list:\n            inputs = self.preprocess(file)\n            batched_inputs.append(inputs)\n        batched_inputs = [\n            np.concatenate([item[i] for item in batched_inputs])\n            for i in range(len(batched_inputs[0]))\n        ]\n        self.input_file = file_list\n        return batched_inputs"
        },
        {
            "comment": "This function postprocesses output scores from a model, accepting batched output scores as input. It checks if the input file is a list and reshapes the output array accordingly. The code applies softmax to each individual output tensor along the last axis, then iterates over the number of inputs (N) to generate class predictions. Classes are sorted based on their scores, and the results are stored in a list for further use.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":122-146",
            "content": "    def postprocess(self,\n                    output: np.ndarray,\n                    print_output: bool = True,\n                    return_result: bool = False):\n        \"\"\"postprocess\n        Args:\n            output (np.ndarray): batched output scores, shape of (batch_size, class_num).\n            print_output (bool, optional): whether to print result. Defaults to True.\n        \"\"\"\n        if not isinstance(self.input_file, list):\n            self.input_file = [\n                self.input_file,\n            ]\n        output = output[0]  # [B, num_cls]\n        N = len(self.input_file)\n        if output.shape[0] != N:\n            output = output.reshape([N] + [output.shape[0] // N] +\n                                    list(output.shape[1:]))  # [N, T, C]\n            output = output.mean(axis=1)  # [N, C]\n        output = F.softmax(paddle.to_tensor(output), axis=-1).numpy()\n        results_list = []\n        for i in range(N):\n            classes = np.argpartition(output[i], -self.top_k)[-self.top_k:]\n            classes = classes[np.argsort(-output[i, classes])]"
        },
        {
            "comment": "This code is creating a helper class for inference tasks. It takes input files, performs video classification using the PaddleVideo framework, and returns top-k class results for each video file. The class also has options to print output and return results as a list. The user can customize the number of segments, segment length, short side size, target size, and top-k values for the classification.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":147-175",
            "content": "            scores = output[i, classes]\n            topk_class = classes[:self.top_k]\n            topk_scores = scores[:self.top_k]\n            result = {\n                \"video_id\": self.input_file[i],\n                \"topk_class\": topk_class,\n                \"topk_scores\": topk_scores\n            }\n            results_list.append(result)\n            if print_output:\n                print(\"Current video file: {0}\".format(self.input_file[i]))\n                print(\"\\ttop-{0} class: {1}\".format(self.top_k, topk_class))\n                print(\"\\ttop-{0} score: {1}\".format(self.top_k, topk_scores))\n        if return_result:\n            return results_list\n@INFERENCE.register()\nclass ppTSM_Inference_helper(Base_Inference_helper):\n    def __init__(self,\n                 num_seg=8,\n                 seg_len=1,\n                 short_size=256,\n                 target_size=224,\n                 top_k=1):\n        self.num_seg = num_seg\n        self.seg_len = seg_len\n        self.short_size = short_size\n        self.target_size = target_size"
        },
        {
            "comment": "This code defines a class that takes in an input file path, applies several image preprocessing operations such as decoding, sampling, resizing, cropping, and normalization, then returns the processed image data in a list. The class also initializes some parameters like the number of segments, segment length, short size for resizing, target size for cropping, and top k value. The code is part of PaddleVideo library.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":176-210",
            "content": "        self.top_k = top_k\n    def preprocess(self, input_file):\n        \"\"\"\n        input_file: str, file path\n        return: list\n        \"\"\"\n        assert os.path.isfile(input_file) is not None, \"{0} not exists\".format(\n            input_file)\n        results = {'filename': input_file}\n        img_mean = [0.485, 0.456, 0.406]\n        img_std = [0.229, 0.224, 0.225]\n        ops = [\n            VideoDecoder(backend=\"decord\"),\n            Sampler(self.num_seg, self.seg_len, valid_mode=True),\n            Scale(self.short_size),\n            CenterCrop(self.target_size),\n            Image2Array(),\n            Normalization(img_mean, img_std)\n        ]\n        for op in ops:\n            results = op(results)\n        res = np.expand_dims(results['imgs'], axis=0).copy()\n        return [res]\n@INFERENCE.register()\nclass ppTSN_Inference_helper(Base_Inference_helper):\n    def __init__(self,\n                 num_seg=25,\n                 seg_len=1,\n                 short_size=256,\n                 target_size=224,\n                 top_k=1):"
        },
        {
            "comment": "This code snippet initializes a class object with several parameters (num_seg, seg_len, short_size, target_size, top_k) and defines a preprocess method. The preprocess method takes an input file path, performs various operations on the image using different ops such as VideoDecoder, Sampler, Scale, TenCrop, Image2Array, Normalization in sequence, and returns an array of processed images.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":211-244",
            "content": "        self.num_seg = num_seg\n        self.seg_len = seg_len\n        self.short_size = short_size\n        self.target_size = target_size\n        self.top_k = top_k\n    def preprocess(self, input_file):\n        \"\"\"\n        input_file: str, file path\n        return: list\n        \"\"\"\n        assert os.path.isfile(input_file) is not None, \"{0} not exists\".format(\n            input_file)\n        results = {'filename': input_file}\n        img_mean = [0.485, 0.456, 0.406]\n        img_std = [0.229, 0.224, 0.225]\n        ops = [\n            VideoDecoder(backend=\"decord\"),\n            Sampler(self.num_seg,\n                    self.seg_len,\n                    valid_mode=True,\n                    select_left=True),\n            Scale(self.short_size,\n                  fixed_ratio=True,\n                  do_round=True,\n                  backend='cv2'),\n            TenCrop(self.target_size),\n            Image2Array(),\n            Normalization(img_mean, img_std)\n        ]\n        for op in ops:\n            results = op(results)\n        res = np.expand_dims(results['imgs'], axis=0).copy()"
        },
        {
            "comment": "This function serves as a helper class for BMN inference and handles preprocessing of input files. It loads and preprocesses the features from the specified file path, converts them to float32 type, and returns the result in a list format. The postprocess function takes outputs as input, assuming it is a list containing predicted BMN, start time, and end time values.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":245-277",
            "content": "        return [res]\n@INFERENCE.register()\nclass BMN_Inference_helper(Base_Inference_helper):\n    def __init__(self, feat_dim, dscale, tscale, result_path):\n        self.feat_dim = feat_dim\n        self.dscale = dscale\n        self.tscale = tscale\n        self.result_path = result_path\n        if not os.path.isdir(self.result_path):\n            os.makedirs(self.result_path)\n    def preprocess(self, input_file):\n        \"\"\"\n        input_file: str, file path\n        return: list\n        \"\"\"\n        assert os.path.isfile(input_file) is not None, \"{0} not exists\".format(\n            input_file)\n        file_info = json.load(open(input_file))\n        self.feat_path = file_info['feat_path']\n        self.video_duration = file_info['duration_second']\n        feat = np.load(self.feat_path).astype('float32').T\n        res = np.expand_dims(feat, axis=0).copy()\n        return [res]\n    def postprocess(self, outputs, print_output=True):\n        \"\"\"\n        output: list\n        \"\"\"\n        pred_bm, pred_start, pred_end = outputs"
        },
        {
            "comment": "This code defines a function _gen_props that calculates snippet xmin and xmax values, generates start and end masks from pred_start and pred_end, and initializes score_vector_list. It iterates over the dscale and tscale to determine start and end indices, checks if valid indices are found, and assigns xmin, xmax, xmin_score, and xmax_score accordingly.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":278-301",
            "content": "        self._gen_props(pred_bm, pred_start[0], pred_end[0], print_output)\n    def _gen_props(self, pred_bm, pred_start, pred_end, print_output):\n        snippet_xmins = [1.0 / self.tscale * i for i in range(self.tscale)]\n        snippet_xmaxs = [\n            1.0 / self.tscale * i for i in range(1, self.tscale + 1)\n        ]\n        pred_bm = pred_bm[0, 0, :, :] * pred_bm[0, 1, :, :]\n        start_mask = boundary_choose(pred_start)\n        start_mask[0] = 1.\n        end_mask = boundary_choose(pred_end)\n        end_mask[-1] = 1.\n        score_vector_list = []\n        for idx in range(self.dscale):\n            for jdx in range(self.tscale):\n                start_index = jdx\n                end_index = start_index + idx\n                if end_index < self.tscale and start_mask[\n                        start_index] == 1 and end_mask[end_index] == 1:\n                    xmin = snippet_xmins[start_index]\n                    xmax = snippet_xmaxs[end_index]\n                    xmin_score = pred_start[start_index]\n                    xmax_score = pred_end[end_index]"
        },
        {
            "comment": "This code performs non-maximum suppression (NMS) on bounding box predictions, selects top-5 predictions for each video feature path, and stores the results in a dictionary. It also prints the top-5 predictions if `print_output` is enabled.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":302-327",
            "content": "                    bm_score = pred_bm[idx, jdx]\n                    conf_score = xmin_score * xmax_score * bm_score\n                    score_vector_list.append([xmin, xmax, conf_score])\n        cols = [\"xmin\", \"xmax\", \"score\"]\n        score_vector_list = np.stack(score_vector_list)\n        df = pandas.DataFrame(score_vector_list, columns=cols)\n        result_dict = {}\n        proposal_list = []\n        df = soft_nms(df, alpha=0.4, t1=0.55, t2=0.9)\n        for idx in range(min(100, len(df))):\n            tmp_prop={\"score\":df.score.values[idx], \\\n                      \"segment\":[max(0,df.xmin.values[idx])*self.video_duration, \\\n                                 min(1,df.xmax.values[idx])*self.video_duration]}\n            proposal_list.append(tmp_prop)\n        result_dict[self.feat_path] = proposal_list\n        # print top-5 predictions\n        if print_output:\n            print(\"Current video file: {0} :\".format(self.feat_path))\n            for pred in proposal_list[:5]:\n                print(pred)\n        # save result"
        },
        {
            "comment": "This code defines a class called TokenShift_Inference_helper, which extends Base_Inference_helper. It has several parameters for customizing the inference process and includes a preprocess method that reads an input file and returns results as a dictionary. The results are then written to a JSON file named \"bmn_results_inference.json\".",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":328-361",
            "content": "        outfile = open(\n            os.path.join(self.result_path, \"bmn_results_inference.json\"), \"w\")\n        json.dump(result_dict, outfile)\n@INFERENCE.register()\nclass TokenShift_Inference_helper(Base_Inference_helper):\n    def __init__(self,\n                 num_seg=8,\n                 seg_len=1,\n                 short_size=256,\n                 target_size=256,\n                 top_k=1,\n                 mean=[0.5, 0.5, 0.5],\n                 std=[0.5, 0.5, 0.5]):\n        self.num_seg = num_seg\n        self.seg_len = seg_len\n        self.short_size = short_size\n        self.target_size = target_size\n        self.top_k = top_k\n        self.mean = mean\n        self.std = std\n    def preprocess(self, input_file):\n        \"\"\"\n        input_file: str, file path\n        return: list\n        \"\"\"\n        assert os.path.isfile(input_file) is not None, \"{0} not exists\".format(\n            input_file)\n        results = {'filename': input_file}\n        ops = [\n            VideoDecoder(backend='pyav', mode='test', num_seg=self.num_seg),"
        },
        {
            "comment": "The code creates a series of data processing operations to preprocess input images for the TimeSformer model. It initializes an instance of TimeSformer_Inference_helper with specified parameters, then applies these operations in order on the input image, resulting in a final tensor ready for model inference.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":362-394",
            "content": "            Sampler(self.num_seg, self.seg_len, valid_mode=True),\n            Normalization(self.mean, self.std, tensor_shape=[1, 1, 1, 3]),\n            Image2Array(data_format='cthw'),\n            JitterScale(self.short_size, self.short_size),\n            MultiCenterCrop(self.target_size)\n        ]\n        for op in ops:\n            results = op(results)\n        # [N,C,Tx3,H,W]\n        res = np.expand_dims(results['imgs'], axis=0).copy()\n        return [res]\n@INFERENCE.register()\nclass TimeSformer_Inference_helper(Base_Inference_helper):\n    def __init__(self,\n                 num_seg=8,\n                 seg_len=1,\n                 short_size=224,\n                 target_size=224,\n                 top_k=1,\n                 mean=[0.45, 0.45, 0.45],\n                 std=[0.225, 0.225, 0.225]):\n        self.num_seg = num_seg\n        self.seg_len = seg_len\n        self.short_size = short_size\n        self.target_size = target_size\n        self.top_k = top_k\n        self.mean = mean\n        self.std = std\n    def preprocess(self, input_file):"
        },
        {
            "comment": "This code defines a function that reads an input file, applies a series of operations to it, and returns the processed data. The operations include video decoding, sampling, normalization, image conversion, jitter scaling, and uniform cropping. The result is a tensor in the shape [N,C,Tx3,H,W], where N is the number of segments, C is the number of channels, Tx3 is the number of frames, H is the height, and W is the width.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":395-426",
            "content": "        \"\"\"\n        input_file: str, file path\n        return: list\n        \"\"\"\n        assert os.path.isfile(input_file) is not None, \"{0} not exists\".format(\n            input_file)\n        results = {'filename': input_file}\n        ops = [\n            VideoDecoder(backend='pyav', mode='test', num_seg=self.num_seg),\n            Sampler(self.num_seg,\n                    self.seg_len,\n                    valid_mode=True,\n                    linspace_sample=True),\n            Normalization(self.mean, self.std, tensor_shape=[1, 1, 1, 3]),\n            Image2Array(data_format='cthw'),\n            JitterScale(self.short_size, self.short_size),\n            UniformCrop(self.target_size)\n        ]\n        for op in ops:\n            results = op(results)\n        # [N,C,Tx3,H,W]\n        res = np.expand_dims(results['imgs'], axis=0).copy()\n        return [res]\n@INFERENCE.register()\nclass VideoSwin_Inference_helper(Base_Inference_helper):\n    def __init__(self,\n                 num_seg=4,\n                 seg_len=32,\n                 frame_interval=2,"
        },
        {
            "comment": "This code defines a class for video preprocessing, taking input file path as parameter. It checks if the file exists and stores the filename in results dictionary. The class uses Decord backend for video decoding and Sampler to sample frames based on specified parameters.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":427-457",
            "content": "                 short_size=224,\n                 target_size=224,\n                 top_k=1,\n                 mean=[123.675, 116.28, 103.53],\n                 std=[58.395, 57.12, 57.375]):\n        self.num_seg = num_seg\n        self.seg_len = seg_len\n        self.frame_interval = frame_interval\n        self.short_size = short_size\n        self.target_size = target_size\n        self.top_k = top_k\n        self.mean = mean\n        self.std = std\n    def preprocess(self, input_file):\n        \"\"\"\n        input_file: str, file path\n        return: list\n        \"\"\"\n        self.input_file = input_file\n        assert os.path.isfile(input_file) is not None, \"{0} not exists\".format(\n            input_file)\n        results = {'filename': input_file}\n        ops = [\n            VideoDecoder(backend='decord', mode='valid'),\n            Sampler(num_seg=self.num_seg,\n                    frame_interval=self.frame_interval,\n                    seg_len=self.seg_len,\n                    valid_mode=True,\n                    use_pil=False),"
        },
        {
            "comment": "The code preprocesses images by resizing, cropping, normalizing, and converting to arrays. It also provides a postprocessing function that handles outputs for multiple input files if necessary.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":458-488",
            "content": "            Scale(short_size=self.short_size,\n                  fixed_ratio=False,\n                  keep_ratio=True,\n                  backend='cv2',\n                  do_round=True),\n            CenterCrop(target_size=224, backend='cv2'),\n            Normalization(mean=self.mean,\n                          std=self.std,\n                          tensor_shape=[3, 1, 1, 1],\n                          inplace=True),\n            Image2Array(data_format='cthw')\n        ]\n        for op in ops:\n            results = op(results)\n        res = np.expand_dims(results['imgs'], axis=0).copy()\n        return [res]\n    def postprocess(self, output, print_output=True):\n        \"\"\"\n        output: list\n        \"\"\"\n        if not isinstance(self.input_file, list):\n            self.input_file = [\n                self.input_file,\n            ]\n        output = output[0]  # [B, num_cls]\n        N = len(self.input_file)\n        if output.shape[0] != N:\n            output = output.reshape([N] + [output.shape[0] // N] +\n                                    list(output.shape[1:]))  # [N, T, C]"
        },
        {
            "comment": "This code snippet is part of a function that extracts the top k classes and their corresponding scores from an output tensor. It first performs mean pooling along the axis 1 to reshape the tensor to [N, C] format, where N is the number of images and C is the number of channels. Then, it iterates over each image and finds the indexes of top k classes by performing argument partition and sorting them based on their scores. Finally, it prints out these results for each image if the print_output flag is set to True.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":489-515",
            "content": "            output = output.mean(axis=1)  # [N, C]\n        for i in range(N):\n            classes = np.argpartition(output[i], -self.top_k)[-self.top_k:]\n            classes = classes[np.argsort(-output[i, classes])]\n            scores = output[i, classes]\n            if print_output:\n                print(\"Current video file: {0}\".format(self.input_file[i]))\n                for j in range(self.top_k):\n                    print(\"\\ttop-{0} class: {1}\".format(j + 1, classes[j]))\n                    print(\"\\ttop-{0} score: {1}\".format(j + 1, scores[j]))\n@INFERENCE.register()\nclass VideoSwin_TableTennis_Inference_helper(Base_Inference_helper):\n    def __init__(self,\n                 num_seg=1,\n                 seg_len=32,\n                 short_size=256,\n                 target_size=224,\n                 top_k=1):\n        self.num_seg = num_seg\n        self.seg_len = seg_len\n        self.short_size = short_size\n        self.target_size = target_size\n        self.top_k = top_k\n    def preprocess(self, input_file):"
        },
        {
            "comment": "This code defines a function that takes an input file, reads frames from it, applies various transformations including decoding, sampling, scaling, cropping, and normalization, and finally converts the resulting images to a numpy array. It uses the PaddleVideo library and has parameters for short_size, target_size, and num_seg.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":516-541",
            "content": "        \"\"\"\n        input_file: str, file path\n        return: list\n        \"\"\"\n        assert os.path.isfile(input_file) is not None, \"{0} not exists\".format(\n            input_file)\n        results = {'frame_dir': input_file, 'suffix': 'img_{:05}.jpg'}\n        img_mean = [123.675, 116.28, 103.53]\n        img_std = [58.395, 57.12, 57.375]\n        ops = [\n            FrameDecoder(),\n            SamplerPkl(num_seg=self.num_seg,\n                       seg_len=self.seg_len,\n                       backend='cv2',\n                       valid_mode=True),\n            Scale(short_size=self.short_size,\n                  fixed_ratio=False,\n                  keep_ratio=True,\n                  backend='cv2',\n                  do_round=True),\n            UniformCrop(target_size=self.target_size, backend='cv2'),\n            Normalization(mean=img_mean,\n                          std=img_std,\n                          tensor_shape=[3, 1, 1, 1],\n                          inplace=True),\n            Image2Array(data_format='cthw')"
        },
        {
            "comment": "The code snippet is adding text to a video. It creates directories, loads or captures frames from the video, and extracts important information like frame length, FPS, and frame width. The code then calls other functions to manipulate images and add text to each frame before storing or displaying the final result.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":542-572",
            "content": "        ]\n        for op in ops:\n            results = op(results)\n        res = np.expand_dims(results['imgs'], axis=0).copy()\n        return [res]\n    def add_text_to_video(\n            self,\n            video_path,\n            output_dir=\"applications/TableTennis/ActionRecognition/results\",\n            text=None):\n        os.makedirs(output_dir, exist_ok=True)\n        if video_path.endswith('.pkl'):\n            try:\n                import cPickle as pickle\n                from cStringIO import StringIO\n            except ImportError:\n                import pickle\n                from io import BytesIO\n            from PIL import Image\n            data_loaded = pickle.load(open(video_path, 'rb'), encoding='bytes')\n            _, _, frames = data_loaded\n            frames_len = len(frames)\n        else:\n            videoCapture = cv2.VideoCapture()\n            videoCapture.open(video_path)\n            fps = videoCapture.get(cv2.CAP_PROP_FPS)\n            frame_width = int(videoCapture.get(cv2.CAP_PROP_FRAME_WIDTH))"
        },
        {
            "comment": "The code reads the video frames and resizes them, then converts to RGB format. If the file is a .pkl file, it opens the image from binary data. It also adds text to each frame using cv2.putText. The code appends each frame in RGB format to a list, and finally, releases the videoCapture object, closes all windows, and saves the resulting GIF with a specific filename.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":573-594",
            "content": "            frame_height = int(videoCapture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n            frames_len = videoCapture.get(cv2.CAP_PROP_FRAME_COUNT)\n            print(\"fps=\", int(fps), \"frames=\", int(frames_len), \"scale=\",\n                  f\"{frame_height}x{frame_width}\")\n        frames_rgb_list = []\n        for i in range(int(frames_len)):\n            if video_path.endswith('.pkl'):\n                frame = np.array(\n                    Image.open(BytesIO(frames[i])).convert(\"RGB\").resize(\n                        (240, 135)))[:, :, ::-1].astype('uint8')\n            else:\n                _, frame = videoCapture.read()\n            frame = cv2.putText(frame, text, (30, 30), cv2.FONT_HERSHEY_COMPLEX,\n                                1.0, (0, 0, 255), 2)\n            frames_rgb_list.append(frame[:, :, ::-1])  # bgr to rgb\n        if not video_path.endswith('.pkl'):\n            videoCapture.release()\n        cv2.destroyAllWindows()\n        output_filename = os.path.basename(video_path)\n        output_filename = output_filename.split('.')[0] + '.gif'"
        },
        {
            "comment": "The function `postprocess` takes an output list and processes it according to the specified parameters. It ensures that the shape of the input matches with the number of files in the input_file list, then calculates class scores for each video file. If print_output is True, it will print the current video file being processed. Finally, if save_gif is True, it creates a GIF using the frames_rgb_list and saves it to the specified output directory with the filename mentioned in the function call.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":595-619",
            "content": "        imageio.mimsave(f'{output_dir}/{output_filename}',\n                        frames_rgb_list,\n                        'GIF',\n                        duration=0.00085)\n    def postprocess(self, output, print_output=True, save_gif=True):\n        \"\"\"\n        output: list\n        \"\"\"\n        if not isinstance(self.input_file, list):\n            self.input_file = [\n                self.input_file,\n            ]\n        output = output[0]  # [B, num_cls]\n        N = len(self.input_file)\n        if output.shape[0] != N:\n            output = output.reshape([N] + [output.shape[0] // N] +\n                                    list(output.shape[1:]))  # [N, T, C]\n            output = output.mean(axis=1)  # [N, C]\n        for i in range(N):\n            classes = np.argpartition(output[i], -self.top_k)[-self.top_k:]\n            classes = classes[np.argsort(-output[i, classes])]\n            scores = output[i, classes]\n            if print_output:\n                print(\"Current video file: {0}\".format(self.input_file[i]))"
        },
        {
            "comment": "This code is a part of PaddleVideo's utils.py file, specifically the SlowFast_Inference_helper class, which handles video inference using the SlowFast model. The class has attributes for number of frames, sampling rate, target size, alpha value, and top k classes to display. It contains methods like preprocess, add_text_to_video, and infer. In this section, it displays the top-1 class and score for each frame in a video and adds text annotations to the first frame of the video if save_gif is set to True.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":620-650",
            "content": "                for j in range(self.top_k):\n                    print(\"\\ttop-{0} class: {1}\".format(j + 1, classes[j]))\n                    print(\"\\ttop-{0} score: {1}\".format(j + 1, scores[j]))\n            if save_gif:\n                self.add_text_to_video(\n                    self.input_file[0],\n                    text=f\"{str(classes[0])} {float(scores[0]):.5f}\")\n@INFERENCE.register()\nclass SlowFast_Inference_helper(Base_Inference_helper):\n    def __init__(self,\n                 num_frames=32,\n                 sampling_rate=2,\n                 target_size=256,\n                 alpha=8,\n                 top_k=1):\n        self.num_frames = num_frames\n        self.sampling_rate = sampling_rate\n        self.target_size = target_size\n        self.alpha = alpha\n        self.top_k = top_k\n    def preprocess(self, input_file):\n        \"\"\"\n        input_file: str, file path\n        return: list\n        \"\"\"\n        assert os.path.isfile(input_file) is not None, \"{0} not exists\".format(\n            input_file)\n        results = {"
        },
        {
            "comment": "This code defines a function for preprocessing and postprocessing video frames. It initializes parameters like filename, sampling rate, target size, and normalization values. The function applies a series of operations to the input image, such as decoding, jitter scaling, cropping, converting to array format, normalizing pixel values, and packing the output. Finally, it expands the result along an axis and returns the processed frames.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":651-681",
            "content": "            'filename': input_file,\n            'temporal_sample_index': 0,\n            'spatial_sample_index': 0,\n            'temporal_num_clips': 1,\n            'spatial_num_clips': 1\n        }\n        img_mean = [0.45, 0.45, 0.45]\n        img_std = [0.225, 0.225, 0.225]\n        ops = [\n            DecodeSampler(self.num_frames, self.sampling_rate, test_mode=True),\n            JitterScale(self.target_size, self.target_size),\n            MultiCrop(self.target_size),\n            Image2Array(transpose=False),\n            Normalization(img_mean, img_std, tensor_shape=[1, 1, 1, 3]),\n            PackOutput(self.alpha),\n        ]\n        for op in ops:\n            results = op(results)\n        res = []\n        for item in results['imgs']:\n            res.append(np.expand_dims(item, axis=0).copy())\n        return res\n    def postprocess(self, output, print_output=True):\n        \"\"\"\n        output: list\n        \"\"\"\n        if not isinstance(self.input_file, list):\n            self.input_file = [\n                self.input_file,"
        },
        {
            "comment": "This function reshapes the output tensor based on the number of input files, then calculates top classes and scores for each file. If print_output is True, it prints the top classes and scores for each video file. The output is from a STGCN (Spatio-Temporal Graph Convolutional Network) inference process.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":682-705",
            "content": "            ]\n        output = output[0]  # [B, num_cls]\n        N = len(self.input_file)\n        if output.shape[0] != N:\n            output = output.reshape([N] + [output.shape[0] // N] +\n                                    list(output.shape[1:]))  # [N, T, C]\n            output = output.mean(axis=1)  # [N, C]\n        # output = F.softmax(paddle.to_tensor(output), axis=-1).numpy() # done in it's head\n        for i in range(N):\n            classes = np.argpartition(output[i], -self.top_k)[-self.top_k:]\n            classes = classes[np.argsort(-output[i, classes])]\n            scores = output[i, classes]\n            if print_output:\n                print(\"Current video file: {0}\".format(self.input_file[i]))\n                for j in range(self.top_k):\n                    print(\"\\ttop-{0} class: {1}\".format(j + 1, classes[j]))\n                    print(\"\\ttop-{0} score: {1}\".format(j + 1, scores[j]))\n@INFERENCE.register()\nclass STGCN_Inference_helper(Base_Inference_helper):\n    def __init__(self,\n                 num_channels,"
        },
        {
            "comment": "This code defines a class `CTRGCN_Inference_helper` that preprocesses data for CTRGCN inference. It takes input file path as parameter and returns processed data as list. The preprocessing includes applying auto-padding, skeleton normalization operations on the input data. The window size, vertex numbers, person numbers can be specified during initialization of the class.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":706-739",
            "content": "                 window_size,\n                 vertex_nums,\n                 person_nums,\n                 top_k=1):\n        self.num_channels = num_channels\n        self.window_size = window_size\n        self.vertex_nums = vertex_nums\n        self.person_nums = person_nums\n        self.top_k = top_k\n    def preprocess(self, input_file):\n        \"\"\"\n        input_file: str, file path\n        return: list\n        \"\"\"\n        assert os.path.isfile(input_file) is not None, \"{0} not exists\".format(\n            input_file)\n        data = np.load(input_file)\n        results = {'data': data}\n        ops = [AutoPadding(window_size=self.window_size), SkeletonNorm()]\n        for op in ops:\n            results = op(results)\n        res = np.expand_dims(results['data'], axis=0).copy()\n        return [res]\n@INFERENCE.register()\nclass CTRGCN_Inference_helper(Base_Inference_helper):\n    def __init__(self,\n                 num_channels=3,\n                 vertex_nums=25,\n                 person_nums=2,\n                 window_size=64,"
        },
        {
            "comment": "This code defines a class for preprocessing data and applying operations. It has an `__init__` method to initialize the window size, number of channels, vertex numbers, person numbers, and top k. The `preprocess` method takes a file path, asserts that it exists, loads the data, applies operations defined in ops, expands dimensions, and returns the processed data. It also registers this class for inference using the @INFERENCE decorator.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":740-774",
            "content": "                 p_interval=[0.95],\n                 top_k=1):\n        self.window_size = window_size\n        self.p_interval = p_interval\n        self.num_channels = num_channels\n        self.vertex_nums = vertex_nums\n        self.person_nums = person_nums\n        self.top_k = top_k\n    def preprocess(self, input_file):\n        \"\"\"\n        input_file: str, file path\n        return: list\n        \"\"\"\n        assert os.path.isfile(input_file) is not None, \"{0} not exists\".format(\n            input_file)\n        data = np.load(input_file)\n        results = {'data': data}\n        ops = [\n            SketeonCropSample(window_size=self.window_size,\n                              p_interval=self.p_interval)\n        ]\n        for op in ops:\n            results = op(results)\n        res = np.expand_dims(results['data'], axis=0).copy()\n        return [res]\n@INFERENCE.register()\nclass AGCN2s_Inference_helper(Base_Inference_helper):\n    def __init__(self,\n                 window_size=300,\n                 num_channels=3,\n                 vertex_nums=25,"
        },
        {
            "comment": "This code defines a class for preprocessing input data and an inference helper class for MSTCN. It initializes the class with parameters like window size, number of channels, vertex numbers, and top k. The `preprocess` method loads data from a file path and returns it as a list. The `MSTCN_Inference_helper` registers itself to be used by INFERENCE.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":775-806",
            "content": "                 person_nums=2,\n                 top_k=1):\n        self.window_size = window_size\n        self.num_channels = num_channels\n        self.vertex_nums = vertex_nums\n        self.person_nums = person_nums\n        self.top_k = top_k\n    def preprocess(self, input_file):\n        \"\"\"\n        input_file: str, file path\n        return: list\n        \"\"\"\n        assert os.path.isfile(input_file) is not None, \"{0} not exists\".format(\n            input_file)\n        data = np.load(input_file)\n        results = {'data': data}\n        res = np.expand_dims(results['data'], axis=0).copy()\n        return [res]\n@INFERENCE.register()\nclass MSTCN_Inference_helper(Base_Inference_helper):\n    def __init__(self, num_channels, actions_map_file_path, feature_path=None):\n        self.num_channels = num_channels\n        file_ptr = open(actions_map_file_path, 'r')\n        actions = file_ptr.read().split('\\n')[:-1]\n        file_ptr.close()\n        self.actions_dict = dict()\n        for a in actions:\n            self.actions_dict[a.split()[1]] = int(a.split()[0])"
        },
        {
            "comment": "The code defines a class with methods to handle video feature files. It initializes the feature path and creates an empty list for file names. The `get_process_file` method reads the input text file, checks if each file exists, appends file paths to `self.file_name_list`, and returns a list of files. The `preprocess` method loads a feature file into data, creates a dictionary with 'video_feat' key, and returns it as output_list.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":808-839",
            "content": "        self.feature_path = feature_path\n        self.file_name_list = []\n    def get_process_file(self, input_file_txt):\n        with open(input_file_txt, 'r') as file_ptr:\n            info = file_ptr.read().split('\\n')[:-1]\n        files = []\n        for video_name in info:\n            if self.feature_path is not None:\n                file_name = video_name.split('.')[0] + \".npy\"\n                input_file = os.path.join(self.feature_path, file_name)\n            else:\n                input_file = video_name\n            assert os.path.isfile(\n                input_file) is not None, \"{0} not exists\".format(input_file)\n            files.append(input_file)\n            self.file_name_list.append(input_file.split('/')[-1].split('.')[0])\n        return files\n    def preprocess(self, input_file):\n        \"\"\"\n        input_file: str, feature file list txt path\n        return: list\n        \"\"\"\n        output_list = []\n        data = np.load(input_file)\n        results = {'video_feat': data, 'video_gt': None}\n        ops = []"
        },
        {
            "comment": "The code processes video features, performs post-processing by creating a directory if it doesn't exist, appends the processed output to the output list and then creates separate text files for each result in the output list. The text files contain the recognized actions and are saved in the specified directory with corresponding filenames.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":840-866",
            "content": "        for op in ops:\n            results = op(results)\n        res = np.expand_dims(results['video_feat'], axis=0).copy()\n        output_list.append(res)\n        return output_list\n    def postprocess(self, output, print_output=True):\n        reslut_path = os.path.join(\"./inference/infer_results/\")\n        if not os.path.isdir(reslut_path):\n            os.makedirs(reslut_path)\n        output = [output]\n        for outputs in output:\n            output_np = outputs[0]\n            recognition = []\n            for i in range(output_np.shape[0]):\n                recognition = np.concatenate((recognition, [\n                    list(self.actions_dict.keys())[list(\n                        self.actions_dict.values()).index(output_np[i])]\n                ]))\n            recog_content = list(recognition)\n            recog_content = [line + \"\\n\" for line in recog_content]\n            filename = self.file_name_list.pop(0)\n            write_path = os.path.join(reslut_path, filename + \".txt\")\n            f = open(write_path, \"w\")"
        },
        {
            "comment": "This code initializes an instance of the ASRF_Inference_helper class, which takes in parameters such as num_channels, actions_map_file_path, postprocessing_method, boundary_threshold, and feature_path. It reads the actions map file, splits the lines into separate action names and their corresponding indices, and stores them in a dictionary called self.actions_dict. The code also creates an empty list called self.file_name_list. Additionally, it defines another function called get_process_file that takes input_file_txt as a parameter and reads its content to store information for further processing.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":867-897",
            "content": "            f.writelines(recog_content)\n            f.close()\n        print(\"result write in : \" + write_path)\n@INFERENCE.register()\nclass ASRF_Inference_helper(Base_Inference_helper):\n    def __init__(self,\n                 num_channels,\n                 actions_map_file_path,\n                 postprocessing_method,\n                 boundary_threshold,\n                 feature_path=None):\n        self.num_channels = num_channels\n        file_ptr = open(actions_map_file_path, 'r')\n        actions = file_ptr.read().split('\\n')[:-1]\n        file_ptr.close()\n        self.actions_dict = dict()\n        for a in actions:\n            self.actions_dict[a.split()[1]] = int(a.split()[0])\n        self.postprocessing_method = postprocessing_method\n        self.boundary_threshold = boundary_threshold\n        self.feature_path = feature_path\n        self.file_name_list = []\n    def get_process_file(self, input_file_txt):\n        with open(input_file_txt, 'r') as file_ptr:\n            info = file_ptr.read().split('\\n')[:-1]\n        files = []"
        },
        {
            "comment": "The code defines a class with methods for loading feature files, preprocessing data, and post-processing results. The `load_features` method reads the feature file list, checks if each input file exists, and stores their names in `file_name_list`. The `preprocess` method loads the features from a specified input file, applies transformations defined by `ops`, and returns a processed output. The `postprocess` method saves the final results to the specified result path.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":898-931",
            "content": "        for video_name in info:\n            if self.feature_path is not None:\n                file_name = video_name.split('.')[0] + \".npy\"\n                input_file = os.path.join(self.feature_path, file_name)\n            else:\n                input_file = video_name\n            assert os.path.isfile(\n                input_file) is not None, \"{0} not exists\".format(input_file)\n            files.append(input_file)\n            self.file_name_list.append(input_file.split('/')[-1].split('.')[0])\n        return files\n    def preprocess(self, input_file):\n        \"\"\"\n        input_file: str, feature file list txt path\n        return: list\n        \"\"\"\n        output_list = []\n        data = np.load(input_file)\n        results = {'video_feat': data, 'video_gt': None}\n        ops = []\n        for op in ops:\n            results = op(results)\n        res = np.expand_dims(results['video_feat'], axis=0).copy()\n        output_list.append(res)\n        return output_list\n    def postprocess(self, output, print_output=True):\n        reslut_path = os.path.join(\"./inference/infer_results/\")"
        },
        {
            "comment": "The code is creating a directory if it doesn't exist, then processing and storing video outputs into separate text files based on the actions detected. It uses a dictionary to match action values with corresponding labels. The processed output is written into a new file for each video, using the populated file name list as references.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":932-958",
            "content": "        if not os.path.isdir(reslut_path):\n            os.makedirs(reslut_path)\n        output = [output]\n        for outputs in output:\n            outputs_cls_np = outputs[0]\n            outputs_boundary_np = outputs[1]\n            output_np = ASRFPostProcessing(\n                outputs_cls_np,\n                outputs_boundary_np,\n                self.postprocessing_method,\n                boundary_threshold=self.boundary_threshold).numpy()[0, :]\n            recognition = []\n            for i in range(output_np.shape[0]):\n                recognition = np.concatenate((recognition, [\n                    list(self.actions_dict.keys())[list(\n                        self.actions_dict.values()).index(output_np[i])]\n                ]))\n            recog_content = list(recognition)\n            recog_content = [line + \"\\n\" for line in recog_content]\n            filename = self.file_name_list.pop(0)\n            write_path = os.path.join(reslut_path, filename + \".txt\")\n            f = open(write_path, \"w\")\n            f.writelines(recog_content)"
        },
        {
            "comment": "This code defines a class `AttentionLSTM_Inference_helper` that initializes attributes for processing data, and has a method `preprocess()` to process input file. The method applies feature decoding operations on the input file, stores results in dictionary format, and returns the result as a list.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":959-992",
            "content": "            f.close()\n        print(\"result write in : \" + write_path)\n@INFERENCE.register()\nclass AttentionLSTM_Inference_helper(Base_Inference_helper):\n    def __init__(\n            self,\n            num_classes,  #Optional, the number of classes to be classified.\n            feature_num,\n            feature_dims,\n            embedding_size,\n            lstm_size,\n            top_k=1):\n        self.num_classes = num_classes\n        self.feature_num = feature_num\n        self.feature_dims = feature_dims\n        self.embedding_size = embedding_size\n        self.lstm_size = lstm_size\n        self.top_k = top_k\n    def preprocess(self, input_file):\n        \"\"\"\n        input_file: str, file path\n        return: list\n        \"\"\"\n        assert os.path.isfile(input_file) is not None, \"{0} not exists\".format(\n            input_file)\n        results = {'filename': input_file}\n        ops = [FeatureDecoder(num_classes=self.num_classes, has_label=False)]\n        for op in ops:\n            results = op(results)\n        res = []"
        },
        {
            "comment": "This code snippet defines a function and a class for video inference using the TransNetV2 model. The function takes input frames, processes them by dividing into windows of 100 frames, padding first/last window, and returns the results as a list of arrays representing data, lengths, and masks for 'rgb' and 'audio' modalities. The class initializes an instance with specified parameters for image size, number of channels, threshold value, output path, and visualization flag.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":993-1021",
            "content": "        for modality in ['rgb', 'audio']:\n            res.append(\n                np.expand_dims(results[f'{modality}_data'], axis=0).copy())\n            res.append(\n                np.expand_dims(results[f'{modality}_len'], axis=0).copy())\n            res.append(\n                np.expand_dims(results[f'{modality}_mask'], axis=0).copy())\n        return res\n@INFERENCE.register()\nclass TransNetV2_Inference_helper():\n    def __init__(self,\n                 num_frames,\n                 height,\n                 width,\n                 num_channels,\n                 threshold=0.5,\n                 output_path=None,\n                 visualize=True):\n        self._input_size = (height, width, num_channels)\n        self.output_path = output_path\n        self.len_frames = 0\n        self.threshold = threshold\n        self.visualize = visualize\n    def input_iterator(self, frames):\n        # return windows of size 100 where the first/last 25 frames are from the previous/next batch\n        # the first and last window must be padded by copies of the first and last frame of the video"
        },
        {
            "comment": "This code is part of a function that takes in an input file and preprocesses it. It imports the 'ffmpeg' library, checks if it exists or not, and then proceeds with the data processing operations. The code calculates the number of padded frames based on the total number of frames, concatenates the start frame, frames, and end frame into a single array, and then iteratively yields batches of 100 elements from this array as an iterator for further processing.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":1022-1050",
            "content": "        no_padded_frames_start = 25\n        no_padded_frames_end = 25 + 50 - (\n            len(frames) % 50 if len(frames) % 50 != 0 else 50)  # 25 - 74\n        start_frame = np.expand_dims(frames[0], 0)\n        end_frame = np.expand_dims(frames[-1], 0)\n        padded_inputs = np.concatenate([start_frame] * no_padded_frames_start +\n                                       [frames] +\n                                       [end_frame] * no_padded_frames_end, 0)\n        ptr = 0\n        while ptr + 100 <= len(padded_inputs):\n            out = padded_inputs[ptr:ptr + 100]\n            out = out.astype(np.float32)\n            ptr += 50\n            yield out[np.newaxis]\n    def preprocess(self, input_file):\n        \"\"\"\n        input_file: str, file path\n        return: iterator\n        \"\"\"\n        try:\n            import ffmpeg\n        except ImportError as e:\n            print(\n                f\"Warning! {e}, [ffmpeg-python] package and it's dependencies is required for TransNetV2.\"\n            )\n        assert os.path.isfile(input_file) is not None, \"{0} not exists\".format("
        },
        {
            "comment": "The code initializes a video input and extracts frames from it. It then reshapes the frames into a 3D array and stores them for further processing. The `input_iterator` function returns an iterator over these frames. The `predictions_to_scenes` function takes predictions, converts them to binary format (0 or 1), and iterates through them to identify scene changes based on consecutive 0's and 1's.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":1051-1073",
            "content": "            input_file)\n        self.input_file = input_file\n        self.filename = os.path.splitext(os.path.split(self.input_file)[1])[0]\n        video_stream, err = ffmpeg.input(\n            self.input_file).output(\"pipe:\",\n                                    format=\"rawvideo\",\n                                    pix_fmt=\"rgb24\",\n                                    s=\"48x27\").run(capture_stdout=True,\n                                                   capture_stderr=True)\n        self.frames = np.frombuffer(video_stream,\n                                    np.uint8).reshape([-1, 27, 48, 3])\n        self.len_frames = len(self.frames)\n        return self.input_iterator(self.frames)\n    def predictions_to_scenes(self, predictions):\n        predictions = (predictions > self.threshold).astype(np.uint8)\n        scenes = []\n        t, t_prev, start = -1, 0, 0\n        for i, t in enumerate(predictions):\n            if t_prev == 1 and t == 0:\n                start = i\n            if t_prev == 0 and t == 1 and i != 0:"
        },
        {
            "comment": "The code above is part of a video processing tool. It appends the start and end frames of a scene to a list, skips scenes with no changes in predictions, pads frames to ensure even widths, and then flattens the scene lists into an array. The `visualize_predictions` function takes a sequence of frames and predictions, pads them to match lengths, and splits the frames into a grid based on width.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":1074-1102",
            "content": "                scenes.append([start, i])\n            t_prev = t\n        if t == 0:\n            scenes.append([start, i])\n        # just fix if all predictions are 1\n        if len(scenes) == 0:\n            return np.array([[0, len(predictions) - 1]], dtype=np.int32)\n        return np.array(scenes, dtype=np.int32)\n    def visualize_predictions(self, frames, predictions):\n        from PIL import Image, ImageDraw\n        if isinstance(predictions, np.ndarray):\n            predictions = [predictions]\n        ih, iw, ic = frames.shape[1:]\n        width = 25\n        # pad frames so that length of the video is divisible by width\n        # pad frames also by len(predictions) pixels in width in order to show predictions\n        pad_with = width - len(frames) % width if len(\n            frames) % width != 0 else 0\n        frames = np.pad(frames, [(0, pad_with), (0, 1), (0, len(predictions)),\n                                 (0, 0)])\n        predictions = [np.pad(x, (0, pad_with)) for x in predictions]\n        height = len(frames) // width"
        },
        {
            "comment": "The code takes in a list of predictions and reshapes them into an image. It then iterates over the frames and predictions, drawing lines to visualize multiple predictions per frame. Finally, it returns the processed image.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":1104-1132",
            "content": "        img = frames.reshape([height, width, ih + 1, iw + len(predictions), ic])\n        img = np.concatenate(np.split(\n            np.concatenate(np.split(img, height), axis=2)[0], width),\n                             axis=2)[0, :-1]\n        img = Image.fromarray(img)\n        draw = ImageDraw.Draw(img)\n        # iterate over all frames\n        for i, pred in enumerate(zip(*predictions)):\n            x, y = i % width, i // width\n            x, y = x * (iw + len(predictions)) + iw, y * (ih + 1) + ih - 1\n            # we can visualize multiple predictions per single frame\n            for j, p in enumerate(pred):\n                color = [0, 0, 0]\n                color[(j + 1) % 3] = 255\n                value = round(p * (ih - 1))\n                if value != 0:\n                    draw.line((x + j, y, x + j, y - value),\n                              fill=tuple(color),\n                              width=1)\n        return img\n    def postprocess(self, outputs, print_output=True):\n        \"\"\"\n        output: list\n        \"\"\""
        },
        {
            "comment": "This code generates predictions for single and all frames. It extracts logits from outputs, applies sigmoid function to convert them into probabilities, and stores the results in a list. Finally, it concatenates the lists of single and all frame predictions for further processing.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":1133-1148",
            "content": "        predictions = []\n        for output in outputs:\n            single_frame_logits, all_frames_logits = output\n            single_frame_pred = F.sigmoid(paddle.to_tensor(single_frame_logits))\n            all_frames_pred = F.sigmoid(paddle.to_tensor(all_frames_logits))\n            predictions.append((single_frame_pred.numpy()[0, 25:75, 0],\n                                all_frames_pred.numpy()[0, 25:75, 0]))\n        single_frame_pred = np.concatenate(\n            [single_ for single_, all_ in predictions])\n        all_frames_pred = np.concatenate(\n            [all_ for single_, all_ in predictions])\n        single_frame_predictions, all_frame_predictions = single_frame_pred[:\n                                                                            self\n                                                                            .\n                                                                            len_frames], all_frames_pred[:\n                                                                                                         self"
        },
        {
            "comment": "The code takes in single-frame and all-frame predictions, converts them into shot boundary scenes, and then optionally prints the output. If an output path is provided and it doesn't exist, it creates the directory. It then stacks the two prediction arrays horizontally, saves the frame predictions file with formatted floats, and saves the scene file with formatted integers.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":1149-1168",
            "content": "                                                                                                         .\n                                                                                                         len_frames]\n        scenes = self.predictions_to_scenes(single_frame_predictions)\n        if print_output:\n            print(\"Current video file: {0}\".format(self.input_file))\n            print(\"\\tShot Boundarys: {0}\".format(scenes))\n        if self.output_path:\n            if not os.path.exists(self.output_path):\n                os.makedirs(self.output_path)\n            predictions = np.stack(\n                [single_frame_predictions, all_frame_predictions], 1)\n            predictions_file = os.path.join(self.output_path,\n                                            self.filename + \"_predictions.txt\")\n            np.savetxt(predictions_file, predictions, fmt=\"%.6f\")\n            scenes_file = os.path.join(self.output_path,\n                                       self.filename + \"_scenes.txt\")\n            np.savetxt(scenes_file, scenes, fmt=\"%d\")"
        },
        {
            "comment": "This code initializes an ADDS_Inference_helper object with various parameters such as frame indices, number of scales, side map, height, width, full resolution shape, number of channels, image extension, and K. The visualize feature is also included to display predictions on saved images.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":1170-1200",
            "content": "            if self.visualize:\n                pil_image = self.visualize_predictions(\n                    self.frames,\n                    predictions=(single_frame_predictions,\n                                 all_frame_predictions))\n                image_file = os.path.join(self.output_path,\n                                          self.filename + \"_vis.png\")\n                pil_image.save(image_file)\n@INFERENCE.register()\nclass ADDS_Inference_helper(Base_Inference_helper):\n    def __init__(self,\n                 frame_idxs=[0],\n                 num_scales=4,\n                 side_map={\n                     \"2\": 2,\n                     \"3\": 3,\n                     \"l\": 2,\n                     \"r\": 3\n                 },\n                 height=256,\n                 width=512,\n                 full_res_shape=None,\n                 num_channels=None,\n                 img_ext=\".png\",\n                 K=None):\n        self.frame_idxs = frame_idxs\n        self.num_scales = num_scales\n        self.side_map = side_map"
        },
        {
            "comment": "The code defines a class with attributes 'full_res_shape', 'img_ext', 'height', 'width', and 'K'. It also has a method 'preprocess' that takes an input file path, checks if the file exists, and returns a list. The preprocess method uses three operations: ImageDecoder, GroupResize, and ToArray(). These operations are applied in sequence to preprocess the image data from the given input file.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":1201-1236",
            "content": "        self.full_res_shape = full_res_shape\n        self.img_ext = img_ext\n        self.height = height\n        self.width = width\n        self.K = K\n    def preprocess(self, input_file):\n        \"\"\"\n        input_file: str, file path\n        return: list\n        \"\"\"\n        assert os.path.isfile(input_file) is not None, \"{0} not exists\".format(\n            input_file)\n        results = {\n            'filename': input_file,\n            'mode': 'infer',\n            'day_or_night': 'day',\n        }\n        ops = [\n            ImageDecoder(\n                backend='pil',\n                dataset='kitti',\n                frame_idxs=self.frame_idxs,\n                num_scales=self.num_scales,\n                side_map=self.side_map,\n                full_res_shape=self.full_res_shape,\n                img_ext=self.img_ext,\n            ),\n            GroupResize(\n                height=self.height,\n                width=self.width,\n                K=self.K,\n                scale=1,\n                mode='infer',\n            ),\n            ToArray(),"
        },
        {
            "comment": "This function processes a list of outputs and performs post-processing operations on each output. It checks if the input file is a single item or a list, then iterates over the outputs to extract depth maps, optionally prints information about each input image and saves the associated depth map as an image file in a specified directory. The code also converts the depth maps to PNG format before saving them.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":1237-1263",
            "content": "        ]\n        for op in ops:\n            results = op(results)\n        res = results['imgs'][('color', 0, 0)]\n        res = np.expand_dims(res, axis=0).copy()\n        return [res]\n    def postprocess(self, output, print_output, save_dir='data/'):\n        \"\"\"\n        output: list\n        \"\"\"\n        if not isinstance(self.input_file, list):\n            self.input_file = [\n                self.input_file,\n            ]\n        print(len(output))\n        N = len(self.input_file)\n        for i in range(N):\n            pred_depth = output[i]  # [H, W]\n            if print_output:\n                print(\"Current input image: {0}\".format(self.input_file[i]))\n                file_name = os.path.basename(self.input_file[i]).split('.')[0]\n                save_path = os.path.join(save_dir,\n                                         file_name + \"_depth\" + \".png\")\n                pred_depth_color = self._convertPNG(pred_depth)\n                pred_depth_color.save(save_path)\n                print(f\"pred depth image saved to: {save_path}\")"
        },
        {
            "comment": "This code defines a function `_convertPNG` that converts an image to PNG format after resizing, normalizing, and color mapping. The class `AVA_SlowFast_FastRCNN_Inference_helper` initializes with various parameters for detection model inference and output settings.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":1265-1290",
            "content": "    def _convertPNG(self, image_numpy):\n        disp_resized = cv2.resize(image_numpy, (1280, 640))\n        disp_resized_np = disp_resized\n        vmax = np.percentile(disp_resized_np, 95)\n        normalizer = mpl.colors.Normalize(vmin=disp_resized_np.min(), vmax=vmax)\n        mapper = cm.ScalarMappable(norm=normalizer, cmap='magma')\n        colormapped_im = (mapper.to_rgba(disp_resized_np)[:, :, :3] *\n                          255).astype(np.uint8)\n        im = Image.fromarray(colormapped_im)\n        return im\n@INFERENCE.register()\nclass AVA_SlowFast_FastRCNN_Inference_helper(Base_Inference_helper):\n    def __init__(self,\n                 detection_model_name,\n                 detection_model_weights,\n                 config_file_path,\n                 predict_stepsize=8,\n                 output_stepsize=4,\n                 output_fps=6,\n                 out_filename='ava_det_demo.mp4',\n                 num_frames=32,\n                 alpha=4,\n                 target_size=256):\n        self.detection_model_name = detection_model_name"
        },
        {
            "comment": "The code is initializing some parameters and then extracting frames from the input video file for further processing. It builds a pipeline configuration for testing, sets clip length, and calculates center indices of each clip. The extracted frames will be used for object detection or other tasks in subsequent steps.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":1291-1320",
            "content": "        self.detection_model_weights = detection_model_weights\n        self.config = get_config(config_file_path,\n                                 show=False)  #parse config file\n        self.predict_stepsize = predict_stepsize\n        self.output_stepsize = output_stepsize\n        self.output_fps = output_fps\n        self.out_filename = out_filename\n        self.num_frames = num_frames\n        self.alpha = alpha\n        self.target_size = target_size\n    def preprocess(self, input_file):\n        \"\"\"\n        input_file: str, file path\n        \"\"\"\n        frame_dir = 'tmp_frames'\n        self.frame_paths, frames, FPS = frame_extraction(input_file, frame_dir)\n        num_frame = len(self.frame_paths)  #\u89c6\u9891\u79d2\u6570*FPS\n        assert num_frame != 0\n        # \u5e27\u56fe\u50cf\u9ad8\u5ea6\u548c\u5bbd\u5ea6\n        h, w, _ = frames[0].shape\n        # Get clip_len, frame_interval and calculate center index of each clip\n        data_process_pipeline = build_pipeline(\n            self.config.PIPELINE.test)  #\u6d4b\u8bd5\u65f6\u8f93\u51fa\u5904\u7406\u6d41\u6c34\u914d\u7f6e\n        clip_len = self.config.PIPELINE.test.sample['clip_len']"
        },
        {
            "comment": "The code asserts for an even clip_len and frame_interval, calculates window size, generates timestamps for selecting frames, creates a list of selected frames, reads label map from file and assigns categories to a dictionary.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":1321-1343",
            "content": "        assert clip_len % 2 == 0, 'We would like to have an even clip_len'\n        frame_interval = self.config.PIPELINE.test.sample['frame_interval']\n        # \u6b64\u5904\u5173\u952e\u5e27\u6bcf\u79d2\u53d6\u4e00\u4e2a\n        clip_len = self.config.PIPELINE.test.sample['clip_len']\n        assert clip_len % 2 == 0, 'We would like to have an even clip_len'\n        frame_interval = self.config.PIPELINE.test.sample['frame_interval']\n        window_size = clip_len * frame_interval\n        timestamps = np.arange(window_size // 2,\n                               (num_frame + 1 - window_size // 2),\n                               self.predict_stepsize)\n        selected_frame_list = []\n        for timestamp in timestamps:\n            selected_frame_list.append(self.frame_paths[timestamp - 1])\n        # Load label_map\n        label_map_path = self.config.DATASET.test['label_file']\n        self.categories, self.class_whitelist = read_labelmap(\n            open(label_map_path))\n        label_map = {}\n        for item in self.categories:\n            id = item['id']"
        },
        {
            "comment": "This code is initializing a label map, running object detection inference on a list of frames, and extracting detection results for each timestamp. It then processes these results by getting proposals and scores for each frame, and checks if there are any detections (if not, it proceeds).",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":1344-1368",
            "content": "            name = item['name']\n            label_map[id] = name\n        self.label_map = label_map\n        detection_result_dir = 'tmp_detection'\n        detection_model_name = self.detection_model_name\n        detection_model_weights = self.detection_model_weights\n        detection_txt_list = detection_inference(selected_frame_list,\n                                                 detection_result_dir,\n                                                 detection_model_name,\n                                                 detection_model_weights)\n        assert len(detection_txt_list) == len(timestamps)\n        human_detections = []\n        data_list = []\n        person_num_list = []\n        for timestamp, detection_txt_path in zip(timestamps,\n                                                 detection_txt_list):\n            proposals, scores = get_detection_result(\n                detection_txt_path, h, w,\n                (float)(self.config.DATASET.test['person_det_score_thr']))\n            if proposals.shape[0] == 0:"
        },
        {
            "comment": "This code is part of a data processing pipeline in PaddleVideo. It appends proposals and scores to the result dictionary, reshapes tensors for image and proposal inputs, and converts images and proposal lists to Paddle Tensors for further processing.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":1369-1399",
            "content": "                #person_num_list.append(0)\n                human_detections.append(None)\n                continue\n            human_detections.append(proposals)\n            result = get_timestep_result(frame_dir,\n                                         timestamp,\n                                         clip_len,\n                                         frame_interval,\n                                         FPS=FPS)\n            result[\"proposals\"] = proposals\n            result[\"scores\"] = scores\n            new_result = data_process_pipeline(result)\n            proposals = new_result['proposals']\n            img_slow = new_result['imgs'][0]\n            img_slow = img_slow[np.newaxis, :]\n            img_fast = new_result['imgs'][1]\n            img_fast = img_fast[np.newaxis, :]\n            proposals = proposals[np.newaxis, :]\n            scores = scores[np.newaxis, :]\n            img_shape = np.asarray(new_result['img_shape'])\n            img_shape = img_shape[np.newaxis, :]\n            data = [\n                paddle.to_tensor(img_slow, dtype='float32'),"
        },
        {
            "comment": "This code defines a class with methods to create and post-process human detections. It takes in various directories as input, and outputs lists of data and predictions. The preprocess method converts image, proposals, and shape into tensors, and appends the number of people and data list for each frame. The postprocess method takes output from the model and checks if human_detections is None for each timestamp, then adds predictions to a list.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":1400-1432",
            "content": "                paddle.to_tensor(img_fast, dtype='float32'),\n                paddle.to_tensor(proposals, dtype='float32'),\n                paddle.to_tensor(img_shape, dtype='int32')\n            ]\n            person_num = proposals.shape[1]\n            person_num_list.append(person_num)\n            data_list.append(data)\n        self.human_detections = human_detections\n        self.person_num_list = person_num_list\n        self.timestamps = timestamps\n        self.frame_dir = frame_dir\n        self.detection_result_dir = detection_result_dir\n        return data_list\n    def postprocess(self, outputs, print_output=True):\n        \"\"\"\n        output: list\n        \"\"\"\n        predictions = []\n        assert len(self.person_num_list) == len(outputs)\n        #print(\"***  self.human_detections\",len( self.human_detections))\n        #print(\"***  outputs\",len( outputs))\n        index = 0\n        for t_index in range(len(self.timestamps)):\n            if self.human_detections[t_index] is None:\n                predictions.append(None)"
        },
        {
            "comment": "This code iterates over human detections and their corresponding outputs. If a detection is None, it appends a None value to the predictions list. It then iterates through the result array for each class, checking if the action score exceeds the specified threshold. For each valid action score, it adds the class label and score to the prediction list. Finally, it appends the prediction list to the predictions list.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":1433-1463",
            "content": "                continue\n            human_detection = self.human_detections[t_index]\n            output = outputs[index]\n            result = output  #\u957f\u5ea6\u4e3a\u7c7b\u522b\u4e2a\u6570\uff0c\u4e0d\u5305\u542b\u80cc\u666f\n            person_num = self.person_num_list[index]\n            index = index + 1\n            prediction = []\n            if human_detection is None:\n                predictions.append(None)\n                continue\n            # N proposals\n            for i in range(person_num):\n                prediction.append([])\n            # Perform action score thr\n            for i in range(len(result)):  # for class\n                if i + 1 not in self.class_whitelist:\n                    continue\n                for j in range(person_num):\n                    if result[i][j, 4] > self.config.MODEL.head['action_thr']:\n                        prediction[j].append(\n                            (self.label_map[i + 1], result[i][j, 4]\n                             ))  # label_map is a dict, label index start from 1\n            predictions.append(prediction)"
        },
        {
            "comment": "Code snippet reads frames from specific paths, performs human detections and predictions, and densely samples timestamps to create a sequence of images. It then visualizes these images and attempts to import moviepy library for output file creation.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":1465-1489",
            "content": "        results = []\n        for human_detection, prediction in zip(self.human_detections,\n                                               predictions):\n            results.append(pack_result(human_detection, prediction))\n        def dense_timestamps(timestamps, n):\n            \"\"\"Make it nx frames.\"\"\"\n            old_frame_interval = (timestamps[1] - timestamps[0])\n            start = timestamps[0] - old_frame_interval / n * (n - 1) / 2\n            new_frame_inds = np.arange(\n                len(timestamps) * n) * old_frame_interval / n + start\n            return new_frame_inds.astype(np.int)\n        dense_n = int(self.predict_stepsize / self.output_stepsize)  #30\n        frames = [\n            cv2.imread(self.frame_paths[i - 1])\n            for i in dense_timestamps(self.timestamps, dense_n)\n        ]\n        vis_frames = visualize(frames, results)\n        try:\n            import moviepy.editor as mpy\n        except ImportError:\n            raise ImportError('Please install moviepy to enable output file')"
        },
        {
            "comment": "This code snippet defines a class PoseC3D_Inference_helper that handles image processing and inference for pose estimation. It includes methods for preprocessing, such as loading data from file, defining keypoint indices for left and right body parts, and applying various operations like frame sampling, pose decoding, and compacting the pose results. The code also demonstrates error handling by checking if input files exist before processing them, and performs cleanup of temporary directories after writing video files.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":1491-1522",
            "content": "        vid = mpy.ImageSequenceClip([x[:, :, ::-1] for x in vis_frames],\n                                    fps=self.output_fps)\n        vid.write_videofile(self.out_filename)\n        print(\"finish write !\")\n        # delete tmp files and dirs\n        shutil.rmtree(self.frame_dir)\n        shutil.rmtree(self.detection_result_dir)\n@INFERENCE.register()\nclass PoseC3D_Inference_helper(Base_Inference_helper):\n    def __init__(self, top_k=1):\n        self.top_k = top_k\n    def preprocess(self, input_file):\n        \"\"\"\n        input_file: str, file path\n        return: list\n        \"\"\"\n        assert os.path.isfile(input_file) is not None, \"{0} not exists\".format(\n            input_file)\n        with open(input_file, 'rb') as f:\n            data = pickle.load(f)\n        self.input_file = input_file\n        left_kp = [1, 3, 5, 7, 9, 11, 13, 15]\n        right_kp = [2, 4, 6, 8, 10, 12, 14, 16]\n        ops = [\n            UniformSampleFrames(clip_len=48, num_clips=10, test_mode=True),\n            PoseDecode(),\n            PoseCompact(hw_ratio=1., allow_imgpad=True),"
        },
        {
            "comment": "The code appears to be a part of a PaddleVideo tool that performs image preprocessing, resizing, cropping, and pose estimation. It uses PaddlePaddle library functions such as Resize, CenterCrop_V2, GeneratePoseTarget, FormatShape, Collect, and F.softmax for various operations. The code also calculates the number of segments and performs post-processing on output results.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":1523-1547",
            "content": "            Resize(scale=(-1, 56)),\n            CenterCrop_V2(crop_size=56),\n            GeneratePoseTarget(sigma=0.6,\n                               use_score=True,\n                               with_kp=True,\n                               with_limb=False,\n                               double=True,\n                               left_kp=left_kp,\n                               right_kp=right_kp),\n            FormatShape(input_format='NCTHW'),\n            Collect(keys=['imgs', 'label'], meta_keys=[])\n        ]\n        for op in ops:\n            results = op(data)\n        results = [results[0][np.newaxis, :, :, :, :, :]]\n        self.num_segs = results[0].shape[1]\n        return results\n    def postprocess(self, outputs, print_output=True):\n        batch_size = outputs[0].shape[0]\n        cls_score = outputs[0].reshape(\n            [batch_size // self.num_segs, self.num_segs, outputs[0].shape[-1]])\n        output = F.softmax(paddle.to_tensor(cls_score),\n                           axis=2).mean(axis=1).numpy()"
        },
        {
            "comment": "This code snippet is a part of YOWO_Inference_helper class in PaddleVideo. It initializes the class with parameters such as num_seg, target_size, nms_thresh, conf_thresh_valid, mean, and std. The class seems to be used for image classification or object detection tasks, based on the presence of top-k classes and scores.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":1548-1573",
            "content": "        N = len(self.input_file)\n        for i in range(N):\n            classes = np.argpartition(output[i], -self.top_k)[-self.top_k:]\n            classes = classes[np.argsort(-output[i, classes])]\n            scores = output[i, classes]\n            if print_output:\n                print(\"Current video file: {0}\".format(self.input_file[i]))\n                for j in range(self.top_k):\n                    print(\"\\ttop-{0} class: {1}\".format(j + 1, classes[j]))\n                    print(\"\\ttop-{0} score: {1}\".format(j + 1, scores[j]))\n@INFERENCE.register()\nclass YOWO_Inference_helper(Base_Inference_helper):\n    def __init__(self,\n                 num_seg=16,\n                 target_size=224,\n                 nms_thresh=0.5,\n                 conf_thresh_valid=0.5,\n                 mean=[0.4345, 0.4051, 0.3775],\n                 std=[0.2768, 0.2713, 0.2737]):\n        self.num_seg = num_seg\n        self.target_size = target_size\n        self.nms_thresh = nms_thresh\n        self.conf_thresh_valid = conf_thresh_valid"
        },
        {
            "comment": "This code is initializing a preprocess function for video input. It checks if the input file exists, then uses OpenCV to read frames from the video file. The function populates a queue with initial frames, adds new frames, and resizes them using interpolation. Finally, it converts images to CHW order while keeping BGR values.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":1574-1605",
            "content": "        self.mean = mean\n        self.std = std\n    def preprocess(self, input_file):\n        \"\"\"\n        input_file: str, file path\n        return: list\n        \"\"\"\n        assert os.path.isfile(input_file) is not None, \"{0} not exists\".format(\n            input_file)\n        cap = cv2.VideoCapture(input_file)\n        queue = []\n        inputs = []\n        frames = []\n        while (cap.isOpened()):\n            ret, frame = cap.read()\n            if ret == False:\n                break\n            if len(queue) <= 0:  # At initialization, populate queue with initial frame\n                for i in range(self.num_seg):\n                    queue.append(frame)\n            # Add the read frame to last and pop out the oldest one\n            queue.append(frame)\n            queue.pop(0)\n            # Resize images\n            imgs = [cv2.resize(img, (self.target_size, self.target_size), interpolation=cv2.INTER_LINEAR) for img in\n                    queue]\n            # Convert image to CHW keeping BGR order.\n            imgs = [img.transpose([2, 0, 1]) for img in imgs]"
        },
        {
            "comment": "The code normalizes the image values to [0, 1] range and reshapes them into a specific format. It then concatenates the images to form a single array and expands dimensions as necessary before appending it to the inputs list. The postprocess function takes outputs, frames, frame, and filename as input and returns labels for classification tasks.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":1607-1637",
            "content": "            # Image [0, 255] -> [0, 1].\n            imgs = [img / 255.0 for img in imgs]\n            imgs = [\n                np.ascontiguousarray(\n                    img.reshape((3, imgs[0].shape[1], imgs[0].shape[2]))\n                ).astype(np.float32)\n                for img in imgs\n            ]\n            # Concat list of images to single ndarray.\n            imgs = np.concatenate(\n                [np.expand_dims(img, axis=1) for img in imgs], axis=1\n            )\n            imgs = np.ascontiguousarray(imgs)\n            imgs = np.expand_dims(imgs, axis=0)\n            imgs = np.expand_dims(imgs, axis=0)\n            inputs.append(imgs)\n            frames.append(queue[-1])\n        return inputs, frames\n    def postprocess(self, outputs, frame, filename, save_img=True):\n        \"\"\"\n        outputs: list\n        frames: list\n        \"\"\"\n        labels = [\n            \"Basketball\", \"BasketballDunk\", \"Biking\", \"CliffDiving\", \"CricketBowling\",\n            \"Diving\", \"Fencing\", \"FloorGymnastics\", \"GolfSwing\", \"HorseRiding\","
        },
        {
            "comment": "This code appears to be involved in object detection and recognition. It applies Non-Maximum Suppression (NMS) to the predicted bounding boxes to filter out redundant detections, calculates the adjusted coordinates for each box, and extracts the classification confidence scores for each class of the detected objects. The specific activity being detected or the model architecture used is not specified in this code snippet.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":1638-1659",
            "content": "            \"IceDancing\", \"LongJump\", \"PoleVault\", \"RopeClimbing\", \"SalsaSpin\",\n            \"SkateBoarding\", \"Skiing\", \"Skijet\", \"SoccerJuggling\", \"Surfing\",\n            \"TennisSwing\", \"TrampolineJumping\", \"VolleyballSpiking\", \"WalkingWithDog\"]\n        nms_thresh = 0.5\n        font = cv2.FONT_HERSHEY_SIMPLEX\n        for out in outputs:\n            out = paddle.to_tensor(out)\n            preds = []\n            all_boxes = get_region_boxes(out)\n            for i in range(out.shape[0]):\n                boxes = all_boxes[i]\n                boxes = nms(boxes, nms_thresh)\n                for box in boxes:\n                    x1 = round(float(box[0] - box[2] / 2.0) * 320.0)\n                    y1 = round(float(box[1] - box[3] / 2.0) * 240.0)\n                    x2 = round(float(box[0] + box[2] / 2.0) * 320.0)\n                    y2 = round(float(box[1] + box[3] / 2.0) * 240.0)\n                    det_conf = float(box[4])\n                    for j in range((len(box) - 5) // 2):\n                        cls_conf = float(box[5 + 2 * j].item())"
        },
        {
            "comment": "This code is part of a video object detection system. It calculates the probability (prob) of detections based on confidence (det_conf) and class confidence (cls_conf). The detections are stored in preds list. If the probability is below 0.4, the loop breaks. Then it draws rectangles around detected objects on the frame using their coordinates from preds[0], colors them green, and displays text with object label and probability using cv2.putText(). Finally, it saves the processed frame as a .jpg image named after filename.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/utils.py\":1660-1669",
            "content": "                        prob = det_conf * cls_conf\n                    preds.append([[x1, y1, x2, y2], prob, labels[int(box[6])]])\n            for _, dets in enumerate(preds):\n                if dets[1] < 0.4:\n                    break\n                text = dets[2] + ' ' + '{:.2f}'.format(dets[1])\n                cv2.rectangle(frame, (dets[0][0], dets[0][1]), (dets[0][2], dets[0][3]), (0, 255, 0), 2)\n                cv2.putText(frame, text, (dets[0][0] + 3, dets[0][1] - 5 - 10 * _), font, 0.5, (0, 255, 0), 2)\n            cv2.imwrite('{}.jpg'.format(filename), frame)"
        }
    ]
}