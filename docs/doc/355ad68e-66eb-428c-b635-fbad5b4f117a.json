{
    "summary": "The MSR-VTT dataset contains 10K videos available on its website, organized in a \"data\" directory for ActBERT model use. The lock.mdb file is a database used for storing and managing data related to multi-modal transformers for video retrieval as described in a 2020 ECCV paper.",
    "details": [
        {
            "comment": "This code provides an overview of the MSR-VTT dataset, its download process for T2VLAD and ActBERT applications, and references for more information. It consists of 10K video clips from 20 categories, each with 20 English sentences, and is available on the MSRVTT website.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/dataset/msrvtt.md\":0-28",
            "content": "[\u7b80\u4f53\u4e2d\u6587](../../zh-CN/dataset/msrvtt.md) | English\n# MSR-VTT Preparation\n- [Introduction](#1.1)\n- [Download for T2VLAD](#1.2)\n- [Download for ActBERT](#1.3)\n- [Reference](#1.4)\n<a name=\"1.1\"></a>\n## Introduction\nMSR-VTT(Microsoft Research Video to Text) is a large-scale dataset containing videos and subtitles, which is composed of 10000 video clips from 20 categories, and each video clip is annotated with 20 English sentences. We used 9000 video clips for training and 1000 for testing. For more details, please refer to the website: [MSRVTT](https://www.microsoft.com/en-us/research/publication/msr-vtt-a-large-video-description-dataset-for-bridging-video-and-language/)\n<a name=\"1.2\"></a>\n## Download for T2VLAD\n[T2VLAD doc](../../../applications/T2VLAD/README_en.md)\nFor ease of use, we provided extracted features of video.\nFirst, make sure to enter the following command in the `applications/T2VLAD/data` directory to download the dataset.\n```bash\nbash download_features.sh\n```\nAfter downloading, the files in the data directory are organized as follows:"
        },
        {
            "comment": "Code provides the instructions to download and decompress data features required for ActBERT model, specifically for MSR-VTT dataset. The data is organized in the \"data\" directory with a .lmdb file and a CSV file containing JSFusion test data.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/dataset/msrvtt.md\":30-72",
            "content": "```\n\u251c\u2500\u2500 data\n|   \u251c\u2500\u2500 MSR-VTT\n|   \u2502   \u251c\u2500\u2500 raw-captions.pkl\n|   \u2502   \u251c\u2500\u2500 train_list_jsfusion.txt\n|   \u2502   \u251c\u2500\u2500 val_list_jsfusion.txt\n|   \u2502   \u251c\u2500\u2500 aggregated_text_feats\n|   |   |   \u251c\u2500\u2500 w2v_MSRVTT_openAIGPT.pickle\n|   |   \u251c\u2500\u2500 mmt_feats\n|   \u2502   \u2502   \u251c\u2500\u2500 features.audio.pkl\n|   \u2502   \u2502   \u251c\u2500\u2500 features.face_agg.pkl\n|   \u2502   \u2502   \u251c\u2500\u2500 features.flos_agg.pkl\n|   \u2502   \u2502   \u251c\u2500\u2500 features.ocr.pkl\n|   \u2502   \u2502   \u251c\u2500\u2500 features.rgb_agg.pkl\n|   \u2502   \u2502   \u251c\u2500\u2500 features.s3d.pkl\n|   \u2502   \u2502   \u251c\u2500\u2500 features.scene.pkl\n|   \u2502   \u2502   \u251c\u2500\u2500 features.speech.pkl\n```\n<a name=\"1.3\"></a>\n## Download for ActBERT\n[ActBERT doc](../model_zoo/multimodal/actbert.md)\nDownload data features:\n```\nwget https://videotag.bj.bcebos.com/Data/ActBERT/msrvtt_test.lmdb.tar\nwget https://videotag.bj.bcebos.com/Data/ActBERT/MSRVTT_JSFUSION_test.csv\n```\nDecompress the `msrvtt_test.lmdb.tar`\uff1a\n```\ntar -zxvf msrvtt_test.lmdb.tar\n```\nThe files in the data directory are organized as follows:\n```\n\u251c\u2500\u2500 data\n|   \u251c\u2500\u2500 MSR-VTT\n|   \u2502   \u251c\u2500\u2500 MSRVTT_JSFUSION_test.csv\n|   \u2502   \u251c\u2500\u2500 msrvtt_test.lmdb\n|   \u2502       \u251c\u2500\u2500 data.mdb"
        },
        {
            "comment": "lock.mdb: Database file used for storing and managing data in codebase related to multi-modal transformer for video retrieval as described in the 2020 ECCV paper by Valentin Gabeur et al.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/dataset/msrvtt.md\":73-78",
            "content": "|   \u2502       \u251c\u2500\u2500 lock.mdb\n```\n<a name=\"1.4\"></a>\n## Reference\n- Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. Multi-modal transformer for video retrieval. In ECCV, 2020."
        }
    ]
}