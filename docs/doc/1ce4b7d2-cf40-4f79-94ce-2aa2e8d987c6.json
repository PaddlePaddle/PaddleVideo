{
    "summary": "The PP-TimeSformer model is an enhanced version of TimeSformer for video recognition tasks, trained on Kinetics-400 dataset and supports multi-GPU. It uses PaddleVideo with Vision Transformer backbone for testing and exports PP-TimeSformer for prediction using a specific config file.",
    "details": [
        {
            "comment": "This code describes the PP-TimeSformer video classification model, an improved version of the TimeSformer model. It outlines the training, testing, and inference processes, as well as providing data preparation instructions for Kinetics-400 dataset. The table shows the accuracy of different versions of the model on Kinetics-400 dataset.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/pp-timesformer.md\":0-28",
            "content": "[\u7b80\u4f53\u4e2d\u6587](../../../zh-CN/model_zoo/recognition/pp-timesformer.md) | English\n# TimeSformer Video Classification Model\n## Content\n- [Introduction](#Introduction)\n- [Data](#Data)\n- [Train](#Train)\n- [Test](#Test)\n- [Inference](#Inference)\n- [Reference](#Reference)\n## Introduction\nWe have improved the [TimeSformer model](./timesformer.md) and obtained a more accurate 2D practical video classification model **PP-TimeSformer**. Without increasing the amount of parameters and calculations, the accuracy on the UCF-101, Kinetics-400 and other data sets significantly exceeds the original version. The accuracy on the Kinetics-400 data set is shown in the table below.\n| Version | Top1 |\n| :------ | :----: |\n| Ours ([swa](#refer-anchor-1)+distill+16frame) | 79.44 |\n| Ours ([swa](#refer-anchor-1)+distill)  | 78.87 |\n| Ours ([swa](#refer-anchor-1)) | **78.61** |\n| [mmaction2](https://github.com/open-mmlab/mmaction2/tree/master/configs/recognition/timesformer#kinetics-400) | 77.92 |\n## Data\nK400 data download and preparation please refer to [Kinetics-400 data preparation](../../dataset/k400.md)"
        },
        {
            "comment": "This code snippet explains how to download and prepare data for training a video recognition model. It mentions the required data sets, pre-trained models, and the specific commands to download and configure them.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/pp-timesformer.md\":30-57",
            "content": "UCF101 data download and preparation please refer to [UCF-101 data preparation](../../dataset/ucf101.md)\n## Train\n### Kinetics-400 data set training\n#### Download and add pre-trained models\n1. Download the image pre-training model [ViT_base_patch16_224_miil_21k.pdparams](https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/ViT_base_patch16_224_pretrained.pdparams) as Backbone initialization parameters, or download through wget command\n   ```bash\n   wget https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/ViT_base_patch16_224_pretrained.pdparams\n   ```\n2. Open `PaddleVideo/configs/recognition/pptimesformer/pptimesformer_k400_videos.yaml`, and fill in the downloaded weight storage path below `pretrained:`\n    ```yaml\n    MODEL:\n        framework: \"RecognizerTransformer\"\n        backbone:\n            name: \"VisionTransformer_tweaks\"\n            pretrained: fill in the path here\n    ```\n#### Start training\n- The Kinetics400 data set uses 8 cards for training, and the start command of the training method is as follows:"
        },
        {
            "comment": "This code runs PaddlePaddle's Timesformer model for video recognition using a specific configuration file. It uses multiple GPUs and supports AMP mixed-precision training for faster processing. The script is customizable, allowing you to train or test on different datasets by modifying the configuration file's name.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/pp-timesformer.md\":59-74",
            "content": "    ```bash\n    # videos data format\n    python3.7 -B -m paddle.distributed.launch --gpus=\"0,1,2,3,4,5,6,7\" --log_dir=log_pptimesformer main.py --validate -c configs/recognition/ pptimesformer/pptimesformer_k400_videos.yaml\n    ```\n- Turn on amp mixed-precision training to speed up the training process. The training start command is as follows:\n    ```bash\n    export FLAGS_conv_workspace_size_limit=800 # MB\n    export FLAGS_cudnn_exhaustive_search=1\n    export FLAGS_cudnn_batchnorm_spatial_persistent=1\n    # videos data format\n    python3.7 -B -m paddle.distributed.launch --gpus=\"0,1,2,3,4,5,6,7\" --log_dir=log_pptimesformer main.py --amp --validate -c configs /recognition/pptimesformer/pptimesformer_k400_videos.yaml\n    ```\n- In addition, you can customize and modify the parameter configuration to achieve the purpose of training/testing on different data sets. It is recommended that the naming method of the configuration file is `model_dataset name_file format_data format_sampling method.yaml` , Please refer to [config](../../tutorials/config.md) for parameter usage."
        },
        {
            "comment": "The PP-TimeSformer model is tested during training, and the best test accuracy can be found in the log with keyword \"best\". However, the verification index recorded in the log may not represent the final test score, so a separate testing script should be used to obtain the accurate result. Two such scripts are provided for 8-frames and 16-frames testing.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/pp-timesformer.md\":77-91",
            "content": "## Test\n- The PP-TimeSformer model is verified synchronously during training. You can find the keyword `best` in the training log to obtain the model test accuracy. The log example is as follows:\n  ```\n  Already save the best model (top1 acc)0.7258\n  ```\n- Because the sampling method of the PP-TimeSformer model test mode is a slightly slower but higher accuracy **UniformCrop**, which is different from the **RandomCrop** used in the verification mode during the training process, so the verification index recorded in the training log` topk Acc` does not represent the final test score, so after the training is completed, you can use the test mode to test the best model to obtain the final index. The command is as follows:\n  ```bash\n  # 8-frames testing script\n  python3.7 -B -m paddle.distributed.launch --gpus=\"0,1,2,3,4,5,6,7\"  --log_dir=log_pptimesformer  main.py  --test -c configs/recognition/pptimesformer/pptimesformer_k400_videos.yaml -w \"output/ppTimeSformer/ppTimeSformer_best.pdparams\"\n  # 16-frames testing script"
        },
        {
            "comment": "This code is launching the PaddleVideo model for testing using Vision Transformer backbone with UniformCrop sampling method and 8 segments. It's running on multiple GPUs and using a specific configuration file, yaml, to set parameters like backbone, sampling method, number of segments, target size, and checkpoint file. The resulting test indicators are presented in tabular format for Kinetics-400 validation dataset.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/pp-timesformer.md\":92-107",
            "content": "  python3.7 -B -m paddle.distributed.launch --gpus=\"0,1,2,3,4,5,6,7\"  --log_dir=log_pptimesformer main.py --test \\\n  -c configs/recognition/pptimesformer/pptimesformer_k400_videos.yaml \\\n  -o MODEL.backbone.num_seg=16 \\\n  -o MODEL.runtime_cfg.test.num_seg=16 \\\n  -o PIPELINE.test.decode.num_seg=16 \\\n  -o PIPELINE.test.sample.num_seg=16 \\\n  -w \"data/ppTimeSformer_k400_16f_distill.pdparams\"\n  ```\n  When the test configuration uses the following parameters, the test indicators on the validation data set of Kinetics-400 are as follows:\n   | backbone           | Sampling method | num_seg | target_size | Top-1 | checkpoints |\n   | :----------------: | :-------------: | :-----: | :---------: | :---- | :----------------------------------------------------------: |\n   | Vision Transformer |   UniformCrop   |   8    |     224     | 78.61 | [ppTimeSformer_k400_8f.pdparams](https://videotag.bj.bcebos.com/PaddleVideo-release2.2/ppTimeSformer_k400_8f.pdparams) |\n   | Vision Transformer | UniformCrop | 8 | 224 | "
        },
        {
            "comment": "The code snippet is exporting the PP-TimeSformer model for video recognition. The model uses linspace sampling strategy, uniformly generating sparse sampling points in time and space to create one clip from a single video. The command uses Python script `export_model.py`, with config file `configs/recognition/pptimesformer/pptimesformer_k400_videos.yaml` and model parameters file `data/ppTimeSformer_k400_8f.pdparams`.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/pp-timesformer.md\":107-119",
            "content": "78.87 | [ppTimeSformer_k400_8f_distill.pdparams](https://videotag.bj.bcebos.com/PaddleVideo-release2.2/ppTimeSformer_k400_8f_distill.pdparams) |\n   | Vision Transformer | UniformCrop | 16 | 224 | 79.44 | [ppTimeSformer_k400_16f_distill.pdparams](https://videotag.bj.bcebos.com/PaddleVideo-release2.2/ppTimeSformer_k400_16f_distill.pdparams) |\n- During the test, the PP-TimeSformer video sampling strategy is to use linspace sampling: in time sequence, from the first frame to the last frame of the video sequence to be sampled, `num_seg` sparse sampling points (including endpoints) are uniformly generated; spatially , Select 3 areas to sample at both ends of the long side and the middle position (left, middle, right or top, middle, and bottom). A total of 1 clip is sampled for 1 video.\n## Inference\n### Export inference model\n```bash\npython3.7 tools/export_model.py -c configs/recognition/pptimesformer/pptimesformer_k400_videos.yaml \\\n                                -p data/ppTimeSformer_k400_8f.pdparams \\"
        },
        {
            "comment": "This code is for inference using PaddlePaddle's ppTimeSformer model. The command generates the required model structure and weight files for prediction and then executes the predict.py script with the given input file, configuration, model files, and parameters. It displays the top-1 class and score for the video file provided, trained on Kinetics 400 dataset.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/pp-timesformer.md\":120-146",
            "content": "                                -o inference/ppTimeSformer\n```\nThe above command will generate the model structure file `ppTimeSformer.pdmodel` and the model weight file `ppTimeSformer.pdiparams` required for prediction.\n- For the meaning of each parameter, please refer to [Model Reasoning Method](../../start.md#2-Model Reasoning)\n### Use predictive engine inference\n```bash\npython3.7 tools/predict.py --input_file data/example.avi \\\n                           --config configs/recognition/pptimesformer/pptimesformer_k400_videos.yaml \\\n                           --model_file inference/ppTimeSformer/ppTimeSformer.pdmodel \\\n                           --params_file inference/ppTimeSformer/ppTimeSformer.pdiparams \\\n                           --use_gpu=True \\\n                           --use_tensorrt=False\n```\nThe output example is as follows:\n```\nCurrent video file: data/example.avi\n        top-1 class: 5\n        top-1 score: 0.9997474551200867\n```\nIt can be seen that using the ppTimeSformer model trained on Ki"
        },
        {
            "comment": "This code snippet is discussing the prediction of a category name using the PP-Timesformer model, specifically for predicting the content of `data/example.avi`. The predicted category id is 5 and its corresponding category name is \"archery\". This information is derived from the category id and name correspondence table `data/k400/Kinetics-400_label_list.txt`. The code provides references to several related papers which have influenced or been used in this model's development.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/pp-timesformer.md\":146-155",
            "content": "netics-400 to predict `data/example.avi`, the output top1 category id is `5`, and the confidence is 0.99. By referring to the category id and name correspondence table `data/k400/Kinetics-400_label_list.txt`, it can be known that the predicted category name is `archery`.\n## Reference\n- [Is Space-TimeAttention All You Need for Video Understanding?](https://arxiv.org/pdf/2102.05095.pdf), Gedas Bertasius, Heng Wang, Lorenzo Torresani\n- [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531), Geoffrey Hinton, Oriol Vinyals, Jeff Dean\n<div id=\"refer-anchor-1\"></div>\n- [Averaging Weights Leads to Wider Optima and Better Generalization](https://arxiv.org/abs/1803.05407v3), Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov\n- [ImageNet-21K Pretraining for the Masses](https://arxiv.org/pdf/2104.10972v4.pdf), Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy"
        }
    ]
}