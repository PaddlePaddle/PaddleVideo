{
    "summary": "The code is a Paddle Video inference script with argparse handling and an \"InferModel\" class for model loading and prediction. It supports GPU usage, multimodal video tagging, and customizable parameters. The inference function takes videos, labels, predicts, and outputs results to a JSON file.",
    "details": [
        {
            "comment": "This code is a Paddle Video inference script for a specific model. It parses arguments such as the model name, config file path, output path, use_gpu, and save_inference_model flag. The script utilizes argparse to handle these command-line arguments.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/MultimodalVideoTag/scenario_lib/inference.py\":0-37",
            "content": "#!/usr/bin/env python\n# coding=utf-8\n\"\"\"\ninfer model\n\"\"\"\nimport sys\nimport os\nimport numpy as np\nimport json\nimport pickle\nimport argparse\nimport time\nimport numpy as np\nimport paddle\nfrom datareader import get_reader\nfrom config import merge_configs, parse_config, print_configs\ndef parse_args():\n    \"\"\"parse_args\n    \"\"\"\n    parser = argparse.ArgumentParser(\"Paddle Video infer script\")\n    parser.add_argument('--model_name',\n                        type=str,\n                        default='BaiduNet',\n                        help='name of model to train.')\n    parser.add_argument('--config',\n                        type=str,\n                        default='configs/conf.txt',\n                        help='path to config file of model')\n    parser.add_argument('--output', type=str, default=None, help='output path')\n    parser.add_argument('--use_gpu',\n                        type=bool,\n                        default=True,\n                        help='default use gpu.')\n    parser.add_argument('--save_inference_model',"
        },
        {
            "comment": "The code is defining a class \"InferModel\" which initializes variables such as name, threshold, and label_map. It also defines a method \"load_inference_model\" that loads the model file, configures GPU usage, and creates a predictor for inference. The code takes input arguments like model_dir, use_gpu, and other configuration parameters.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/MultimodalVideoTag/scenario_lib/inference.py\":38-68",
            "content": "                        type=str,\n                        default=None,\n                        help='save inference path')\n    args = parser.parse_args()\n    return args\nclass InferModel(object):\n    \"\"\"lstm infer\"\"\"\n    def __init__(self, cfg, name='ACTION'): \n        name = name.upper()\n        self.name           = name\n        self.threshold      = cfg.INFER.threshold\n        self.cfg            = cfg\n        self.label_map      = load_class_file(cfg.MODEL.class_name_file)\n    def load_inference_model(self, model_dir, use_gpu=True):\n        \"\"\"model_init\n        \"\"\"\n        model_file = os.path.join(model_dir, \"model\")\n        params_file = os.path.join(model_dir, \"params\")\n        config = paddle.inference.Config(model_file, params_file)\n        if use_gpu:\n            config.enable_use_gpu(1024)\n        else:\n            config.disable_gpu()\n        config.switch_ir_optim(True)  # default true\n        config.enable_memory_optim()\n        # use zero copy\n        config.switch_use_feed_fetch_ops(False)\n        self.predictor = paddle.inference.create_predictor(config)"
        },
        {
            "comment": "This code initializes input and output tensors for a multimodal video tagging scenario. It builds the input tensor from RGB frames, audio data, and text data, and retrieves the output tensor. The `preprocess_for_lod_data` function converts input data into a list of arrays with length indicators (LOD) for efficient handling.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/MultimodalVideoTag/scenario_lib/inference.py\":69-97",
            "content": "        # build input tensor and output tensor\n        self.build_input_output()\n    def build_input_output(self):\n        \"\"\"build_input_output\n        \"\"\"\n        input_names = self.predictor.get_input_names()\n        # input\n        self.input_rgb_tensor = self.predictor.get_input_handle(input_names[0])\n        self.input_audio_tensor = self.predictor.get_input_handle(input_names[1])\n        self.input_text_tensor = self.predictor.get_input_handle(input_names[2])\n        # output\n        output_names = self.predictor.get_output_names()\n        self.output_tensor = self.predictor.get_output_handle(output_names[0])\n    def preprocess_for_lod_data(self, input):\n        \"\"\"pre process\"\"\"\n        input_arr = []\n        input_lod = [0]\n        start_lod = 0\n        end_lod = 0\n        for sub_item in input:\n            end_lod = start_lod + len(sub_item)\n            input_lod.append(end_lod)\n            input_arr.extend(sub_item)\n            start_lod = end_lod\n        input_arr = np.array(input_arr)\n        return input_arr, [input_lod]"
        },
        {
            "comment": "In this code, a function named \"predict\" is defined. It uses a reader to process data for inference and iterates through each instance of the data. The instances contain RGB images, audio, text, and video IDs. These instances are preprocessed for LOD (Level Of Detail) data using the preprocess_for_lod_data method. The preprocessed data is then copied to corresponding input tensors (RGB, audio, and text) for inference.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/MultimodalVideoTag/scenario_lib/inference.py\":99-121",
            "content": "    def predict(self):\n        \"\"\"predict\"\"\"\n        infer_reader = get_reader(self.name, 'infer', self.cfg)\n        probs = []\n        video_ids = []\n        label_map_inverse = {value: key for key, value in self.label_map.items()}\n        for infer_iter, data in enumerate(infer_reader()):\n            # video_id = [[items[-2], items[-1]] for items in data]\n            rgb = [items[0] for items in data]\n            audio = [items[1] for items in data]\n            text = np.array([items[2] for items in data])\n            videos = np.array([items[3] for items in data])\n            rgb_arr, rgb_lod = self.preprocess_for_lod_data(rgb)\n            audio_arr, audio_lod = self.preprocess_for_lod_data(audio)\n            self.input_rgb_tensor.copy_from_cpu(rgb_arr.astype('float32'))\n            self.input_rgb_tensor.set_lod(rgb_lod)\n            self.input_audio_tensor.copy_from_cpu(audio_arr.astype('float32'))\n            self.input_audio_tensor.set_lod(audio_lod)\n            self.input_text_tensor.copy_from_cpu(text.astype('int64'))"
        },
        {
            "comment": "The code defines an inference function that takes a set of videos and their associated labels, and returns the predicted labels for each video. It also includes functions to load class information from a file and parse command-line arguments. The inference function first runs the predictor on the input data, then extracts the output probabilities and corresponding video IDs. It checks that the number of video IDs matches the number of probabilities. Then, for each video-probability pair, it identifies the indices where the probability is above a certain threshold and uses these to determine the predicted labels. The resulting dictionary contains the video ID, as well as a list of tuples containing the label name and corresponding probability for each detected label.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/MultimodalVideoTag/scenario_lib/inference.py\":123-160",
            "content": "            self.predictor.run()\n            output = self.output_tensor.copy_to_cpu()\n            probs.extend(list(output))\n            video_ids.extend(videos)\n        assert len(video_ids) == len(probs)\n        result = []\n        for video_id, prob in zip(video_ids, probs):\n            label_idx = list(np.where(prob >= self.threshold)[0])\n            result.append({\n                \"video_id\": video_id,\n                \"labels\": [\n                    (label_map_inverse[str(idx)], float(prob[idx])) for idx in label_idx\n                ]\n            })\n        return result\ndef load_class_file(class_file):\n    \"\"\"\n    load_class_file\n    \"\"\"\n    class_lines = open(class_file, 'r', encoding='utf8').readlines()\n    class_dict = {}\n    for i, line in enumerate(class_lines):\n        tmp = line.strip().split('\\t')\n        word = tmp[0]\n        index = str(i)\n        if len(tmp) == 2:\n            index = tmp[1]\n        class_dict[word] = index\n    return class_dict\ndef infer(args):\n    \"\"\"\n    infer main\n    \"\"\"\n    config = parse_config(args.config)"
        },
        {
            "comment": "This code snippet is from the \"inference.py\" file in the PaddleVideo MultimodalVideoTag application. It defines a function `infer` that performs model inference on input data and outputs the results. The code merges configs for the infer stage, prints them out, creates an InferModel object with those configs, loads the inference model from a given file (if provided), runs inference, and finally saves the results to a JSON file if requested.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/MultimodalVideoTag/scenario_lib/inference.py\":161-172",
            "content": "    infer_config = merge_configs(config, 'infer', vars(args))\n    print_configs(infer_config, 'infer')\n    infer_obj = InferModel(infer_config, name=args.model_name)\n    infer_obj.load_inference_model(args.save_inference_model, use_gpu=args.use_gpu)\n    rt = infer_obj.predict()\n    if args.output:\n        with open(args.output, 'w') as f:\n            json.dump(rt, f, ensure_ascii=False, indent=4)\nif __name__ == \"__main__\":\n    args = parse_args()\n    infer(args)"
        }
    ]
}