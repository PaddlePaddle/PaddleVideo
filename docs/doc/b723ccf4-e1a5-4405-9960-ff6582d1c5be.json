{
    "summary": "The Token Shift Transformer is a versatile video classification model utilizing vision transformer and Token Shift Module, trained on UCF-101 dataset with mixed-precision AMP acceleration, and achieves high accuracy with \"BrushingTeeth.avi\".",
    "details": [
        {
            "comment": "Token Shift Transformer is a video classification model using vision transformer, with a novel Token Shift Module for modeling temporal relations. It offers strong interpretability and flexibility, while being zero-parameter and zero-FLOPs. UCF-101 data preparation guide provided.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/tokenshift_transformer.md\":0-35",
            "content": "[\u7b80\u4f53\u4e2d\u6587](../../../zh-CN/model_zoo/recognition/tokenshift_transformer.md) | English\n# Token Shift Transformer\n## Content\n- [Introduction](#Introduction)\n- [Data](#DATA)\n- [Train](#Train)\n- [Test](#Test)\n- [Inference](#Inference)\n- [Reference](#Reference)\n## Introduction\nToken Shift Transformer is a video classification model based on vision transformer, which shares merits of strong interpretability, high discriminative power on hyper-scale data, and \ufb02exibility in processing varying length inputs. Token Shift Module is a novel, zero-parameter, zero-FLOPs operator, for modeling temporal relations within each transformer encoder.\n<div align=\"center\">\n<img src=\"../../../images/tokenshift_structure.png\">\n</div>\n## Data\nUCF-101 data download and preparation please refer to [UCF-101 data preparation](../../dataset/ucf101.md)\n## Train\n### UCF-101 data set training\n#### Download and add pre-trained models\n1. Download the image pre-training model [ViT_base_patch16_224](https://paddle-imagenet-models-name.bj.bcebos.c"
        },
        {
            "comment": "This code provides instructions on how to download a pre-trained model and modify a configuration file for training the TokenShift Transformer model on the UCF-101 dataset using PaddlePaddle. It also highlights the need for using mixed-precision training with AMP to accelerate the training process.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/tokenshift_transformer.md\":35-62",
            "content": "om/dygraph/ViT_base_patch16_224_pretrained.pdparams) as Backbone initialization parameters, or download through the wget command\n   ```bash\n   wget https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/ViT_base_patch16_224_pretrained.pdparams\n   ```\n2. Open `PaddleVideo/configs/recognition/token_transformer/tokShift_transformer_ucf101_256_videos.yaml`, and fill in the downloaded weight storage path below `pretrained:`\n    ```yaml\n    MODEL:\n        framework: \"RecognizerTransformer\"\n        backbone:\n            name: \"TokenShiftVisionTransformer\"\n            pretrained: fill in the path here\n    ```\n#### Start training\n- The UCF-101 data set uses 1 card for training, and the start command of the training method is as follows:\n```bash\n# videos data format\npython3 main.py -c configs/recognition/token_transformer/tokShift_transformer_ucf101_256_videos.yaml --validate --seed=1234\n```\n- Turn on amp mixed-precision training to speed up the training process. The training start command is as follows:\n```bash"
        },
        {
            "comment": "This code snippet is used to train a Token Shift Transformer model on the UCF101 dataset with a video size of 256. The model configuration file is tokShift_transformer_ucf101_256_videos.yaml, and the training is performed using automatic mixed precision (--amp flag). The model will be validated during training, and the best model's test accuracy can be found in the training log using the keyword \"best\". The test mode sampling method is uniform sampling, which differs from the dense sampling used in verification mode during training.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/tokenshift_transformer.md\":63-77",
            "content": "python3 main.py --amp -c configs/recognition/token_transformer/tokShift_transformer_ucf101_256_videos.yaml --validate --seed=1234\n```\n- In addition, you can customize and modify the parameter configuration to achieve the purpose of training/testing on different data sets. It is recommended that the naming method of the configuration file is `model_dataset name_file format_data format_sampling method.yaml` , Please refer to [config](../../tutorials/config.md) for parameter usage.\n## Test\n- The Token Shift Transformer model is verified synchronously during training. You can find the keyword `best` in the training log to obtain the model test accuracy. The log example is as follows:\n  ```\n  Already save the best model (top1 acc)0.9201\n  ```\n- Since the sampling method of the Token Shift Transformer model test mode is **uniform** sampling, which is different from the **dense** sampling used in the verification mode during the training process, so the verification index recorded in the training log"
        },
        {
            "comment": "This code describes a command for testing the best model after training is complete using the TokenShift VisionTransformer on the UCF-101 dataset. The test configuration parameters include backbone, sampling method, num_seg, and target_size to obtain Top-1 accuracy. The checkpoints are available in a shared Google Drive link. Uniform sampling divides timing equally into `num_seg`.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/tokenshift_transformer.md\":77-92",
            "content": ", called `topk Acc `, does not represent the final test score, so after the training is completed, you can use the test mode to test the best model to obtain the final index, the command is as follows:\n  ```bash\n  python3 main.py --amp -c configs/recognition/token_transformer/tokShift_transformer_ucf101_256_videos.yaml --test --seed=1234 -w 'output/TokenShiftVisionTransformer/TokenShiftVisionTransformer_best.pdparams'\n  ```\n  When the test configuration uses the following parameters, the test indicators on the validation data set of UCF-101 are as follows:\n  | backbone | sampling method | num_seg | target_size | Top-1 | checkpoints |\n  | :----------------: | :-----: | :-----: | :---------: | :----: | :----------------------------------------------------------: |\n  | Vision Transformer | Uniform | 8 | 256 | 92.81 | [TokenShiftTransformer.pdparams](https://drive.google.com/drive/folders/1k_TpAqaJZYJE8C5g5pT9phdyk9DrY_XL?usp=sharing) |\n- Uniform sampling: Timing-wise, equal division into `num_seg"
        },
        {
            "comment": "This code provides instructions for exporting an inference model and using the prediction engine in PaddleVideo's TokenShift Vision Transformer. The first command exports the model structure file (TokenShiftVisionTransformer.pdmodel) and the model weight file (TokenShiftVisionTransformer.pdiparams). The second command uses these files to perform inference on a specific video file (e.g., 'data/BrushingTeeth.avi').",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/tokenshift_transformer.md\":92-115",
            "content": "` segments, 1 frame sampled at the middle of each segment; spatially, sampling at the center. 1 video sampled 1 clip in total.\n## Inference\n### Export inference model\n```bash\npython3 tools/export_model.py -c configs/recognition/token_transformer/tokShift_transformer_ucf101_256_videos.yaml -p 'output/TokenShiftVisionTransformer/TokenShiftVisionTransformer_best.pdparams'\n```\nThe above command will generate the model structure file `TokenShiftVisionTransformer.pdmodel` and the model weight file `TokenShiftVisionTransformer.pdiparams` required for prediction.\n- For the meaning of each parameter, please refer to [Model Reasoning Method](../../usage.md#2-infer)\n### Use prediction engine inference\n```bash\npython3 tools/predict.py -c configs/recognition/token_transformer/tokShift_transformer_ucf101_256_videos.yaml -i 'data/BrushingTeeth.avi' --model_file ./inference/TokenShiftVisionTransformer.pdmodel --params_file ./inference/TokenShiftVisionTransformer.pdiparams\n```\nThe output example is as follows:\n```\nCurrent video file: data/BrushingTeeth.avi"
        },
        {
            "comment": "This code snippet is displaying the top-1 category prediction and confidence score for a given video file \"BrushingTeeth.avi\" using Token Shift Transformer model trained on UCF-101 dataset. The predicted top-1 category id is 19, and its corresponding category name is \"brushing_teeth\", with a high confidence of 0.99.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/tokenshift_transformer.md\":116-124",
            "content": "\ttop-1 class: 19\n\ttop-1 score: 0.9959074258804321\n```\nIt can be seen that using the Token Shift Transformer model trained on UCF-101 to predict `data/BrushingTeeth.avi`, the output top1 category id is `19`, and the confidence is 0.99. By consulting the category id and name correspondence table, it can be seen that the predicted category name is `brushing_teeth`.\n## Reference\n- [Is Space-Time Attention All You Need for Video Understanding?](https://arxiv.org/pdf/2102.05095.pdf), Gedas Bertasius, Heng Wang, Lorenzo Torresani"
        }
    ]
}