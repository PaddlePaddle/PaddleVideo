{
    "summary": "This code introduces TSN, a 2D-CNN-based video classification solution that utilizes sparse sampling and ResNet-50 as its backbone. It trains on Kinetics-400 dataset with pre-trained weights, provides data preparation/model config details, tests different methods/backbones, and exports an \"TSN\" inference model.",
    "details": [
        {
            "comment": "This code introduces TSN (Temporal Segment Network), a 2D-CNN-based solution for video classification. It uses sparse sampling to capture global information, reduce redundancy, and decrease computational burden. The model is based on single-channel RGB images and utilizes ResNet-50 as the backbone.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/tsn.md\":0-19",
            "content": "[\u7b80\u4f53\u4e2d\u6587](../../../zh-CN/model_zoo/recognition/tsn.md) | English\n# TSN\n## Content\n- [Introduction](#Introduction)\n- [Data](#Data)\n- [Train](#Train)\n- [Test](#Test)\n- [Inference](#Inference)\n- [Details](#Details)\n- [Reference](#Reference)\n## Introduction\nTemporal Segment Network (TSN) is a classic 2D-CNN-based solution in the field of video classification. This method mainly solves the problem of long-term behavior recognition of video, and replaces dense sampling by sparsely sampling video frames, which can not only capture the global information of the video, but also remove redundancy and reduce the amount of calculation. The core idea is to average the features of each frame as the overall feature of the video, and then enter the classifier for classification. The model implemented by this code is a TSN network based on a single-channel RGB image, and Backbone uses the ResNet-50 structure.\n<div align=\"center\">\n<img src=\"../../../images/tsn_architecture.png\" height=350 width=80000 hspace='10'/> <br />"
        },
        {
            "comment": "This code provides instructions for training the Temporal Segment Networks model on the Kinetics-400 dataset. It explains how to download and add pre-trained ResNet50 weights as initialization parameters, and specifies where to find more information about data preparation and model configuration.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/tsn.md\":20-47",
            "content": "</div>\nFor details, please refer to the ECCV 2016 paper [Temporal Segment Networks: Towards Good Practices for Deep Action Recognition](https://arxiv.org/abs/1608.00859)\n## Data\nPaddleVide provides training and testing scripts on the Kinetics-400 dataset. Kinetics-400 data download and preparation please refer to [Kinetics-400 data preparation](../../dataset/k400.md)\n## Train\n### Kinetics-400 data set training\n#### Download and add pre-trained models\n1. Load the ResNet50 weights trained on ImageNet1000 as Backbone initialization parameters [ResNet50_pretrain.pdparams](https://videotag.bj.bcebos.com/PaddleVideo/PretrainModel/ResNet50_pretrain.pdparams), or download through the command line\n   ```bash\n   wget https://videotag.bj.bcebos.com/PaddleVideo/PretrainModel/ResNet50_pretrain.pdparams\n   ```\n2. Open `PaddleVideo/configs/recognition/tsn/tsn_k400_frames.yaml`, and fill in the downloaded weight path below `pretrained:`\n   ```yaml\n   MODEL:\n       framework: \"Recognizer2D\"\n       backbone:\n           name: \"ResNet\""
        },
        {
            "comment": "Start training: Use Kinetics-400 dataset and 8 GPUs for training, command to start the training process.\nTest: TSN model test mode uses TenCrop method for better accuracy, different from training's CenterCrop; obtain final index by testing best model after training completes.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/tsn.md\":48-64",
            "content": "           pretrained: fill in the path here\n   ```\n#### Start training\n- Kinetics-400 data set uses 8 cards for training, the training start command for frames format data is as follows\n  ```bash\n  python3.7 -B -m paddle.distributed.launch --gpus=\"0,1,2,3,4,5,6,7\" --log_dir=log_tsn main.py --validate -c configs/recognition/ tsn/tsn_k400_frames.yaml\n  ```\n## Test\nSince the sampling method of the TSN model test mode is **TenCrop** with a slower speed but higher accuracy, which is different from the **CenterCrop** used in the verification mode during the training process, the verification index `topk Acc` recorded in the training log It does not represent the final test score, so after the training is completed, you can use the test mode to test the best model to obtain the final index. The command is as follows:\n```bash\npython3.7 -B -m paddle.distributed.launch --gpus=\"0,1,2,3,4,5,6,7\" --log_dir=log_tsn main.py --test -c configs/recognition/ tsn/tsn_k400_frames.yaml -w \"output/TSN/TSN_best.pdparams\""
        },
        {
            "comment": "The code is providing test indicator results for TSN model on the validation dataset of Kinetics-400 using different backbone, sampling methods, and training strategies. It also shows the checkpoints' URLs. Additionally, it exports an inference model named \"TSN\" into a folder called \"inference/TSN\" from the specified configuration file, model parameters, and output directory.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/tsn.md\":65-80",
            "content": "```\nWhen the test configuration uses the following parameters, the test indicators on the validation data set of Kinetics-400 are as follows:\n| backbone | Sampling method | Training Strategy | num_seg | target_size | Top-1 |                         checkpoints                          |\n| :------: | :-------------: | :---------------: | :-----: | :---------: | :---: | :----------------------------------------------------------: |\n| ResNet50 |     TenCrop     |       NCHW        |   3    |     224     | 69.81 | [TSN_k400.pdparams](https://videotag.bj.bcebos.com/PaddleVideo-release2.2/TSN_k400.pdparams) |\n| ResNet50 |     TenCrop     |       NCHW        |   8    |     224     | 71.70 | [TSN_k400_8.pdparams](https://videotag.bj.bcebos.com/PaddleVideo-release2.2/TSN_k400_8.pdparams) |\n## Inference\n### export inference model\n```bash\npython3.7 tools/export_model.py -c configs/recognition/tsn/tsn_k400_frames.yaml \\\n                                -p data/TSN_k400.pdparams \\\n                                -o inference/TSN"
        },
        {
            "comment": "This code is for generating and using the TSN model in PaddlePaddle for video recognition. It generates a model structure file (TSN.pdmodel) and weight file (TSN.pdiparams), and then uses predict.py to predict the labels of frames from a video file (example.avi) using the generated files, with GPU acceleration enabled. The model reads frames sparsely sampled from videos in the Kinetics-400 dataset, divides them into segments, extracts one frame per segment, and applies random data augmentation.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/tsn.md\":81-102",
            "content": "```\nThe above command will generate the model structure file `TSN.pdmodel` and the model weight file `TSN.pdiparams` required for prediction.\nFor the meaning of each parameter, please refer to [Model Reasoning Method](https://github.com/PaddlePaddle/PaddleVideo/blob/release/2.0/docs/zh-CN/start.md#2-Model Reasoning)\n### infer\n```bash\npython3.7 tools/predict.py --input_file data/example.avi \\\n                           --config configs/recognition/tsn/tsn_k400_frames.yaml \\\n                           --model_file inference/TSN/TSN.pdmodel \\\n                           --params_file inference/TSN/TSN.pdiparams \\\n                           --use_gpu=True \\\n                           --use_tensorrt=False\n```\n## Details\n**data processing:**\n- The model reads the `mp4` data in the Kinetics-400 data set, first divides each piece of video data into `num_seg` segments, and then evenly extracts 1 frame of image from each segment to obtain sparsely sampled `num_seg` video frames , And then do the same random da"
        },
        {
            "comment": "Enhances `num_seg` frame image with multi-scale random cropping, flips, normalization, and zooms to `target_size`. Momentum optimization is used for training, L2 decay with 1e-4 attenuation coefficient, global gradient clipping with a factor of 40.0. Total epochs are 100, learning rate decreases at epochs 40 and 80, dropout_ratio=0.4. KaimingNormal and Constant initializers used for convolutional layers and FC layer weights, respectively.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/tsn.md\":102-118",
            "content": "ta enhancement to this `num_seg` frame image, including multi-scale random cropping, random left and right flips, data normalization, etc., and finally zoom to `target_size`\n**training strategy:**\n- Use Momentum optimization algorithm for training, momentum=0.9\n- Using L2_Decay, the weight attenuation coefficient is 1e-4\n- Use global gradient clipping, with a clipping factor of 40.0\n- The total number of epochs is 100, and the learning rate will be attenuated by 0.1 times when the epoch reaches 40 and 80\n- Dropout_ratio=0.4\n**parameter initialization**\n- The convolutional layer of the TSN model uses Paddle's default [KaimingNormal](https://www.paddlepaddle.org.cn/documentation/docs/zh/develop/api/paddle/nn/initializer/KaimingNormal_cn.html#kaimingnormal) and [Constant](https://www.paddlepaddle.org.cn/documentation/docs/en/develop/api/paddle/nn/initializer/Constant_cn.html#constant) initialization method, with Normal(mean=0, std= 0.01) normal distribution to initialize the weight of the FC layer, and a constant 0 to initialize the bias of the FC layer"
        },
        {
            "comment": "The code contains a reference to the paper \"Temporal Segment Networks: Towards Good Practices for Deep Action Recognition\" by Limin Wang et al., which provides information on the implementation of TSN model in PaddleVideo.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/tsn.md\":120-122",
            "content": "## Reference\n- [Temporal Segment Networks: Towards Good Practices for Deep Action Recognition](https://arxiv.org/abs/1608.00859), Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, Luc Van Gool"
        }
    ]
}