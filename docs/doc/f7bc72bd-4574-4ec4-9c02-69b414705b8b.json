{
    "summary": "The code utilizes PaddlePaddle's Fleet API for distributed training, defines models/metrics, and uses AMP to speed up gradient descent via DataParallel. It logs performance data, evaluates using PaddleVideo, saves the best model/optimizer, and periodically saves state during training.",
    "details": [
        {
            "comment": "The code imports necessary libraries, defines functions to build data loaders, datasets, models, and metrics using a builder pattern. It also includes functions for logging progress and saving results. The code is licensed under the Apache License 2.0, and it might be part of a larger framework or application dealing with video analysis tasks.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/tasks/train.py\":0-26",
            "content": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os.path as osp\nimport time\nimport paddle\nimport paddle.amp as amp\nimport paddle.distributed as dist\nimport paddle.distributed.fleet as fleet\nfrom paddlevideo.utils import (add_profiler_step, build_record, get_logger,\n                               load, log_batch, log_epoch, mkdir, save)\nfrom ..loader.builder import build_dataloader, build_dataset\nfrom ..metrics.ava_utils import collect_results_cpu\nfrom ..modeling.builder import build_model"
        },
        {
            "comment": "The code defines a train_model function for training the model using given configuration (cfg). It takes optional arguments like weights path, parallel training flag, validation enablement, automatic mixed precision usage, and more.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/tasks/train.py\":27-50",
            "content": "from ..solver import build_lr, build_optimizer\nfrom ..utils import do_preciseBN\ndef train_model(cfg,\n                weights=None,\n                parallel=True,\n                validate=True,\n                use_amp=False,\n                amp_level=None,\n                max_iters=None,\n                use_fleet=False,\n                profiler_options=None):\n    \"\"\"Train model entry\n    Args:\n        cfg (dict): configuration.\n        weights (str, optional): weights path for finetuning. Defaults to None.\n        parallel (bool, optional): whether multi-cards training. Defaults to True.\n        validate (bool, optional): whether to do evaluation. Defaults to True.\n        use_amp (bool, optional): whether to use automatic mixed precision during training. Defaults to False.\n        amp_level (str, optional): amp optmization level, must be 'O1' or 'O2' when use_amp is True. Defaults to None.\n        max_iters (int, optional): max running iters in an epoch. Defaults to None.\n        use_fleet (bool, optional): whether to use fleet. Defaults to False."
        },
        {
            "comment": "This code sets up gradient accumulation and global batch size for distributed training using PaddlePaddle's Fleet API. It retrieves batch and validation batch sizes from the configuration, then checks if gradient accumulation is enabled and the world size of the distributed setup. If so, it calculates the global batch size based on these settings and asserts that global_batch_size is greater than the current batch size.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/tasks/train.py\":51-74",
            "content": "        profiler_options (str, optional): configuration for the profiler function. Defaults to None.\n    \"\"\"\n    if use_fleet:\n        fleet.init(is_collective=True)\n    logger = get_logger(\"paddlevideo\")\n    batch_size = cfg.DATASET.get('batch_size', 8)\n    valid_batch_size = cfg.DATASET.get('valid_batch_size', batch_size)\n    # gradient accumulation settings\n    use_gradient_accumulation = cfg.get('GRADIENT_ACCUMULATION', None)\n    if use_gradient_accumulation and dist.get_world_size() >= 1:\n        global_batch_size = cfg.GRADIENT_ACCUMULATION.get(\n            'global_batch_size', None)\n        num_gpus = dist.get_world_size()\n        assert isinstance(\n            global_batch_size, int\n        ), f\"global_batch_size must be int, but got {type(global_batch_size)}\"\n        assert batch_size <= global_batch_size, \\\n            f\"global_batch_size({global_batch_size}) must not be less than batch_size({batch_size})\"\n        cur_global_batch_size = batch_size * num_gpus  # The number of batches calculated by all GPUs at one time"
        },
        {
            "comment": "The code ensures the global batch size is divisible by cur_global_batch_size, sets the number of iterations needed to reach the global batch size, and sets the device type (NPU, XPU, or GPU) based on config values. It also allows for setting the number of workers for training and validation data loading.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/tasks/train.py\":75-95",
            "content": "        assert global_batch_size % cur_global_batch_size == 0, \\\n            f\"The global batchsize({global_batch_size}) must be divisible by cur_global_batch_size({cur_global_batch_size})\"\n        cfg.GRADIENT_ACCUMULATION[\n            \"num_iters\"] = global_batch_size // cur_global_batch_size\n        # The number of iterations required to reach the global batchsize\n        logger.info(\n            f\"Using gradient accumulation training strategy, \"\n            f\"global_batch_size={global_batch_size}, \"\n            f\"num_gpus={num_gpus}, \"\n            f\"num_accumulative_iters={cfg.GRADIENT_ACCUMULATION.num_iters}\")\n    if cfg.get('use_npu', False):\n        places = paddle.set_device('npu')\n    elif cfg.get('use_xpu', False):\n        places = paddle.set_device('xpu')\n    else:\n        places = paddle.set_device('gpu')\n    # default num worker: 0, which means no subprocess will be created\n    num_workers = cfg.DATASET.get('num_workers', 0)\n    valid_num_workers = cfg.DATASET.get('valid_num_workers', num_workers)"
        },
        {
            "comment": "Code snippet builds a model, creates dataset and dataloader for training and validation, and optionally converts the model to static using Paddle.jit.to_static(). It saves the output in the specified directory and logs if @to_static is applied successfully.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/tasks/train.py\":96-123",
            "content": "    model_name = cfg.model_name\n    output_dir = cfg.get(\"output_dir\", f\"./output/{model_name}\")\n    mkdir(output_dir)\n    # 1. Construct model\n    model = build_model(cfg.MODEL)\n    if cfg.get('to_static', False):\n        specs = None\n        model = paddle.jit.to_static(model, input_spec=specs)\n        logger.info(\n            \"Successfully to apply @to_static with specs: {}\".format(specs))\n    # 2. Construct dataset and dataloader for training and evaluation\n    train_dataset = build_dataset((cfg.DATASET.train, cfg.PIPELINE.train))\n    train_dataloader_setting = dict(\n        batch_size=batch_size,\n        num_workers=num_workers,\n        collate_fn_cfg=cfg.get('MIX', None),\n        places=places)\n    train_loader = build_dataloader(train_dataset, **train_dataloader_setting)\n    if validate:\n        valid_dataset = build_dataset((cfg.DATASET.valid, cfg.PIPELINE.valid))\n        validate_dataloader_setting = dict(\n            batch_size=valid_batch_size,\n            num_workers=valid_num_workers,\n            places=places,"
        },
        {
            "comment": "This code is setting up a training process for the PaddleVideo framework. It first creates train and validation dataloaders with specified settings, then constructs a learning rate scheduler and optimizer based on provided configurations. Optionally, it converts model parameters to fp16 using AMP if needed.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/tasks/train.py\":124-149",
            "content": "            drop_last=False,\n            shuffle=cfg.DATASET.get(\n                'shuffle_valid',\n                False)  # NOTE: attention_LSTM needs to shuffle valid data.\n        )\n        valid_loader = build_dataloader(valid_dataset,\n                                        **validate_dataloader_setting)\n    # 3. Construct learning rate scheduler(lr) and optimizer\n    lr = build_lr(cfg.OPTIMIZER.learning_rate, len(train_loader))\n    optimizer = build_optimizer(\n        cfg.OPTIMIZER, lr, model=model, use_amp=use_amp, amp_level=amp_level)\n    # 4. Construct scalar and convert parameters for amp(optional)\n    if use_amp:\n        scaler = amp.GradScaler(\n            init_loss_scaling=2.0**16,\n            incr_every_n_steps=2000,\n            decr_every_n_nan_or_inf=1)\n        # convert model parameters to fp16 when amp_level is O2(pure fp16)\n        model, optimizer = amp.decorate(\n            models=model,\n            optimizers=optimizer,\n            level=amp_level,\n            master_weight=True,\n            save_dtype=None)"
        },
        {
            "comment": "The code checks if training in amp mode or fp32 mode. If in amp mode, it asserts that the amp_level is not None and logs the current level. If in fp32 mode, it asserts that amp_level is None and logs the mode. It then handles optional resume and finetuning steps if specified by loading model weights from a file, setting the model state dictionary to the loaded dictionary, and logging the checkpoint used.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/tasks/train.py\":150-171",
            "content": "        # NOTE: save_dtype is set to float32 now.\n        logger.info(f\"Training in amp mode, amp_level={amp_level}.\")\n    else:\n        assert amp_level is None, f\"amp_level must be None when training in fp32 mode, but got {amp_level}.\"\n        logger.info(\"Training in fp32 mode.\")\n    # 5. Resume(optional)\n    resume_epoch = cfg.get(\"resume_epoch\", 0)\n    if resume_epoch:\n        filename = osp.join(output_dir,\n                            model_name + f\"_epoch_{resume_epoch:05d}\")\n        resume_model_dict = load(filename + '.pdparams')\n        resume_opt_dict = load(filename + '.pdopt')\n        model.set_state_dict(resume_model_dict)\n        optimizer.set_state_dict(resume_opt_dict)\n        logger.info(\"Resume from checkpoint: {}\".format(filename))\n    # 6. Finetune(optional)\n    if weights:\n        assert resume_epoch == 0, f\"Conflict occurs when finetuning, please switch resume function off by setting resume_epoch to 0 or not indicating it.\"\n        model_dict = load(weights)\n        model.set_state_dict(model_dict)"
        },
        {
            "comment": "The code finetunes a model from a specified checkpoint. It optionally parallelizes the training process using Paddle's DataParallel API and Fleet distributed computing for further optimization. The code trains the model for a specified number of epochs, continuing from a previous resume_epoch if needed. Performance information is collected when profiler options are activated.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/tasks/train.py\":172-203",
            "content": "        logger.info(\"Finetune from checkpoint: {}\".format(weights))\n    # 7. Parallelize(optional)\n    if parallel:\n        model = paddle.DataParallel(model)\n    if use_fleet:\n        model = fleet.distributed_model(model)\n        optimizer = fleet.distributed_optimizer(optimizer)\n    # 8. Train Model\n    best = 0.0\n    for epoch in range(0, cfg.epochs):\n        if epoch < resume_epoch:\n            logger.info(\n                f\"| epoch: [{epoch + 1}] <= resume_epoch: [{resume_epoch}], continue...\"\n            )\n            continue\n        model.train()\n        record_list = build_record(cfg.MODEL)\n        tic = time.time()\n        for i, data in enumerate(train_loader):\n            \"\"\"Next two line of code only used in test_tipc,\n            ignore it most of the time\"\"\"\n            if max_iters is not None and i >= max_iters:\n                break\n            record_list['reader_time'].update(time.time() - tic)\n            # Collect performance information when profiler_options is activate\n            add_profiler_step(profiler_options)"
        },
        {
            "comment": "Applies Automatic Mixed Precision (AMP) for faster training, calculates average loss, performs gradient accumulation, and scales backpropagation to reduce memory usage.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/tasks/train.py\":205-228",
            "content": "            # 8.1 forward\n            # AMP #\n            if use_amp:\n                with amp.auto_cast(\n                        custom_black_list={\"reduce_mean\", \"conv3d\"},\n                        level=amp_level):\n                    outputs = model(data, mode='train')\n                avg_loss = outputs['loss']\n                if use_gradient_accumulation:\n                    # clear grad at when epoch begins\n                    if i == 0:\n                        optimizer.clear_grad()\n                    # Loss normalization\n                    avg_loss /= cfg.GRADIENT_ACCUMULATION.num_iters\n                    # Loss scaling\n                    scaled = scaler.scale(avg_loss)\n                    # 8.2 backward\n                    scaled.backward()\n                    # 8.3 minimize\n                    if (i + 1) % cfg.GRADIENT_ACCUMULATION.num_iters == 0:\n                        scaler.minimize(optimizer, scaled)\n                        optimizer.clear_grad()\n                else:  # general case\n                    # Loss scaling"
        },
        {
            "comment": "This code calculates the average loss, scales it if necessary, performs backward pass, and applies gradient descent to minimize the loss. If gradient accumulation is used, the gradients are cleared at the start of each epoch and after every accumulated number of iterations.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/tasks/train.py\":229-252",
            "content": "                    scaled = scaler.scale(avg_loss)\n                    # 8.2 backward\n                    scaled.backward()\n                    # 8.3 minimize\n                    scaler.minimize(optimizer, scaled)\n                    optimizer.clear_grad()\n            else:\n                outputs = model(data, mode='train')\n                avg_loss = outputs['loss']\n                if use_gradient_accumulation:\n                    # clear grad at when epoch begins\n                    if i == 0:\n                        optimizer.clear_grad()\n                    # Loss normalization\n                    avg_loss /= cfg.GRADIENT_ACCUMULATION.num_iters\n                    # 8.2 backward\n                    avg_loss.backward()\n                    # 8.3 minimize\n                    if (i + 1) % cfg.GRADIENT_ACCUMULATION.num_iters == 0:\n                        optimizer.step()\n                        optimizer.clear_grad()\n                else:  # general case\n                    # 8.2 backward\n                    avg_loss.backward()"
        },
        {
            "comment": "Optimizer step and gradient clearance followed by logging records, updating logs, calculating instantaneous performance (ips), determining progress and estimated time of arrival (eta), and calling log_batch function.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/tasks/train.py\":253-276",
            "content": "                    # 8.3 minimize\n                    optimizer.step()\n                    optimizer.clear_grad()\n            # log record\n            record_list['lr'].update(optimizer.get_lr(), batch_size)\n            for name, value in outputs.items():\n                if name in record_list:\n                    record_list[name].update(value, batch_size)\n            record_list['batch_time'].update(time.time() - tic)\n            tic = time.time()\n            if i % cfg.get(\"log_interval\", 10) == 0:\n                ips = \"ips: {:.5f} instance/sec,\".format(\n                    batch_size / record_list[\"batch_time\"].val)\n                cur_progress = ((i + 1) + epoch * len(train_loader)) / (\n                    len(train_loader) * cfg.epochs)\n                eta = int(record_list[\"batch_time\"].sum *\n                          (1 - cur_progress) / cur_progress + 0.5)\n                log_batch(record_list, i, epoch + 1, cfg.epochs, \"train\", ips,\n                          eta)\n            # learning rate iter step"
        },
        {
            "comment": "This code snippet is from the PaddleVideo library and it contains code for training a model. It uses an optimizer with a learning rate that can be stepped based on whether it's an iterative step or not. After performing an epoch, it logs the average instances per second processed. The code then evaluates the model by setting it to evaluation mode and collecting test results using a record list. It also records the time taken for testing in 'tic'.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/tasks/train.py\":277-305",
            "content": "            if cfg.OPTIMIZER.learning_rate.get(\"iter_step\"):\n                lr.step()\n        # learning rate epoch step\n        if not cfg.OPTIMIZER.learning_rate.get(\"iter_step\"):\n            lr.step()\n        ips = \"avg_ips: {:.5f} instance/sec.\".format(\n            batch_size * record_list[\"batch_time\"].count /\n            record_list[\"batch_time\"].sum)\n        log_epoch(record_list, epoch + 1, \"train\", ips)\n        def evaluate(best):\n            model.eval()\n            results = []\n            record_list = build_record(cfg.MODEL)\n            record_list.pop('lr')\n            tic = time.time()\n            if parallel:\n                rank = dist.get_rank()\n            # single_gpu_test and multi_gpu_test\n            for i, data in enumerate(valid_loader):\n                \"\"\"Next two line of code only used in test_tipc,\n                ignore it most of the time\"\"\"\n                if max_iters is not None and i >= max_iters:\n                    break\n                if use_amp:\n                    with amp.auto_cast("
        },
        {
            "comment": "This code snippet is from the PaddleVideo library and appears to be handling model training for a specific framework. It calculates outputs, updates records for non-FastRCNN models, logs batch information, and handles FastRCNN-specific operations. The code also includes functionality for updating batch time and logging progress at regular intervals.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/tasks/train.py\":306-329",
            "content": "                            custom_black_list={\"reduce_mean\", \"conv3d\"},\n                            level=amp_level):\n                        outputs = model(data, mode='valid')\n                else:\n                    outputs = model(data, mode='valid')\n                if cfg.MODEL.framework == \"FastRCNN\":\n                    results.extend(outputs)\n                # log_record\n                if cfg.MODEL.framework != \"FastRCNN\":\n                    for name, value in outputs.items():\n                        if name in record_list:\n                            record_list[name].update(value, batch_size)\n                record_list['batch_time'].update(time.time() - tic)\n                tic = time.time()\n                if i % cfg.get(\"log_interval\", 10) == 0:\n                    ips = \"ips: {:.5f} instance/sec.\".format(\n                        valid_batch_size / record_list[\"batch_time\"].val)\n                    log_batch(record_list, i, epoch + 1, cfg.epochs, \"val\", ips)\n            if cfg.MODEL.framework == \"FastRCNN\":"
        },
        {
            "comment": "Code section checks if parallel processing is enabled, collects results for CPU, and evaluates the dataset. It calculates average instance processing speed and logs it. If using specific models like FastRCNN or YOWOLocalizer, compares current performance metrics with the best values achieved so far and returns them along with a flag indicating if a new best value was found.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/tasks/train.py\":330-350",
            "content": "                if parallel:\n                    results = collect_results_cpu(results, len(valid_dataset))\n                if not parallel or (parallel and rank == 0):\n                    eval_res = valid_dataset.evaluate(results)\n                    for name, value in eval_res.items():\n                        record_list[name].update(value, valid_batch_size)\n            ips = \"avg_ips: {:.5f} instance/sec.\".format(\n                valid_batch_size * record_list[\"batch_time\"].count /\n                record_list[\"batch_time\"].sum)\n            log_epoch(record_list, epoch + 1, \"val\", ips)\n            best_flag = False\n            if cfg.MODEL.framework == \"FastRCNN\" and (not parallel or\n                                                      (parallel and rank == 0)):\n                if record_list[\"mAP@0.5IOU\"].val > best:\n                    best = record_list[\"mAP@0.5IOU\"].val\n                    best_flag = True\n                return best, best_flag\n            if cfg.MODEL.framework == \"YOWOLocalizer\" and (not parallel or"
        },
        {
            "comment": "This code is updating the best value and flag based on various metrics (fscore, hit_at_one, top1, rmse, F1@0.50) in a parallel setting with rank 0. It also checks if using precise batch normalization improves accuracy every 'preciseBN_interval' epochs.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/tasks/train.py\":351-372",
            "content": "                                                           (parallel and rank == 0)):\n                if record_list[\"fscore\"].avg > best:\n                    best = record_list[\"fscore\"].avg\n                    best_flag = True\n                return best, best_flag\n            # forbest2, cfg.MODEL.framework != \"FastRCNN\":\n            for top_flag in ['hit_at_one', 'top1', 'rmse', \"F1@0.50\"]:\n                if record_list.get(top_flag):\n                    if top_flag != 'rmse' and record_list[top_flag].avg > best:\n                        best = record_list[top_flag].avg\n                        best_flag = True\n                    elif top_flag == 'rmse' and (\n                            best == 0.0 or record_list[top_flag].avg < best):\n                        best = record_list[top_flag].avg\n                        best_flag = True\n            return best, best_flag\n        # use precise bn to improve acc\n        if cfg.get(\"PRECISEBN\") and (\n                epoch % cfg.PRECISEBN.preciseBN_interval == 0"
        },
        {
            "comment": "This code block is responsible for the precise Batch Normalization and validation steps in a deep learning training process. It applies PreciseBN for specific number of iterations, performs validation every \"val_interval\" epochs or at the last epoch, saves best model state if validation accuracy improves, and handles model saving differently depending on the framework used (Distillation vs others).",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/tasks/train.py\":373-394",
            "content": "                or epoch == cfg.epochs - 1):\n            do_preciseBN(model, train_loader, parallel,\n                         min(cfg.PRECISEBN.num_iters_preciseBN,\n                             len(train_loader)), use_amp, amp_level)\n        # 9. Validation\n        if validate and (epoch % cfg.get(\"val_interval\", 1) == 0\n                         or epoch == cfg.epochs - 1):\n            with paddle.no_grad():\n                best, save_best_flag = evaluate(best)\n            # save best\n            if save_best_flag:\n                save(optimizer.state_dict(),\n                     osp.join(output_dir, model_name + \"_best.pdopt\"))\n                save_student_model_flag = True if \"Distillation\" in cfg.MODEL.framework else False\n                save(\n                    model.state_dict(),\n                    osp.join(output_dir, model_name + \"_best.pdparams\"),\n                    save_student_model=save_student_model_flag)\n                if model_name == \"AttentionLstm\":\n                    logger.info(\n                        f\"Already save the best model (hit_at_one){best}\")"
        },
        {
            "comment": "This code block checks the current model framework and logs the metric used to identify the best model saved, followed by saving the best model and optimizer.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/tasks/train.py\":395-416",
            "content": "                elif cfg.MODEL.framework == \"FastRCNN\":\n                    logger.info(\n                        f\"Already save the best model (mAP@0.5IOU){int(best * 10000) / 10000}\"\n                    )\n                elif cfg.MODEL.framework == \"DepthEstimator\":\n                    logger.info(\n                        f\"Already save the best model (rmse){int(best * 10000) / 10000}\"\n                    )\n                elif cfg.MODEL.framework in ['MSTCN', 'ASRF']:\n                    logger.info(\n                        f\"Already save the best model (F1@0.50){int(best * 10000) / 10000}\"\n                    )\n                elif cfg.MODEL.framework in ['YOWOLocalizer']:\n                    logger.info(\n                        f\"Already save the best model (fsocre){int(best * 10000) / 10000}\"\n                    )\n                else:\n                    logger.info(\n                        f\"Already save the best model (top1 acc){int(best * 10000) / 10000}\"\n                    )\n        # 10. Save model and optimizer"
        },
        {
            "comment": "This code saves the optimizer and model state dictionaries at specific intervals during training. The optimizer state is saved with a .pdopt extension and the model state is saved with a .pdparams extension. This occurs if the current epoch is either divisible by the save_interval or is the final epoch, to preserve progress during training. Finally, it logs that training for the specified model has finished.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/tasks/train.py\":417-425",
            "content": "        if epoch % cfg.get(\"save_interval\", 1) == 0 or epoch == cfg.epochs - 1:\n            save(optimizer.state_dict(),\n                 osp.join(output_dir,\n                          model_name + f\"_epoch_{epoch + 1:05d}.pdopt\"))\n            save(model.state_dict(),\n                 osp.join(output_dir,\n                          model_name + f\"_epoch_{epoch + 1:05d}.pdparams\"))\n    logger.info(f'training {model_name} finished')"
        }
    ]
}