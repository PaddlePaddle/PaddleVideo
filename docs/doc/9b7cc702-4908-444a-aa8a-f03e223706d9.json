{
    "summary": "This code constructs an optimizer and learning rate scheduler for parameter optimization, adjustable parameters, and applies regularizers to prevent overfitting. It sets weight decay based on name and value from configuration and returns the optimizer with specified parameters.",
    "details": [
        {
            "comment": "This code builds an optimizer and learning rate scheduler for parameter optimization based on the given configuration file. It allows for different optimizer types (e.g., Momentum) with adjustable parameters like momentum and weight decay.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/VideoQualityAssessment/paddlevideo/solver/optimizer.py\":0-35",
            "content": "\"\"\"\n# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nimport copy\nimport paddle\ndef build_optimizer(cfg, lr_scheduler, parameter_list=None):\n    \"\"\"\n    Build an optimizer and learning rate scheduler to optimize parameters accroding to ```OPTIMIZER``` field in configuration .\n    In configuration:\n    OPTIMIZER:\n        name: Momentum\n        momentum: 0.9\n        weight_decay: 0.001\n    or\n    OPTIMIZER:\n        name: Momentum\n        momentum: 0.9\n        weight_decay:"
        },
        {
            "comment": "This code defines an optimizer function that creates an optimizer based on the provided configuration. It uses an Adam optimizer to optimize a network and applies an L2Decay regularizer to avoid overfitting. The L1Decay regularizer can also be applied. The function takes an optimizer configuration dictionary, learning rate scheduler, and a list of parameters to be optimized as inputs and returns a paddle optimizer object. It checks for none and illegal configurations.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/VideoQualityAssessment/paddlevideo/solver/optimizer.py\":36-67",
            "content": "            name: \"L1\"\n            value: 0.001\n    Momentum optimizer will be applied to optimize network and L1Decay regularizer will be applied to avoid overfit.\n    OPTIMIZER:\n        name: Adam\n        weight_decay:\n            name: \"L2\"\n            value: 0.001\n    Adam optimizer will be applied to optimize network and L2Decay regularizer will applied to avoid overfit.\n    Refer to ```https://www.paddlepaddle.org.cn/documentation/docs/en/develop/api/paddle/regularizer/L2Decay_en.html``` for more details.\n    Args:\n        cfg (dict): optimizer configuration.\n        lr_schduler: learning rate scheduler.\n        parameter_list (list): parameters to be optimized.\n    Returns:\n        optimizer (paddle.optimizer): paddle optimizer.\n    \"\"\"\n    cfg_copy = cfg.copy()\n    #XXX check none and illegal cfg!!!\n    opt_name = cfg_copy.pop('name')\n    # deal with weight decay\n    if cfg_copy.get('weight_decay'):\n        if isinstance(cfg_copy.get('weight_decay'), float) or 'L1' in cfg_copy.get('weight_decay').get('name').upper():"
        },
        {
            "comment": "This code sets the weight decay based on its name and value from configuration. If 'L2' is in the name, it adds L2 Decay regularizer. Otherwise, it raises a ValueError. It then removes learning_rate from config and returns an optimizer with specified parameters and other configurations.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/VideoQualityAssessment/paddlevideo/solver/optimizer.py\":68-78",
            "content": "            cfg_copy['weight_decay'] = cfg_copy.get('weight_decay').get('value')\n        elif 'L2' in cfg_copy.get('weight_decay').get('name').upper():\n            cfg_copy['weight_decay'] = paddle.regularizer.L2Decay(cfg_copy.get('weight_decay').get('value'))\n        else:\n            raise ValueError\n    cfg_copy.pop('learning_rate')\n    return getattr(paddle.optimizer, opt_name)(lr_scheduler,\n                                               parameters=parameter_list,\n                                               **cfg_copy)"
        }
    ]
}