{
    "summary": "This code defines MSR-Vtt dataset paths, performs type checking, and loads features for a specific expert, handling aggregation, caching, and saving raw captions. It also checks text features, verifies format, size, and number of test captions, calculates missing queries, and raises errors for incorrect query mask sum.",
    "details": [
        {
            "comment": "The code snippet is part of the MSRVTT class in the PaddleVideo library. It defines a dataset for MSR-Vtt, a large-scale video description dataset. The dataset_paths method returns the paths to train and test data splits. This method is typechecked to ensure that input types match expected data structures.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/data_loader/MSRVTT_dataset.py\":0-28",
            "content": "# Copyright (c) 2021  PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nimport copy\nfrom pathlib import Path\nfrom utils import memory_summary\nfrom typeguard import typechecked\nfrom typing import Dict, Union, List\nfrom base.base_dataset import BaseDataset\nfrom zsvision.zs_utils import memcache, concat_features\nclass MSRVTT(BaseDataset):\n    @staticmethod\n    @typechecked\n    def dataset_paths() -> Dict[str, Union[str, List[str], Path, Dict]]:\n        subset_paths = {}\n        split_name = \"jsfusion\"\n        train_list_path = \"train_list_jsfusion.txt\"\n        test_list_path = \"val_list_jsfusion.txt\""
        },
        {
            "comment": "This code defines the data split paths for training and validation sets, as well as custom feature paths for different types of features. The JSFusion test caption indices path is also specified to reproduce a specific evaluation subset.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/data_loader/MSRVTT_dataset.py\":29-45",
            "content": "        # NOTE: The JSFusion split (referred to as 1k-A in the paper) uses all\n        # videos, but randomly samples a single caption per video from the test\n        # set for evaluation. To reproduce this evaluation, we use the indices\n        # of the test captions, and restrict to this subset during eval.\n        js_test_cap_idx_path = \"jsfusion_val_caption_idx.pkl\"\n        subset_paths[split_name] = {\"train\": train_list_path, \"val\": test_list_path}\n        custom_paths = {\n            \"features_audio\": [\"mmt_feats/features.audio.pkl\"],\n            \"features_flow\": [\"mmt_feats/features.flow_agg.pkl\"],\n            \"features_rgb\": [\"mmt_feats/features.rgb_agg.pkl\"],\n            \"features_scene\": [\"mmt_feats/features.scene.pkl\"],\n            \"features_face\": [\"mmt_feats/features.face_agg.pkl\"],\n            \"features_ocr\": [\"mmt_feats/features.ocr.pkl\"],\n            \"features_s3d\": [\"mmt_feats/features.s3d.pkl\"],\n            \"features_speech\": [\"mmt_feats/features.speech.pkl\"],\n        }\n        text_feat_paths = {"
        },
        {
            "comment": "This code is loading features from the MSRVTT dataset. It defines paths for text features and raw captions, and then updates a dictionary with custom paths, subset list paths, text feature paths, raw caption path, and JS test caption index path. The load_features method retrieves these paths and loads the features accordingly.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/data_loader/MSRVTT_dataset.py\":46-70",
            "content": "            \"openai\": \"w2v_MSRVTT_openAIGPT.pickle\",\n        }\n        text_feat_paths = {key: Path(\"aggregated_text_feats\") / fname\n                           for key, fname in text_feat_paths.items()}\n        feature_info = {\n            \"custom_paths\": custom_paths,\n            \"subset_list_paths\": subset_paths,\n            \"text_feat_paths\": text_feat_paths,\n            \"raw_captions_path\": \"raw-captions.pkl\",\n            \"js_test_cap_idx_path\": js_test_cap_idx_path,\n        }\n        return feature_info\n    def load_features(self):\n        root_feat = Path(self.root_feat)\n        feat_names = {}\n        custom_path_key = \"custom_paths\"\n        feat_names.update(self.paths[custom_path_key])\n        features = {}\n        for expert, rel_names in feat_names.items():\n            if expert not in self.ordered_experts:\n                continue\n            feat_paths = tuple([root_feat / rel_name for rel_name in rel_names])\n            if len(feat_paths) == 1:\n                features[expert] = memcache(feat_paths[0])"
        },
        {
            "comment": "This code is handling feature aggregation for a specific expert. It checks if the aggregation method is \"concat\" and then concatenates the features based on the given axis. If not, it throws an error message. The code also caches information about the concatenated features, copies the features for each split, and stores them in the 'features' dictionary. Finally, it saves raw captions using memcache and updates self.raw_captions.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/data_loader/MSRVTT_dataset.py\":71-88",
            "content": "            else:\n                # support multiple forms of feature (e.g. max and avg pooling). For\n                # now, we only support direct concatenation\n                msg = f\"{expert}: Only direct concatenation of muliple feats is possible\"\n                print(f\"Concatenating aggregates for {expert}....\")\n                is_concat = self.feat_aggregation[expert][\"aggregate\"] == \"concat\"\n                self.log_assert(is_concat, msg=msg)\n                axis = self.feat_aggregation[expert][\"aggregate-axis\"]\n                x = concat_features.cache_info()  # pylint: disable=no-value-for-parameter\n                print(f\"concat cache info: {x}\")\n                features_ = concat_features(feat_paths, axis=axis)\n                memory_summary()\n                # Make separate feature copies for each split to allow in-place filtering\n                features[expert] = copy.deepcopy(features_)\n        self.features = features\n        self.raw_captions = memcache(root_feat / self.paths[\"raw_captions_path\"])"
        },
        {
            "comment": "This code retrieves text features from the cache and checks if they belong to a specific training set. It also verifies that the train text features are in the expected format (a list with length 19 or 20).",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/data_loader/MSRVTT_dataset.py\":89-107",
            "content": "        text_feat_path = root_feat / self.paths[\"text_feat_paths\"][self.text_feat]\n        self.text_features = memcache(text_feat_path)\n        if self.restrict_train_captions:\n            # hash the video names to avoid O(n) lookups in long lists\n            train_list = set(self.partition_lists[\"train\"])\n            for key, val in self.text_features.items():\n                if key not in train_list:\n                    continue\n                if not self.split_name == \"full-test\":\n                    # Note that we do not perform this sanity check for the full-test\n                    # split, because the text features in the cached dataset will\n                    # already have been cropped to the specified\n                    # `resstrict_train_captions`\n                    expect = {19, 20}\n                    msg = f\"expected train text feats as lists with length {expect}\"\n                    has_expected_feats = isinstance(val, list) and len(val) in expect\n                    self.log_assert(has_expected_feats, msg=msg)"
        },
        {
            "comment": "The code checks if the number of test captions is set to 20 and verifies that the corresponding validation list size matches expected values. It calculates the missing queries based on the validation list size and raises a ValueError for unrecognized test sets. The code asserts that the difference between query mask sum and its size should be equal to the number of missing queries, with an error message if not correct.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/data_loader/MSRVTT_dataset.py\":109-125",
            "content": "                # restrict to the first N captions (deterministic)\n                self.text_features[key] = val[:self.restrict_train_captions]\n        self.summary_stats()\n    def sanity_checks(self):\n        if self.num_test_captions == 20:\n            if len(self.partition_lists[\"val\"]) == 2990:\n                missing = 6\n            elif len(self.partition_lists[\"val\"]) == 1000:\n                missing = 2\n            elif len(self.partition_lists[\"val\"]) == 497:\n                missing = 0\n            else:\n                raise ValueError(\"unrecognised test set\")\n            msg = \"Expected to find two missing queries in MSRVTT for full eval\"\n            correct_missing = self.query_masks.sum() == self.query_masks.size - missing\n            self.log_assert(correct_missing, msg=msg)"
        }
    ]
}