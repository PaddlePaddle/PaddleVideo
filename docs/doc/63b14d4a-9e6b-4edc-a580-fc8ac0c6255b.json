{
    "summary": "PaddleVideo library enables feature extraction and map creation, while GetVideoLabel class calculates IoU for object detection tasks. The code stores max IoU values and prepares data for evaluation or processing.",
    "details": [
        {
            "comment": "This code is part of PaddleVideo library, specifically for loading feature data from a given path. It defines a class \"LoadFeat\" and uses the numpy library to load .npy files based on the video name provided in the results dictionary. The file path is constructed using the specified feat_path and video name.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/anet_pipeline.py\":0-31",
            "content": "#  Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\nimport numpy as np\nfrom ..registry import PIPELINES\n\"\"\"pipeline ops for Activity Net.\n\"\"\"\n@PIPELINES.register()\nclass LoadFeat(object):\n    def __init__(self, feat_path):\n        self.feat_path = feat_path\n    def __call__(self, results):\n        video_name = results['video_name']\n        file_name = video_name + \".npy\"\n        file_path = os.path.join(self.feat_path, file_name)\n        #TODO: check path\n        video_feat = np.load(file_path)"
        },
        {
            "comment": "This code defines a pipeline function that generates matching maps for an input video. It creates temporal matching windows of varying sizes and reshapes the result into a specific format. The anchor positions are also extracted for later use.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/anet_pipeline.py\":32-61",
            "content": "        video_feat = video_feat.T\n        video_feat = video_feat.astype(\"float32\")\n        results['video_feat'] = video_feat\n        return results\n@PIPELINES.register()\nclass GetMatchMap(object):\n    def __init__(self, tscale):\n        self.tscale = tscale\n        self.tgap = 1. / self.tscale\n    def __call__(self, results):\n        match_map = []\n        for idx in range(self.tscale):\n            tmp_match_window = []\n            xmin = self.tgap * idx\n            for jdx in range(1, self.tscale + 1):\n                xmax = xmin + self.tgap * jdx\n                tmp_match_window.append([xmin, xmax])\n            match_map.append(tmp_match_window)\n        match_map = np.array(match_map)\n        match_map = np.transpose(match_map, [1, 0, 2])\n        match_map = np.reshape(match_map, [-1, 2])\n        anchor_xmin = [self.tgap * i for i in range(self.tscale)]\n        anchor_xmax = [self.tgap * i for i in range(1, self.tscale + 1)]\n        results['match_map'] = match_map\n        results['anchor_xmin'] = anchor_xmin"
        },
        {
            "comment": "This code defines a class called \"GetVideoLabel\" which calculates the Intersection over Union (IOU) and intersection scores between a box and the anchors. It also initializes variables for time and distance scaling, and box type data types. The \"iou_with_anchors\" method calculates the Jaccard score and the \"ioa_with_anchors\" method computes the intersection. These methods can be used to determine the best match between an anchor box and a target box in object detection tasks.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/anet_pipeline.py\":62-89",
            "content": "        results['anchor_xmax'] = anchor_xmax\n        return results\n@PIPELINES.register()\nclass GetVideoLabel(object):\n    def __init__(self, tscale, dscale, datatype=\"float32\"):\n        self.tscale = tscale\n        self.dscale = dscale\n        self.tgap = 1. / self.tscale\n        self.datatype = datatype\n    def iou_with_anchors(self, anchors_min, anchors_max, box_min, box_max):\n        \"\"\"Compute jaccard score between a box and the anchors.\n        \"\"\"\n        len_anchors = anchors_max - anchors_min\n        int_xmin = np.maximum(anchors_min, box_min)\n        int_xmax = np.minimum(anchors_max, box_max)\n        inter_len = np.maximum(int_xmax - int_xmin, 0.)\n        union_len = len_anchors - inter_len + box_max - box_min\n        jaccard = np.divide(inter_len, union_len)\n        return jaccard\n    def ioa_with_anchors(self, anchors_min, anchors_max, box_min, box_max):\n        \"\"\"Compute intersection between score a box and the anchors.\n        \"\"\"\n        len_anchors = anchors_max - anchors_min\n        int_xmin = np.maximum(anchors_min, box_min)"
        },
        {
            "comment": "The function initializes gt_bbox and gt_iou_map variables to store ground truth bounding box coordinates and their IoU with anchor boxes. It then iterates through video labels, calculating the start and end timestamps in video seconds for each ground truth box. The IoU between match map and the current ground truth is computed using the iou_with_anchors function and stored in gt_iou_map, reshaped to match the dimensions of dscale and tscale.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/anet_pipeline.py\":90-114",
            "content": "        int_xmax = np.minimum(anchors_max, box_max)\n        inter_len = np.maximum(int_xmax - int_xmin, 0.)\n        scores = np.divide(inter_len, len_anchors)\n        return scores\n    def __call__(self, results):\n        video_info = results['video_info']\n        match_map = results['match_map']\n        anchor_xmin = results['anchor_xmin']\n        anchor_xmax = results['anchor_xmax']\n        video_second = video_info['duration_second']\n        video_labels = video_info['annotations']\n        gt_bbox = []\n        gt_iou_map = []\n        for gt in video_labels:\n            tmp_start = max(min(1, gt[\"segment\"][0] / video_second), 0)\n            tmp_end = max(min(1, gt[\"segment\"][1] / video_second), 0)\n            gt_bbox.append([tmp_start, tmp_end])\n            tmp_gt_iou_map = self.iou_with_anchors(match_map[:, 0],\n                                                   match_map[:, 1], tmp_start,\n                                                   tmp_end)\n            tmp_gt_iou_map = np.reshape(tmp_gt_iou_map,\n                                        [self.dscale, self.tscale])"
        },
        {
            "comment": "This code calculates the intersection over union (IoU) between ground truth bounding boxes and anchor boxes. It stores the maximum IoU values for each ground truth box and anchor pair, then calculates the maximum IoU values for start and end positions of anchor boxes. This information will be used to determine if a prediction matches with a ground truth box and assign appropriate scores.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/anet_pipeline.py\":115-139",
            "content": "            gt_iou_map.append(tmp_gt_iou_map)\n        gt_iou_map = np.array(gt_iou_map)\n        gt_iou_map = np.max(gt_iou_map, axis=0)\n        gt_bbox = np.array(gt_bbox)\n        gt_xmins = gt_bbox[:, 0]\n        gt_xmaxs = gt_bbox[:, 1]\n        gt_len_small = 3 * self.tgap\n        gt_start_bboxs = np.stack(\n            (gt_xmins - gt_len_small / 2, gt_xmins + gt_len_small / 2), axis=1)\n        gt_end_bboxs = np.stack(\n            (gt_xmaxs - gt_len_small / 2, gt_xmaxs + gt_len_small / 2), axis=1)\n        match_score_start = []\n        for jdx in range(len(anchor_xmin)):\n            match_score_start.append(\n                np.max(\n                    self.ioa_with_anchors(anchor_xmin[jdx], anchor_xmax[jdx],\n                                          gt_start_bboxs[:, 0],\n                                          gt_start_bboxs[:, 1])))\n        match_score_end = []\n        for jdx in range(len(anchor_xmin)):\n            match_score_end.append(\n                np.max(\n                    self.ioa_with_anchors(anchor_xmin[jdx], anchor_xmax[jdx],"
        },
        {
            "comment": "This code is storing ground truth (gt) IOU map, start and end indices for the annotations into the 'results' dictionary. The IOU map is converted to specified datatype before storage. These values will be used later for evaluation or further processing.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/anet_pipeline.py\":140-149",
            "content": "                                          gt_end_bboxs[:, 0], gt_end_bboxs[:,\n                                                                           1])))\n        gt_start = np.array(match_score_start)\n        gt_end = np.array(match_score_end)\n        results['gt_iou_map'] = gt_iou_map.astype(self.datatype)\n        results['gt_start'] = gt_start.astype(self.datatype)\n        results['gt_end'] = gt_end.astype(self.datatype)\n        return results"
        }
    ]
}