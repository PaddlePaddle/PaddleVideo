{
    "summary": "This code constructs a learning rate scheduler based on the 'OPTIMIZER' configuration provided, returns it with specified iterations, and handles custom cases such as converting 'learning_rate' to a custom object.",
    "details": [
        {
            "comment": "This code is building a learning rate scheduler according to the \"OPTIMIZER\" configuration provided in the cfg dictionary. The scheduler is based on the 'PiecewiseDecay' name, and has boundaries and values for adjusting the learning rate at specified iterations.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/solver/lr.py\":0-27",
            "content": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom typing import Dict\nfrom paddle.optimizer.lr import LRScheduler\nfrom . import custom_lr\ndef build_lr(cfg: Dict, num_iters: int) -> LRScheduler:\n    \"\"\"Build a learning rate scheduler accroding to ```OPTIMIZER``` configuration, and it always pass into the optimizer.\n    In configuration:\n    learning_rate:\n        name: 'PiecewiseDecay'\n        boundaries: [20, 60]\n        values: [0.00025, 0.000025, 0.0000025]"
        },
        {
            "comment": "This function takes a learning rate configuration and the number of iterations, and returns a learning rate scheduler. If the configuration includes a 'learning_rate' key with a dictionary value, it converts it to a custom learning rate object using the build_lr() function. It also handles cases where 'iter_step' is present in the configuration, replacing it with 'num_iters'. The returned scheduler is obtained from the 'custom_lr' module with the specified 'name'.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/solver/lr.py\":29-51",
            "content": "    Args:\n        cfg (Dict): learning rate configuration.\n        num_iters (int): The number of iterations that may be used when calculating the learning rate\n    Returns:\n        LRScheduler: learning rate scheduler.\n    \"\"\"\n    cfg_copy = cfg.copy()\n    #when learning_rate is LRScheduler\n    if cfg_copy.get('learning_rate') and isinstance(cfg_copy['learning_rate'],\n                                                    dict):\n        cfg_copy['learning_rate'] = build_lr(\n            cfg_copy['learning_rate'],\n            num_iters)  #not support only inner iter_step\n    lr_name = cfg_copy.pop('name')\n    if cfg_copy.get('iter_step'):\n        cfg_copy['num_iters'] = num_iters\n        cfg_copy.pop('iter_step')\n    return getattr(custom_lr, lr_name)(**cfg_copy)"
        }
    ]
}