{
    "summary": "The code sets command-line arguments for Paddle Video tool, configures the predictor, supports GPU/NPU usage, and utilizes TensorRT engine with YOWO model. It creates a directory, preprocesses data, performs inference, post-processes output, benchmarks, and guides users to install \"auto_log\".",
    "details": [
        {
            "comment": "Code snippet is an import-heavy function definition. It begins with a lengthy comment mentioning the copyright and license details, followed by multiple imports from various modules. The only executable code present is the \"parse_args\" function definition. This function uses argparse to create a parser for general parameters of PaddleVideo Inference model script.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/predict.py\":0-31",
            "content": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport argparse\nimport os\nfrom os import path as osp\nimport paddle\nfrom paddle import inference\nfrom paddle.inference import Config, create_predictor\nfrom utils import build_inference_helper\nfrom paddlevideo.utils import get_config\ndef parse_args():\n    def str2bool(v):\n        return v.lower() in (\"true\", \"t\", \"1\")\n    # general params\n    parser = argparse.ArgumentParser(\"PaddleVideo Inference model script\")\n    parser.add_argument("
        },
        {
            "comment": "The code defines command-line arguments for a Paddle Video tool. It allows the user to specify the config file, input file, model and parameters files, batch size, and GPU/XPU usage. The `str2bool` type converts string inputs to boolean values.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/predict.py\":32-58",
            "content": "        '-c',\n        '--config',\n        type=str,\n        default='configs/example.yaml',\n        help='config file path')\n    parser.add_argument(\n        '-o',\n        '--override',\n        action='append',\n        default=[],\n        help='config options to be overridden')\n    parser.add_argument(\"-i\", \"--input_file\", type=str, help=\"input file path\")\n    parser.add_argument(\n        \"--time_test_file\",\n        type=str2bool,\n        default=False,\n        help=\"whether input time test file\")\n    parser.add_argument(\"--model_file\", type=str)\n    parser.add_argument(\"--params_file\", type=str)\n    # params for paddle predict\n    parser.add_argument(\"-b\", \"--batch_size\", type=int, default=1)\n    parser.add_argument(\"--use_gpu\", type=str2bool, default=True)\n    parser.add_argument(\"--use_xpu\", type=str2bool, default=False)\n    parser.add_argument(\"--use_npu\", type=str2bool, default=False)\n    parser.add_argument(\"--precision\", type=str, default=\"fp32\")\n    parser.add_argument(\"--ir_optim\", type=str2bool, default=True)"
        },
        {
            "comment": "This code is parsing arguments to configure a Paddle video predictor. It adds various arguments for use_tensorrt, gpu_mem, enable_benchmark, enable_mkldnn, cpu_threads, and disable_glog. The create_paddle_predictor function creates a config object with the provided arguments, enabling GPU or NPU usage if specified, and disabling GPU if not. It also sets the number of CPU threads if provided, and enables MKLDNN if enabled.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/predict.py\":59-83",
            "content": "    parser.add_argument(\"--use_tensorrt\", type=str2bool, default=False)\n    parser.add_argument(\"--gpu_mem\", type=int, default=8000)\n    parser.add_argument(\"--enable_benchmark\", type=str2bool, default=False)\n    parser.add_argument(\"--enable_mkldnn\", type=str2bool, default=False)\n    parser.add_argument(\"--cpu_threads\", type=int, default=None)\n    parser.add_argument(\"--disable_glog\", type=str2bool, default=False)\n    # parser.add_argument(\"--hubserving\", type=str2bool, default=False)  #TODO\n    return parser.parse_args()\ndef create_paddle_predictor(args, cfg):\n    config = Config(args.model_file, args.params_file)\n    if args.use_gpu:\n        config.enable_use_gpu(args.gpu_mem, 0)\n    elif args.use_npu:\n        config.enable_npu()\n    elif args.use_xpu:\n        config.enable_xpu()\n    else:\n        config.disable_gpu()\n        if args.cpu_threads:\n            config.set_cpu_math_library_num_threads(args.cpu_threads)\n        if args.enable_mkldnn:\n            # cache 10 different shapes for mkldnn to avoid memory leak"
        },
        {
            "comment": "The code configures the PaddleVideo model for inference by setting the MKLDNN cache capacity, enabling MKLDNN and optionally BFloat16, disabling GLOG info, switching IR optim, and handling precision and batch size when TensorRT is enabled.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/predict.py\":84-106",
            "content": "            config.set_mkldnn_cache_capacity(10)\n            config.enable_mkldnn()\n            if args.precision == \"fp16\":\n                config.enable_mkldnn_bfloat16()\n    # config.disable_glog_info()\n    config.switch_ir_optim(args.ir_optim)  # default true\n    if args.use_tensorrt:\n        # choose precision\n        if args.precision == \"fp16\":\n            precision = inference.PrecisionType.Half\n        elif args.precision == \"int8\":\n            precision = inference.PrecisionType.Int8\n        else:\n            precision = inference.PrecisionType.Float32\n        # calculate real max batch size during inference when tenrotRT enabled\n        max_batch_size = args.batch_size\n        if 'num_seg' in cfg.INFERENCE:\n            # num_seg: number of segments when extracting frames.\n            # seg_len: number of frames extracted within a segment, default to 1.\n            # num_views: the number of video frame groups obtained by cropping and flipping,\n            # uniformcrop=3, tencrop=10, centercrop=1."
        },
        {
            "comment": "The code sets the number of segments and views based on the model name, calculates the maximum batch size, enables TensorRT engine with specified precision mode, enables memory optimization, disables glog if instructed to do so, and potentially deletes a pass for ST-GCN TensorRT case usage.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/predict.py\":107-133",
            "content": "            num_seg = cfg.INFERENCE.num_seg\n            seg_len = cfg.INFERENCE.get('seg_len', 1)\n            num_views = 1\n            if 'tsm' in cfg.model_name.lower():\n                num_views = 1  # CenterCrop\n            elif 'tsn' in cfg.model_name.lower():\n                num_views = 10  # TenCrop\n            elif 'timesformer' in cfg.model_name.lower():\n                num_views = 3  # UniformCrop\n            elif 'videoswin' in cfg.model_name.lower():\n                num_views = 3  # UniformCrop\n            elif 'tokenshift' in cfg.model_name.lower():\n                num_views = 3  # UniformCrop\n            max_batch_size = args.batch_size * num_views * num_seg * seg_len\n        config.enable_tensorrt_engine(\n            precision_mode=precision, max_batch_size=max_batch_size)\n    config.enable_memory_optim()\n    # use zero copy\n    config.switch_use_feed_fetch_ops(False)\n    # disable glog\n    if args.disable_glog:\n        config.disable_glog_info()\n    # for ST-GCN tensorRT case usage\n    # config.delete_pass(\"shuffle_channel_detect_pass\")"
        },
        {
            "comment": "The code is implementing a main function for predicting using Paddle Inference model. It first parses arguments from command-line, then retrieves configuration and overrides for the inference task, prints an informative message, builds the inference helper, and creates paddle predictor with the given arguments and configuration. After this, it gets input and output names, initializes empty lists for input and output tensors, and iterates through input names to populate these lists.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/predict.py\":135-172",
            "content": "    predictor = create_predictor(config)\n    return config, predictor\ndef parse_file_paths(input_path: str) -> list:\n    if osp.isfile(input_path):\n        files = [\n            input_path,\n        ]\n    else:\n        files = os.listdir(input_path)\n        files = [\n            file for file in files\n            if (file.endswith(\".avi\") or file.endswith(\".mp4\"))\n        ]\n        files = [osp.join(input_path, file) for file in files]\n    return files\ndef main():\n    \"\"\"predict using paddle inference model\n    \"\"\"\n    args = parse_args()\n    cfg = get_config(args.config, overrides=args.override, show=False)\n    model_name = cfg.model_name\n    print(f\"Inference model({model_name})...\")\n    InferenceHelper = build_inference_helper(cfg.INFERENCE)\n    inference_config, predictor = create_paddle_predictor(args, cfg)\n    # get input_tensor and output_tensor\n    input_names = predictor.get_input_names()\n    output_names = predictor.get_output_names()\n    input_tensor_list = []\n    output_tensor_list = []\n    for item in input_names:"
        },
        {
            "comment": "The code is processing input files for a specific model and running inference. For certain models, it preprocesses the input files using InferenceHelper and then runs inference by setting input tensors and calling predictor.run(). Finally, if the model is AVA_SlowFast_FastRcnn, it post-processes the output.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/predict.py\":173-200",
            "content": "        input_tensor_list.append(predictor.get_input_handle(item))\n    for item in output_names:\n        output_tensor_list.append(predictor.get_output_handle(item))\n    # get the absolute file path(s) to be processed\n    if model_name in [\"MSTCN\", \"ASRF\"]:\n        files = InferenceHelper.get_process_file(args.input_file)\n    else:\n        files = parse_file_paths(args.input_file)\n    if model_name == 'TransNetV2':\n        for file in files:\n            inputs = InferenceHelper.preprocess(file)\n            outputs = []\n            for input in inputs:\n                # Run inference\n                for i in range(len(input_tensor_list)):\n                    input_tensor_list[i].copy_from_cpu(input)\n                predictor.run()\n                output = []\n                for j in range(len(output_tensor_list)):\n                    output.append(output_tensor_list[j].copy_to_cpu())\n                outputs.append(output)\n            # Post process output\n            InferenceHelper.postprocess(outputs)\n    elif model_name == 'AVA_SlowFast_FastRcnn':"
        },
        {
            "comment": "Iterates through each video file in the list. \nPreprocesses the input data for a model. \nRuns inference for each input, copying CPU memory. \nStores the output of each run. \nPost processes the outputs using InferenceHelper function. For YOWO model, also does filename operations and saves results to specified directory.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/predict.py\":201-226",
            "content": "        for file in files:  # for videos\n            inputs = InferenceHelper.preprocess(file)\n            outputs = []\n            for input in inputs:\n                # Run inference\n                input_len = len(input_tensor_list)\n                for i in range(input_len):\n                    if type(input[i]) == paddle.Tensor:\n                        input_tmp = input[i].numpy()\n                    else:\n                        input_tmp = input[i]\n                    input_tensor_list[i].copy_from_cpu(input_tmp)\n                predictor.run()\n                output = []\n                for j in range(len(output_tensor_list)):\n                    output.append(output_tensor_list[j].copy_to_cpu())\n                outputs.append(output)\n            # Post process output\n            InferenceHelper.postprocess(outputs)\n    elif model_name == 'YOWO':\n        for file in files:  # for videos\n            (_, filename) = os.path.split(file)\n            (filename, _) = os.path.splitext(filename)\n            save_dir = osp.join('inference', 'YOWO_infer')"
        },
        {
            "comment": "This code creates a directory and checks if the save path exists, then preprocesses input data for inference. It runs inference using a predictor, post-processes the output, and if benchmarking is enabled, it instantiates auto log.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/predict.py\":227-250",
            "content": "            if not osp.exists('inference'):\n                os.mkdir('inference')\n            if not osp.exists(save_dir):\n                os.mkdir(save_dir)\n            save_path = osp.join(save_dir, filename)\n            if not osp.exists(save_path):\n                os.mkdir(save_path)\n            inputs, frames = InferenceHelper.preprocess(file)\n            for idx, input in enumerate(inputs):\n                # Run inference\n                outputs = []\n                input_len = len(input_tensor_list)\n                for i in range(input_len):\n                    input_tensor_list[i].copy_from_cpu(input[i])\n                predictor.run()\n                for j in range(len(output_tensor_list)):\n                    outputs.append(output_tensor_list[j].copy_to_cpu())\n                # Post process output\n                InferenceHelper.postprocess(outputs, frames[idx], osp.join(save_path, str(idx).zfill(3)))\n    else:\n        if args.enable_benchmark:\n            num_warmup = 3\n            # instantiate auto log"
        },
        {
            "comment": "This code snippet attempts to import the \"auto_log\" package and if it fails, provides instructions on how to install it. Then, it creates an instance of AutoLogger, configuring various parameters like model name, batch size, data shape, etc., and specifies which timing metrics to track during inference. If no time test file is provided, the code sets the number of test videos to 15 and assigns all input files to these tests.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/predict.py\":251-274",
            "content": "            try:\n                import auto_log\n            except ImportError as e:\n                print(f\"{e}, [git+https://github.com/LDOUBLEV/AutoLog] \"\n                      f\"package and it's dependencies is required for \"\n                      f\"python-inference when enable_benchmark=True.\")\n            pid = os.getpid()\n            autolog = auto_log.AutoLogger(\n                model_name=cfg.model_name,\n                model_precision=args.precision,\n                batch_size=args.batch_size,\n                data_shape=\"dynamic\",\n                save_path=\"./output/auto_log.lpg\",\n                inference_config=inference_config,\n                pids=pid,\n                process_name=None,\n                gpu_ids=0 if args.use_gpu else None,\n                time_keys=[\n                    'preprocess_time', 'inference_time', 'postprocess_time'\n                ],\n                warmup=num_warmup)\n            if not args.time_test_file:\n                test_video_num = 15\n                files = [args.input_file for _ in range(test_video_num)]"
        },
        {
            "comment": "This code reads input files, processes them in batches, runs inference on a model, and collects output. It also supports benchmarking and logs processing times for each step.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/predict.py\":275-305",
            "content": "            else:\n                f_input = open(args.input_file, 'r')\n                files = [i.strip() for i in f_input.readlines()]\n                test_video_num = len(files)\n                f_input.close()\n        # Inferencing process\n        batch_num = args.batch_size\n        for st_idx in range(0, len(files), batch_num):\n            ed_idx = min(st_idx + batch_num, len(files))\n            # auto log start\n            if args.enable_benchmark:\n                autolog.times.start()\n            # Pre process batched input\n            batched_inputs = InferenceHelper.preprocess_batch(\n                files[st_idx:ed_idx])\n            # get pre process time cost\n            if args.enable_benchmark:\n                autolog.times.stamp()\n            # run inference\n            for i in range(len(input_tensor_list)):\n                input_tensor_list[i].copy_from_cpu(batched_inputs[i])\n            predictor.run()\n            batched_outputs = []\n            for j in range(len(output_tensor_list)):\n                batched_outputs.append(output_tensor_list[j].copy_to_cpu())"
        },
        {
            "comment": "Enables benchmarking for inference time, processes outputs and records post-processing time, then reports the benchmark log if enabled.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/predict.py\":307-326",
            "content": "            # get inference process time cost\n            if args.enable_benchmark:\n                autolog.times.stamp()\n            InferenceHelper.postprocess(batched_outputs,\n                                        not args.enable_benchmark)\n            # get post process time cost\n            if args.enable_benchmark:\n                autolog.times.end(stamp=True)\n            # time.sleep(0.01)  # sleep for T4 GPU\n    # report benchmark log if enabled\n    if args.enable_benchmark:\n        autolog.report()\nif __name__ == \"__main__\":\n    main()"
        }
    ]
}