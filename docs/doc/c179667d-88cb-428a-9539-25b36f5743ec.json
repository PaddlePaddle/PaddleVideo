{
    "summary": "The Temporal Shift Module (TSM) is an efficient video understanding technique that balances performance and efficiency, capturing spatial-temporal features. Suitable for both online and offline videos, it focuses on temporal information and has a simple 2-line implementation.",
    "details": [
        {
            "comment": "This code snippet provides background and motivation for TSM (Temporal Shift Module), a classic model in video understanding proposed by MIT and IBM Watson AI Lab. The TSM aims to balance efficiency and performance while improving the ability to analyze video content in various dimensions. It is related to the Temporal Segment Network (TSN) published by Limin Wang.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/tutorials/TSM.md\":0-4",
            "content": "# 1. Background&Motivation\nAt present, the video data on the Internet is increasing rapidly, and the time users spend watching short videos and small videos is also increasing rapidly. How to analyze, process and classify the massive video resources quickly and accurately is an urgent problem to be solved. The video understanding technology can analyze the video content in multiple dimensions, understand the video semantics, and automatically classify and label the video, which greatly saves the efficiency of manual audit and costs. At the same time, accurate user recommendation is realized to improve the experience effect.\nIn this paper, we will introduce the classic model **TSM (Temporal Shift Module)** in the field of video understanding, which is proposed by **MIT** and **IBM Watson AI Lab** `Ji Lin, Chuang Gan and Songhan, etc`, to achieve the balance between effeiciency and performance and improve video understanding ability.\nThe most relevant video understanding model to TSM is the **Temporal Segment Network (TSN)** published by Limin Wang"
        },
        {
            "comment": "This code describes the Temporal Shift Module (TSM), which is a method for efficient video understanding that avoids extra computation by using temporal dimension feature map shift. It is based on the concept of capturing spatial-temporal features, with a focus on temporal information in videos. This approach aims to achieve feature fusion and joint modeling among different frames without adding extra computational overhead compared to TSN.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/tutorials/TSM.md\":5-9",
            "content": "a series of works represented such as I3D, S3D and P3D, which carry out end-to-end joint spatial-temporal modeling through 3D convolution. Although this series of works can capture spatial-temporal features, compared with TSN, the transition from 2D convolution to 3D convolution inevitably introduces extra computation. TSM cleverly uses the idea of temporal dimension feature map shift, theoretically achieving the purpose of feature fusion and joint modeling among different frames with zero extra computing overhead compared with TSN.\n**Paper Address:** [Temporal Shift Module for Efficient Video Understanding](https://arxiv.org/pdf/1811.08383v2.pdf)\nLet's have a look at the following example: if the video is played from left to right and then from right to left respectively, the subjects will give different but correct interpretation of the video, indicating that the understanding of the video is strongly dependent on the temporal information of the video. Yes !, It is the motivation why TSM is proposed."
        },
        {
            "comment": "The code is an introduction to Temporal Shift Module (TSM) in video understanding, highlighting the trade-offs between 2D and 3D CNN methods, and how TSM embeds time displacement into 2D CNN for equivalent performance without additional computation or parameters.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/tutorials/TSM.md\":10-20",
            "content": "<p align=\"center\">\n<img src=\"../../images/temporal.png\" height=188 width=500 hspace='10'/> <br />\n</p>\nIt looks interesting, next,let's dive into the core modules of TSM.\n# 2. Dark technologies used in TSM\nOn the basis of traditional image analysis, video analysis needs researchers to supplement the modeling structure of temporal information. At present, 2D CNN and 3D CNN are the two most commonly used methods in video understanding: using 2D CNN model requires less computation but will lose part of the time information; While using 3D CNN has a good effect but a large amount of computation. Faced with such a situation, Ji Lin, Chuang Gan and Song Han et al. from MIT and IBM Watson AI Lab proposed the Temp Shift Module (TSM) Module. By embedding the time displacement module into 2D CNN, they can easily achieve the same video understanding ability as 3D CNN without adding any additional calculation and parameters.\n<p align=\"center\">\n<img src=\"../../images/tsm_intr.png\" height=188 width=500 hspace='10'/> <br />"
        },
        {
            "comment": "This code describes the TSM (Temporal Segment Networks) module, which introduces context interaction on the temporal dimension in feature graphs. It does this by moving some channels forward and backward one step in the temporal dimension, filling gaps with zeros. The channel movement allows 2D convolution to extract spatial-temporal information like 3D convolution. This improves model ability in time dimension and has TSM modules suitable for online and offline videos.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/tutorials/TSM.md\":21-26",
            "content": "</p>\nThe rows and columns of the matrix in the figure above represent the temporal and channel dimensions of the feature graph, respectively. In TSM module, some channels are moved forward one step int the temporal dimension, and some channels are moved backward one step in the temporal dimension, and the gaps after the displacement are zeroed. In this way, context interaction on the temporal dimension is introduced into the feature graph. The channel movement operation can make the current frame contain the channel information of the two adjacent frames. In this way, the 2D convolution operation can directly extract the spatial-temporal information of the video just like the 3D convolution.\nIt improves the modeling ability of the model in time dimension. based on this basis, the researchers further subdivided the module into TSM module suitable for online video and TSM module suitable for offline video.\n<p align=\"center\">\n<img src=\"../../images/tsm_architecture.png\" height=188 width=500 hspace='10'/> <br />"
        },
        {
            "comment": "Bi-Direction TSM module handles past and future spatial and temporal information, suitable for high throughput offline videos. UNI-Direction TSM is more appropriate for low delay online video recognition. Residual TSM performs better than in-place TSM but may affect spatial information extraction. Torch version tsm implementation to follow.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/tutorials/TSM.md\":27-39",
            "content": "</p>\nBi-Direction TSM module can obtain past and future spatial and temporal information, which is suitable for offline video with high throughput. However, UNI-Direction TSM module is only suitable for low delay online video recognition compared with the present and past spatio-temporal information.\nIn addition, the author also considered the insertion position of TSM modules and compared two TSM insertion methods: **Residual TSM** and **in-place TSM**. The author found that **Residual TSM** could achieve better performance than **in-place TSM**, At the same time, author explained that **in-place TSM** may affect the extraction of spatial information.\n<p align=\"center\">\n<img src=\"../../images/residual_tsm.png\" height=188 width=500 hspace='10'/> <br />\n</p>\nTSM module looks **So Easy!!**, the next question is how to implement ?\n# 3. The core codes of TSM\nNow that the principle is clear, let's look at how the code works. First let's have a look the torch version tsm. Unfortunately, the Torch fr"
        },
        {
            "comment": "The code demonstrates a TSM model implementation in PaddlePaddle framework, allowing users to achieve Temporal Shift Module (TSM) operations without writing additional code. It significantly improves accuracy and efficiency on Something-Something datasets. The provided images visually explain the TSM implementation and the optimized version (TSM OP). Additionally, the documentation refers users to the acceleration documentation for further information on speeding up the model while reducing memory consumption.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/tutorials/TSM.md\":39-57",
            "content": "amework does not provide an API for TSM, so we will have to do it by ourselves. The code is shown below:\n<p align=\"center\">\n<img src=\"../../images/torch_tsm.png\" height=160 width=500 hspace='10'/> <br />\n</p>\nThis means that you only need to add four lines of code to TSN's codebase then you can **double the accuracy in Something-Something datasets!!** what a simple and efficient model!\nBut...\uff0c\n**paddlepaddle** framework take the needs of the majority of users into account and have achieve TSM OP,then users can use it easily.\n<p align=\"center\">\n<img src=\"../../images/tsm_op.png\" height=300 width=400 hspace='10'/> <br />\n</p>\nSo you no longer have to achieve it by yourself, **it cab be called directly!!! , it can be called directly!!! , it can be called directly!!!** The important thing must say three times.\nDo you think that it is the end of the this topic?  **Too young Too simple !!!**\nWe have also optimized it to increase speed by 5 times while reducing memory consumption. See the acceleration documentation [accelerate.md](./accelerate.md) for more information."
        },
        {
            "comment": "The provided code demonstrates an implementation of the Temporal Shift Module (TSM) in PaddlePaddle. It only requires two lines of code and uses the `temporal_shift` function from `paddle.nn.functional`. This makes it easy to implement the TSM for efficient video understanding, as referenced by Lin Ji et al. and Limin Wang et al.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/tutorials/TSM.md\":59-72",
            "content": "Let's have a look at how TSM is implemented using **paddlepaddle**:\n`import paddle.nn.functional as F`\n`shifts = F.temporal_shift(inputs, self.num_seg, 1.0 / self.num_seg)`\n**Only two lines codes !!!**, isn't it easy ?\n# Reference\n[1] [Lin Ji , Gan Chuang , Han Song . TSM: Temporal Shift Module for Efficient Video Understanding. arXiv:1811.08383,2018](https://arxiv.org/pdf/1811.08383v2.pdf).\n[2] [Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoo Tang,and Luc Van Gool. Temporal segment networks for action recognition in videos? In Proceedings of the European Conference on Computer Vision,pages 20\u201336. Springer, 2016](https://arxiv.org/abs/1608.00859)."
        }
    ]
}