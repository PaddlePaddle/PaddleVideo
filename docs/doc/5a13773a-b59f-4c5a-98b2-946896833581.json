{
    "summary": "ActBERT is a multimodal pretrain task using global action info and TaNgled Transformer block (TNT) for text-object interactions. It outperforms state-of-the-art in video-language tasks and can be trained on HowTo100M dataset with AMP for faster training, evaluated on MSR-VTT, and found at the provided link.",
    "details": [
        {
            "comment": "This is an introduction to ActBERT, a multimodal pretrain task proposed by Baidu in CVPR2020. It uses global action information to analyze mutual interactions between linguistic texts and local regional objects. The method introduces TaNgled Transformer block (TNT) to encode three sources of information. ActBERT outperforms state-of-the-art in five video-and-language tasks, including text-video clip retrieval, video captioning, and action segmentation.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/multimodal/actbert.md\":0-24",
            "content": "[\u7b80\u4f53\u4e2d\u6587](../../../zh-CN/model_zoo/multimodal/actbert.md) | English\n# ActBERT\n---\n## Contents\n- [Introduction](#Introduction)\n- [Data](#Data)\n- [Train](#Train)\n- [Test](#Test)\n- [Reference](#Reference)\nBefore getting started, you need to install additional dependencies as follows:\n```bash\npython -m pip install paddlenlp\npython -m pip install lmdb\n```\n## Introduction\nActbert is proposed by Baidu in CVPR2020 for multimodal pretrain task. It leverage global action information to cat- alyze mutual interactions between linguistic texts and local regional objects.  This method introduce a TaNgled Transformer block (TNT) to encode three sources of information, i.e., global actions, local regional objects, and linguistic descriptions. ActBERT significantly outperforms the state- of-the-art in five downstream video-and-language tasks, i.e., text-video clip retrieval, video captioning, video question answering, action segmentation, and action step localization.\n<div align=\"center\">\n<img src=\"../../../images/actbert.png\" height=400 width=500 hspace='10'/> <br />"
        },
        {
            "comment": "This code describes how to train ActBERT on HowTo100M dataset. It first requires downloading the pretrain-model \"bert-base-uncased\" from a specified URL and adding its path to the config file. Then, it provides the command to start training using the provided script with specific configuration and GPU allocation.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/multimodal/actbert.md\":25-64",
            "content": "</div>\n## Data\nPlease refer to Kinetics400 data download and preparation doc [HowTo100M-data](../../dataset/howto100m.md)\nPlease refer to MSR-VTT data download and preparation doc [MSR-VTT-data](../../dataset/umsrvtt.md)\n## Train\n### Train on HowTo100M\n#### download pretrain-model\nPlease download [bert-base-uncased](https://videotag.bj.bcebos.com/PaddleVideo-release2.2/bert-base-uncased.pdparams) as pretraind model:\n```bash\nwget https://videotag.bj.bcebos.com/PaddleVideo-release2.2/bert-base-uncased.pdparams\n```\nand add path to `MODEL.framework.backbone.pretrained` in config file as\uff1a\n```yaml\nMODEL:\n    framework: \"ActBert\"\n    backbone:\n        name: \"BertForMultiModalPreTraining\"\n        pretrained: your weight path\n```\n- We provide training option on small data, config file is for reference only.\n#### Start training\n- Train ActBERT on HowTo100M scripts:\n```bash\npython3.7 -B -m paddle.distributed.launch --gpus=\"0,1,2,3,4,5,6,7\"  --log_dir=log_actbert  main.py  --validate -c configs/multimodal/actbert/actbert.yaml"
        },
        {
            "comment": "This code shows how to train a model using PaddlePaddle with Automatic Mixed Precision (AMP) for faster training, evaluate it on the MSR-VTT dataset, and provides metrics such as R@1, R@5, R@10, and Median R. The ActBERT model can be found at the provided link in the reference section.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/multimodal/actbert.md\":65-97",
            "content": "```\n- AMP is useful for speeding up training:\n```bash\nexport FLAGS_conv_workspace_size_limit=800 #MB\nexport FLAGS_cudnn_exhaustive_search=1\nexport FLAGS_cudnn_batchnorm_spatial_persistent=1\npython3.7 -B -m paddle.distributed.launch --gpus=\"0,1,2,3,4,5,6,7\"  --log_dir=log_actbert  main.py  --amp --validate -c configs/multimodal/actbert/actbert.yaml\n```\n## Test\n- Evaluation performs on downstream task, i.e. text-video clip retrieval on MSR-VTT dataset, test accuracy can be obtained using scripts:\n```bash\npython3.7 main.py --test -c configs/multimodal/actbert/actbert_msrvtt.yaml -w Actbert.pdparams\n```\nMetrics on MSR-VTT:\n| R@1 | R@5 | R@10 | Median R | Mean R | checkpoints |\n| :------: | :----------: | :----: | :----: | :----: | :----: |\n| 8.6 | 31.2 | 45.5 | 13.0 | 28.5 | [ActBERT.pdparams](https://videotag.bj.bcebos.com/PaddleVideo-release2.2/ActBERT.pdparams) |\n## Reference\n- [ActBERT: Learning Global-Local Video-Text Representations\n](https://arxiv.org/abs/2011.07231), Linchao Zhu, Yi Yang"
        }
    ]
}