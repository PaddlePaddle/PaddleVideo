{
    "summary": "The code trains and validates PoseC3D, a skeleton-based action recognition model, on the UCF101 dataset, using pre-trained weights. It details testing and inference processes without GPU acceleration or TensorRT.",
    "details": [
        {
            "comment": "PoseC3D is a skeleton-based action recognition approach that utilizes 3D head pose features and aims to overcome the limitations of GCN-based methods in terms of robustness, interoperability, and scalability. It involves training on UCF101, testing on UCF101, exporting an inference model, and inferring using the model.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/posec3d.md\":0-23",
            "content": "[\u7b80\u4f53\u4e2d\u6587](../../../zh-CN/model_zoo/recognition/posec3d.md) | English\n# PoseC3D\n---\n## Contents\n- [PoseC3D](#PoseC3D)\n  - [Contents](#contents)\n  - [Introduction](#introduction)\n  - [Data](#data)\n  - [Train](#train)\n    - [Train on UCF101.](#train-on-ucf101)\n  - [Test](#test)\n    - [Test onf UCF101](#test-onf-ucf101)\n  - [Inference](#inference)\n    - [export inference model](#export-inference-model)\n    - [infer](#infer)\n  - [Reference](#reference)\n## Introduction\nHuman  skeleton,  as  a  compact  representation  of  hu-man  action,  has  received  increasing  attention  in  recentyears.    Many  skeleton-based  action  recognition  methodsadopt graph convolutional networks (GCN) to extract fea-tures on top of human skeletons.   Despite the positive re-sults  shown  in  previous  works,  GCN-based  methods  aresubject  to  limitations  in  robustness,  interoperability,  andscalability.  In this work, we propose PoseC3D, a new ap-proach  to  skeleton-based  action  recognition,  which  relieson  a  3D  hea"
        },
        {
            "comment": "This code is for training the PoseC3D model on UCF101 dataset. It requires downloading pre-trained model weights from a specific URL. The command \"python3.7 main.py --validate -c configs/recognition/posec3d/posec3d.yaml --weights res3d_k400.pdparams\" is used to train the PoseC3D model using a provided configuration file and pre-trained weights. The trained model will be validated, likely to assess its performance.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/posec3d.md\":23-38",
            "content": "tmap  stack  instead  of  a  graph  sequence  asthe base representation of human skeletons.  Compared toGCN-based methods, PoseC3D is more effective in learningspatiotemporal features, more robust against pose estima-tion noises, and generalizes better in cross-dataset settings.Also, PoseC3D can handle multiple-person scenarios with-out additional computation cost, and its features can be eas-ily integrated with other modalities at early fusion stages,which  provides  a  great  design  space  to  further  boost  theperformance. On four challenging datasets, PoseC3D con-sistently obtains superior performance, when used alone onskeletons and in combination with the RGB modality.\n## Data\nPlease download UCF101 skeletons datasets and pretraind model weights.\n[https://aistudio.baidu.com/aistudio/datasetdetail/140593](https://aistudio.baidu.com/aistudio/datasetdetail/140593)\n## Train\n### Train on UCF101.\n- Train PoseC3D model:\n```bash\npython3.7 main.py --validate -c configs/recognition/posec3d/posec3d.yaml --weights res3d_k400.pdparams"
        },
        {
            "comment": "This code provides instructions for testing and inference of the PoseC3D model on UCF101 dataset. The test script specifies the config file and weight path, while the inference steps explain how to export the model architecture and parameters for further usage. The link leads to additional information on model inference.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/posec3d.md\":39-81",
            "content": "```\n## Test\n### Test onf UCF101\n- Test scripts\uff1a\n```bash\npython3.7 main.py --test -c configs/recognition/posec3d/posec3d.yaml  -w output/PoseC3D/PoseC3D_epoch_0012.pdparams\n```\n- Specify the config file with `-c`, specify the weight path with `-w`.\nAccuracy on UCF101 dataset:\n| Test_Data | Top-1 | checkpoints |\n| :----: | :----: | :---- |\n| UCF101 test1 | 87.05 | [PoseC3D_ucf101.pdparams]() |\n## Inference\n### export inference model\n To get model architecture file `PoseC3D.pdmodel` and parameters file `PoseC3D.pdiparams`, use:\n```bash\npython3.7 tools/export_model.py -c configs/recognition/posec3d/posec3d.yaml \\\n                                -p data/PoseC3D_ucf101.pdparams \\\n                                -o inference/PoseC3D\n```\n- Args usage please refer to [Model Inference](https://github.com/PaddlePaddle/PaddleVideo/blob/release/2.0/docs/zh-CN/start.md#2-%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86).\n### infer\n```bash\npython3.7 tools/predict.py --input_file data/example_UCF101_skeleton.pkl\\\n                           --config configs/recognition/posec3d/posec3d.yaml \\"
        },
        {
            "comment": "Running PoseC3D model for inference with GPU acceleration and without TensorRT.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/posec3d.md\":82-99",
            "content": "                           --model_file inference/PoseC3D/PoseC3D.pdmodel \\\n                           --params_file inference/PoseC3D/PoseC3D.pdiparams \\\n                           --use_gpu=True \\\n                           --use_tensorrt=False\n```\nexample of logs:\n```\nCurrent video file: data/example_UCF101_skeleton.pkl\n\ttop-1 class: 0\n\ttop-1 score: 0.6731489896774292\n```\n## Reference\n- [Revisiting Skeleton-based Action Recognition](https://arxiv.org/pdf/2104.13586v1.pdf), Haodong Duan, Yue Zhao, Kai Chen, Dian Shao, Dahua Lin, Bo Dai"
        }
    ]
}