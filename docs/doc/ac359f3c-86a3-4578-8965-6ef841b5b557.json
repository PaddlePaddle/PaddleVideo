{
    "summary": "The code establishes paths, defines functions for AVA model in PaddleVideo with OpenCV, creates a video analysis model, extracts frames, predicts label scores, detects humans, performs inference, and identifies spatio-temporal actions.",
    "details": [
        {
            "comment": "This code is a Python script for the AVA (Action Unit Detection) model in PaddleVideo. It imports necessary libraries, checks for missing dependencies, and sets up paths for model building.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/ava_predict.py\":0-31",
            "content": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport argparse\nimport paddle\nimport os, sys\nimport copy as cp\nimport cv2\nimport math\ntry:\n    import ppdet\nexcept ImportError as e:\n    print(\n        f\"Warning! {e}, [paddledet] package and it's dependencies is required for AVA.\"\n    )\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.abspath(os.path.join(__dir__, '../')))\nfrom paddlevideo.modeling.builder import build_model\nfrom paddlevideo.utils import get_config"
        },
        {
            "comment": "This code snippet is a part of the PaddleVideo library. It defines several color schemes and abbreviation functions related to video analysis tasks. The color schemes are used for annotations, while the abbreviation function is for simplifying label names in the AVA dataset.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/ava_predict.py\":32-67",
            "content": "from paddlevideo.loader.builder import build_dataloader, build_dataset, build_pipeline\nfrom paddlevideo.metrics.ava_utils import read_labelmap\nimport time\nfrom os import path as osp\nimport numpy as np\nfrom paddlevideo.utils import get_config\nimport pickle\nfrom paddlevideo.utils import (get_logger, load, mkdir, save)\nimport shutil\nFONTFACE = cv2.FONT_HERSHEY_DUPLEX\nFONTSCALE = 0.5\nFONTCOLOR = (255, 255, 255)  # BGR, white\nMSGCOLOR = (128, 128, 128)  # BGR, gray\nTHICKNESS = 1\nLINETYPE = 1\ndef hex2color(h):\n    \"\"\"Convert the 6-digit hex string to tuple of 3 int value (RGB)\"\"\"\n    return (int(h[:2], 16), int(h[2:4], 16), int(h[4:], 16))\nplate_blue = '03045e-023e8a-0077b6-0096c7-00b4d8-48cae4'\nplate_blue = plate_blue.split('-')\nplate_blue = [hex2color(h) for h in plate_blue]\nplate_green = '004b23-006400-007200-008000-38b000-70e000'\nplate_green = plate_green.split('-')\nplate_green = [hex2color(h) for h in plate_green]\ndef abbrev(name):\n    \"\"\"Get the abbreviation of label name:\n    'take (an object) from (a person)' -> 'take ... from ...'"
        },
        {
            "comment": "This function visualizes frames with predicted annotations, requiring the number of frames and annotations to be multiples. It asserts that the max_num is less than or equal to the length of the plate used for visualization and ensures that frames are a deep copy before processing. The assertions check if the number of frames is divisible by the number of annotations, and calculates the number of frames per annotation. The function also initializes the annotation variable and stores the image height and width for later use.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/ava_predict.py\":68-97",
            "content": "    \"\"\"\n    while name.find('(') != -1:\n        st, ed = name.find('('), name.find(')')\n        name = name[:st] + '...' + name[ed + 1:]\n    return name\n# annotations is pred results\ndef visualize(frames, annotations, plate=plate_blue, max_num=5):\n    \"\"\"Visualize frames with predicted annotations.\n    Args:\n        frames (list[np.ndarray]): Frames for visualization, note that\n            len(frames) % len(annotations) should be 0.\n        annotations (list[list[tuple]]): The predicted results.\n        plate (str): The plate used for visualization. Default: plate_blue.\n        max_num (int): Max number of labels to visualize for a person box.\n            Default: 5\uff0c\u76ee\u524d\u4e0d\u80fd\u5927\u4e8e5.\n    Returns:\n        list[np.ndarray]: Visualized frames.\n    \"\"\"\n    assert max_num + 1 <= len(plate)\n    plate = [x[::-1] for x in plate]\n    frames_ = cp.deepcopy(frames)\n    nf, na = len(frames), len(annotations)\n    assert nf % na == 0\n    nfpa = len(frames) // len(annotations)\n    anno = None\n    h, w, _ = frames[0].shape\n    # proposals\u88ab\u5f52\u4e00\u5316\u9700\u8981\u8fd8\u539f\u771f\u5b9e\u5750\u6807\u503c"
        },
        {
            "comment": "This code is iterating through annotations and frames, scaling box coordinates based on image size, drawing rectangles around objects in frames using OpenCV, and displaying labels above the rectangles with their corresponding scores.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/ava_predict.py\":98-124",
            "content": "    scale_ratio = np.array([w, h, w, h])\n    for i in range(na):\n        anno = annotations[i]\n        if anno is None:\n            continue\n        for j in range(nfpa):\n            ind = i * nfpa + j\n            frame = frames_[ind]\n            for ann in anno:\n                box = ann[0]\n                label = ann[1]\n                if not len(label):\n                    continue\n                score = ann[2]\n                box = (box * scale_ratio).astype(np.int64)\n                st, ed = tuple(box[:2]), tuple(box[2:])\n                cv2.rectangle(frame, st, ed, plate[0], 2)\n                for k, lb in enumerate(label):\n                    if k >= max_num:\n                        break\n                    text = abbrev(lb)\n                    text = ': '.join([text, str(score[k])])\n                    location = (0 + st[0], 18 + k * 18 + st[1])\n                    textsize = cv2.getTextSize(text, FONTFACE, FONTSCALE,\n                                               THICKNESS)[0]\n                    textwidth = textsize[0]"
        },
        {
            "comment": "This code is part of the \"ava_predict.py\" file in the PaddleVideo library. It defines a function called \"frame_extraction\" that takes a video path and target directory as arguments. The function extracts frames from the given video_path and saves them to the specified target directory. It reads each frame of the video, appends it to the \"frames\" list, writes it to disk using cv2.imwrite, and increments the index for frame naming. The target directory is created if it doesn't exist already. This function handles videos with a maximum length of several hours, as indicated by the FPS (Frames Per Second) value obtained from the video.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/ava_predict.py\":125-159",
            "content": "                    diag0 = (location[0] + textwidth, location[1] - 14)\n                    diag1 = (location[0], location[1] + 2)\n                    cv2.rectangle(frame, diag0, diag1, plate[k + 1], -1)\n                    cv2.putText(frame, text, location, FONTFACE, FONTSCALE,\n                                FONTCOLOR, THICKNESS, LINETYPE)\n    return frames_\ndef frame_extraction(video_path, target_dir):\n    \"\"\"Extract frames given video_path.\n    Args:\n        video_path (str): The video_path.\n    \"\"\"\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir, exist_ok=True)\n    # Should be able to handle videos up to several hours\n    frame_tmpl = osp.join(target_dir, '{:05d}.jpg')\n    vid = cv2.VideoCapture(video_path)\n    FPS = int(vid.get(5))\n    frames = []\n    frame_paths = []\n    flag, frame = vid.read()\n    index = 1\n    while flag:\n        frames.append(frame)\n        frame_path = frame_tmpl.format(index)\n        frame_paths.append(frame_path)\n        cv2.imwrite(frame_path, frame)\n        index += 1"
        },
        {
            "comment": "This code is for running PaddleVideo inference model. It takes a video file or URL, config file path, and overrides options as input parameters. The model can be finetuned or tested using specified weights. The detection model name is also an optional parameter.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/ava_predict.py\":160-190",
            "content": "        flag, frame = vid.read()\n    return frame_paths, frames, FPS\ndef parse_args():\n    def str2bool(v):\n        return v.lower() in (\"true\", \"t\", \"1\")\n    # general params\n    parser = argparse.ArgumentParser(\"PaddleVideo Inference model script\")\n    parser.add_argument('-c',\n                        '--config',\n                        type=str,\n                        default='configs/example.yaml',\n                        help='config file path')\n    parser.add_argument('--video_path', help='video file/url')\n    parser.add_argument('-o',\n                        '--override',\n                        action='append',\n                        default=[],\n                        help='config options to be overridden')\n    parser.add_argument('-w',\n                        '--weights',\n                        type=str,\n                        help='weights for finetuning or testing')\n    #detection_model_name\n    parser.add_argument('--detection_model_name',\n                        help='the name of detection model ')"
        },
        {
            "comment": "This code is parsing arguments for the ava_predict function, including detection model weights path, output filename, predict step size, output step size, and output FPS. The pack_result function combines human detection results with a given result, sorting them by probability size.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/ava_predict.py\":191-221",
            "content": "    # detection_model_weights\n    parser.add_argument('--detection_model_weights',\n                        help='the weights path of detection model ')\n    # params for predict\n    parser.add_argument('--out-filename',\n                        default='ava_det_demo.mp4',\n                        help='output filename')\n    parser.add_argument('--predict-stepsize',\n                        default=8,\n                        type=int,\n                        help='give out a prediction per n frames')\n    parser.add_argument(\n        '--output-stepsize',\n        default=4,\n        type=int,\n        help=('show one frame per n frames in the demo, we should have: '\n              'predict_stepsize % output_stepsize == 0'))\n    parser.add_argument('--output-fps',\n                        default=6,\n                        type=int,\n                        help='the fps of demo video output')\n    return parser.parse_args()\n# \u4e00\u5e27\u7684\u7ed3\u679c\u3002\u6839\u636e\u6982\u7387\u5927\u5c0f\u8fdb\u884c\u6392\u5e8f\ndef pack_result(human_detection, result):\n    \"\"\"Short summary.\n    Args:\n        human_detection (np.ndarray): Human detection result."
        },
        {
            "comment": "This function takes the predicted label of each human proposal and returns a tuple containing the human proposal, label name, and label score. It also constructs data processing results for frame directory, timestamp, clip length, frame interval, and frames per second.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/ava_predict.py\":222-263",
            "content": "        result (type): The predicted label of each human proposal.\n    Returns:\n        tuple: Tuple of human proposal, label name and label score.\n    \"\"\"\n    results = []\n    if result is None:\n        return None\n    for prop, res in zip(human_detection, result):\n        res.sort(key=lambda x: -x[1])\n        results.append((prop, [x[0] for x in res], [x[1] for x in res]))\n    return results\n# \u6784\u9020\u6570\u636e\u5904\u7406\u9700\u8981\u7684results\ndef get_timestep_result(frame_dir, timestamp, clip_len, frame_interval, FPS):\n    result = {}\n    result[\"frame_dir\"] = frame_dir\n    frame_num = len(os.listdir(frame_dir))\n    dir_name = frame_dir.split(\"/\")[-1]\n    result[\"video_id\"] = dir_name\n    result['timestamp'] = timestamp\n    timestamp_str = '{:04d}'.format(timestamp)\n    img_key = dir_name + \",\" + timestamp_str\n    result['img_key'] = img_key\n    result['shot_info'] = (1, frame_num)\n    result['fps'] = FPS\n    result['suffix'] = '{:05}.jpg'\n    result['timestamp_start'] = 1\n    result['timestamp_end'] = int(frame_num / result['fps'])\n    return result"
        },
        {
            "comment": "This function performs human detection on a list of frame paths using a specified model and weight file. It uses the trainer object to predict human boxes in each frame, saving the results as text files in the specified output directory. The function then returns a list of paths for these detection results.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/ava_predict.py\":266-293",
            "content": "def detection_inference(frame_paths, output_dir, model_name, weights_path):\n    \"\"\"Detect human boxes given frame paths.\n    Args:\n        frame_paths (list[str]): The paths of frames to do detection inference.\n    Returns:\n        list[np.ndarray]: The human detection results.\n    \"\"\"\n    detection_cfg = ppdet.model_zoo.get_config_file(model_name)\n    detection_cfg = ppdet.core.workspace.load_config(detection_cfg)\n    detection_trainer = ppdet.engine.Trainer(detection_cfg, mode='test')\n    detection_trainer.load_weights(weights_path)\n    print('Performing Human Detection for each frame')\n    detection_trainer.predict(frame_paths, output_dir=output_dir, save_txt=True)\n    print(\"finish object detection\")\n    results = []\n    for frame_path in frame_paths:\n        (file_dir, file_name) = os.path.split(frame_path)\n        (file_path, ext) = os.path.splitext(frame_path)\n        txt_file_name = file_name.replace(ext, \".txt\")\n        txt_path = os.path.join(output_dir, txt_file_name)\n        results.append(txt_path)"
        },
        {
            "comment": "This function reads a detection result file and returns the bounding box proposals (proposals) and corresponding scores for people in the image. It takes the path to the txt file, image height, and image width as input parameters. The function first splits the lines of the file and then checks each line to see if it corresponds to a person detection result. If so, it extracts the score and bounding box coordinates (x1, y1, x2, y2) for that object and adds them to separate lists, scores and proposals. Finally, it returns numpy arrays of the extracted proposals and scores.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/ava_predict.py\":295-333",
            "content": "    return results\ndef get_detection_result(txt_file_path, img_h, img_w, person_det_score_thr):\n    \"\"\"\n    \u6839\u636e\u68c0\u6d4b\u7ed3\u679c\u6587\u4ef6\u5f97\u5230\u56fe\u50cf\u4e2d\u4eba\u7684\u68c0\u6d4b\u6846(proposals)\u548c\u7f6e\u4fe1\u5ea6\uff08scores\uff09\n    txt_file_path:\u68c0\u6d4b\u7ed3\u679c\u5b58\u653e\u8def\u5f84\n    img_h:\u56fe\u50cf\u9ad8\u5ea6\n    img_w:\u56fe\u50cf\u5bbd\u5ea6\n    \"\"\"\n    proposals = []\n    scores = []\n    with open(txt_file_path, 'r') as detection_file:\n        lines = detection_file.readlines()\n        for line in lines:  # person 0.9842637181282043 0.0 469.1407470703125 944.7770385742188 831.806396484375\n            items = line.split(\" \")\n            if items[0] != 'person':  #\u53ea\u8981\u4eba\n                continue\n            score = items[1]\n            if (float)(score) < person_det_score_thr:\n                continue\n            x1 = (float(items[2])) / img_w\n            y1 = ((float)(items[3])) / img_h\n            box_w = ((float)(items[4]))\n            box_h = ((float)(items[5]))\n            x2 = (float(items[2]) + box_w) / img_w\n            y2 = (float(items[3]) + box_h) / img_h\n            scores.append(score)\n            proposals.append([x1, y1, x2, y2])\n    return np.array(proposals), np.array(scores)"
        },
        {
            "comment": "This code function is extracting frames from a video, parsing config files, and setting up processing pipelines for testing. The frame extraction process involves specifying the input video path and output directory for storing frames. It calculates the number of frames in the video and ensures it's not zero. It asserts that clip_len and frame_interval are even numbers to create equal-sized clips. Finally, it calculates the window size based on these parameters.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/ava_predict.py\":336-364",
            "content": "@paddle.no_grad()\ndef main(args):\n    config = get_config(args.config, show=False)  #parse config file\n    # extract frames from video\n    video_path = args.video_path\n    frame_dir = 'tmp_frames'\n    frame_paths, frames, FPS = frame_extraction(video_path, frame_dir)\n    num_frame = len(frame_paths)  #\u89c6\u9891\u79d2\u6570*FPS\n    assert num_frame != 0\n    print(\"Frame Number\uff1a\", num_frame)\n    # \u5e27\u56fe\u50cf\u9ad8\u5ea6\u548c\u5bbd\u5ea6\n    h, w, _ = frames[0].shape\n    # Get clip_len, frame_interval and calculate center index of each clip\n    data_process_pipeline = build_pipeline(config.PIPELINE.test)  #\u6d4b\u8bd5\u65f6\u8f93\u51fa\u5904\u7406\u6d41\u6c34\u914d\u7f6e\n    clip_len = config.PIPELINE.test.sample['clip_len']\n    assert clip_len % 2 == 0, 'We would like to have an even clip_len'\n    frame_interval = config.PIPELINE.test.sample['frame_interval']\n    # \u6b64\u5904\u5173\u952e\u5e27\u6bcf\u79d2\u53d6\u4e00\u4e2a\n    clip_len = config.PIPELINE.test.sample['clip_len']\n    assert clip_len % 2 == 0, 'We would like to have an even clip_len'\n    frame_interval = config.PIPELINE.test.sample['frame_interval']\n    window_size = clip_len * frame_interval\n    timestamps = np.arange(window_size // 2, (num_frame + 1 - window_size // 2),"
        },
        {
            "comment": "This code snippet is parsing timestamps from a file, selecting frames based on those timestamps, loading a label map, constructing a model, and setting its state dictionary. The selected frames are passed to the `detection_inference` function which performs inference using the specified detection model with given weights.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/ava_predict.py\":365-394",
            "content": "                           args.predict_stepsize)\n    print(\"timetamps number:\", len(timestamps))\n    # get selected frame list according to timestamps\n    selected_frame_list = []\n    for timestamp in timestamps:\n        selected_frame_list.append(frame_paths[timestamp - 1])\n    # Load label_map\n    label_map_path = config.DATASET.test['label_file']\n    categories, class_whitelist = read_labelmap(open(label_map_path))\n    label_map = {}\n    for item in categories:\n        id = item['id']\n        name = item['name']\n        label_map[id] = name\n    # Construct model.\n    if config.MODEL.backbone.get('pretrained'):\n        config.MODEL.backbone.pretrained = ''  # disable pretrain model init\n    model = build_model(config.MODEL)\n    model.eval()\n    state_dicts = load(args.weights)\n    model.set_state_dict(state_dicts)\n    detection_result_dir = 'tmp_detection'\n    detection_model_name = args.detection_model_name\n    detection_model_weights = args.detection_model_weights\n    detection_txt_list = detection_inference(selected_frame_list,"
        },
        {
            "comment": "This code performs SpatioTemporal Action Detection for each clip. It first retrieves detection results from various txt files, ensuring their lengths match the timestamps. Then, it extracts human detections and predictions for each timestamp using get_detection_result() and get_timestep_result(). If there are no detections in a frame, None values are appended to the lists.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/ava_predict.py\":395-420",
            "content": "                                             detection_result_dir,\n                                             detection_model_name,\n                                             detection_model_weights)\n    assert len(detection_txt_list) == len(timestamps)\n    print('Performing SpatioTemporal Action Detection for each clip')\n    human_detections = []\n    predictions = []\n    index = 0\n    for timestamp, detection_txt_path in zip(timestamps, detection_txt_list):\n        proposals, scores = get_detection_result(\n            detection_txt_path, h, w,\n            (float)(config.DATASET.test['person_det_score_thr']))\n        if proposals.shape[0] == 0:\n            predictions.append(None)\n            human_detections.append(None)\n            continue\n        human_detections.append(proposals)\n        result = get_timestep_result(frame_dir,\n                                     timestamp,\n                                     clip_len,\n                                     frame_interval,\n                                     FPS=FPS)"
        },
        {
            "comment": "This code prepares input data for a model by converting images, proposals, and shapes to tensors. It then feeds the prepared data into the model in order mode='infer'. The output is stored in 'result' and used to generate predictions based on number of proposals.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/ava_predict.py\":421-454",
            "content": "        result[\"proposals\"] = proposals\n        result[\"scores\"] = scores\n        new_result = data_process_pipeline(result)\n        proposals = new_result['proposals']\n        img_slow = new_result['imgs'][0]\n        img_slow = img_slow[np.newaxis, :]\n        img_fast = new_result['imgs'][1]\n        img_fast = img_fast[np.newaxis, :]\n        proposals = proposals[np.newaxis, :]\n        scores = scores[np.newaxis, :]\n        img_shape = np.asarray(new_result['img_shape'])\n        img_shape = img_shape[np.newaxis, :]\n        data = [\n            paddle.to_tensor(img_slow, dtype='float32'),\n            paddle.to_tensor(img_fast, dtype='float32'),\n            paddle.to_tensor(proposals, dtype='float32'), scores,\n            paddle.to_tensor(img_shape, dtype='int32')\n        ]\n        with paddle.no_grad():\n            result = model(data, mode='infer')\n            result = result[0]\n            prediction = []\n            person_num = proposals.shape[1]\n            # N proposals\n            for i in range(person_num):"
        },
        {
            "comment": "This code performs action score thresholding for each detected person in the video. It appends labels and corresponding scores to a prediction list, then appends the predictions to a list of lists for all detected humans. The code also prints progress updates every 10 iterations, and finally, it creates denser timestamps using an older frame interval.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/ava_predict.py\":455-480",
            "content": "                prediction.append([])\n            # Perform action score thr\n            for i in range(len(result)):\n                if i + 1 not in class_whitelist:\n                    continue\n                for j in range(person_num):\n                    if result[i][j, 4] > config.MODEL.head['action_thr']:\n                        prediction[j].append((label_map[i + 1], result[i][j,\n                                                                          4]))\n            predictions.append(prediction)\n        index = index + 1\n        if index % 10 == 0:\n            print(index, \"/\", len(timestamps))\n    results = []\n    for human_detection, prediction in zip(human_detections, predictions):\n        results.append(pack_result(human_detection, prediction))\n    def dense_timestamps(timestamps, n):\n        \"\"\"Make it nx frames.\"\"\"\n        old_frame_interval = (timestamps[1] - timestamps[0])\n        start = timestamps[0] - old_frame_interval / n * (n - 1) / 2\n        new_frame_inds = np.arange(\n            len(timestamps) * n) * old_frame_interval / n + start"
        },
        {
            "comment": "The code reads video frames, performs visualization, and writes the processed frames into a new video file. It requires moviepy to be installed for output functionality and deletes temporary files after use.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/tools/ava_predict.py\":481-508",
            "content": "        return new_frame_inds.astype(np.int)\n    dense_n = int(args.predict_stepsize / args.output_stepsize)  #30\n    frames = [\n        cv2.imread(frame_paths[i - 1])\n        for i in dense_timestamps(timestamps, dense_n)\n    ]\n    vis_frames = visualize(frames, results)\n    try:\n        import moviepy.editor as mpy\n    except ImportError:\n        raise ImportError('Please install moviepy to enable output file')\n    vid = mpy.ImageSequenceClip([x[:, :, ::-1] for x in vis_frames],\n                                fps=args.output_fps)\n    vid.write_videofile(args.out_filename)\n    print(\"finish write !\")\n    # delete tmp files and dirs\n    shutil.rmtree(frame_dir)\n    shutil.rmtree(detection_result_dir)\nif __name__ == '__main__':\n    args = parse_args()  #\u89e3\u6790\u53c2\u6570\n    main(args)"
        }
    ]
}