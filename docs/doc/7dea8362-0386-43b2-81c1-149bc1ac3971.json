{
    "summary": "The TextEmbedding interface utilizes Word2Vec for embedding video descriptions and queries. It initializes with model, dimensionality, and optional parameters, providing methods for GPT or Word2Vec extraction while ensuring CPU-only execution. The code initializes an OpenAI GPT model, tokenizes input, converts to vocabulary indices, obtains embeddings from hidden states, and returns squeezed dimensions.",
    "details": [
        {
            "comment": "This module defines the TextEmbedding interface for converting video descriptions and queries into embeddings. The class, TextEmbedding, initializes with a model and dimensionality of embedding. It has an abstract method, text2vec, that converts a string of text into an embedding, returning a (d x n) array, where d is the dimensionality of the embedding and `n` is the number of words successfully parsed from the text string. Some text embedding models may drop certain kinds of stop words.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/model/text.py\":0-36",
            "content": "\"\"\"This module defines the TextEmbedding interface for converting video descriptions and\nqueries into embeddings.\n\"\"\"\nimport zipfile\nimport functools\nfrom abc import abstractmethod\nfrom pathlib import Path\nimport numpy as np\nimport paddle\nimport gensim\nimport requests\nimport transformers\nfrom typeguard import typechecked\nfrom zsvision.zs_utils import BlockTimer\nfrom model.s3dg import S3D\nclass TextEmbedding:\n    def __init__(self, model, dim: int):\n        self.model = model\n        self.dim = dim\n        #self.device = None\n    @abstractmethod\n    def text2vec(self, text: str) -> np.ndarray:\n        \"\"\"Convert a string of text into an embedding.\n        Args:\n            text: the content to be embedded\n        Returns:\n            (d x n) array, where d is the dimensionality of the embedding and `n` is the\n                number of words that were successfully parsed from the text string.\n        NOTE: For some text embedding models (such as word2vec), not all words are\n        converted to vectors (e.g. certain kinds of stop words) - these are dropped from"
        },
        {
            "comment": "This code defines a class for the W2VEmbedding model, which embeds text using the Word2Vec algorithm. It has methods for loading pre-trained Word2Vec models from disk or fetching them online. The set_device method allows specifying the device to use (CPU or GPU). The load_w2v_model_from_cache function loads a Word2Vec model from disk, and the fetch_model function downloads it from a given URL.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/model/text.py\":37-72",
            "content": "        the output.\n        \"\"\"\n        raise NotImplementedError\n    #@typechecked\n    #def set_device(self, device: torch.device):\n    #    self.model = self.model.to(device)\n    #    self.device = device\n@functools.lru_cache(maxsize=64, typed=False)\ndef load_w2v_model_from_cache(\n        w2v_weights: Path,\n) -> gensim.models.keyedvectors.Word2VecKeyedVectors:\n    with BlockTimer(\"Loading w2v from disk\"):\n        model = gensim.models.KeyedVectors.load_word2vec_format(\n            fname=w2v_weights,\n            binary=True,\n        )\n    return model\n@typechecked\ndef fetch_model(url: str, weights_path: Path):\n    weights_path.parent.mkdir(exist_ok=True, parents=True)\n    with BlockTimer(f\"Fetching weights {url} -> {weights_path}\"):\n        resp = requests.get(url, verify=False)\n        with open(weights_path, \"wb\") as f:\n            f.write(resp.content)\nclass W2VEmbedding(TextEmbedding):\n    \"\"\"This model embeds text using the google-released implementation of the word2vec\n    model introduced in:\n        Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013)."
        },
        {
            "comment": "The code initializes a class with dimensions, mirror, and weights_path parameters. If the weights path doesn't exist, it fetches them or raises an error. It then loads the word2vec model from the cache and initializes the superclass. The text2vec method converts input text to tokens processed by w2v, excluding 'a' and tokens not in vocab.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/model/text.py\":73-100",
            "content": "        Distributed representations of words and phrases and their compositionality.\n        In Advances in neural information processing systems (pp. 3111-3119).\n    For words that are present in the w2v vocabulary, a 300-dimensional embedding is\n    produced via a lookup table.\n    \"\"\"\n    @typechecked\n    def __init__(\n            self,\n            dim: int,\n            mirror: str,\n            weights_path: Path,\n            fetch_weights: bool = True,\n    ):\n        if not weights_path.exists():\n            if fetch_weights:\n                fetch_model(url=mirror, weights_path=weights_path)\n            else:\n                raise ValueError(f\"w2v weights missing at {weights_path}\")\n        model = load_w2v_model_from_cache(weights_path)\n        super().__init__(model=model, dim=dim)\n    @typechecked\n    def text2vec(self, text: str) -> np.ndarray:\n        # convert the text string to tokens that can be processed by w2v.  We handle\n        # 'a' as a special case.\n        tokens = [x for x in text.split(\" \") if x != \"a\" and x in self.model.vocab]"
        },
        {
            "comment": "The code defines a class \"TextEmbedding\" that provides methods to extract embeddings from text tokens using either the GPT model or Word2Vec. The \"get_vector\" method returns embeddings in the expected format for the CE codebase, and it handles empty sequences by returning zeros with the correct dimensionality. The class also includes a \"set_device\" method that asserts the device type is CPU-only, as GPT model only supports CPU execution.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/model/text.py\":102-129",
            "content": "        embeddings = []\n        for token in tokens:\n            embeddings.append(self.model.get_vector(token))\n        embeddings = np.array(embeddings)\n        # For empty sequences, we use zeros with the dimensionality of the features on\n        # the second dimension (this is the format expected by the CE codebase)\n        if embeddings.size == 0:\n            embeddings = np.zeros((0, self.dim))\n        return embeddings\n    #@typechecked\n    #def set_device(self, device: torch.device):\n    #    msg = f\"w2v only supports CPU-based execution found {device.type}\"\n    #    assert device.type == \"cpu\", msg\nclass OpenAI_GPT(TextEmbedding):\n    \"\"\"This model produces 768-embeddings using a pretrained GPT model, introduced\n    in the paper:\n    Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018).\n    Improving language understanding by generative pre-training,\n    https://cdn.openai.com/research-covers/language-unsupervised/language_understanding\n    _paper.pdf\n    \"\"\"\n    def __init__(self):\n        self.tokenizer = transformers.OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")"
        },
        {
            "comment": "This code initializes an OpenAI GPT model, tokenizes text input, converts tokens to vocabulary indices, and obtains embeddings from the model's hidden states. The embeddings are then returned after squeezing dimensions.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/model/text.py\":130-145",
            "content": "        model = transformers.OpenAIGPTModel.from_pretrained(\"openai-gpt\")\n        model.eval()\n        super().__init__(model=model)\n    @typechecked\n    def text2vec(self, text: str) -> np.ndarray:\n        tokenized_text = self.tokenizer.tokenize(text)\n        # Convert token to vocabulary indices\n        indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)\n        tokens_tensor = paddle.to_tensor(indexed_tokens, dtype='int64') #tokens_tensor = torch.LongTensor([indexed_tokens]).to(self.model.device)\n        with paddle.no_grad():\n            hidden_states = self.model(tokens_tensor)\n            embeddings = hidden_states[0].numpy()\n        return embeddings.squeeze(0)"
        }
    ]
}