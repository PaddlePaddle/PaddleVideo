{
    "summary": "The code provides data conversion, tensor handling, input compatibility, gradient norm clipping, and gain calculations for PyTorch and PaddlePaddle. It also offers tensor initialization using various methods with backward compatibility.",
    "details": [
        {
            "comment": "This code file contains utility functions for converting data from PyTorch to Paddle, filling tensors with values, zeroing out tensor values, and changing the tensor's dtype. It also includes a function that attempts to import PyTorch and converts corresponding data types accordingly.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/utils/api.py\":0-48",
            "content": "import math\nimport warnings\nimport numpy\nimport numpy as np\nfrom numpy import inf\nfrom paddle import Tensor, concat, reshape, nn\nimport paddle\nfrom typing import Union, Iterable\n_tensor_or_tensors = Union[paddle.Tensor, Iterable[paddle.Tensor]]\nimport paddle\nimport PIL\nimport numbers\nimport numpy as np\nfrom PIL import Image\nfrom paddle.vision.transforms import BaseTransform\nfrom paddle.vision.transforms import functional as F\ndef torch2paddle(data):\n    try:\n        import torch\n        if isinstance(data, dict):\n            np_data = {}\n            for k, v in data.items():\n                np_data[k] = paddle.to_tensor(v.detach().numpy())\n            return np_data\n        else:\n            return paddle.to_tensor(data.detach().numpy())\n    except:\n        pass\ndef fill_(tensor: Tensor, value):\n    return tensor.set_value(paddle.full_like(tensor, value))\ndef zero_(tensor: Tensor):\n    return tensor.set_value(paddle.zeros_like(tensor))\ndef float_(tensor: Tensor):\n    return paddle.to_tensor(tensor, dtype='float32')\ndef long_(tensor: Tensor):"
        },
        {
            "comment": "The code provides three tensor conversion functions: `int64`, `int32`, and `byte` that convert tensors into specific data types. It also includes a class `ToPILImage` for converting images to PIL Image format with the option to specify mode and keys. The function `_apply_image` checks if the input is a tensor or numpy array, raises an error if not, and then proceeds to convert 2D or 3D images into PIL Image format by adding channel dimension if necessary.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/utils/api.py\":49-82",
            "content": "    return paddle.to_tensor(tensor, dtype='int64')\ndef int_(tensor: Tensor):\n    return paddle.to_tensor(tensor, dtype='int32')\ndef byte_(tensor: Tensor):\n    return paddle.to_tensor(tensor, dtype='bool')\nclass ToPILImage(BaseTransform):\n    def __init__(self, mode=None, keys=None):\n        super(ToPILImage, self).__init__(keys)\n    def _apply_image(self, pic):\n        \"\"\"\n        Args:\n            pic (Tensor|np.ndarray): Image to be converted to PIL Image.\n        Returns:\n            PIL: Converted image.\n        \"\"\"\n        if not (isinstance(pic, paddle.Tensor) or isinstance(pic, np.ndarray)):\n            raise TypeError('pic should be Tensor or ndarray. Got {}.'.format(\n                type(pic)))\n        elif isinstance(pic, paddle.Tensor):\n            if pic.ndimension() not in {2, 3}:\n                raise ValueError(\n                    'pic should be 2/3 dimensional. Got {} dimensions.'.format(\n                        pic.ndimension()))\n            elif pic.ndimension() == 2:\n                # if 2D image, add channel dimension (CHW)"
        },
        {
            "comment": "Code checks if the input 'pic' is a Paddle or numpy array, adjusts dimensions if necessary, and converts data types accordingly. If 'pic' is not compatible with the code, it raises an error. This code ensures that the input image is in the correct format for further processing.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/utils/api.py\":83-111",
            "content": "                pic = pic.unsqueeze(0)\n        elif isinstance(pic, np.ndarray):\n            if pic.ndim not in {2, 3}:\n                raise ValueError(\n                    'pic should be 2/3 dimensional. Got {} dimensions.'.format(\n                        pic.ndim))\n            elif pic.ndim == 2:\n                # if 2D image, add channel dimension (HWC)\n                pic = np.expand_dims(pic, 2)\n        npimg = pic\n        if isinstance(pic, paddle.Tensor) and \"float\" in str(\n                pic.numpy().dtype) and self.mode != 'F':\n            pic = pic.mul(255).byte()\n        if isinstance(pic, paddle.Tensor):\n            npimg = np.transpose(pic.numpy(), (1, 2, 0))\n        if not isinstance(npimg, np.ndarray):\n            raise TypeError(\n                'Input pic must be a paddle.Tensor or NumPy ndarray, ' +\n                'not {}'.format(type(npimg)))\n        if npimg.shape[2] == 1:\n            expected_mode = None\n            npimg = npimg[:, :, 0]\n            if npimg.dtype == np.uint8:\n                expected_mode = 'L'"
        },
        {
            "comment": "This code checks the data type of npimg and sets the expected mode accordingly. It then compares the input's mode to the expected mode, raising a ValueError if they don't match. For 2D inputs with 2 channels, only 'LA' mode is supported; it sets self.mode to 'LA' if necessary. For 4-channel inputs, the code supports modes like 'RGBA', 'CMYK', and 'RGBX'.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/utils/api.py\":112-135",
            "content": "            elif npimg.dtype == np.int16:\n                expected_mode = 'I;16'\n            elif npimg.dtype == np.int32:\n                expected_mode = 'I'\n            elif npimg.dtype == np.float32:\n                expected_mode = 'F'\n            if self.mode is not None and self.mode != expected_mode:\n                raise ValueError(\n                    \"Incorrect self.mode ({}) supplied for input type {}. Should be {}\"\n                    .format(self.mode, np.dtype, expected_mode))\n            self.mode = expected_mode\n        elif npimg.shape[2] == 2:\n            permitted_2_channel_modes = ['LA']\n            if self.mode is not None and self.mode not in permitted_2_channel_modes:\n                raise ValueError(\n                    \"Only self.modes {} are supported for 2D inputs\".format(\n                        permitted_2_channel_modes))\n            if self.mode is None and npimg.dtype == np.uint8:\n                self.mode = 'LA'\n        elif npimg.shape[2] == 4:\n            permitted_4_channel_modes = ['RGBA', 'CMYK', 'RGBX']"
        },
        {
            "comment": "This code checks the input image mode and data type, ensuring it matches with the supported modes for 3D or 4D inputs. If the mode is not recognized, a ValueError is raised. If no mode is provided and the data type is np.uint8, it assigns the appropriate default mode (RGB or RGBA). Finally, if there is no mode specified and the input data type is unsupported, a TypeError is raised. The code is part of a class called Identity which seems to be an identity operator for neural networks.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/utils/api.py\":136-160",
            "content": "            if self.mode is not None and self.mode not in permitted_4_channel_modes:\n                raise ValueError(\n                    \"Only self.modes {} are supported for 4D inputs\".format(\n                        permitted_4_channel_modes))\n            if self.mode is None and npimg.dtype == np.uint8:\n                self.mode = 'RGBA'\n        else:\n            permitted_3_channel_modes = ['RGB', 'YCbCr', 'HSV']\n            if self.mode is not None and self.mode not in permitted_3_channel_modes:\n                raise ValueError(\n                    \"Only self.modes {} are supported for 3D inputs\".format(\n                        permitted_3_channel_modes))\n            if self.mode is None and npimg.dtype == np.uint8:\n                self.mode = 'RGB'\n        if self.mode is None:\n            raise TypeError('Input type {} is not supported'.format(\n                npimg.dtype))\n        return Image.fromarray(npimg, mode=self.mode)\nclass Identity(nn.Layer):\n    r\"\"\"A placeholder identity operator that is argument-insensitive."
        },
        {
            "comment": "This code defines a class \"Identity\" that performs identity forwarding and a function \"convert\" to convert data between Paddle and Torch formats. It takes a dictionary of data, converts it into either Paddle or Torch format based on the specified type, and returns a new dictionary with the converted data. If the data is a numpy ndarray, it can also be casted to a specific dtype.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/utils/api.py\":162-197",
            "content": "    Args:\n        args: any argument (unused)\n        kwargs: any keyword argument (unused)\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super(Identity, self).__init__()\n    def forward(self, input):\n        return input\ndef convert(data: dict, to, dtype=None):\n    assert isinstance(data, dict)\n    input = {}\n    for k, v in data.items():\n        if 'paddle' == to:\n            if isinstance(v, np.ndarray):\n                if dtype is not None:\n                    input[k] = paddle.to_tensor(v.astype(dtype))\n                else:\n                    input[k] = paddle.to_tensor(v)\n            else:\n                input[k] = v\n        elif 'torch' == to:\n            try:\n                import torch\n                if isinstance(v, np.ndarray):\n                    if dtype is not None:\n                        input[k] = torch.tensor(v.astype(dtype))\n                    else:\n                        input[k] = torch.tensor(v)\n                else:\n                    input[k] = v\n            except:\n                pass"
        },
        {
            "comment": "This code defines a function that clips the gradient norm of an iterable of parameters. It takes in an iterable of Tensors (parameters) and a maximum norm value, computes the norm over all gradients, and modifies them in-place if necessary. The norm type can be specified as well, with options including 'inf' for infinity norm. If nonfinite norms are present, an error will occur unless error_if_nonfinite is set to False.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/utils/api.py\":198-222",
            "content": "        else:\n            if isinstance(v, np.ndarray):\n                input[k] = v.astype(to)\n            else:\n                input[k] = v\n    return input\ndef clip_grad_norm_(parameters: _tensor_or_tensors,\n                    max_norm: float,\n                    norm_type: float = 2.0,\n                    error_if_nonfinite: bool = False) -> paddle.Tensor:\n    r\"\"\"Clips gradient norm of an iterable of parameters.\n    The norm is computed over all gradients together, as if they were\n    concatenated into a single vector. Gradients are modified in-place.\n    Args:\n        parameters (Iterable[Tensor] or Tensor): an iterable of Tensors or a\n            single Tensor that will have gradients normalized\n        max_norm (float or int): max norm of the gradients\n        norm_type (float or int): type of the used p-norm. Can be ``'inf'`` for\n            infinity norm.\n        error_if_nonfinite (bool): if True, an error is thrown if the total\n            norm of the gradients from :attr:``parameters`` is ``nan``,"
        },
        {
            "comment": "This function calculates the total norm of parameters viewed as a single vector. It takes parameters and optional arguments max_norm and norm_type for the maximum allowed norm value and type of norm (inf or other), respectively. If no parameters are provided, it returns 0. If max_norm is infinity, it finds the maximum absolute value among parameters. Otherwise, it calculates the norm of gradients using the provided norm_type. If error_if_nonfinite is True and the total norm is NaN or Inf, a RuntimeError is raised.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/utils/api.py\":223-249",
            "content": "            ``inf``, or ``-inf``. Default: False (will switch to True in the future)\n    Returns:\n        Total norm of the parameters (viewed as a single vector).\n    \"\"\"\n    import time\n    if isinstance(parameters, paddle.Tensor):\n        parameters = [parameters]\n    parameters = [p for p in parameters if p.grad is not None]\n    detached_grads = [p.grad.detach() for p in parameters]\n    max_norm = float(max_norm)\n    norm_type = float(norm_type)\n    if len(parameters) == 0:\n        return paddle.to_tensor(0.)\n    if norm_type == inf:\n        norms = [p.abs().max() for p in parameters]\n        total_norm = norms[0] if len(norms) == 1 else paddle.max(\n            paddle.stack(norms))\n    else:\n        total_norm = paddle.norm(\n            paddle.stack([paddle.norm(g, norm_type) for g in detached_grads]),\n            norm_type)\n    if error_if_nonfinite and paddle.logical_or(total_norm.isnan(),\n                                                total_norm.isinf()):\n        raise RuntimeError(\n            f'The total norm of order {norm_type} for gradients from '"
        },
        {
            "comment": "This code snippet is a part of the PaddleVideo framework's Ma-Net application. It checks if 'parameters' are non-finite and clips them if not. It also defines a function to find the maximum value in a tensor, similar to numpy's max() function.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/utils/api.py\":250-273",
            "content": "            '`parameters` is non-finite, so it cannot be clipped. To disable '\n            'this error and scale the gradients by the non-finite norm anyway, '\n            'set `error_if_nonfinite=False`')\n    clip_coef = max_norm / (total_norm + 1e-6)\n    # Note: multiplying by the clamped coef is redundant when the coef is clamped to 1, but doing so\n    # avoids a `if clip_coef < 1:` conditional which can require a CPU <=> device synchronization\n    # when the gradients do not reside in CPU memory.\n    clip_coef_clamped = paddle.clip(clip_coef, max=1.0)\n    for i, p in enumerate(parameters):\n        p.grad.set_value(detached_grads[i] * clip_coef_clamped)  # fixed\n    return total_norm\ndef max(a: paddle.Tensor, axis=0, keepdim=True):\n    \"\"\"ndarray=numpy.array([[1, 2, 3, 4],\n           [4, 3, 2, 1],\n           [5, 6, 7, 8],\n           [8, 7, 6, 5]])\n    np.where(ndarray == np.max(ndarray))\n    (array([2, 3]), array([3, 0]))\n    ndarray[np.where(ndarray == np.max(ndarray))]\n    array([8, 8])\n    \"\"\"\n    max_ = a.max(axis).unsqueeze(-1)"
        },
        {
            "comment": "This code calculates the maximum value in a tensor and returns the corresponding index for each dimension. It also provides functions to gather data along different dimensions using gather or index_sample operations, depending on the shape of the input tensor.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/utils/api.py\":274-306",
            "content": "    index = paddle.argmax(a, axis=axis, keepdim=keepdim)\n    max_ = max_.numpy()\n    index = index.numpy()\n    # index = paddle.argmax(a, axis=axis, keepdim=keepdim)[-1].flatten()\n    return max_, index\ndef gather(tmp: paddle.Tensor, ind: paddle.Tensor):\n    shape = tmp.shape\n    tmp = paddle.to_tensor(tmp)\n    ind = paddle.to_tensor(ind)\n    if len(shape) == 2:\n        b = shape[0]\n        return concat([\n            reshape(paddle.gather(tmp[i, :], ind[i, :]), [1, -1])\n            for i in range(b)\n        ],\n                      axis=0)\n    elif len(shape) == 3:\n        out = []\n        for i in range(tmp.shape[0]):\n            _ = paddle.index_sample(tmp[i], ind[i])\n            out.append(_)\n        return paddle.to_tensor(out)\n    elif len(shape) == 4:\n        b, c, d = shape[:3]\n        return concat([\n            reshape(\n                concat([\n                    reshape(\n                        concat([\n                            reshape(\n                                paddle.gather(tmp[i, j, k, :], ind[i, j, k, :]),"
        },
        {
            "comment": "This code defines three functions (_no_grad_uniform_, _no_grad_normal_, and _no_grad_trunc_normal_) to initialize the weights of a tensor using different distributions while ensuring the computations are performed without gradient calculation. The main purpose is to prevent unnecessary memory usage and computation time for backpropagation in cases where gradients are not required.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/utils/api.py\":307-337",
            "content": "                                [1, -1]) for k in range(d)\n                        ],\n                               axis=0), [1, d, -1]) for j in range(c)\n                ],\n                       axis=0), [1, c, d, -1]) for i in range(b)\n        ],\n                      axis=0)\n    else:\n        pass\n# These no_grad_* functions are necessary as wrappers around the parts of these\n# functions that use `with torch.no_grad()`. The JIT doesn't support context\n# managers, so these need to be implemented as builtins. Using these wrappers\n# lets us keep those builtins small and re-usable.\ndef _no_grad_uniform_(tensor, a, b):\n    with paddle.no_grad():\n        tensor.set_value(paddle.uniform(tensor.shape, min=a, max=b))\n        return tensor\ndef _no_grad_normal_(tensor, mean, std):\n    with paddle.no_grad():\n        tensor.set_value(paddle.normal(shape=tensor.shape, mean=mean, std=std))\n        return tensor\ndef _no_grad_trunc_normal_(tensor, mean, std, a, b):\n    from scipy import special\n    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf"
        },
        {
            "comment": "This function initializes the weights of a neural network using truncated normal distribution. It first computes the standard normal cumulative distribution function and checks if the mean is more than 2 std away from [a, b]. If so, it issues a warning. Then it generates uniform values in [l, u] and transforms them to truncated standard normal distribution.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/utils/api.py\":338-363",
            "content": "    def norm_cdf(x):\n        # Computes standard normal cumulative distribution function\n        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n    if (mean < a - 2 * std) or (mean > b + 2 * std):\n        warnings.warn(\n            \"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n            \"The distribution of values may be incorrect.\",\n            stacklevel=2)\n    with paddle.no_grad():\n        # Values are generated by using a truncated uniform distribution and\n        # then using the inverse CDF for the normal distribution.\n        # Get upper and lower cdf values\n        l = norm_cdf((a - mean) / std)\n        u = norm_cdf((b - mean) / std)\n        # Uniformly fill tensor with values from [l, u], then translate to\n        # [2l-1, 2u-1].\n        tensor.set_value(\n            paddle.uniform(tensor.shape, min=2 * l - 1, max=2 * u - 1))\n        # tensor.uniform_(2 * l - 1, 2 * u - 1)\n        # Use inverse cdf transform for normal distribution to get truncated\n        # standard normal\n        tensor.set_value(special.erfinv(tensor))"
        },
        {
            "comment": "The provided code contains functions for transforming, filling, and zeroing tensors. It also includes a function that calculates the recommended gain value for different nonlinearity functions. The gain values are 1 for Linear/Identity, Conv{1,2,3}D, and Sigmoid; and 5/3 for Tanh, while for ReLU it is sqrt(2).",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/utils/api.py\":365-397",
            "content": "        # Transform to proper mean, std\n        tensor.set_value(tensor.multiply(paddle.to_tensor(std * math.sqrt(2.))))\n        tensor.add_(mean)\n        # Clamp to ensure it's in the proper range\n        tensor.clip_(min=a, max=b)\n        return tensor\ndef _no_grad_fill_(tensor, val):\n    with paddle.no_grad():\n        tensor.set_value(paddle.full_like(tensor, fill_value=val))\n        return tensor\ndef _no_grad_zero_(tensor):\n    with paddle.no_grad():\n        tensor.set_value(paddle.zeros_like(tensor))\n        return tensor\ndef calculate_gain(nonlinearity, param=None):\n    r\"\"\"Return the recommended gain value for the given nonlinearity function.\n    The values are as follows:\n    ================= ====================================================\n    nonlinearity      gain\n    ================= ====================================================\n    Linear / Identity :math:`1`\n    Conv{1,2,3}D      :math:`1`\n    Sigmoid           :math:`1`\n    Tanh              :math:`\\frac{5}{3}`\n    ReLU              :math:`\\sqrt{2}`"
        },
        {
            "comment": "This function calculates the gain value for different non-linear functions used in neural networks, such as Leaky ReLU, SELU, and others. It returns appropriate gain values depending on the specified nonlinearity parameter, considering any optional parameters as well.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/utils/api.py\":398-424",
            "content": "    Leaky Relu        :math:`\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}`\n    SELU              :math:`\\frac{3}{4}`\n    ================= ====================================================\n    Args:\n        nonlinearity: the non-linear function (`nn.functional` name)\n        param: optional parameter for the non-linear function\n    Examples:\n        >>> gain = nn.init.calculate_gain('leaky_relu', 0.2)  # leaky_relu with negative_slope=0.2\n    \"\"\"\n    linear_fns = [\n        'linear', 'conv1d', 'conv2d', 'conv3d', 'conv_transpose1d',\n        'conv_transpose2d', 'conv_transpose3d'\n    ]\n    if nonlinearity in linear_fns or nonlinearity == 'sigmoid':\n        return 1\n    elif nonlinearity == 'tanh':\n        return 5.0 / 3\n    elif nonlinearity == 'relu':\n        return math.sqrt(2.0)\n    elif nonlinearity == 'leaky_relu':\n        if param is None:\n            negative_slope = 0.01\n        elif not isinstance(param, bool) and isinstance(\n                param, int) or isinstance(param, float):\n            # True/False are instances of int, hence check above"
        },
        {
            "comment": "The code defines two functions, `uniform_` and `normal_`, used for initializing tensors with uniform or normal distribution respectively. The `_no_grad_uniform_` function is used internally by `uniform_`. The `else` and `if-elif` structures are used to handle nonlinearity cases in the `init_` function.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/utils/api.py\":425-453",
            "content": "            negative_slope = param\n        else:\n            raise ValueError(\n                \"negative_slope {} not a valid number\".format(param))\n        return math.sqrt(2.0 / (1 + negative_slope**2))\n    elif nonlinearity == 'selu':\n        return 3.0 / 4  # Value found empirically (https://github.com/pytorch/pytorch/pull/50664)\n    else:\n        raise ValueError(\"Unsupported nonlinearity {}\".format(nonlinearity))\ndef uniform_(tensor: Tensor, a: float = 0., b: float = 1.) -> Tensor:\n    r\"\"\"Fills the input Tensor with values drawn from the uniform\n    distribution :math:`\\mathcal{U}(a, b)`.\n    Args:\n        tensor: an n-dimensional `torch.Tensor`\n        a: the lower bound of the uniform distribution\n        b: the upper bound of the uniform distribution\n    Examples:\n        >>> w = torch.empty(3, 5)\n        >>> nn.init.uniform_(w)\n    \"\"\"\n    return _no_grad_uniform_(tensor, a, b)\ndef normal_(tensor: Tensor, mean: float = 0., std: float = 1.) -> Tensor:\n    r\"\"\"Fills the input Tensor with values drawn from the normal"
        },
        {
            "comment": "This code initializes or fills a tensor with values drawn from a normal distribution, truncating values outside the specified range [a, b]. The function `_no_grad_normal_` initializes a tensor with values from a normal distribution with given mean and standard deviation. The `trunc_normal_` function initializes a tensor with values from a truncated normal distribution within the specified bounds [a, b].",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/utils/api.py\":454-482",
            "content": "    distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`.\n    Args:\n        tensor: an n-dimensional `torch.Tensor`\n        mean: the mean of the normal distribution\n        std: the standard deviation of the normal distribution\n    Examples:\n        >>> w = torch.empty(3, 5)\n        >>> nn.init.normal_(w)\n    \"\"\"\n    return _no_grad_normal_(tensor, mean, std)\ndef trunc_normal_(tensor: Tensor,\n                  mean: float = 0.,\n                  std: float = 1.,\n                  a: float = -2.,\n                  b: float = 2.) -> Tensor:\n    r\"\"\"Fills the input Tensor with values drawn from a truncated\n    normal distribution. The values are effectively drawn from the\n    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n    with values outside :math:`[a, b]` redrawn until they are within\n    the bounds. The method used for generating the random values works\n    best when :math:`a \\leq \\text{mean} \\leq b`.\n    Args:\n        tensor: an n-dimensional `torch.Tensor`\n        mean: the mean of the normal distribution"
        },
        {
            "comment": "This code defines several functions for initializing Tensor objects in PyTorch. These functions include `trunc_normal_`, `constant_`, `ones_`, and `zeros_`. The `trunc_normal_` function initializes a tensor with values drawn from a truncated normal distribution, while the other three functions fill the tensor with constant values (specified by the user), ones, or zeros respectively. These functions can be used to set the initial values of a tensor before training a neural network model.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/utils/api.py\":483-525",
            "content": "        std: the standard deviation of the normal distribution\n        a: the minimum cutoff value\n        b: the maximum cutoff value\n    Examples:\n        >>> w = torch.empty(3, 5)\n        >>> nn.init.trunc_normal_(w)\n    \"\"\"\n    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\ndef constant_(tensor: Tensor, val: float) -> Tensor:\n    r\"\"\"Fills the input Tensor with the value :math:`\\text{val}`.\n    Args:\n        tensor: an n-dimensional `torch.Tensor`\n        val: the value to fill the tensor with\n    Examples:\n        >>> w = torch.empty(3, 5)\n        >>> nn.init.constant_(w, 0.3)\n    \"\"\"\n    return _no_grad_fill_(tensor, val)\ndef ones_(tensor: Tensor) -> Tensor:\n    r\"\"\"Fills the input Tensor with the scalar value `1`.\n    Args:\n        tensor: an n-dimensional `torch.Tensor`\n    Examples:\n        >>> w = torch.empty(3, 5)\n        >>> nn.init.ones_(w)\n    \"\"\"\n    return _no_grad_fill_(tensor, 1.)\ndef zeros_(tensor: Tensor) -> Tensor:\n    r\"\"\"Fills the input Tensor with the scalar value `0`.\n    Args:\n        tensor: an n-dimensional `torch.Tensor`"
        },
        {
            "comment": "The code above contains three functions: _no_grad_zero_, eye_, and dirac_. The _no_grad_zero_ function returns a tensor with all elements set to zero while preserving the identity of inputs in Linear layers. The eye_ function fills a 2-dimensional input tensor with an identity matrix, preserving as many inputs as possible in Linear layers. Lastly, the dirac_ function fills a 3, 4, or 5-dimensional input tensor with Dirac delta functions while preserving the identity of inputs in Convolutional layers, considering groups if specified.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/utils/api.py\":527-561",
            "content": "    Examples:\n        >>> w = torch.empty(3, 5)\n        >>> nn.init.zeros_(w)\n    \"\"\"\n    return _no_grad_zero_(tensor)\ndef eye_(tensor):\n    r\"\"\"Fills the 2-dimensional input `Tensor` with the identity\n    matrix. Preserves the identity of the inputs in `Linear` layers, where as\n    many inputs are preserved as possible.\n    Args:\n        tensor: a 2-dimensional `torch.Tensor`\n    Examples:\n        >>> w = torch.empty(3, 5)\n        >>> nn.init.eye_(w)\n    \"\"\"\n    if tensor.ndimension() != 2:\n        raise ValueError(\"Only tensors with 2 dimensions are supported\")\n    with paddle.no_grad():\n        tensor.set_value(paddle.eye(*tensor.shape))\n    return tensor\ndef dirac_(tensor, groups=1):\n    r\"\"\"Fills the {3, 4, 5}-dimensional input `Tensor` with the Dirac\n    delta function. Preserves the identity of the inputs in `Convolutional`\n    layers, where as many input channels are preserved as possible. In case\n    of groups>1, each group of channels preserves identity\n    Args:\n        tensor: a {3, 4, 5}-dimensional `torch.Tensor`"
        },
        {
            "comment": "The code is a function that initializes the convolutional layer weights using Dirac delta distribution for 3, 4, or 5-dimensional tensors. It first checks if the tensor dimensions are supported and then raises an error if not. Then it calculates the number of output channels per group and minimum dimension. The code then zeroes out the tensor and initializes the weights with Dirac delta distribution for specified groups and dimensions, performing a temporal convolution in 3-dimensions or spatial convolution in 4-dimensions.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/utils/api.py\":562-591",
            "content": "        groups (optional): number of groups in the conv layer (default: 1)\n    Examples:\n        >>> w = torch.empty(3, 16, 5, 5)\n        >>> nn.init.dirac_(w)\n        >>> w = torch.empty(3, 24, 5, 5)\n        >>> nn.init.dirac_(w, 3)\n    \"\"\"\n    dimensions = tensor.ndimension()\n    if dimensions not in [3, 4, 5]:\n        raise ValueError(\n            \"Only tensors with 3, 4, or 5 dimensions are supported\")\n    sizes = tensor.shape\n    if sizes[0] % groups != 0:\n        raise ValueError('dim 0 must be divisible by groups')\n    out_chans_per_grp = sizes[0] // groups\n    min_dim = min(out_chans_per_grp, sizes[1])\n    with paddle.no_grad():\n        tensor.zero_()\n        for g in range(groups):\n            for d in range(min_dim):\n                if dimensions == 3:  # Temporal convolution\n                    tensor[g * out_chans_per_grp + d, d,\n                           tensor.shape[2] // 2] = 1\n                elif dimensions == 4:  # Spatial convolution\n                    tensor[g * out_chans_per_grp + d, d, tensor.shape[2] // 2,"
        },
        {
            "comment": "This code defines several utility functions related to tensors in PaddlePaddle. The `_calculate_fan_in_and_fan_out` function calculates the fan-in and fan-out of a tensor, while the `LongTensor` function converts an input to a long tensor (dtype: int64). The `IntTensor` function does the same but with an int32 dtype. Lastly, `xavier_uniform_` initializes a tensor's parameters using Xavier Uniform initialization.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/utils/api.py\":592-626",
            "content": "                           tensor.shape[3] // 2] = 1\n                else:  # Volumetric convolution\n                    tensor[g * out_chans_per_grp + d, d, tensor.shape[2] // 2,\n                           tensor.shape[3] // 2, tensor.shape[4] // 2] = 1\n    return tensor\ndef _calculate_fan_in_and_fan_out(tensor):\n    dimensions = tensor.dim()\n    if dimensions < 2:\n        raise ValueError(\n            \"Fan in and fan out can not be computed for tensor with fewer than 2 dimensions\"\n        )\n    num_input_fmaps = tensor.shape[1]  # .size(1)\n    num_output_fmaps = tensor.shape[0]  # .size(0)\n    receptive_field_size = 1\n    if tensor.dim() > 2:\n        for s in tensor.shape[2:]:\n            receptive_field_size *= s  # fixed\n    fan_in = num_input_fmaps * receptive_field_size\n    fan_out = num_output_fmaps * receptive_field_size\n    return fan_in, fan_out\ndef LongTensor(x):\n    return paddle.to_tensor(x, dtype='int64')\ndef IntTensor(x):\n    return paddle.to_tensor(x, dtype='int32')\ndef xavier_uniform_(tensor: Tensor, gain: float = 1.) -> Tensor:"
        },
        {
            "comment": "This code initializes the input tensor with values following the Glorot initialization method. It uses a uniform distribution and calculates the scaling factor 'a' based on gain, fan_in, and fan_out dimensions of the tensor. The resulting tensor is sampled from a uniform distribution between -a and a. The function xavier_normal_ is a variation that also fills the input tensor but with values from a normal distribution instead of uniform.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/utils/api.py\":627-654",
            "content": "    r\"\"\"Fills the input `Tensor` with values according to the method\n    described in `Understanding the difficulty of training deep feedforward\n    neural networks` - Glorot, X. & Bengio, Y. (2010), using a uniform\n    distribution. The resulting tensor will have values sampled from\n    :math:`\\mathcal{U}(-a, a)` where\n    .. math::\n        a = \\text{gain} \\times \\sqrt{\\frac{6}{\\text{fan\\_in} + \\text{fan\\_out}}}\n    Also known as Glorot initialization.\n    Args:\n        tensor: an n-dimensional `torch.Tensor`\n        gain: an optional scaling factor\n    Examples:\n        >>> w = torch.empty(3, 5)\n        >>> nn.init.xavier_uniform_(w, gain=nn.init.calculate_gain('relu'))\n    \"\"\"\n    fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)\n    std = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n    a = math.sqrt(3.0) * std  # Calculate uniform bounds from standard deviation\n    return _no_grad_uniform_(tensor, -a, a)\ndef xavier_normal_(tensor: Tensor, gain: float = 1.) -> Tensor:\n    r\"\"\"Fills the input `Tensor` with values according to the method"
        },
        {
            "comment": "This function initializes a tensor with Xavier/Glorot normal distribution, using a normal distribution and a scaling factor 'gain'. The resulting tensor values are sampled from the normal distribution N(0, std^2), where std = gain * sqrt(2 / (fan_in + fan_out)). It also includes a function _calculate_correct_fan that checks for valid modes 'fan_in' or 'fan_out'.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/utils/api.py\":655-686",
            "content": "    described in `Understanding the difficulty of training deep feedforward\n    neural networks` - Glorot, X. & Bengio, Y. (2010), using a normal\n    distribution. The resulting tensor will have values sampled from\n    :math:`\\mathcal{N}(0, \\text{std}^2)` where\n    .. math::\n        \\text{std} = \\text{gain} \\times \\sqrt{\\frac{2}{\\text{fan\\_in} + \\text{fan\\_out}}}\n    Also known as Glorot initialization.\n    Args:\n        tensor: an n-dimensional `torch.Tensor`\n        gain: an optional scaling factor\n    Examples:\n        >>> w = torch.empty(3, 5)\n        >>> nn.init.xavier_normal_(w)\n    \"\"\"\n    fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)\n    std = gain * math.sqrt(2.0 / float(fan_in + fan_out))\n    return _no_grad_normal_(tensor, 0., std)\ndef _calculate_correct_fan(tensor, mode):\n    mode = mode.lower()\n    valid_modes = ['fan_in', 'fan_out']\n    if mode not in valid_modes:\n        raise ValueError(\"Mode {} not supported, please use one of {}\".format(\n            mode, valid_modes))\n    fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)"
        },
        {
            "comment": "This function fills a tensor with values from a uniform distribution according to the method described in He et al.'s (2015) paper, using either fan_in or fan_out mode. It also takes an optional argument for the negative slope of the rectifier used after this layer when 'leaky_relu' is specified.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/utils/api.py\":687-708",
            "content": "    return fan_in if mode == 'fan_in' else fan_out\ndef kaiming_uniform_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu'):\n    r\"\"\"Fills the input `Tensor` with values according to the method\n    described in `Delving deep into rectifiers: Surpassing human-level\n    performance on ImageNet classification` - He, K. et al. (2015), using a\n    uniform distribution. The resulting tensor will have values sampled from\n    :math:`\\mathcal{U}(-\\text{bound}, \\text{bound})` where\n    .. math::\n        \\text{bound} = \\text{gain} \\times \\sqrt{\\frac{3}{\\text{fan\\_mode}}}\n    Also known as He initialization.\n    Args:\n        tensor: an n-dimensional `torch.Tensor`\n        a: the negative slope of the rectifier used after this layer (only\n            used with ``'leaky_relu'``)\n        mode: either ``'fan_in'`` (default) or ``'fan_out'``. Choosing ``'fan_in'``\n            preserves the magnitude of the variance of the weights in the\n            forward pass. Choosing ``'fan_out'`` preserves the magnitudes in the"
        },
        {
            "comment": "This code is a PyTorch implementation of the Kaiming Uniform initialization method, used for initializing weights in neural networks. The function takes in a tensor and sets its values according to a uniform distribution with bounds calculated based on the tensor shape and nonlinearity (default is 'leaky_relu'). It also calculates the fan and gain based on the mode and nonlinearity to determine the standard deviation for the uniform distribution. The function then uses Paddle's `uniform` method to set the values in the tensor with the calculated bounds.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/utils/api.py\":709-731",
            "content": "            backwards pass.\n        nonlinearity: the non-linear function (`nn.functional` name),\n            recommended to use only with ``'relu'`` or ``'leaky_relu'`` (default).\n    Examples:\n        >>> w = torch.empty(3, 5)\n        >>> nn.init.kaiming_uniform_(w, mode='fan_in', nonlinearity='relu')\n    \"\"\"\n    fan = _calculate_correct_fan(tensor, mode)\n    gain = calculate_gain(nonlinearity, a)\n    std = gain / math.sqrt(fan)\n    bound = math.sqrt(\n        3.0) * std  # Calculate uniform bounds from standard deviation\n    with paddle.no_grad():\n        tensor.set_value(paddle.uniform(tensor.shape, min=-bound, max=bound))\n        return tensor\ndef kaiming_normal_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu'):\n    r\"\"\"Fills the input `Tensor` with values according to the method\n    described in `Delving deep into rectifiers: Surpassing human-level\n    performance on ImageNet classification` - He, K. et al. (2015), using a\n    normal distribution. The resulting tensor will have values sampled from"
        },
        {
            "comment": "Function initializes weights using Kaiming normal distribution. It takes in an n-dimensional tensor, a slope value for rectifier (optional), mode as 'fan_in' or 'fan_out', and nonlinearity function. It preserves weight variance in forward pass with 'fan_in' and backward pass with 'fan_out'. Recommended to use with 'relu' or 'leaky_relu'.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/utils/api.py\":732-757",
            "content": "    :math:`\\mathcal{N}(0, \\text{std}^2)` where\n    .. math::\n        \\text{std} = \\frac{\\text{gain}}{\\sqrt{\\text{fan\\_mode}}}\n    Also known as He initialization.\n    Args:\n        tensor: an n-dimensional `torch.Tensor`\n        a: the negative slope of the rectifier used after this layer (only\n            used with ``'leaky_relu'``)\n        mode: either ``'fan_in'`` (default) or ``'fan_out'``. Choosing ``'fan_in'``\n            preserves the magnitude of the variance of the weights in the\n            forward pass. Choosing ``'fan_out'`` preserves the magnitudes in the\n            backwards pass.\n        nonlinearity: the non-linear function (`nn.functional` name),\n            recommended to use only with ``'relu'`` or ``'leaky_relu'`` (default).\n    Examples:\n        >>> w = torch.empty(3, 5)\n        >>> kaiming_normal_(w, mode='fan_out', nonlinearity='relu')\n    \"\"\"\n    fan = _calculate_correct_fan(tensor, mode)\n    gain = calculate_gain(nonlinearity, a)\n    std = gain / math.sqrt(fan)\n    with paddle.no_grad():"
        },
        {
            "comment": "The code initializes a tensor with a (semi) orthogonal matrix based on the input tensor. The tensor must have at least 2 dimensions, and the trailing dimensions are flattened. If rows are less than columns, transpose the tensor. It computes the QR factorization of the tensor.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/utils/api.py\":758-788",
            "content": "        tensor.set_value(paddle.normal(shape=tensor.shape, mean=0, std=std))\n        return tensor\ndef orthogonal_(tensor, gain=1):\n    r\"\"\"Fills the input `Tensor` with a (semi) orthogonal matrix, as\n    described in `Exact solutions to the nonlinear dynamics of learning in deep\n    linear neural networks` - Saxe, A. et al. (2013). The input tensor must have\n    at least 2 dimensions, and for tensors with more than 2 dimensions the\n    trailing dimensions are flattened.\n    Args:\n        tensor: an n-dimensional `torch.Tensor`, where :math:`n \\geq 2`\n        gain: optional scaling factor\n    Examples:\n        >>> w = torch.empty(3, 5)\n        >>> nn.init.orthogonal_(w)\n    \"\"\"\n    if tensor.ndimension() < 2:\n        raise ValueError(\"Only tensors with 2 or more dimensions are supported\")\n    rows = tensor.shape[0]  # .size(0)\n    cols = tensor.numel() // rows\n    flattened = tensor.new(rows, cols).normal_(0, 1)\n    if rows < cols:\n        flattened.t_()\n    # Compute the qr factorization\n    q, r = paddle.to_tensor(np.linalg.qr(flattened.numpy()))"
        },
        {
            "comment": "Line 789: q, r = torch.qr(flattened) - Performs QR decomposition on flattened tensor\nLine 790-792: Make Q uniform according to https://arxiv.org/pdf/math-ph/0609050.pdf - Applies diag function on r, multiplies q by ph, and transposes q if rows < cols\nLine 794-798: with paddle.no_grad(): tensor.view_as(q).copy_(q) - Uses no_grad context manager to prevent gradients from being recorded during the operation\nLine 799: tensor.mul_(gain) - Multiplies tensor by a gain factor\nLine 802: return tensor - Returns the modified tensor after applying QR decomposition and scaling",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/utils/api.py\":789-821",
            "content": "    # q, r = torch.qr(flattened)\n    # Make Q uniform according to https://arxiv.org/pdf/math-ph/0609050.pdf\n    d = paddle.diag(r, 0)\n    ph = d.sign()\n    q *= ph\n    if rows < cols:\n        q.t_()\n    with paddle.no_grad():\n        tensor.view_as(q).copy_(q)\n        tensor.mul_(gain)\n    return tensor\ndef sparse_(tensor, sparsity, std=0.01):\n    r\"\"\"Fills the 2D input `Tensor` as a sparse matrix, where the\n    non-zero elements will be drawn from the normal distribution\n    :math:`\\mathcal{N}(0, 0.01)`, as described in `Deep learning via\n    Hessian-free optimization` - Martens, J. (2010).\n    Args:\n        tensor: an n-dimensional `torch.Tensor`\n        sparsity: The fraction of elements in each column to be set to zero\n        std: the standard deviation of the normal distribution used to generate\n            the non-zero values\n    Examples:\n        >>> w = torch.empty(3, 5)\n        >>> nn.init.sparse_(w, sparsity=0.1)\n    \"\"\"\n    if tensor.ndimension() != 2:\n        raise ValueError(\"Only tensors with 2 dimensions are supported\")"
        },
        {
            "comment": "This code defines a function that initializes the values of a tensor using the Kaiming normal distribution. It also includes a deprecated method for backward compatibility, warning users to use the new method instead.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/utils/api.py\":823-856",
            "content": "    rows, cols = tensor.shape\n    num_zeros = int(math.ceil(sparsity * rows))\n    with paddle.no_grad():\n        tensor.normal_(0, std)\n        for col_idx in range(cols):\n            row_indices = paddle.randperm(rows)\n            zero_indices = row_indices[:num_zeros]\n            tensor[zero_indices, col_idx] = 0\n    return tensor\n# for backward compatibility\ndef _make_deprecate(meth):\n    new_name = meth.__name__\n    old_name = new_name[:-1]\n    def deprecated_init(*args, **kwargs):\n        warnings.warn(\n            \"nn.init.{} is now deprecated in favor of nn.init.{}.\".format(\n                old_name, new_name),\n            stacklevel=2)\n        return meth(*args, **kwargs)\n    deprecated_init.__doc__ = r\"\"\"\n    {old_name}(...)\n    .. warning::\n        This method is now deprecated in favor of :func:`torch.nn.init.{new_name}`.\n    See :func:`~torch.nn.init.{new_name}` for details.\"\"\".format(\n        old_name=old_name, new_name=new_name)\n    deprecated_init.__name__ = old_name\n    return deprecated_init"
        }
    ]
}