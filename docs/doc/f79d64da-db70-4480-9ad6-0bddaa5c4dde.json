{
    "summary": "The code prepares the environment for training PaddleVideo models, builds a multigrid configuration, handles device and parallelism, trains the model, optimizes it using specified optimizer, logs progress/learning rate updates, evaluates performance, saves state, and saves model & optimizer.",
    "details": [
        {
            "comment": "The code snippet is the opening section of the file \"train_multigrid.py\" within the PaddleVideo library. It starts by declaring copyright, licensing information, and importing necessary modules. It also includes functions to build datasets, models, loaders, solvers, and utilities for logging, saving, and loading model parameters and progress. This section sets up the environment for training video models in the PaddleVideo framework.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/tasks/train_multigrid.py\":0-26",
            "content": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport time\nimport os.path as osp\nimport paddle\nimport paddle.distributed as dist\nfrom ..loader.builder import build_dataloader, build_dataset\nfrom ..modeling.builder import build_model\nfrom ..solver import build_lr, build_optimizer\nfrom ..utils import do_preciseBN\nfrom paddlevideo.utils import get_logger, coloring\nfrom paddlevideo.utils import (AverageMeter, build_record, log_batch, log_epoch,\n                               save, load, mkdir)"
        },
        {
            "comment": "This function constructs data loaders for training a model with the multigrid approach. It takes several arguments including configuration (cfg), places to distribute the data, whether to use precise batch normalization (precise_bn), number of iterations for precise BN (num_iters_precise_bn), and world size. If precise BN is enabled, it adjusts the number of samples in the training dataset, creates a separate loader for precise BN, and sets the adjusted number of samples back to None. If not, it sets the precise BN loader to None. The code also checks if a short cycle multigrid approach is being used.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/tasks/train_multigrid.py\":27-49",
            "content": "from paddlevideo.utils.multigrid import MultigridSchedule, aggregate_sub_bn_stats, subn_load, subn_save, is_eval_epoch\ndef construct_loader(cfg, places, validate, precise_bn, num_iters_precise_bn,\n                     world_size):\n    batch_size = cfg.DATASET.get('batch_size', 2)\n    train_dataset = build_dataset((cfg.DATASET.train, cfg.PIPELINE.train))\n    precise_bn_dataloader_setting = dict(\n        batch_size=batch_size,\n        num_workers=cfg.DATASET.get('num_workers', 0),\n        places=places,\n    )\n    if precise_bn:\n        cfg.DATASET.train.num_samples_precise_bn = num_iters_precise_bn * batch_size * world_size\n        precise_bn_dataset = build_dataset((cfg.DATASET.train,\n                                            cfg.PIPELINE.train))\n        precise_bn_loader = build_dataloader(precise_bn_dataset,\n                                             **precise_bn_dataloader_setting)\n        cfg.DATASET.train.num_samples_precise_bn = None\n    else:\n        precise_bn_loader = None\n    if cfg.MULTIGRID.SHORT_CYCLE:"
        },
        {
            "comment": "The code adjusts the batch size in a short cycle schedule based on target image size, multi-grid factors and default crop size. It then sets up a train_dataloader with these batch sizes and other parameters. If validate is True, it also builds a valid_dataset and valid_dataloader with the given configurations.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/tasks/train_multigrid.py\":50-76",
            "content": "        # get batch size list in short cycle schedule\n        bs_factor = [\n            int(\n                round((float(cfg.PIPELINE.train.transform[1]['MultiCrop'][\n                    'target_size']) / (s * cfg.MULTIGRID.default_crop_size))\n                      **2)) for s in cfg.MULTIGRID.short_cycle_factors\n        ]\n        batch_sizes = [\n            batch_size * bs_factor[0],\n            batch_size * bs_factor[1],\n            batch_size,\n        ]\n        train_dataloader_setting = dict(\n            batch_size=batch_sizes,\n            multigrid=True,\n            num_workers=cfg.DATASET.get('num_workers', 0),\n            places=places,\n        )\n    else:\n        train_dataloader_setting = precise_bn_dataloader_setting\n    train_loader = build_dataloader(train_dataset, **train_dataloader_setting)\n    if validate:\n        valid_dataset = build_dataset((cfg.DATASET.valid, cfg.PIPELINE.valid))\n        validate_dataloader_setting = dict(\n            batch_size=batch_size,\n            num_workers=cfg.DATASET.get('num_workers', 0),"
        },
        {
            "comment": "This code is creating training and validation data loaders for a PaddleVideo model. It also builds the model, and if parallelization is enabled, it wraps the model with Paddle's DataParallel API to distribute computation across multiple GPUs. The function returns the trained model, its optimizer, and the various data loaders required for training and validation.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/tasks/train_multigrid.py\":77-109",
            "content": "            places=places,\n            drop_last=False,\n            shuffle=False)\n        valid_loader = build_dataloader(valid_dataset,\n                                        **validate_dataloader_setting)\n    else:\n        valid_loader = None\n    return train_loader, valid_loader, precise_bn_loader\ndef build_trainer(cfg, places, parallel, validate, precise_bn,\n                  num_iters_precise_bn, world_size):\n    \"\"\"\n    Build training model and its associated tools, including optimizer,\n    dataloaders and meters.\n    Args:\n        cfg (CfgNode): configs.\n    Returns:\n        model: training model.\n        optimizer: optimizer.\n        train_loader: training data loader.\n        val_loader: validatoin data loader.\n        precise_bn_loader: training data loader for computing\n            precise BN.\n    \"\"\"\n    model = build_model(cfg.MODEL)\n    if parallel:\n        model = paddle.DataParallel(model)\n    train_loader, valid_loader, precise_bn_loader = \\\n        construct_loader(cfg,\n                         places,"
        },
        {
            "comment": "This code initializes a multigrid training configuration and builds the model, learning rate, optimizer, and loaders for training, validation, and precise Batch Normalization. It also includes an optional multigrid schedule for long or short cycles if specified in the configuration.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/tasks/train_multigrid.py\":110-145",
            "content": "                         validate,\n                         precise_bn,\n                         num_iters_precise_bn,\n                         world_size,\n                         )\n    lr = build_lr(cfg.OPTIMIZER.learning_rate, len(train_loader))\n    optimizer = build_optimizer(cfg.OPTIMIZER, lr, model=model)\n    return (\n        model,\n        lr,\n        optimizer,\n        train_loader,\n        valid_loader,\n        precise_bn_loader,\n    )\ndef train_model_multigrid(cfg, world_size=1, validate=True):\n    \"\"\"Train model entry\n    Args:\n    \tcfg (dict): configuration.\n    \tparallel (bool): Whether multi-card training. Default: True\n        validate (bool): Whether to do evaluation. Default: False.\n    \"\"\"\n    # Init multigrid.\n    multigrid = None\n    if cfg.MULTIGRID.LONG_CYCLE or cfg.MULTIGRID.SHORT_CYCLE:\n        multigrid = MultigridSchedule()\n        cfg = multigrid.init_multigrid(cfg)\n        if cfg.MULTIGRID.LONG_CYCLE:\n            cfg, _ = multigrid.update_long_cycle(cfg, cur_epoch=0)\n    multi_save_epoch = [i[-1] - 1 for i in multigrid.schedule]"
        },
        {
            "comment": "This code sets the device (npu, xpu or gpu) based on configuration and creates the model, dataloaders for training, validation, and precise BN if needed. It also initializes a logger and handles distributed training using parallel models and dataloaders.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/tasks/train_multigrid.py\":147-178",
            "content": "    parallel = world_size != 1\n    logger = get_logger(\"paddlevideo\")\n    batch_size = cfg.DATASET.get('batch_size', 2)\n    if cfg.get('use_npu', False):\n        places = paddle.set_device('npu')\n    elif cfg.get('use_xpu', False):\n        places = paddle.set_device('xpu')\n    else:\n        places = paddle.set_device('gpu')\n    model_name = cfg.model_name\n    output_dir = cfg.get(\"output_dir\", f\"./output/{model_name}\")\n    mkdir(output_dir)\n    local_rank = dist.ParallelEnv().local_rank\n    precise_bn = cfg.get(\"PRECISEBN\")\n    num_iters_precise_bn = cfg.PRECISEBN.num_iters_preciseBN\n    # 1. Construct model\n    model = build_model(cfg.MODEL)\n    if parallel:\n        model = paddle.DataParallel(model)\n    # 2. Construct dataloader\n    train_loader, valid_loader, precise_bn_loader = \\\n        construct_loader(cfg,\n                         places,\n                         validate,\n                         precise_bn,\n                         num_iters_precise_bn,\n                         world_size,\n                         )"
        },
        {
            "comment": "Constructing the optimizer, resuming training from a previous checkpoint if specified in the config file, and updating the long cycle configuration for multi-grid training.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/tasks/train_multigrid.py\":180-209",
            "content": "    # 3. Construct optimizer\n    lr = build_lr(cfg.OPTIMIZER.learning_rate, len(train_loader))\n    optimizer = build_optimizer(\n        cfg.OPTIMIZER, lr, parameter_list=model.parameters())\n    # Resume\n    resume_epoch = cfg.get(\"resume_epoch\", 0)\n    if resume_epoch:\n        filename = osp.join(\n            output_dir,\n            model_name + str(local_rank) + '_' + f\"{resume_epoch:05d}\")\n        subn_load(model, filename, optimizer)\n    # 4. Train Model\n    best = 0.\n    total_epochs = int(cfg.epochs * cfg.MULTIGRID.epoch_factor)\n    for epoch in range(total_epochs):\n        if epoch < resume_epoch:\n            logger.info(\n                f\"| epoch: [{epoch+1}] <= resume_epoch: [{ resume_epoch}], continue... \"\n            )\n            continue\n        if cfg.MULTIGRID.LONG_CYCLE:\n            cfg, changed = multigrid.update_long_cycle(cfg, epoch)\n            if changed:\n                logger.info(\"====== Rebuild model/optimizer/loader =====\")\n                (\n                    model,\n                    lr,"
        },
        {
            "comment": "The code builds a trainer with specified configurations, optimizer, train and validation loaders. It loads checkpoints if the epoch is not zero and updates the learning rate for the next epoch before training the model on the given data.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/tasks/train_multigrid.py\":210-234",
            "content": "                    optimizer,\n                    train_loader,\n                    valid_loader,\n                    precise_bn_loader,\n                ) = build_trainer(cfg, places, parallel, validate, precise_bn,\n                                  num_iters_precise_bn, world_size)\n                #load checkpoint after re-build model\n                if epoch != 0:\n                    #epoch no need to -1, haved add 1 when save\n                    filename = osp.join(\n                        output_dir,\n                        model_name + str(local_rank) + '_' + f\"{(epoch):05d}\")\n                    subn_load(model, filename, optimizer)\n                #update lr last epoch, not to use saved params\n                lr.last_epoch = epoch\n                lr.step(rebuild=True)\n        model.train()\n        record_list = build_record(cfg.MODEL)\n        tic = time.time()\n        for i, data in enumerate(train_loader):\n            record_list['reader_time'].update(time.time() - tic)\n            # 4.1 forward\n            outputs = model(data, mode='train')"
        },
        {
            "comment": "Performing backward pass, optimizing using given optimizer, logging progress, and updating learning rate in both iteration step and epoch step.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/tasks/train_multigrid.py\":235-261",
            "content": "            # 4.2 backward\n            avg_loss = outputs['loss']\n            avg_loss.backward()\n            # 4.3 minimize\n            optimizer.step()\n            optimizer.clear_grad()\n            # log record\n            record_list['lr'].update(\n                float(optimizer._global_learning_rate()), batch_size)\n            for name, value in outputs.items():\n                record_list[name].update(float(value), batch_size)\n            record_list['batch_time'].update(time.time() - tic)\n            tic = time.time()\n            if i % cfg.get(\"log_interval\", 10) == 0:\n                ips = \"ips: {:.5f} instance/sec.\".format(\n                    batch_size / record_list[\"batch_time\"].val)\n                log_batch(record_list, i, epoch + 1, total_epochs, \"train\", ips)\n            # learning rate iter step\n            if cfg.OPTIMIZER.learning_rate.get(\"iter_step\"):\n                lr.step()\n        # learning rate epoch step\n        if not cfg.OPTIMIZER.learning_rate.get(\"iter_step\"):\n            lr.step()"
        },
        {
            "comment": "This code snippet evaluates the model's performance during training and updates the record list with new values. It also logs the progress at certain intervals, displaying the number of instances processed per second (ips). The function 'evaluate' is called to perform this evaluation for each data batch in the valid_loader, updating the record list accordingly.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/tasks/train_multigrid.py\":263-287",
            "content": "        ips = \"ips: {:.5f} instance/sec.\".format(\n            batch_size * record_list[\"batch_time\"].count /\n            record_list[\"batch_time\"].sum)\n        log_epoch(record_list, epoch + 1, \"train\", ips)\n        def evaluate(best):\n            model.eval()\n            record_list = build_record(cfg.MODEL)\n            record_list.pop('lr')\n            tic = time.time()\n            for i, data in enumerate(valid_loader):\n                outputs = model(data, mode='valid')\n                # log_record\n                for name, value in outputs.items():\n                    record_list[name].update(float(value), batch_size)\n                record_list['batch_time'].update(time.time() - tic)\n                tic = time.time()\n                if i % cfg.get(\"log_interval\", 10) == 0:\n                    ips = \"ips: {:.5f} instance/sec.\".format(\n                        batch_size / record_list[\"batch_time\"].val)\n                    log_batch(record_list, i, epoch + 1, total_epochs, \"val\",\n                              ips)"
        },
        {
            "comment": "The code calculates the instantaneous processing speed (ips) and checks if a new best performance has been achieved. It then logs this information. If it's an evaluation epoch, it performs precise batch normalization, aggregates sub-batch normalization stats, and validates the model.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/tasks/train_multigrid.py\":289-312",
            "content": "            ips = \"ips: {:.5f} instance/sec.\".format(\n                batch_size * record_list[\"batch_time\"].count /\n                record_list[\"batch_time\"].sum)\n            log_epoch(record_list, epoch + 1, \"val\", ips)\n            best_flag = False\n            if record_list.get('top1') and record_list['top1'].avg > best:\n                best = record_list['top1'].avg\n                best_flag = True\n            return best, best_flag\n        # use precise bn to improve acc\n        if is_eval_epoch(cfg, epoch, total_epochs, multigrid.schedule):\n            logger.info(f\"do precise BN in {epoch+1} ...\")\n            do_preciseBN(model, precise_bn_loader, parallel,\n                         min(num_iters_precise_bn, len(precise_bn_loader)))\n        #  aggregate sub_BN stats\n        logger.info(\"Aggregate sub_BatchNorm stats...\")\n        aggregate_sub_bn_stats(model)\n        # 5. Validation\n        if is_eval_epoch(cfg, epoch, total_epochs, multigrid.schedule):\n            logger.info(f\"eval in {epoch+1} ...\")"
        },
        {
            "comment": "The code saves the best model if it outperforms previous results, and periodically saves the current model parameters during training. It uses the evaluate function to measure performance, the save function to store state dictionaries, and the subn_save function for saving models and optimizers at certain epochs. The logger is used for informative messages about saving and training completion.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/tasks/train_multigrid.py\":313-334",
            "content": "            with paddle.no_grad():\n                best, save_best_flag = evaluate(best)\n            # save best\n            if save_best_flag:\n                save(optimizer.state_dict(),\n                     osp.join(output_dir, model_name + \"_best.pdopt\"))\n                save(model.state_dict(),\n                     osp.join(output_dir, model_name + \"_best.pdparams\"))\n                logger.info(\n                    f\"Already save the best model (top1 acc){int(best * 10000) / 10000}\"\n                )\n        # 6. Save model and optimizer\n        if is_eval_epoch(\n                cfg, epoch,\n                total_epochs, multigrid.schedule) or epoch % cfg.get(\n                    \"save_interval\", 10) == 0 or epoch in multi_save_epoch:\n            logger.info(\"[Save parameters] ======\")\n            subn_save(output_dir, model_name + str(local_rank) + '_', epoch + 1,\n                      model, optimizer)\n    logger.info(f'training {model_name} finished')"
        }
    ]
}