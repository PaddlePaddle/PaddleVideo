{
    "summary": "This code introduces the PP-TSN model, an enhanced version of TSN. It describes implementation, data preparation and training processes, using mixed-precision training for speed. The PP-TSN model can be customized and tested on Kinetics-400, providing models for video file inference.",
    "details": [
        {
            "comment": "This code is a documentation for the PP-TSN model, which is an improved version of the TSN model. The documentation includes sections on introduction, data, train, test, inference, and reference. It also provides accuracy information and guidance on how to download and prepare K400 and UCF101 data.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/pp-tsn.md\":0-29",
            "content": "[\u7b80\u4f53\u4e2d\u6587](../../../zh-CN/model_zoo/recognition/pp-tsn.md) | English\n# PP-TSN\n## Content\n- [Introduction](#Introduction)\n- [Data](#Data)\n- [Train](#Train)\n- [Test](#Test)\n- [Inference](#Inference)\n- [Reference](#Reference)\n## Introduction\nWe have improved the [TSN model](./tsn.md) and obtained a more accurate 2D practical video classification model **PP-TSN**. Without increasing the amount of parameters and calculations, the accuracy on the UCF-101, Kinetics-400 and other data sets significantly exceeds the original version. The accuracy on the Kinetics-400 data set is shown in the following table.\n| Version | Top1 |\n| :------ | :----: |\n| Ours (distill) | 75.06 |\n| Ours | **73.68** |\n| [mmaction2](https://github.com/open-mmlab/mmaction2/tree/master/configs/recognition/tsn#kinetics-400) | 71.80 |\n## Data\nK400 data download and preparation please refer to [Kinetics-400 data preparation](../../dataset/k400.md)\nUCF101 data download and preparation please refer to [UCF-101 data preparation](../../dataset/ucf101.md)"
        },
        {
            "comment": "This code describes how to train the \"PPTSN\" model on the Kinetics-400 dataset using 8 GPUs. It first requires downloading a pre-trained ResNet50_vd_ssld_v2 model, then configuring its path in the yaml file, and finally running training with the provided command.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/pp-tsn.md\":32-60",
            "content": "## Train\n### Kinetics-400 data set training\n#### Download and add pre-trained models\n1. Download the image distillation pre-training model [ResNet50_vd_ssld_v2.pdparams](https://videotag.bj.bcebos.com/PaddleVideo/PretrainModel/ResNet50_vd_ssld_v2_pretrained.pdparams) as the Backbone initialization parameter, or download it through wget\n   ```bash\n   wget https://videotag.bj.bcebos.com/PaddleVideo/PretrainModel/ResNet50_vd_ssld_v2_pretrained.pdparams\n   ```\n2. Open `PaddleVideo/configs/recognition/pptsn/pptsn_k400_frames.yaml`, and fill in the downloaded weight storage path below `pretrained:`\n    ```yaml\n    MODEL:\n        framework: \"Recognizer2D\"\n        backbone:\n            name: \"ResNetTweaksTSN\"\n            pretrained: fill in the path here\n    ```\n#### Start training\n- The Kinetics400 data set uses 8 cards for training, and the start command of the training method is as follows:\n    ```bash\n    # frames data format\n    python3.7 -B -m paddle.distributed.launch --gpus=\"0,1,2,3,4,5,6,7\" --log_dir=log_pptsn main.py --validate -c configs/recognition/ pptsn/pptsn_k400_frames.yaml"
        },
        {
            "comment": "This code demonstrates how to run PaddleVideo's pp-tsn model with amp mixed-precision training for faster processing. It supports both videos and frames data formats, and allows customization of parameter configurations for different datasets.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/pp-tsn.md\":62-80",
            "content": "    # videos data format\n    python3.7 -B -m paddle.distributed.launch --gpus=\"0,1,2,3,4,5,6,7\" --log_dir=log_pptsn main.py --validate -c configs/recognition/ pptsn/pptsn_k400_videos.yaml\n    ```\n- Turn on amp mixed-precision training to speed up the training process. The training start command is as follows:\n    ```bash\n    export FLAGS_conv_workspace_size_limit=800 # MB\n    export FLAGS_cudnn_exhaustive_search=1\n    export FLAGS_cudnn_batchnorm_spatial_persistent=1\n    # frames data format\n    python3.7 -B -m paddle.distributed.launch --gpus=\"0,1,2,3,4,5,6,7\" --log_dir=log_pptsn main.py --amp --validate -c configs /recognition/pptsn/pptsn_k400_frames.yaml\n    # videos data format\n    python3.7 -B -m paddle.distributed.launch --gpus=\"0,1,2,3,4,5,6,7\" --log_dir=log_pptsn main.py --amp --validate -c configs /recognition/pptsn/pptsn_k400_videos.yaml\n    ```\n- In addition, you can customize and modify the parameter configuration to achieve the purpose of training/testing on different data sets. It is "
        },
        {
            "comment": "The PP-TSN model's testing process is different from training verification due to the sampling method used. The final test score should be obtained after testing the best model in test mode, as opposed to using the top-k accuracy recorded during training.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/pp-tsn.md\":80-94",
            "content": "recommended that the naming method of the configuration file is `model_dataset name_file format_data format_sampling method.yaml` , Please refer to [config](../../tutorials/config.md) for parameter usage.\n## Test\n- The PP-TSN model is verified during training. You can find the keyword `best` in the training log to obtain the model test accuracy. The log example is as follows:\n\t```\n  Already save the best model (top1 acc)0.7004\n\t```\n- Since the sampling method of the PP-TSN model test mode is **TenCrop**, which is slightly slower but more accurate, it is different from the **CenterCrop** used in the verification mode during the training process, so the verification index recorded in the training log is `topk Acc `Does not represent the final test score, so after the training is completed, you can use the test mode to test the best model to obtain the final index, the command is as follows:\n\t```bash\n  python3.7 -B -m paddle.distributed.launch --gpus=\"0,1,2,3,4,5,6,7\" --log_dir=log_pptsn main.py --test -c configs/recognition/ pptsn/pptsn_k400_frames.yaml -w \"output/ppTSN/ppTSN_best.pdparams\""
        },
        {
            "comment": "This code outlines the test results of PP-TSN model using different configurations on the validation dataset of Kinetics-400. The table presents backbone, sampling method, distillation method, number of segments, target image size, and Top-1 accuracy for each configuration. Checkpoints are also provided for each configuration. The PP-TSN video sampling strategy is TenCrop sampling, which samples frames from different positions in the video sequence and spatial areas.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/pp-tsn.md\":95-104",
            "content": "\t```\n\tWhen the test configuration uses the following parameters, the test indicators on the validation data set of Kinetics-400 are as follows:\n\t| backbone | Sampling method | distill | num_seg | target_size | Top-1 |       checkpoints       |\n\t| :------: | :-------------: | :-----: | :-----: | :---------: | :---- | :---------------------: |\n\t| ResNet50 |     TenCrop     |  False  |    3    |     224     | 73.68 | [ppTSN_k400.pdparams](https://videotag.bj.bcebos.com/PaddleVideo-release2.2/ppTSN_k400.pdparams) |\n\t| ResNet50 |     TenCrop     |  True   |    8    |     224     | 75.06 | [ppTSN_k400_8.pdparams](https://videotag.bj.bcebos.com/PaddleVideo-release2.2/ppTSN_k400_8.pdparams) |\n- The PP-TSN video sampling strategy is TenCrop sampling: in time sequence, the input video is evenly divided into num_seg segments, and the middle position of each segment is sampled 1 frame; spatially, from the upper left corner, upper right corner, center point, lower left corner, and lower right corner Each"
        },
        {
            "comment": "The code exports the pre-trained model for inference and uses the prediction engine to perform predictions on input video files. Distillation is used for obtaining the pre-trained model, and the generated model structure file and weight files are stored in the `inference/ppTSN/` directory. The provided bash commands assist in exporting and predicting with the model respectively.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/pp-tsn.md\":104-124",
            "content": " of the 5 sub-regions sampled an area of 224x224, and the horizontal flip was added to obtain a total of 10 sampling results. A total of 1 clip is sampled for 1 video.\n- Distill is `True`, which means that the pre-trained model obtained by distillation is used. For the specific distillation scheme, please refer to [ppTSM Distillation Scheme]().\n## Inference\n### Export inference model\n```bash\npython3.7 tools/export_model.py -c configs/recognition/pptsn/pptsn_k400_frames.yaml -p data/ppTSN_k400.pdparams -o inference/ppTSN\n```\nThe above command will generate the model structure file `ppTSN.pdmodel` and model weight files `ppTSN.pdiparams` and `ppTSN.pdiparams.info` files required for prediction, all of which are stored in the `inference/ppTSN/` directory\nFor the meaning of each parameter in the above bash command, please refer to [Model Reasoning Method](https://github.com/HydrogenSulfate/PaddleVideo/blob/PPTSN-v1/docs/en/start.md#2-infer)\n### Use prediction engine inference\n```bash\npython3.7 tools/predict.py --input_file data/example.avi \\"
        },
        {
            "comment": "This code is running an inference on a video file using the PP-TSN model trained on Kinetics-400. The top-1 category and its corresponding confidence are being outputted for the given video file.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/pp-tsn.md\":125-145",
            "content": "                           --config configs/recognition/pptsn/pptsn_k400_frames.yaml \\\n                           --model_file inference/ppTSN/ppTSN.pdmodel \\\n                           --params_file inference/ppTSN/ppTSN.pdiparams \\\n                           --use_gpu=True \\\n                           --use_tensorrt=False\n```\nThe output example is as follows:\n```bash\nCurrent video file: data/example.avi\n        top-1 class: 5\n        top-1 score: 0.998979389667511\n```\nIt can be seen that using the PP-TSN model trained on Kinetics-400 to predict `data/example.avi`, the output top1 category id is `5`, and the confidence is 0.99. By consulting the category id and name correspondence table `data/k400/Kinetics-400_label_list.txt`, it can be known that the predicted category name is `archery`.\n## Reference\n- [Temporal Segment Networks: Towards Good Practices for Deep Action Recognition](https://arxiv.org/pdf/1608.00859.pdf), Limin Wang, Yuanjun Xiong, Zhe Wang\n- [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531), Geoffrey Hinton, Oriol Vinyals, Jeff Dean"
        }
    ]
}