{
    "summary": "The model employs PaddlePaddle for object matching, k-nearest neighbor search, SpatialCorrelationSampler for pairwise distances, and a neural network with separable convolutional layers for semantic segmentation. The Ma-Net's int_seghead updates global and local maps for sequence processing and performs tensor operations for video object segmentation.",
    "details": [
        {
            "comment": "This code snippet is from a PaddlePaddle-based video object detection model. It defines functions for calculating pairwise distances between embeddings and initializes some global variables. The model is designed to take reference and query embeddings as input, compute pairwise squared L2 distances, and returns the flattened tensor of distances. This distance calculation is likely used in the matching process of objects in the video frames.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/networks/IntVOS.py\":0-41",
            "content": "import os\nimport numpy as np\nimport paddle\nimport paddle.nn as nn\nimport sys\nsys.path.append(\"..\")\nfrom config import cfg\nimport time\nimport paddle.nn.functional as F\nfrom utils.api import int_, float_, long_\nfrom utils.api import kaiming_normal_\n#############################################################GLOBAL_DIST_MAP\nMODEL_UNFOLD = True\nWRONG_LABEL_PADDING_DISTANCE = 1e20\ndef _pairwise_distances(x, y, ys=None):\n    \"\"\"Computes pairwise squared l2 distances between tensors x and y.\n    Args:\n    x: Tensor of shape [n, feature_dim].\n    y: Tensor of shape [m, feature_dim].\n    Returns:\n    Float32 distances tensor of shape [n, m].\n    \"\"\"\n    xs = paddle.sum(x * x, 1)\n    xs = xs.unsqueeze(1)\n    if ys is None:\n        ys = paddle.sum(y * y, 1)\n        ys = ys.unsqueeze(0)\n    else:\n        ys = ys\n    d = xs + ys - 2. * paddle.matmul(x, paddle.t(y))\n    return d, ys\n##################\ndef _flattened_pairwise_distances(reference_embeddings, query_embeddings, ys):\n    \"\"\"Calculates flattened tensor of pairwise distances between ref and query."
        },
        {
            "comment": "The code calculates the distance between reference and query embeddings, performing pairwise distances calculations using the _pairwise_distances function. The result is a distance tensor with shape [reference_embeddings.size / embedding_dim, query_embeddings.size / embedding_dim]. This function also includes _nn_features_per_object_for_chunk which extracts features for each object using nearest neighbor attention.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/networks/IntVOS.py\":42-64",
            "content": "    Args:\n    reference_embeddings: Tensor of shape [..., embedding_dim],\n      the embedding vectors for the reference frame\n    query_embeddings: Tensor of shape [n_query_images, height, width,\n      embedding_dim], the embedding vectors for the query frames.\n    Returns:\n    A distance tensor of shape [reference_embeddings.size / embedding_dim,\n    query_embeddings.size / embedding_dim]\n    \"\"\"\n    embedding_dim = query_embeddings.shape[-1]\n    reference_embeddings = reference_embeddings.reshape([-1, embedding_dim])\n    first_dim = -1\n    query_embeddings = query_embeddings.reshape([first_dim, embedding_dim])\n    dists, ys = _pairwise_distances(query_embeddings, reference_embeddings, ys)\n    return dists, ys\ndef _nn_features_per_object_for_chunk(reference_embeddings, query_embeddings,\n                                      wrong_label_mask, k_nearest_neighbors,\n                                      ys):\n    \"\"\"Extracts features for each object using nearest neighbor attention.\n  Args:\n    reference_embeddings: Tensor of shape [n_chunk, embedding_dim],"
        },
        {
            "comment": "This code calculates pairwise distances between reference and query embedding vectors, selects the k-nearest neighbors, and returns the nearest neighbor features. It takes into account a wrong_label_mask and padding distance, which helps handle incorrect labels and avoid noisy data.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/networks/IntVOS.py\":65-87",
            "content": "      the embedding vectors for the reference frame.\n    query_embeddings: Tensor of shape [m_chunk, embedding_dim], the embedding\n      vectors for the query frames.\n    wrong_label_mask:\n    k_nearest_neighbors: Integer, the number of nearest neighbors to use.\n  Returns:\n    nn_features: A float32 tensor of nearest neighbor features of shape\n      [m_chunk, n_objects, feature_dim].\n    \"\"\"\n    #    reference_embeddings_key = reference_embeddings\n    #    query_embeddings_key = query_embeddings\n    dists, ys = _flattened_pairwise_distances(reference_embeddings,\n                                              query_embeddings, ys)\n    dists = (paddle.unsqueeze(dists, 1) +\n             paddle.unsqueeze(float_(wrong_label_mask), 0) *\n             WRONG_LABEL_PADDING_DISTANCE)\n    if k_nearest_neighbors == 1:\n        features = paddle.min(dists, 2, keepdim=True)\n    else:\n        dists, _ = paddle.topk(-dists, k=k_nearest_neighbors, axis=2)\n        dists = -dists\n        valid_mask = (dists < WRONG_LABEL_PADDING_DISTANCE)"
        },
        {
            "comment": "The code calculates the nearest neighbor features for each object in chunks to save memory. It starts by masking and averaging distances between reference and query embeddings, then selects relevant indices from flattened arrays to calculate nearest neighbors for objects. The function takes `reference_embeddings_flat`, `query_embeddings_flat`, `reference_labels_flat`, `ref_obj_ids`, `k_nearest_neighbors`, and `n_chunks` as input and returns the features and labels.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/networks/IntVOS.py\":88-117",
            "content": "        masked_dists = dists * valid_mask.float()\n        pad_dist = paddle.max(masked_dists, axis=2, keepdim=True)[0].tile(\n            (1, 1, masked_dists.shape[-1]))\n        dists = paddle.where(valid_mask, dists, pad_dist)\n        # take mean of distances\n        features = paddle.mean(dists, axis=2, keepdim=True)\n    return features, ys\n###\ndef _selected_pixel(ref_labels_flat, ref_emb_flat):\n    index_list = paddle.arange(len(ref_labels_flat))\n    index_list = index_list\n    index_ = paddle.masked_select(index_list, ref_labels_flat != -1)\n    index_ = long_(index_)\n    ref_labels_flat = paddle.index_select(ref_labels_flat, index_, 0)\n    ref_emb_flat = paddle.index_select(ref_emb_flat, index_, 0)\n    return ref_labels_flat, ref_emb_flat\n###\ndef _nearest_neighbor_features_per_object_in_chunks(\n        reference_embeddings_flat, query_embeddings_flat, reference_labels_flat,\n        ref_obj_ids, k_nearest_neighbors, n_chunks):\n    \"\"\"Calculates the nearest neighbor features per object in chunks to save mem."
        },
        {
            "comment": "This code performs k-nearest neighbor search using chunking to save memory. It takes embedding vectors for reference and query frames, their class labels, object ids, the number of nearest neighbors, and the number of chunks as input. It calculates the chunk size based on the number of query frames and the specified number of chunks. If TEST_MODE is enabled, it selects some pixels from the input. Then, it checks if the reference labels are equal to unsqueezed object ids for each query frame and creates a mask for wrong labels. It returns nearest neighbor features of shape [m, n_objects, feature_dim].",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/networks/IntVOS.py\":118-140",
            "content": "    Uses chunking to bound the memory use.\n    Args:\n    reference_embeddings_flat: Tensor of shape [n, embedding_dim],\n      the embedding vectors for the reference frame.\n    query_embeddings_flat: Tensor of shape [m, embedding_dim], the embedding\n      vectors for the query frames.\n    reference_labels_flat: Tensor of shape [n], the class labels of the\n      reference frame.\n    ref_obj_ids: int tensor of unique object ids in the reference labels.\n    k_nearest_neighbors: Integer, the number of nearest neighbors to use.\n    n_chunks: Integer, the number of chunks to use to save memory\n      (set to 1 for no chunking).\n    Returns:\n    nn_features: A float32 tensor of nearest neighbor features of shape\n      [m, n_objects, feature_dim].\n    \"\"\"\n    chunk_size = int_(\n        np.ceil((float_(query_embeddings_flat.shape[0]) / n_chunks).numpy()))\n    if cfg.TEST_MODE:\n        reference_labels_flat, reference_embeddings_flat = _selected_pixel(\n            reference_labels_flat, reference_embeddings_flat)\n    wrong_label_mask = (reference_labels_flat != paddle.unsqueeze("
        },
        {
            "comment": "This code calculates nearest neighbor features for each object across multiple chunks. It splits the query embeddings into different chunks, then computes the features for each chunk individually. If there is only one chunk, it returns the features directly. Otherwise, it concatenates all the computed features along axis 0 and returns them as nearest neighbor features.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/networks/IntVOS.py\":141-168",
            "content": "        ref_obj_ids, 1))\n    all_features = []\n    for n in range(n_chunks):\n        if n == 0:\n            ys = None\n        if n_chunks == 1:\n            query_embeddings_flat_chunk = query_embeddings_flat\n        else:\n            chunk_start = n * chunk_size\n            chunk_end = (n + 1) * chunk_size\n            query_embeddings_flat_chunk = query_embeddings_flat[\n                chunk_start:chunk_end]\n        features, ys = _nn_features_per_object_for_chunk(\n            reference_embeddings_flat, query_embeddings_flat_chunk,\n            wrong_label_mask, k_nearest_neighbors, ys)\n        all_features.append(features)\n    if n_chunks == 1:\n        nn_features = all_features[0]\n    else:\n        nn_features = paddle.concat(all_features, axis=0)\n    return nn_features\ndef nearest_neighbor_features_per_object(reference_embeddings,\n                                         query_embeddings,\n                                         reference_labels,\n                                         k_nearest_neighbors,\n                                         gt_ids=None,"
        },
        {
            "comment": "This function calculates the distance between nearest neighbors in reference_embeddings and query_embeddings for each object. It uses the provided reference_labels to determine objects, subsamples if max_neighbors_per_object is specified, and considers k_nearest_neighbors. The gt_ids are used for determining unique ground truth ids in the first frame.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/networks/IntVOS.py\":169-185",
            "content": "                                         n_chunks=100):\n    \"\"\"Calculates the distance to the nearest neighbor per object.\n    For every pixel of query_embeddings calculate the distance to the\n    nearest neighbor in the (possibly subsampled) reference_embeddings per object.\n    Args:\n    reference_embeddings: Tensor of shape [height, width, embedding_dim],\n      the embedding vectors for the reference frame.\n    query_embeddings: Tensor of shape [n_query_images, height, width,\n      embedding_dim], the embedding vectors for the query frames.\n    reference_labels: Tensor of shape [height, width, 1], the class labels of\n      the reference frame.\n    max_neighbors_per_object: Integer, the maximum number of candidates\n      for the nearest neighbor query per object after subsampling,\n      or 0 for no subsampling.\n    k_nearest_neighbors: Integer, the number of nearest neighbors to use.\n    gt_ids: Int tensor of shape [n_objs] of the sorted unique ground truth\n      ids in the first frame. If None, it will be derived from"
        },
        {
            "comment": "This function calculates the nearest neighbor features for query images using reference embeddings and labels. It first asserts that the shape of reference embeddings matches the shape of reference labels. Then, it flattens the reference labels and checks if gt_ids (ground truth ids) are provided. If not, it finds unique object ids in the reference labels, creates a tensor with those ids, and converts them to integer type. Else, it converts the given gt_ids to integers. The function reshapes the query and reference embeddings, calculates embedding dimensions, and returns the nearest neighbor features and gt_ids.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/networks/IntVOS.py\":186-210",
            "content": "      reference_labels.\n    n_chunks: Integer, the number of chunks to use to save memory\n      (set to 1 for no chunking).\n    Returns:\n    nn_features: A float32 tensor of nearest neighbor features of shape\n      [n_query_images, height, width, n_objects, feature_dim].\n    gt_ids: An int32 tensor of the unique sorted object ids present\n      in the reference labels.\n    \"\"\"\n    assert (reference_embeddings.shape[:2] == reference_labels.shape[:2])\n    h, w, _ = query_embeddings.shape\n    reference_labels_flat = reference_labels.reshape([-1])\n    if gt_ids is None:\n        ref_obj_ids = paddle.unique(reference_labels_flat)[-1]\n        ref_obj_ids = np.arange(0, ref_obj_ids + 1)\n        gt_ids = paddle.to_tensor(ref_obj_ids)\n        gt_ids = int_(gt_ids)\n    else:\n        gt_ids = int_(paddle.arange(0, gt_ids + 1))\n    embedding_dim = query_embeddings.shape[-1]\n    query_embeddings_flat = query_embeddings.reshape([-1, embedding_dim])\n    reference_embeddings_flat = reference_embeddings.reshape(\n        [-1, embedding_dim])"
        },
        {
            "comment": "This code chunk performs nearest neighbor feature extraction for each object in the image, reshapes it, and then returns it along with gt_ids. The local_pairwise_distances function computes pairwise squared l2 distances using a local search window. It is used to compare features between different points in an optimized manner, considering a maximum distance per dimension.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/networks/IntVOS.py\":211-234",
            "content": "    nn_features = _nearest_neighbor_features_per_object_in_chunks(\n        reference_embeddings_flat, query_embeddings_flat, reference_labels_flat,\n        gt_ids, k_nearest_neighbors, n_chunks)\n    nn_features_dim = nn_features.shape[-1]\n    nn_features = nn_features.reshape(\n        [1, h, w, gt_ids.shape[0], nn_features_dim])\n    return nn_features.cuda(), gt_ids\n########################################################################LOCAL_DIST_MAP\ndef local_pairwise_distances(x, y, max_distance=9):\n    \"\"\"Computes pairwise squared l2 distances using a local search window.\n    Optimized implementation using correlation_cost.\n    Args:\n    x: Float32 tensor of shape [height, width, feature_dim].\n    y: Float32 tensor of shape [height, width, feature_dim].\n    max_distance: Integer, the maximum distance in pixel coordinates\n      per dimension which is considered to be in the search window.\n    Returns:\n    Float32 distances tensor of shape\n      [height, width, (2 * max_distance + 1) ** 2].\n    \"\"\"\n    if cfg.MODEL_LOCAL_DOWNSAMPLE:"
        },
        {
            "comment": "This code is performing cross-correlation between two input tensors and applying a boundary condition to the resulting tensor. It then applies sigmoid activation, resizes the tensor to original dimensions, and transposes it back to the original shape before unsqueezing the last dimension.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/networks/IntVOS.py\":235-260",
            "content": "        #####\n        ori_h, ori_w, _ = x.shape\n        x = x.transpose([2, 0, 1]).unsqueeze(0)\n        x = F.avg_pool2d(x, (2, 2), (2, 2))\n        y = y.transpose([2, 0, 1]).unsqueeze(0)\n        y = F.avg_pool2d(y, (2, 2), (2, 2))\n        x = x.squeeze(0).transpose([1, 2, 0])\n        y = y.squeeze(0).transpose([1, 2, 0])\n        corr = cross_correlate(x, y, max_distance=max_distance)\n        xs = paddle.sum(x * x, 2, keepdim=True)\n        ys = paddle.sum(y * y, 2, keepdim=True)\n        ones_ys = paddle.ones_like(ys)\n        ys = cross_correlate(ones_ys, ys, max_distance=max_distance)\n        d = xs + ys - 2 * corr\n        # Boundary should be set to Inf.\n        tmp = paddle.zeros_like(d)\n        boundary = paddle.equal(\n            cross_correlate(ones_ys, ones_ys, max_distance=max_distance), 0)\n        d = paddle.where(boundary, tmp.fill_(float_('inf')), d)\n        d = (paddle.nn.functional.sigmoid(d) - 0.5) * 2\n        d = d.transpose([2, 0, 1]).unsqueeze(0)\n        d = F.interpolate(d,\n                          size=(ori_h, ori_w),"
        },
        {
            "comment": "This code calculates the pairwise squared l2 distances between two tensors using either correlation or cross-correlation method, depending on the mode. In correlation mode, it uses bilinear interpolation and aligns corners. Otherwise, it uses cross-correlate function with a max_distance parameter. It also handles boundary cases by setting values to infinity where necessary.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/networks/IntVOS.py\":261-286",
            "content": "                          mode='bilinear',\n                          align_corners=True)\n        d = d.squeeze(0).transpose([1, 2, 0])\n    else:\n        corr = cross_correlate(x, y, max_distance=max_distance)\n        xs = paddle.sum(x * x, 2, keepdim=True)\n        ys = paddle.sum(y * y, 2, keepdim=True)\n        ones_ys = paddle.ones_like(ys)\n        ys = cross_correlate(ones_ys, ys, max_distance=max_distance)\n        d = xs + ys - 2 * corr\n        # Boundary should be set to Inf.\n        tmp = paddle.zeros_like(d)\n        boundary = paddle.equal(\n            cross_correlate(ones_ys, ones_ys, max_distance=max_distance), 0)\n        d = paddle.where(boundary, tmp.fill_(float_('inf')), d)\n    return d\ndef local_pairwise_distances2(x, y, max_distance=9):\n    \"\"\"Computes pairwise squared l2 distances using a local search window.\n    Naive implementation using map_fn.\n    Used as a slow fallback for when correlation_cost is not available.\n    Args:\n    x: Float32 tensor of shape [height, width, feature_dim].\n    y: Float32 tensor of shape [height, width, feature_dim]."
        },
        {
            "comment": "This code section performs local downsampling on the input tensors x and y. It first transposes the tensors and applies average pooling with a 2x2 kernel to reduce their size. Then, it pads the result of y with a large value, calculates offsets using unfolding, subtracts them from x, and sums squared differences across channels. The result is a distances tensor of shape [height, width, (2 * max_distance + 1) ** 2].",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/networks/IntVOS.py\":287-311",
            "content": "    max_distance: Integer, the maximum distance in pixel coordinates\n      per dimension which is considered to be in the search window.\n    Returns:\n    Float32 distances tensor of shape\n      [height, width, (2 * max_distance + 1) ** 2].\n    \"\"\"\n    if cfg.MODEL_LOCAL_DOWNSAMPLE:\n        ori_h, ori_w, _ = x.shape\n        x = paddle.transpose(x, [2, 0, 1]).unsqueeze(0)\n        x = F.avg_pool2d(x, (2, 2), (2, 2))\n        y = paddle.transpose(y, [2, 0, 1]).unsqueeze(0)\n        y = F.avg_pool2d(y, (2, 2), (2, 2))\n        _, channels, height, width = x.shape\n        padding_val = 1e20\n        padded_y = F.pad(\n            y, (max_distance, max_distance, max_distance, max_distance),\n            mode='constant',\n            value=padding_val)\n        offset_y = F.unfold(padded_y, kernel_sizes=[height, width]).reshape(\n            [1, channels, height, width, -1])\n        x = x.reshape([1, channels, height, width, 1])\n        minus = x - offset_y\n        dists = paddle.sum(paddle.multiply(minus, minus),\n                           axis=1).reshape([1, height, width,"
        },
        {
            "comment": "This code calculates the distance between a set of 2D points and another point, in a sliding window manner. It handles two cases: when the first point set has been divided into smaller blocks for faster computation, and when it hasn't. The result is stored in dists as a list of distance matrices.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/networks/IntVOS.py\":312-335",
            "content": "                                            -1]).transpose([0, 3, 1, 2])\n        dists = (paddle.nn.functional.sigmoid(dists) - 0.5) * 2\n        dists = F.interpolate(dists,\n                              size=[ori_h, ori_w],\n                              mode='bilinear',\n                              align_corners=True)\n        dists = dists.squeeze(0).transpose([1, 2, 0])\n    else:\n        padding_val = 1e20\n        padded_y = nn.functional.pad(\n            y, (0, 0, max_distance, max_distance, max_distance, max_distance),\n            mode='constant',\n            value=padding_val)\n        height, width, _ = x.shape\n        dists = []\n        for y_start in range(2 * max_distance + 1):\n            y_end = y_start + height\n            y_slice = padded_y[y_start:y_end]\n            for x_start in range(2 * max_distance + 1):\n                x_end = x_start + width\n                offset_y = y_slice[:, x_start:x_end]\n                dist = paddle.sum(paddle.pow((x - offset_y), 2), dim=2)\n                dists.append(dist)"
        },
        {
            "comment": "This code defines the SpatialCorrelationSampler class and a function called cross_correlate. The cross_correlate function takes two tensors, x and y, of shape [height, width, feature_dim] as inputs. It computes the cross correlation of these tensors using an optimized implementation from the SpatialCorrelationSampler class. The output tensor has a shape of [height, width, (2 * max_distance + 1) ** 2].",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/networks/IntVOS.py\":336-364",
            "content": "        dists = paddle.stack(dists, dim=2)\n    return dists\nclass SpatialCorrelationSampler:\n    pass\ndef cross_correlate(x, y, max_distance=9):\n    \"\"\"Efficiently computes the cross correlation of x and y.\n  Optimized implementation using correlation_cost.\n  Note that we do not normalize by the feature dimension.\n  Args:\n    x: Float32 tensor of shape [height, width, feature_dim].\n    y: Float32 tensor of shape [height, width, feature_dim].\n    max_distance: Integer, the maximum distance in pixel coordinates\n      per dimension which is considered to be in the search window.\n  Returns:\n    Float32 tensor of shape [height, width, (2 * max_distance + 1) ** 2].\n    \"\"\"\n    corr_op = SpatialCorrelationSampler(kernel_size=1,\n                                        patch_size=2 * max_distance + 1,\n                                        stride=1,\n                                        dilation_patch=1,\n                                        padding=0)\n    xs = x.transpose(2, 0, 1)\n    xs = paddle.unsqueeze(xs, 0)"
        },
        {
            "comment": "This code is part of the IntVOS model and defines a function that computes nearest neighbor features, allowing only local matches. It takes previous frame embedding, query embedding, previous frame labels, ground truth IDs and maximum distance as input. It transposes and reshapes the tensors before returning the computed correlations.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/networks/IntVOS.py\":365-391",
            "content": "    ys = y.transpose(2, 0, 1)\n    ys = paddle.unsqueeze(ys, 0)\n    corr = corr_op(xs, ys)\n    bs, _, _, hh, ww = corr.shape\n    corr = corr.reshape([bs, -1, hh, ww])\n    corr = paddle.squeeze(corr, 0)\n    corr = corr.transpose(1, 2, 0)\n    return corr\ndef local_previous_frame_nearest_neighbor_features_per_object(\n        prev_frame_embedding,\n        query_embedding,\n        prev_frame_labels,\n        gt_ids,\n        max_distance=12):\n    \"\"\"Computes nearest neighbor features while only allowing local matches.\n  Args:\n    prev_frame_embedding: Tensor of shape [height, width, embedding_dim],\n      the embedding vectors for the last frame.\n    query_embedding: Tensor of shape [height, width, embedding_dim],\n      the embedding vectors for the query frames.\n    prev_frame_labels: Tensor of shape [height, width, 1], the class labels of\n      the previous frame.\n    gt_ids: Int Tensor of shape [n_objs] of the sorted unique ground truth\n      ids in the first frame.\n    max_distance: Integer, the maximum distance allowed for local matching."
        },
        {
            "comment": "This function calculates the nearest neighbor features using local pairwise distances. If MODEL_UNFOLD is set, it pads and unfolds the labels for offset masks generation. Else, it directly creates masks by comparing prev_frame_labels to gt_ids.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/networks/IntVOS.py\":392-420",
            "content": "  Returns:\n    nn_features: A float32 np.array of nearest neighbor features of shape\n      [1, height, width, n_objects, 1].\n    \"\"\"\n    d = local_pairwise_distances2(query_embedding,\n                                  prev_frame_embedding,\n                                  max_distance=max_distance)\n    height, width = prev_frame_embedding.shape[:2]\n    if MODEL_UNFOLD:\n        labels = float_(prev_frame_labels).transpose([2, 0, 1]).unsqueeze(0)\n        padded_labels = F.pad(labels, (\n            2 * max_distance,\n            2 * max_distance,\n            2 * max_distance,\n            2 * max_distance,\n        ))\n        offset_labels = F.unfold(padded_labels,\n                                 kernel_sizes=[height, width],\n                                 strides=[2, 2]).reshape([height, width, -1, 1])\n        offset_masks = paddle.equal(\n            offset_labels,\n            float_(gt_ids).unsqueeze(0).unsqueeze(0).unsqueeze(0))\n    else:\n        masks = paddle.equal(prev_frame_labels,\n                             gt_ids.unsqueeze(0).unsqueeze(0))"
        },
        {
            "comment": "This code applies padding to masks and creates offset masks by slicing the padded masks. It then constructs a 3D tensor of offset masks using Paddle's stack function. It also tiles the input data 'd' along the gt_ids dimension and creates a padding tensor. The code then computes the minimum distance between the tiled input and the masked data, resulting in distances tensor. Finally, it reshapes the distances tensor to have a specific shape and returns it.\nThe _res_block class is a layer that takes an input dimension (in_dim) and an output dimension (out_dim).",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/networks/IntVOS.py\":421-453",
            "content": "        padded_masks = nn.functional.pad(masks, (\n            0,\n            0,\n            max_distance,\n            max_distance,\n            max_distance,\n            max_distance,\n        ))\n        offset_masks = []\n        for y_start in range(2 * max_distance + 1):\n            y_end = y_start + height\n            masks_slice = padded_masks[y_start:y_end]\n            for x_start in range(2 * max_distance + 1):\n                x_end = x_start + width\n                offset_mask = masks_slice[:, x_start:x_end]\n                offset_masks.append(offset_mask)\n        offset_masks = paddle.stack(offset_masks, axis=2)\n    d_tiled = d.unsqueeze(-1).tile((1, 1, 1, gt_ids.shape[0]))\n    pad = paddle.ones_like(d_tiled)\n    d_masked = paddle.where(offset_masks, d_tiled, pad)\n    dists = paddle.min(d_masked, axis=2)\n    dists = dists.reshape([1, height, width, gt_ids.shape[0], 1])\n    return dists\n##############################################################\n#################\nclass _res_block(nn.Layer):\n    def __init__(self, in_dim, out_dim):"
        },
        {
            "comment": "This code defines a Residual Block and an Instance Segmentation Head for the Ma-Net model. The Residual Block consists of two 3x3 convolutions, batch normalization, and ReLU activations, while the IntSegHead layer takes in a specific input dimension for instance segmentation tasks.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/networks/IntVOS.py\":454-485",
            "content": "        super(_res_block, self).__init__()\n        self.conv1 = nn.Conv2D(in_dim,\n                               out_dim,\n                               kernel_size=3,\n                               stride=1,\n                               padding=1)\n        self.relu1 = nn.ReLU()\n        self.bn1 = paddle.nn.BatchNorm2D(out_dim, momentum=cfg.TRAIN_BN_MOM)\n        self.conv2 = nn.Conv2D(out_dim,\n                               out_dim,\n                               kernel_size=3,\n                               stride=1,\n                               padding=1)\n        self.relu2 = nn.ReLU()\n        self.bn2 = paddle.nn.BatchNorm2D(out_dim, momentum=cfg.TRAIN_BN_MOM)\n    def forward(self, x):\n        res = x\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu1(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu2(x)\n        x += res\n        return x\n####################\nclass IntSegHead(nn.Layer):\n    def __init__(self,\n                 in_dim=(cfg.MODEL_SEMANTIC_EMBEDDING_DIM + 3),"
        },
        {
            "comment": "This code defines a neural network class called \"IntSegHead\" for segmentation tasks. It consists of multiple convolutional and batch normalization layers, followed by ReLU activations. The output is passed through another convolutional layer before being fed into the final convolutional layer to produce the result.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/networks/IntVOS.py\":486-512",
            "content": "                 emb_dim=cfg.MODEL_HEAD_EMBEDDING_DIM):\n        super(IntSegHead, self).__init__()\n        self.conv1 = nn.Conv2D(in_dim,\n                               emb_dim,\n                               kernel_size=7,\n                               stride=1,\n                               padding=3)\n        self.bn1 = paddle.nn.BatchNorm2D(emb_dim, momentum=cfg.TRAIN_BN_MOM)\n        self.relu1 = nn.ReLU(True)\n        self.res1 = _res_block(emb_dim, emb_dim)\n        self.res2 = _res_block(emb_dim, emb_dim)\n        self.conv2 = nn.Conv2D(256, emb_dim, kernel_size=3, stride=1, padding=1)\n        self.bn2 = paddle.nn.BatchNorm2D(emb_dim, momentum=cfg.TRAIN_BN_MOM)\n        self.relu2 = nn.ReLU(True)\n        self.conv3 = nn.Conv2D(emb_dim, 1, 1, 1)\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu1(x)\n        x = self.res1(x)\n        x = self.res2(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu2(x)\n        x = self.conv3(x)\n        return x"
        },
        {
            "comment": "This code defines a custom layer \"_split_separable_conv2d\" that performs separable convolution using two consecutive 2D convolutions. It consists of two 2D convolutions separated by Batch Normalization and ReLU activation functions. The first convolution is followed by Batch Normalization and ReLU, while the second convolution is also followed by another Batch Normalization and ReLU. Weights are initialized using Kaiming Normal initialization for both convolutions.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/networks/IntVOS.py\":515-536",
            "content": "class _split_separable_conv2d(nn.Layer):\n    def __init__(self, in_dim, out_dim, kernel_size=7):\n        super(_split_separable_conv2d, self).__init__()\n        self.conv1 = nn.Conv2D(in_dim,\n                               in_dim,\n                               kernel_size=kernel_size,\n                               stride=1,\n                               padding=int((kernel_size - 1) / 2),\n                               groups=in_dim)\n        self.relu1 = nn.ReLU(True)\n        self.bn1 = paddle.nn.BatchNorm2D(in_dim, momentum=cfg.TRAIN_BN_MOM)\n        self.conv2 = nn.Conv2D(in_dim, out_dim, kernel_size=1, stride=1)\n        self.relu2 = nn.ReLU(True)\n        self.bn2 = paddle.nn.BatchNorm2D(out_dim, momentum=cfg.TRAIN_BN_MOM)\n        kaiming_normal_(self.conv1.weight, mode='fan_out', nonlinearity='relu')\n        kaiming_normal_(self.conv2.weight, mode='fan_out', nonlinearity='relu')\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu1(x)\n        x = self.conv2(x)"
        },
        {
            "comment": "The code defines a DynamicSegHead class with four split separable convolutional layers, followed by a 1x1 convolution. It also initializes an IntVOS class that takes in the configuration and feature extractor as parameters. The classes are used for semantic segmentation tasks.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/networks/IntVOS.py\":537-570",
            "content": "        x = self.bn2(x)\n        x = self.relu2(x)\n        return x\nclass DynamicSegHead(nn.Layer):\n    def __init__(self,\n                 in_dim=(cfg.MODEL_SEMANTIC_EMBEDDING_DIM + 3),\n                 embed_dim=cfg.MODEL_HEAD_EMBEDDING_DIM,\n                 kernel_size=1):\n        super(DynamicSegHead, self).__init__()\n        self.layer1 = _split_separable_conv2d(in_dim, embed_dim)\n        self.layer2 = _split_separable_conv2d(embed_dim, embed_dim)\n        self.layer3 = _split_separable_conv2d(embed_dim, embed_dim)\n        self.layer4 = _split_separable_conv2d(embed_dim, embed_dim)\n        self.conv = nn.Conv2D(embed_dim, 1, 1, 1)\n        kaiming_normal_(self.conv.weight, mode='fan_out', nonlinearity='relu')\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.conv(x)\n        return x\n##################\n###############\nclass IntVOS(nn.Layer):\n    def __init__(self, cfg, feature_extracter):\n        super(IntVOS, self).__init__()"
        },
        {
            "comment": "The code initializes components for a network architecture. It creates feature extractors, convolutional layers, batch normalization layers, and ReLU activation functions to process and extract semantic features from input data. These features will be used for tasks such as object detection or image classification.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/networks/IntVOS.py\":571-588",
            "content": "        self.feature_extracter = feature_extracter  ##embedding extractor\n        self.feature_extracter.cls_conv = nn.Sequential()\n        self.feature_extracter.upsample4 = nn.Sequential()\n        self.semantic_embedding = None\n        self.seperate_conv = nn.Conv2D(cfg.MODEL_ASPP_OUTDIM,\n                                       cfg.MODEL_ASPP_OUTDIM,\n                                       kernel_size=3,\n                                       stride=1,\n                                       padding=1,\n                                       groups=cfg.MODEL_ASPP_OUTDIM)\n        self.bn1 = paddle.nn.BatchNorm2D(cfg.MODEL_ASPP_OUTDIM,\n                                         momentum=cfg.TRAIN_BN_MOM)\n        self.relu1 = nn.ReLU(True)\n        self.embedding_conv = nn.Conv2D(cfg.MODEL_ASPP_OUTDIM,\n                                        cfg.MODEL_SEMANTIC_EMBEDDING_DIM, 1, 1)\n        self.relu2 = nn.ReLU(True)\n        self.bn2 = paddle.nn.BatchNorm2D(cfg.MODEL_SEMANTIC_EMBEDDING_DIM,\n                                         momentum=cfg.TRAIN_BN_MOM)"
        },
        {
            "comment": "The code initializes the network's semantic embedding layer, consisting of a sequence of convolutional layers, and applies Kaiming initialization to the weights. It also creates a dynamic segmentation head (seghead) for propagation and an interaction segmentation head based on the configuration flag MODEL_USEIntSeg. The function defines the forward pass for the network, taking in various inputs such as image data, reference labels, and masks.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/networks/IntVOS.py\":589-615",
            "content": "        self.semantic_embedding = nn.Sequential(*[\n            self.seperate_conv, self.bn1, self.relu1, self.embedding_conv,\n            self.bn2, self.relu2\n        ])\n        for m in self.semantic_embedding:\n            if isinstance(m, nn.Conv2D):\n                kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n        self.dynamic_seghead = DynamicSegHead()  # propagation segm head\n        if cfg.MODEL_USEIntSeg:\n            self.inter_seghead = IntSegHead(\n                in_dim=cfg.MODEL_SEMANTIC_EMBEDDING_DIM + 3)\n        else:\n            self.inter_seghead = DynamicSegHead(\n                in_dim=cfg.MODEL_SEMANTIC_EMBEDDING_DIM +\n                2)  # interaction segm head\n    def forward(self,\n                x=None,\n                ref_scribble_label=None,\n                previous_frame_mask=None,\n                normalize_nearest_neighbor_distances=True,\n                use_local_map=True,\n                seq_names=None,\n                gt_ids=None,\n                k_nearest_neighbors=1,"
        },
        {
            "comment": "This code splits the input feature into three parts, then if `global_map_tmp_dic` is None, it passes these parts and other parameters to `prop_seghead()`, which returns a dictionary. If `global_map_tmp_dic` is not None, it also passes `global_map_tmp_dic` as an additional parameter before calling `prop_seghead()`. The function then returns the returned dictionary and updates `global_map_tmp_dic`.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/networks/IntVOS.py\":616-639",
            "content": "                global_map_tmp_dic=None,\n                local_map_dics=None,\n                interaction_num=None,\n                start_annotated_frame=None,\n                frame_num=None):\n        x = self.extract_feature(x)\n        #         print('extract_feature:', x.mean().item())\n        ref_frame_embedding, previous_frame_embedding, current_frame_embedding = paddle.split(\n            x, num_or_sections=3, axis=0)\n        if global_map_tmp_dic is None:\n            dic = self.prop_seghead(\n                ref_frame_embedding, previous_frame_embedding,\n                current_frame_embedding, ref_scribble_label,\n                previous_frame_mask, normalize_nearest_neighbor_distances,\n                use_local_map, seq_names, gt_ids, k_nearest_neighbors,\n                global_map_tmp_dic, local_map_dics, interaction_num,\n                start_annotated_frame, frame_num, self.dynamic_seghead)\n            return dic\n        else:\n            dic, global_map_tmp_dic = self.prop_seghead(\n                ref_frame_embedding, previous_frame_embedding,"
        },
        {
            "comment": "The code defines a function that takes various inputs including frame embeddings, scribble labels, and masked frames. It performs feature extraction using a predefined feature extractor and semantic embedding. The function then returns the extracted features and temporary global map dictionaries.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/networks/IntVOS.py\":640-663",
            "content": "                current_frame_embedding, ref_scribble_label,\n                previous_frame_mask, normalize_nearest_neighbor_distances,\n                use_local_map, seq_names, gt_ids, k_nearest_neighbors,\n                global_map_tmp_dic, local_map_dics, interaction_num,\n                start_annotated_frame, frame_num, self.dynamic_seghead)\n            return dic, global_map_tmp_dic\n    def extract_feature(self, x):\n        x = self.feature_extracter(x)\n        x = self.semantic_embedding(x)\n        return x\n    def prop_seghead(self,\n                     ref_frame_embedding=None,\n                     previous_frame_embedding=None,\n                     current_frame_embedding=None,\n                     ref_scribble_label=None,\n                     previous_frame_mask=None,\n                     normalize_nearest_neighbor_distances=True,\n                     use_local_map=True,\n                     seq_names=None,\n                     gt_ids=None,\n                     k_nearest_neighbors=1,\n                     global_map_tmp_dic=None,"
        },
        {
            "comment": "This code defines a function that takes various inputs and returns feature_embedding, global_match_map, local_match_map, and previous_frame_mask. It performs interpolation on the ref_scribble_label and previous_frame_mask using nearest mode to resize them to the same size as current_frame_embedding.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/networks/IntVOS.py\":664-684",
            "content": "                     local_map_dics=None,\n                     interaction_num=None,\n                     start_annotated_frame=None,\n                     frame_num=None,\n                     dynamic_seghead=None):\n        \"\"\"return: feature_embedding,global_match_map,local_match_map,previous_frame_mask\"\"\"\n        ###############\n        global_map_tmp_dic = global_map_tmp_dic\n        dic_tmp = {}\n        bs, c, h, w = current_frame_embedding.shape\n        if cfg.TEST_MODE:\n            scale_ref_scribble_label = float_(ref_scribble_label)\n        else:\n            scale_ref_scribble_label = paddle.nn.functional.interpolate(\n                float_(ref_scribble_label), size=(h, w), mode='nearest')\n        scale_ref_scribble_label = int_(scale_ref_scribble_label)\n        scale_previous_frame_label = paddle.nn.functional.interpolate(\n            float_(previous_frame_mask), size=(h, w), mode='nearest')\n        #         print(scale_previous_frame_label.sum())  # xx\n        #         print(previous_frame_mask.sum().item())  # xx"
        },
        {
            "comment": "In this code snippet, we see the process of extracting nearest neighbor features per object for each batch of frames. The frames are transposed and labeled before finding the k_nearest_neighbors. These operations are performed within a loop for every frame in the batch (bs).",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/networks/IntVOS.py\":685-703",
            "content": "        scale_previous_frame_label = int_(scale_previous_frame_label)\n        #         print(scale_previous_frame_label.sum().item())  # xx\n        for n in range(bs):\n            seq_current_frame_embedding = current_frame_embedding[n]\n            seq_ref_frame_embedding = ref_frame_embedding[n]\n            seq_prev_frame_embedding = previous_frame_embedding[n]\n            seq_ref_frame_embedding = seq_ref_frame_embedding.transpose(\n                [1, 2, 0])\n            seq_current_frame_embedding = seq_current_frame_embedding.transpose(\n                [1, 2, 0])\n            seq_ref_scribble_label = scale_ref_scribble_label[n].transpose(\n                [1, 2, 0])\n            #########Global Map\n            nn_features_n, ref_obj_ids = nearest_neighbor_features_per_object(\n                reference_embeddings=seq_ref_frame_embedding,\n                query_embeddings=seq_current_frame_embedding,\n                reference_labels=seq_ref_scribble_label,\n                k_nearest_neighbors=k_nearest_neighbors,"
        },
        {
            "comment": "This code segment checks if the current sequence name exists in the global map temporary dictionary. If it does not exist, a paddle.ones_like(nn_features_n) is created and assigned to the dictionary with shape [104, 1, 1, 1, 1]. Then, the code performs a where operation using nn_features_n, comparing it to the global map value for the current sequence name and frame number. If nn_features_n is less than or equal to the global map value, it remains unchanged; otherwise, the global map value overwrites nn_features_n. The last line transposes seq_prev_frame_embedding before continuing with the next chunk of code.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/networks/IntVOS.py\":704-724",
            "content": "                gt_ids=gt_ids[n],\n                n_chunks=10)\n            if normalize_nearest_neighbor_distances:\n                nn_features_n = (paddle.nn.functional.sigmoid(nn_features_n) -\n                                 0.5) * 2\n            if global_map_tmp_dic is not None:  ###when testing, use global map memory\n                if seq_names[n] not in global_map_tmp_dic:\n                    global_map_tmp_dic[seq_names[n]] = paddle.ones_like(\n                        nn_features_n).tile([104, 1, 1, 1, 1])\n                nn_features_n = paddle.where(\n                    nn_features_n <=\n                    global_map_tmp_dic[seq_names[n]][frame_num[n]].unsqueeze(0),\n                    nn_features_n,\n                    global_map_tmp_dic[seq_names[n]][frame_num[n]].unsqueeze(0))\n                global_map_tmp_dic[seq_names[n]][\n                    frame_num[n]] = nn_features_n.detach()[0]\n            #########################Local dist map\n            seq_prev_frame_embedding = seq_prev_frame_embedding.transpose("
        },
        {
            "comment": "The code is performing nearest neighbor feature extraction for previous frames in a video sequence. It checks if the use_local_map flag is set, and depending on its value, either uses local_previous_frame_nearest_neighbor_features_per_object function or nearest_neighbor_features_per_object function to extract features. If use_local_map is true, it takes previous frame embedding, current frame embedding, previous frame labels, reference object IDs and max distance as inputs. Otherwise, it takes previous frame embeddings, current frame embeddings, previous frame labels, k-nearest neighbors, gt_ids (for current iteration), and number of chunks as inputs. The code then assigns the extracted features to prev_frame_nn_features_n variable.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/networks/IntVOS.py\":725-744",
            "content": "                [1, 2, 0])\n            seq_previous_frame_label = scale_previous_frame_label[n].transpose(\n                [1, 2, 0])\n            if use_local_map:\n                prev_frame_nn_features_n = local_previous_frame_nearest_neighbor_features_per_object(\n                    prev_frame_embedding=seq_prev_frame_embedding,\n                    query_embedding=seq_current_frame_embedding,\n                    prev_frame_labels=seq_previous_frame_label,\n                    gt_ids=ref_obj_ids,\n                    max_distance=cfg.MODEL_MAX_LOCAL_DISTANCE)\n            else:\n                prev_frame_nn_features_n, _ = nearest_neighbor_features_per_object(\n                    reference_embeddings=seq_prev_frame_embedding,\n                    query_embeddings=seq_current_frame_embedding,\n                    reference_labels=seq_previous_frame_label,\n                    k_nearest_neighbors=k_nearest_neighbors,\n                    gt_ids=gt_ids[n],\n                    n_chunks=20)\n                prev_frame_nn_features_n = ("
        },
        {
            "comment": "This code segment is checking if the current sequence name is present in the local map dictionaries for distance and temporary maps. If it's not, it creates new entries with zeros initialized. The local map distance value is then updated based on the frame number and interaction number, using the absolute difference from a start annotated frame to determine the distance. This could be used in a video sequence processing context where local map dictionaries store temporary and distance maps for different sequences.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/networks/IntVOS.py\":745-763",
            "content": "                    paddle.nn.functional.sigmoid(prev_frame_nn_features_n) -\n                    0.5) * 2\n#             print(prev_frame_nn_features_n.mean().item(), prev_frame_nn_features_n.shape, interaction_num)  # o\n#############\n            if local_map_dics is not None:  ##When testing, use local map memory\n                local_map_tmp_dic, local_map_dist_dic = local_map_dics\n                if seq_names[n] not in local_map_dist_dic:\n                    print(seq_names[n], 'not in local_map_dist_dic')\n                    local_map_dist_dic[seq_names[n]] = paddle.zeros(104, 9)\n                if seq_names[n] not in local_map_tmp_dic:\n                    print(seq_names[n], 'not in local_map_tmp_dic')\n                    local_map_tmp_dic[seq_names[n]] = paddle.zeros_like(\n                        prev_frame_nn_features_n).unsqueeze(0).tile(\n                            [104, 9, 1, 1, 1, 1])\n                local_map_dist_dic[seq_names[n]][\n                    frame_num[n], interaction_num -\n                    1] = 1.0 / (abs(frame_num[n] - start_annotated_frame)"
        },
        {
            "comment": "This code block appears to be part of a larger function. It seems to store and update the features of previous frames for a given sequence, based on the interaction number and frame number. If the current interaction's distance is greater than the previous one, it updates the previous frame features. The code uses dictionaries to store these features, with the frame number and interaction number as keys. The detach() function seems to remove the feature tensor from the computation graph for memory efficiency, while unsqueeze(0) reshapes the tensor to have a batch dimension.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/networks/IntVOS.py\":764-780",
            "content": "                                )  # bugs fixed.\n                local_map_tmp_dic[seq_names[n]][\n                    frame_num[n],\n                    interaction_num - 1] = prev_frame_nn_features_n.squeeze(\n                        0).detach()  # bugs fixed.\n                if interaction_num == 1:\n                    prev_frame_nn_features_n = local_map_tmp_dic[seq_names[n]][\n                        frame_num[n]][interaction_num - 1]\n                    prev_frame_nn_features_n = prev_frame_nn_features_n.unsqueeze(\n                        0)\n                else:\n                    if local_map_dist_dic[seq_names[n]][frame_num[n]][interaction_num - 1] > \\\n                            local_map_dist_dic[seq_names[n]][frame_num[n]][interaction_num - 2]:\n                        prev_frame_nn_features_n = local_map_tmp_dic[\n                            seq_names[n]][frame_num[n]][interaction_num - 1]\n                        prev_frame_nn_features_n = prev_frame_nn_features_n.unsqueeze(\n                            0)"
        },
        {
            "comment": "This code snippet is part of a neural network model for video object detection. It deals with handling previous frame features, categorizing frames based on the reference object IDs, and concatenating different tensor inputs together. The code checks if the current frame's label matches the reference object ID, unsqueezes and tiles the tensors accordingly, transposes them, and finally concatenates these transformed tensors using `paddle.concat`.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/networks/IntVOS.py\":781-802",
            "content": "                    else:\n                        prev_frame_nn_features_n = local_map_tmp_dic[\n                            seq_names[n]][frame_num[n]][interaction_num - 2]\n                        prev_frame_nn_features_n = prev_frame_nn_features_n.unsqueeze(\n                            0)\n                local_map_dics = (local_map_tmp_dic, local_map_dist_dic)\n            to_cat_previous_frame = (\n                float_(seq_previous_frame_label) == float_(ref_obj_ids)\n            )  # float comparision?\n            to_cat_current_frame_embedding = current_frame_embedding[\n                n].unsqueeze(0).tile((ref_obj_ids.shape[0], 1, 1, 1))\n            to_cat_nn_feature_n = nn_features_n.squeeze(0).transpose(\n                [2, 3, 0, 1])\n            to_cat_previous_frame = float_(\n                to_cat_previous_frame.unsqueeze(-1).transpose([2, 3, 0, 1]))\n            to_cat_prev_frame_nn_feature_n = prev_frame_nn_features_n.squeeze(\n                0).transpose([2, 3, 0, 1])\n            to_cat = paddle.concat("
        },
        {
            "comment": "This function, int_seghead, takes various inputs such as reference frame embedding, scribble label, previous round label etc. It normalizes nearest neighbor distances if specified and returns the dictionary temporary (dic_tmp) containing predicted results for each sequence, along with optional global map temporary dictionary (global_map_tmp_dic) and local map dictionaries (local_map_dics). The interaction number (interaction_num), frame number (frame_num) and list of sequence names (seq_names) are also used.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/networks/IntVOS.py\":803-828",
            "content": "                (to_cat_current_frame_embedding, to_cat_nn_feature_n,\n                 to_cat_prev_frame_nn_feature_n, to_cat_previous_frame), 1)\n            pred_ = dynamic_seghead(to_cat)\n            pred_ = pred_.transpose([1, 0, 2, 3])\n            dic_tmp[seq_names[n]] = pred_\n        if global_map_tmp_dic is None:\n            return dic_tmp\n        else:\n            if local_map_dics is None:\n                return dic_tmp, global_map_tmp_dic\n            else:\n                return dic_tmp, global_map_tmp_dic, local_map_dics\n    def int_seghead(self,\n                    ref_frame_embedding=None,\n                    ref_scribble_label=None,\n                    prev_round_label=None,\n                    normalize_nearest_neighbor_distances=True,\n                    global_map_tmp_dic=None,\n                    local_map_dics=None,\n                    interaction_num=None,\n                    seq_names=None,\n                    gt_ids=None,\n                    k_nearest_neighbors=1,\n                    frame_num=None,"
        },
        {
            "comment": "This code segment is part of the Ma-Net network in PaddleVideo. It interpolates the reference scribble label and previous round label images, assigns ground truth IDs, and performs local distance map calculations on a sequence of frames for a batch of videos.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/networks/IntVOS.py\":829-852",
            "content": "                    first_inter=True):\n        dic_tmp = {}\n        bs, c, h, w = ref_frame_embedding.shape\n        scale_ref_scribble_label = paddle.nn.functional.interpolate(\n            float_(ref_scribble_label), size=(h, w), mode='nearest')\n        scale_ref_scribble_label = int_(scale_ref_scribble_label)\n        if not first_inter:\n            scale_prev_round_label = paddle.nn.functional.interpolate(\n                float_(prev_round_label), size=(h, w), mode='nearest')\n            scale_prev_round_label = int_(scale_prev_round_label)\n        n_chunks = 500\n        for n in range(bs):\n            gt_id = paddle.arange(0, gt_ids[n] + 1)\n            gt_id = int_(gt_id)\n            seq_ref_frame_embedding = ref_frame_embedding[n]\n            ########################Local dist map\n            seq_ref_frame_embedding = paddle.transpose(seq_ref_frame_embedding,\n                                                       [1, 2, 0])\n            seq_ref_scribble_label = paddle.transpose(\n                scale_ref_scribble_label[n], [1, 2, 0])"
        },
        {
            "comment": "This code segment is updating the global and local maps for a given sequence of frames. It first calculates the nearest neighbor features (nn_features_n) using the previous frame embedding, query embedding, previous frame labels, and ground truth IDs. Then it checks if this current sequence name exists in the global map temporary dictionary (global_map_tmp_dic). If not, it initializes a one-tensor for that sequence and tiles it to match the shape of nn_features_n. It then applies a where statement to compare nn_features_n with the global map tensor, selecting either nn_features_n or the global map tensor depending on which is smaller. Finally, it updates the global map temporary dictionary entry for this sequence at the current frame number with the selected tensor from the where statement.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/networks/IntVOS.py\":853-876",
            "content": "            nn_features_n = local_previous_frame_nearest_neighbor_features_per_object(\n                prev_frame_embedding=seq_ref_frame_embedding,\n                query_embedding=seq_ref_frame_embedding,\n                prev_frame_labels=seq_ref_scribble_label,\n                gt_ids=gt_id,\n                max_distance=cfg.MODEL_MAX_LOCAL_DISTANCE)\n            #######\n            ######################Global map update\n            if seq_names[n] not in global_map_tmp_dic:\n                global_map_tmp_dic[seq_names[n]] = paddle.ones_like(\n                    nn_features_n).tile([104, 1, 1, 1, 1])\n            nn_features_n_ = paddle.where(\n                nn_features_n <=\n                global_map_tmp_dic[seq_names[n]][frame_num[n]].unsqueeze(0),\n                nn_features_n,\n                global_map_tmp_dic[seq_names[n]][frame_num[n]].unsqueeze(0))\n            ###\n            ###\n            global_map_tmp_dic[seq_names[n]][\n                frame_num[n]] = nn_features_n_.detach()[0]\n            ##################Local map update"
        },
        {
            "comment": "The code checks if a dictionary of local maps is provided. If so, it retrieves the temporary and distance dictionaries from it. It then updates these dictionaries for the current sequence name (seq_names[n]), adding a 0 to a specific element if the sequence name is not already in the distance dictionary. Finally, it creates embedding tensors for frame and feature comparison and prepares them for concatenation.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/networks/IntVOS.py\":877-896",
            "content": "            if local_map_dics is not None:\n                local_map_tmp_dic, local_map_dist_dic = local_map_dics\n                if seq_names[n] not in local_map_dist_dic:\n                    local_map_dist_dic[seq_names[n]] = paddle.zeros([104, 9])\n                if seq_names[n] not in local_map_tmp_dic:\n                    local_map_tmp_dic[seq_names[n]] = paddle.ones_like(\n                        nn_features_n).unsqueeze(0).tile([104, 9, 1, 1, 1, 1])\n                local_map_dist_dic[seq_names[n]][frame_num[n]][interaction_num -\n                                                               1] = 0\n                local_map_dics = (local_map_tmp_dic, local_map_dist_dic)\n            ##################\n            to_cat_current_frame_embedding = ref_frame_embedding[n].unsqueeze(\n                0).tile((gt_id.shape[0], 1, 1, 1))\n            to_cat_nn_feature_n = nn_features_n.squeeze(0).transpose(\n                [2, 3, 0, 1])\n            to_cat_scribble_mask_to_cat = (\n                float_(seq_ref_scribble_label) == float_(gt_id)"
        },
        {
            "comment": "This code is performing a series of operations on tensors to create the 'to_cat' tensor for use in the model. It checks if it's the first iteration and adjusts the previous round label accordingly. Then, it concatenates three different tensor inputs along the 1st axis (channel dimension) and passes the result through a segmentation head network to get the final prediction 'pred_'. This code seems to be part of a larger neural network for video object segmentation.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/networks/IntVOS.py\":897-920",
            "content": "            )  # float comparision?\n            to_cat_scribble_mask_to_cat = float_(\n                to_cat_scribble_mask_to_cat.unsqueeze(-1).transpose(\n                    [2, 3, 0, 1]))\n            if not first_inter:\n                seq_prev_round_label = scale_prev_round_label[n].transpose(\n                    [1, 2, 0])\n                to_cat_prev_round_to_cat = (\n                    float_(seq_prev_round_label) == float_(gt_id)\n                )  # float comparision?\n                to_cat_prev_round_to_cat = float_(\n                    to_cat_prev_round_to_cat.unsqueeze(-1).transpose(\n                        [2, 3, 0, 1]))\n            else:\n                to_cat_prev_round_to_cat = paddle.zeros_like(\n                    to_cat_scribble_mask_to_cat)\n                to_cat_prev_round_to_cat[0] = 1.\n            to_cat = paddle.concat(\n                (to_cat_current_frame_embedding, to_cat_scribble_mask_to_cat,\n                 to_cat_prev_round_to_cat), 1)\n            pred_ = self.inter_seghead(to_cat)"
        },
        {
            "comment": "This code is transposing the tensor 'pred_' and storing it in 'dic_tmp' with corresponding sequence name as key. It then checks if 'local\\_map\\_dics' is None, and returns 'dic\\_tmp' or returns both 'dic\\_tmp' and 'local\\_map\\_dics'.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/networks/IntVOS.py\":921-926",
            "content": "            pred_ = pred_.transpose([1, 0, 2, 3])\n            dic_tmp[seq_names[n]] = pred_\n        if local_map_dics is None:\n            return dic_tmp\n        else:\n            return dic_tmp, local_map_dics"
        }
    ]
}