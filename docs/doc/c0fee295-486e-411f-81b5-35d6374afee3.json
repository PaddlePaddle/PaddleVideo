{
    "summary": "This code uses PaddleVideo for video segment matching, ASPP-based deep learning models for object size determination and feature extraction, handles padding, computes distances, prepares data, performs feature selection and masking, and utilizes parallel processing in PaddlePaddle.",
    "details": [
        {
            "comment": "This code defines a function \"foreground2background\" that takes distance (dis) and object number (obj_num) as inputs. It returns the background distances for each foreground object when obj_num is greater than 1 by concatenating the unsqueezed distance of other objects along axis 1.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/framework/segment/utils.py\":0-30",
            "content": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport numpy as np\nimport paddle\nimport paddle.nn as nn\nimport paddle.nn.functional as F\ndef foreground2background(dis, obj_num):\n    if obj_num == 1:\n        return dis\n    bg_dis = []\n    for i in range(obj_num):\n        obj_back = []\n        for j in range(obj_num):\n            if i == j:\n                continue\n            obj_back.append(paddle.unsqueeze(dis[j], axis=0))\n        obj_back = paddle.concat(x=obj_back, axis=1)"
        },
        {
            "comment": "This function calculates the pairwise squared L2 distances between tensors x and y, returns them in a matrix d. The function takes x and y as input, which are [n, feature_dim] and [m, feature_dim] respectively. It then performs matrix calculations to compute the pairwise distances and returns d, which is of size [n, m].",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/framework/segment/utils.py\":31-67",
            "content": "        obj_back = paddle.min(x=obj_back, axis=1, keepdim=True)\n        bg_dis.append(obj_back)\n    bg_dis = paddle.concat(x=bg_dis, axis=0)\n    return bg_dis\nWRONG_LABEL_PADDING_DISTANCE = 5e4\n#GLOBAL_DIST_MAP\ndef _pairwise_distances(x, x2, y, y2):\n    \"\"\"\n    Computes pairwise squared l2 distances between tensors x and y.\n    Args:\n    x: [n, feature_dim].\n    y: [m, feature_dim].\n    Returns:\n    d: [n, m].\n    \"\"\"\n    xs = x2\n    ys = y2\n    xs = paddle.unsqueeze(xs, axis=1)\n    ys = paddle.unsqueeze(ys, axis=0)\n    d = xs + ys - 2. * paddle.matmul(x, y, transpose_y=True)\n    return d\ndef _flattened_pairwise_distances(reference_embeddings, ref_square,\n                                  query_embeddings, query_square):\n    \"\"\"\n    Calculates flattened tensor of pairwise distances between ref and query.\n    Args:\n        reference_embeddings: [..., embedding_dim],\n          the embedding vectors for the reference frame\n        query_embeddings: [..., embedding_dim],\n          the embedding vectors for the query frames."
        },
        {
            "comment": "This code computes pairwise distances between query and reference embeddings, then extracts features for each object using nearest neighbor attention. It takes embedding vectors for the reference frame and query frames as input, along with a mask for pixels not used for matching. The output is a tensor of nearest neighbor features shape [m_chunk, n_objects, n_chunk]. The code also checks the dtype of reference_embeddings to handle float16 data.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/framework/segment/utils.py\":68-91",
            "content": "    Returns:\n        dists: [reference_embeddings.size / embedding_dim, query_embeddings.size / embedding_dim]\n    \"\"\"\n    dists = _pairwise_distances(query_embeddings, query_square,\n                                reference_embeddings, ref_square)\n    return dists\ndef _nn_features_per_object_for_chunk(reference_embeddings, ref_square,\n                                      query_embeddings, query_square,\n                                      wrong_label_mask):\n    \"\"\"Extracts features for each object using nearest neighbor attention.\n    Args:\n        reference_embeddings: [n_chunk, embedding_dim],\n          the embedding vectors for the reference frame.\n        query_embeddings: [m_chunk, embedding_dim],\n          the embedding vectors for the query frames.\n        wrong_label_mask: [n_objects, n_chunk],\n          the mask for pixels not used for matching.\n    Returns:\n        nn_features: A float32 tensor of nearest neighbor features of shape\n          [m_chunk, n_objects, n_chunk].\n    \"\"\"\n    if reference_embeddings.dtype == \"float16\":"
        },
        {
            "comment": "This function calculates the nearest neighbor features per object in chunks to save memory. It takes reference embeddings, query embeddings, and reference labels as inputs. The function first casts the wrong_label_mask based on its type (float16 or float32). Then it calculates pairwise distances between reference and query embeddings. Distances for incorrect matches are set to a specific padding distance using wrong_label_mask. Finally, it returns the features by taking the minimum value across chunks in each dimension.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/framework/segment/utils.py\":92-112",
            "content": "        wrong_label_mask = paddle.cast(wrong_label_mask, dtype=\"float16\")\n    else:\n        wrong_label_mask = paddle.cast(wrong_label_mask, dtype=\"float32\")\n    reference_embeddings_key = reference_embeddings\n    query_embeddings_key = query_embeddings\n    dists = _flattened_pairwise_distances(reference_embeddings_key, ref_square,\n                                          query_embeddings_key, query_square)\n    dists = (paddle.unsqueeze(dists, axis=1) +\n             paddle.unsqueeze(wrong_label_mask, axis=0) *\n             WRONG_LABEL_PADDING_DISTANCE)\n    features = paddle.min(dists, axis=2, keepdim=True)\n    return features\ndef _nearest_neighbor_features_per_object_in_chunks(reference_embeddings_flat,\n                                                    query_embeddings_flat,\n                                                    reference_labels_flat,\n                                                    n_chunks):\n    \"\"\"Calculates the nearest neighbor features per object in chunks to save mem.\n    Uses chunking to bound the memory use."
        },
        {
            "comment": "This function computes the features for a set of query frames against a reference frame. It takes in embedding vectors for reference and query frames, as well as their respective class labels. The function uses chunking to handle large feature dimensions, with the number of chunks adjustable by the user. It returns a tensor of shape [m, n_objects, n] which represents the features for each query frame.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/framework/segment/utils.py\":113-137",
            "content": "    Args:\n        reference_embeddings_flat: [n, embedding_dim],\n          the embedding vectors for the reference frame.\n        query_embeddings_flat: [m, embedding_dim],\n          the embedding vectors for the query frames.\n        reference_labels_flat: [n, n_objects],\n          the class labels of the reference frame.\n        n_chunks: Integer, the number of chunks to use to save memory\n          (set to 1 for no chunking).\n    Returns:\n        nn_features: [m, n_objects, n].\n    \"\"\"\n    feature_dim, embedding_dim = query_embeddings_flat.shape\n    chunk_size = int(np.ceil(float(feature_dim) / n_chunks))\n    wrong_label_mask = reference_labels_flat < 0.1\n    wrong_label_mask = paddle.transpose(x=wrong_label_mask, perm=[1, 0])\n    ref_square = paddle.sum(paddle.pow(reference_embeddings_flat, 2), axis=1)\n    query_square = paddle.sum(paddle.pow(query_embeddings_flat, 2), axis=1)\n    all_features = []\n    for n in range(n_chunks):\n        if n_chunks == 1:\n            query_embeddings_flat_chunk = query_embeddings_flat"
        },
        {
            "comment": "This function is performing global matching on query embeddings and reference embeddings. It breaks down the embeddings into chunks, calculates features for each chunk, and concatenates these features to get the final nn_features. The number of chunks is determined by n_chunks, which default to 100. If n_chunks = 1, it returns the features from the only chunk.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/framework/segment/utils.py\":138-166",
            "content": "            query_square_chunk = query_square\n            chunk_start = 0\n        else:\n            chunk_start = n * chunk_size\n            chunk_end = (n + 1) * chunk_size\n            query_square_chunk = query_square[chunk_start:chunk_end]\n            if query_square_chunk.shape[0] == 0:\n                continue\n            query_embeddings_flat_chunk = query_embeddings_flat[\n                chunk_start:chunk_end]\n        features = _nn_features_per_object_for_chunk(\n            reference_embeddings_flat, ref_square, query_embeddings_flat_chunk,\n            query_square_chunk, wrong_label_mask)\n        all_features.append(features)\n    if n_chunks == 1:\n        nn_features = all_features[0]\n    else:\n        nn_features = paddle.concat(all_features, axis=0)\n    return nn_features\ndef global_matching(reference_embeddings,\n                    query_embeddings,\n                    reference_labels,\n                    n_chunks=100,\n                    dis_bias=0.,\n                    ori_size=None,\n                    atrous_rate=1,"
        },
        {
            "comment": "This code calculates the distance to the nearest neighbor per object for query_embeddings and reference_embeddings, given class labels and other parameters. It uses chunks to save memory and takes into account the atrous rate of reference_embeddings.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/framework/segment/utils.py\":167-185",
            "content": "                    use_float16=True,\n                    atrous_obj_pixel_num=0):\n    \"\"\"\n    Calculates the distance to the nearest neighbor per object.\n    For every pixel of query_embeddings calculate the distance to the\n    nearest neighbor in the (possibly subsampled) reference_embeddings per object.\n    Args:\n        reference_embeddings: [height, width, embedding_dim],\n          the embedding vectors for the reference frame.\n        query_embeddings: [height, width,\n          embedding_dim], the embedding vectors for the query frames.\n        reference_labels: [height, width, obj_nums],\n          the class labels of the reference frame.\n        n_chunks: Integer, the number of chunks to use to save memory\n          (set to 1 for no chunking).\n        dis_bias: [n_objects], foreground and background bias\n        ori_size: (ori_height, ori_width),\n          the original spatial size. If \"None\", (ori_height, ori_width) = (height, width).\n        atrous_rate: Integer, the atrous rate of reference_embeddings."
        },
        {
            "comment": "This code snippet calculates and pads the selected points for spatial pyramid pooling in a segmentation model. It checks if float16 is used, then prepares padding based on the atrous rate. The resulting tensor of selected points is reshaped to match the input shape before returning it.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/framework/segment/utils.py\":186-208",
            "content": "        use_float16: Bool, if \"True\", use float16 type for matching.\n    Returns:\n        nn_features: [1, ori_height, ori_width, n_objects, feature_dim].\n    \"\"\"\n    assert (reference_embeddings.shape[:2] == reference_labels.shape[:2])\n    if use_float16:\n        query_embeddings = paddle.cast(query_embeddings, dtype=\"float16\")\n        reference_embeddings = paddle.cast(reference_embeddings,\n                                           dtype=\"float16\")\n    h, w, embedding_dim = query_embeddings.shape\n    obj_nums = reference_labels.shape[2]\n    if atrous_rate > 1:\n        h_pad = (atrous_rate - h % atrous_rate) % atrous_rate\n        w_pad = (atrous_rate - w % atrous_rate) % atrous_rate\n        selected_points = paddle.zeros([h + h_pad, w + w_pad])\n        selected_points = selected_points.view(\n            (h + h_pad) // atrous_rate, atrous_rate, (w + w_pad) // atrous_rate,\n            atrous_rate)\n        selected_points[:, 0, :, 0] = 1.\n        selected_points = paddle.reshape(selected_points,\n                                         [h + h_pad, w + w_pad, 1])[:h, :w]"
        },
        {
            "comment": "The code is implementing a segmentation method in the PaddleVideo library. It first determines if an object is big or small based on the sum of reference labels. Then, it reshapes the reference embeddings and labels for further processing. It checks if any reference labels are present and returns default values if none are found.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/framework/segment/utils.py\":209-229",
            "content": "        is_big_obj = (paddle.sum(\n            reference_labels,\n            axis=(0, 1))) > (atrous_obj_pixel_num * atrous_rate**2)\n        reference_labels[:, :,\n                         is_big_obj] = reference_labels[:, :,\n                                                        is_big_obj] * selected_points\n    reference_embeddings_flat = paddle.reshape(reference_embeddings,\n                                               [-1, embedding_dim])\n    reference_labels_flat = paddle.reshape(reference_labels, [-1, obj_nums])\n    query_embeddings_flat = paddle.reshape(query_embeddings,\n                                           [-1, embedding_dim])\n    all_ref_fg = paddle.sum(reference_labels_flat, axis=1, keepdim=True) > 0.9\n    reference_labels_flat = paddle.reshape(\n        paddle.masked_select(reference_labels_flat,\n                             paddle.expand(all_ref_fg, [-1, obj_nums])),\n        [-1, obj_nums])\n    if reference_labels_flat.shape[0] == 0:\n        return paddle.ones([1, h, w, obj_nums, 1])\n    reference_embeddings_flat = paddle.reshape("
        },
        {
            "comment": "This function performs nearest neighbor feature extraction for video segment matching using reference and query embeddings, reference labels, and other parameters such as number of chunks, displacement bias, original size, and atrous rate. It returns the normalized nearest neighbor features in a reshaped format.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/framework/segment/utils.py\":230-256",
            "content": "        paddle.masked_select(reference_embeddings_flat,\n                             paddle.expand(all_ref_fg, [-1, embedding_dim])),\n        [-1, embedding_dim])\n    nn_features = _nearest_neighbor_features_per_object_in_chunks(\n        reference_embeddings_flat, query_embeddings_flat, reference_labels_flat,\n        n_chunks)\n    nn_features_reshape = paddle.reshape(nn_features, [1, h, w, obj_nums, 1])\n    nn_features_reshape = (\n        F.sigmoid(nn_features_reshape +\n                  paddle.reshape(dis_bias, [1, 1, 1, -1, 1])) - 0.5) * 2\n    #TODO: ori_size is not None\n    if use_float16:\n        nn_features_reshape = paddle.cast(nn_features_reshape, dtype=\"float32\")\n    return nn_features_reshape\ndef global_matching_for_eval(all_reference_embeddings,\n                             query_embeddings,\n                             all_reference_labels,\n                             n_chunks=20,\n                             dis_bias=0.,\n                             ori_size=None,\n                             atrous_rate=1,"
        },
        {
            "comment": "This code calculates the distance to the nearest neighbor per object for query embeddings in a list of reference embeddings, considering potentially subsampled frames. It takes query_embeddings of size [n_query_images, height, width, embedding_dim], all_reference_embeddings and all_reference_labels lists with size [height, width, obj_nums] each, n_chunks, dis_bias, and ori_size as input arguments.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/framework/segment/utils.py\":257-276",
            "content": "                             use_float16=True,\n                             atrous_obj_pixel_num=0):\n    \"\"\"\n    Calculates the distance to the nearest neighbor per object.\n    For every pixel of query_embeddings calculate the distance to the\n    nearest neighbor in the (possibly subsampled) reference_embeddings per object.\n    Args:\n        all_reference_embeddings: A list of reference_embeddings,\n          each with size [height, width, embedding_dim],\n          the embedding vectors for the reference frame.\n        query_embeddings: [n_query_images, height, width,\n          embedding_dim], the embedding vectors for the query frames.\n        all_reference_labels: A list of reference_labels,\n          each with size [height, width, obj_nums],\n          the class labels of the reference frame.\n        n_chunks: Integer, the number of chunks to use to save memory\n          (set to 1 for no chunking).\n        dis_bias: [n_objects], foreground and background bias\n        ori_size: (ori_height, ori_width),\n          the original spatial size. If \"None\", (ori_height, ori_width) = (height, width)."
        },
        {
            "comment": "This function is responsible for creating a tensor of reference embeddings and labels for a given set of query embeddings, based on the provided atrous rate. The function first calculates the shape of the input tensors, then initializes empty lists for flat versions of reference embeddings and labels. It then determines the padding needed to match the atrous rate, creates a selection matrix with ones at the selected points, and reshapes it according to the atrous rate. Finally, it prepares the tensor for matching by flattening the reference embeddings and labels lists.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/framework/segment/utils.py\":277-298",
            "content": "        atrous_rate: Integer, the atrous rate of reference_embeddings.\n        use_float16: Bool, if \"True\", use float16 type for matching.\n    Returns:\n        nn_features: [n_query_images, ori_height, ori_width, n_objects, feature_dim].\n    \"\"\"\n    h, w, embedding_dim = query_embeddings.shape\n    obj_nums = all_reference_labels[0].shape[2]\n    all_reference_embeddings_flat = []\n    all_reference_labels_flat = []\n    ref_num = len(all_reference_labels)\n    n_chunks *= ref_num\n    if atrous_obj_pixel_num > 0:\n        if atrous_rate > 1:\n            h_pad = (atrous_rate - h % atrous_rate) % atrous_rate\n            w_pad = (atrous_rate - w % atrous_rate) % atrous_rate\n            selected_points = paddle.zeros([h + h_pad, w + w_pad])\n            selected_points = paddle.reshape(\n                selected_points, [(h + h_pad) // atrous_rate, atrous_rate,\n                                  (w + w_pad) // atrous_rate, atrous_rate])\n            selected_points[:, 0, :, 0] = 1.\n            selected_points = paddle.reshape(selected_points,"
        },
        {
            "comment": "This code segment appears to be a part of image segmentation or object detection algorithm. It processes reference embeddings and labels, potentially for each detected object in the image. The atrous rate determines if an object is big or small, with larger objects being processed separately by multiplying selected points to corresponding regions in reference_labels. The embeddings are flattened into 1D arrays, as well as reference_labels.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/framework/segment/utils.py\":299-317",
            "content": "                                             [h + h_pad, w + w_pad, 1])[:h, :w]\n        for reference_embeddings, reference_labels, idx in zip(\n                all_reference_embeddings, all_reference_labels, range(ref_num)):\n            if atrous_rate > 1:\n                is_big_obj = paddle.sum(\n                    reference_labels,\n                    axis=(0, 1)) > (atrous_obj_pixel_num * atrous_rate**2)\n                is_big_obj = list(np.array(is_big_obj))\n                for j in range(len(is_big_obj)):\n                    if is_big_obj[j] == True:\n                        reference_labels[:, :, j:j +\n                                         1] = reference_labels[:, :, j:j +\n                                                               1] * selected_points\n            reference_embeddings_flat = paddle.reshape(reference_embeddings,\n                                                       [-1, embedding_dim])\n            reference_labels_flat = paddle.reshape(reference_labels,\n                                                   [-1, obj_nums])"
        },
        {
            "comment": "The code concatenates reference embeddings and labels, then pads them if necessary based on the atrous rate. If there is only one reference, it directly selects the first item from all_reference_embeddings and all_reference_labels lists.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/framework/segment/utils.py\":319-337",
            "content": "            all_reference_embeddings_flat.append(reference_embeddings_flat)\n            all_reference_labels_flat.append(reference_labels_flat)\n        reference_embeddings_flat = paddle.concat(\n            x=all_reference_embeddings_flat, axis=0)\n        reference_labels_flat = paddle.concat(x=all_reference_labels_flat,\n                                              axis=0)\n    else:\n        if ref_num == 1:\n            reference_embeddings, reference_labels = all_reference_embeddings[\n                0], all_reference_labels[0]\n            if atrous_rate > 1:\n                h_pad = (atrous_rate - h % atrous_rate) % atrous_rate\n                w_pad = (atrous_rate - w % atrous_rate) % atrous_rate\n                if h_pad > 0 or w_pad > 0:\n                    reference_embeddings = F.pad(reference_embeddings,\n                                                 [0, h_pad, 0, w_pad, 0, 0])\n                    reference_labels = F.pad(reference_labels,\n                                             [0, h_pad, 0, w_pad, 0, 0])"
        },
        {
            "comment": "This code reshapes the reference embeddings and labels to match a specific pattern, then flattens the reference embeddings while preserving their data type and shape.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/framework/segment/utils.py\":338-355",
            "content": "                reference_embeddings = paddle.reshape(\n                    reference_embeddings,\n                    [(h + h_pad) // atrous_rate, atrous_rate,\n                     (w + w_pad) // atrous_rate, atrous_rate, 32])\n                reference_labels = paddle.reshape(\n                    reference_labels,\n                    [(h + h_pad) // atrous_rate, atrous_rate,\n                     (w + w_pad) // atrous_rate, atrous_rate, -1])\n                reference_embeddings = paddle.reshape(\n                    reference_embeddings[:, 0, :, 0, :],\n                    reference_embeddings[:, 0, :, 0, :].shape)\n                reference_labels = paddle.reshape(\n                    reference_labels[:, 0, :, 0, :],\n                    reference_labels[:, 0, :, 0, :].shape)\n            reference_embeddings_flat = paddle.reshape(reference_embeddings,\n                                                       [-1, embedding_dim])\n            reference_labels_flat = paddle.reshape(reference_labels,\n                                                   [-1, obj_nums])"
        },
        {
            "comment": "This code segment handles the case where atrous_rate is greater than 1. It pads reference embeddings and labels if needed, then reshapes them to have a shape compatible with Atrous Spatial Pyramid Pooling (ASPP) in deep learning models for image classification or detection tasks.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/framework/segment/utils.py\":356-374",
            "content": "        else:\n            for reference_embeddings, reference_labels, idx in zip(\n                    all_reference_embeddings, all_reference_labels,\n                    range(ref_num)):\n                if atrous_rate > 1:\n                    h_pad = (atrous_rate - h % atrous_rate) % atrous_rate\n                    w_pad = (atrous_rate - w % atrous_rate) % atrous_rate\n                    if h_pad > 0 or w_pad > 0:\n                        reference_embeddings = F.pad(reference_embeddings,\n                                                     [0, h_pad, 0, w_pad, 0, 0])\n                        reference_labels = F.pad(reference_labels,\n                                                 [0, h_pad, 0, w_pad, 0, 0])\n                    reference_embeddings = paddle.reshape(\n                        reference_embeddings,\n                        [(h + h_pad) // atrous_rate, atrous_rate,\n                         (w + w_pad) // atrous_rate, atrous_rate, -1])\n                    reference_labels = paddle.reshape(\n                        reference_labels,"
        },
        {
            "comment": "This code reshapes the reference embeddings and labels into a flattened format, appends them to lists, and then concatenates all the flattened reference embeddings along axis 0. This is likely for use in a deep learning model that requires the data in this specific format for training or prediction.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/framework/segment/utils.py\":375-393",
            "content": "                        [(h + h_pad) // atrous_rate, atrous_rate,\n                         (w + w_pad) // atrous_rate, atrous_rate, -1])\n                    reference_embeddings = paddle.reshape(\n                        reference_embeddings[:, 0, :, 0, :],\n                        reference_embeddings[:, 0, :, 0, :].shape)\n                    reference_labels = paddle.reshape(\n                        reference_labels[:, 0, :, 0, :],\n                        reference_labels[:, 0, :, 0, :].shape)\n                reference_embeddings_flat = paddle.reshape(\n                    reference_embeddings, [-1, embedding_dim])\n                reference_labels_flat = paddle.reshape(reference_labels,\n                                                       [-1, obj_nums])\n                all_reference_embeddings_flat.append(reference_embeddings_flat)\n                all_reference_labels_flat.append(reference_labels_flat)\n            reference_embeddings_flat = paddle.concat(\n                all_reference_embeddings_flat, axis=0)"
        },
        {
            "comment": "This code segment performs feature selection and reshaping of query and reference embeddings for the segment matching process. It concatenates all reference labels, flattens the query embeddings, masks the selected reference labels and embeddings based on a threshold, and finally reshapes them before returning a tensor of ones if no references are found or casting the embeddings to float16 datatype if specified.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/framework/segment/utils.py\":394-414",
            "content": "            reference_labels_flat = paddle.concat(all_reference_labels_flat,\n                                                  axis=0)\n    query_embeddings_flat = paddle.reshape(query_embeddings,\n                                           [-1, embedding_dim])\n    all_ref_fg = paddle.sum(reference_labels_flat, axis=1, keepdim=True) > 0.9\n    reference_labels_flat = paddle.reshape(\n        paddle.masked_select(reference_labels_flat,\n                             paddle.expand(all_ref_fg, [-1, obj_nums])),\n        [-1, obj_nums])\n    if reference_labels_flat.shape[0] == 0:\n        return paddle.ones([1, h, w, obj_nums, 1])\n    reference_embeddings_flat = paddle.reshape(\n        paddle.masked_select(reference_embeddings_flat,\n                             paddle.expand(all_ref_fg, [-1, embedding_dim])),\n        [-1, embedding_dim])\n    if use_float16:\n        query_embeddings_flat = paddle.cast(query_embeddings_flat,\n                                            dtype=\"float16\")\n        reference_embeddings_flat = paddle.cast(reference_embeddings_flat,"
        },
        {
            "comment": "This code calculates pairwise squared L2 distances using a local search window, and then computes the nearest neighbor features for each object in image chunks. The result is reshaped into an appropriate format and can be used for further processing or analysis.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/framework/segment/utils.py\":415-441",
            "content": "                                                dtype=\"float16\")\n    nn_features = _nearest_neighbor_features_per_object_in_chunks(\n        reference_embeddings_flat, query_embeddings_flat, reference_labels_flat,\n        n_chunks)\n    nn_features_reshape = paddle.reshape(nn_features, [1, h, w, obj_nums, 1])\n    nn_features_reshape = (\n        F.sigmoid(nn_features_reshape +\n                  paddle.reshape(dis_bias, [1, 1, 1, -1, 1])) - 0.5) * 2\n    # TODO: ori_size is not None\n    if use_float16:\n        nn_features_reshape = paddle.cast(nn_features_reshape, dtype=\"float32\")\n    return nn_features_reshape\n#LOCAL_DIST_MAP\ndef local_pairwise_distances(x,\n                             y,\n                             max_distance=9,\n                             atrous_rate=1,\n                             allow_downsample=False):\n    \"\"\"Computes pairwise squared l2 distances using a local search window.\n        Use for-loop for saving memory.\n    Args:\n        x: Float32 tensor of shape [height, width, feature_dim]."
        },
        {
            "comment": "This function takes in a tensor 'x' and 'y', along with parameters such as max_distance, atrous_rate, and allow_downsample. It returns a distances tensor of shape [height, width, (2 * max_distance + 1) ** 2]. If downsampling is allowed, the original height and width are saved and the tensors 'x' and 'y' are reshaped. Then, using bilinear interpolation, 'x' and 'y' are downsampled to half their size while preserving values at borders.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/framework/segment/utils.py\":442-463",
            "content": "        y: Float32 tensor of shape [height, width, feature_dim].\n        max_distance: Integer, the maximum distance in pixel coordinates\n          per dimension which is considered to be in the search window.\n        atrous_rate: Integer, the atrous rate of local matching.\n        allow_downsample: Bool, if \"True\", downsample x and y\n          with a stride of 2.\n    Returns:\n        Float32 distances tensor of shape [height, width, (2 * max_distance + 1) ** 2].\n    \"\"\"\n    if allow_downsample:\n        ori_height = x.shape[0]\n        ori_width = x.shape[1]\n        x = paddle.unsqueeze(paddle.transpose(x, [2, 0, 1]), axis=0)\n        y = paddle.unsqueeze(paddle.transpose(y, [2, 0, 1]), axis=0)\n        down_size = (int(ori_height / 2) + 1, int(ori_width / 2) + 1)\n        x = F.interpolate(x,\n                          size=down_size,\n                          mode='bilinear',\n                          align_corners=True)\n        y = F.interpolate(y,\n                          size=down_size,\n                          mode='bilinear',"
        },
        {
            "comment": "This code computes local pairwise distances between the input tensors x and y, accounting for atrous dilation. It first pads y with wrong label padding distance to match the size of x. Then it loops through the range of possible offsets for each pixel and calculates the sum of squared differences between the current pixel and all potential offsets in y. These distances are then stacked along the channel axis before being returned.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/framework/segment/utils.py\":464-491",
            "content": "                          align_corners=True)\n        x = paddle.unsqueeze(paddle.transpose(x, [1, 2, 0]), axis=0)\n        y = paddle.unsqueeze(paddle.transpose(y, [1, 2, 0]), axis=0)\n    pad_max_distance = max_distance - max_distance % atrous_rate\n    # no change pad\n    padded_y = F.pad(y, (0, 0, pad_max_distance, pad_max_distance,\n                         pad_max_distance, pad_max_distance),\n                     value=WRONG_LABEL_PADDING_DISTANCE)\n    height, width, _ = x.shape\n    dists = []\n    for y in range(2 * pad_max_distance // atrous_rate + 1):\n        y_start = y * atrous_rate\n        y_end = y_start + height\n        y_slice = padded_y[y_start:y_end]\n        for x in range(2 * max_distance + 1):\n            x_start = x * atrous_rate\n            x_end = x_start + width\n            offset_y = y_slice[:, x_start:x_end]\n            dist = paddle.sum(paddle.pow((x - offset_y), 2), axis=2)\n            dists.append(dist)\n    dists = paddle.stack(dists, axis=2)\n    return dists\ndef local_pairwise_distances_parallel(x,"
        },
        {
            "comment": "This function computes pairwise squared L2 distances using a local search window. It takes two tensors x and y of shape [height, width, feature_dim] as input. The maximum distance (max\\_distance) in pixel coordinates per dimension is considered in the search window. Atrous rate determines the local matching rate. If downsampling is allowed, the function downsamples the tensors with a stride of 2. It returns a float32 distances tensor of shape [height, width, (2 * max_distance + 1) ** 2].",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/framework/segment/utils.py\":492-512",
            "content": "                                      y,\n                                      max_distance=9,\n                                      atrous_rate=1,\n                                      allow_downsample=True):\n    \"\"\"Computes pairwise squared l2 distances using a local search window.\n    Args:\n        x: Float32 tensor of shape [height, width, feature_dim].\n        y: Float32 tensor of shape [height, width, feature_dim].\n        max_distance: Integer, the maximum distance in pixel coordinates\n          per dimension which is considered to be in the search window.\n        atrous_rate: Integer, the atrous rate of local matching.\n        allow_downsample: Bool, if \"True\", downsample x and y\n          with a stride of 2.\n    Returns:\n        Float32 distances tensor of shape [height, width, (2 * max_distance + 1) ** 2].\n    \"\"\"\n    ori_height, ori_width, _ = x.shape\n    x = paddle.unsqueeze(paddle.transpose(x, [2, 0, 1]), axis=0)\n    y = paddle.unsqueeze(paddle.transpose(y, [2, 0, 1]), axis=0)\n    if allow_downsample:"
        },
        {
            "comment": "The code resizes the input tensors x and y to a downsized version of half the original size using bilinear interpolation. It then calculates the squared values for x and y, reshapes them, pads the tensors with WRONG_LABEL_PADDING_DISTANCE to match the atrous rate, and assigns them to padded_y and padded_y2 variables.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/framework/segment/utils.py\":513-536",
            "content": "        down_size = (int(ori_height / 2) + 1, int(ori_width / 2) + 1)\n        x = F.interpolate(x,\n                          size=down_size,\n                          mode='bilinear',\n                          align_corners=True)\n        y = F.interpolate(y,\n                          size=down_size,\n                          mode='bilinear',\n                          align_corners=True)\n    _, channels, height, width = x.shape\n    x2 = paddle.reshape(paddle.sum(paddle.pow(x, 2), axis=1),\n                        [height, width, 1])\n    y2 = paddle.reshape(paddle.sum(paddle.pow(y, 2), axis=1),\n                        [1, 1, height, width])\n    pad_max_distance = max_distance - max_distance % atrous_rate\n    # no change pad\n    padded_y = F.pad(y, (pad_max_distance, pad_max_distance, pad_max_distance,\n                         pad_max_distance))\n    padded_y2 = F.pad(y2, (pad_max_distance, pad_max_distance, pad_max_distance,\n                           pad_max_distance),\n                      value=WRONG_LABEL_PADDING_DISTANCE)"
        },
        {
            "comment": "This code snippet calculates the distance between embeddings of frames to measure similarity. It takes in two frame embeddings, their corresponding labels, and several optional parameters, including an atomic number for local distances, a size parameter, and whether to allow downsampling. The function uses Paddle's unfold operation to reshape the data, then calculates distances between these reshaped embeddings using a formula that involves matmul (matrix multiplication) operations. The final step is returning the calculated distances.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/framework/segment/utils.py\":538-565",
            "content": "    offset_y = paddle.transpose(\n        paddle.reshape(\n            F.unfold(x=padded_y,\n                     kernel_sizes=[height, width],\n                     strides=[atrous_rate, atrous_rate]),\n            [channels, height * width, -1]), [1, 0, 2])\n    offset_y2 = paddle.reshape(\n        F.unfold(padded_y2,\n                 kernel_sizes=[height, width],\n                 strides=[atrous_rate, atrous_rate]), [height, width, -1])\n    x = paddle.transpose(paddle.reshape(x, [channels, height * width, -1]),\n                         [1, 2, 0])\n    dists = x2 + offset_y2 - 2. * paddle.reshape(paddle.matmul(x, offset_y),\n                                                 [height, width, -1])\n    return dists\ndef local_matching(prev_frame_embedding,\n                   query_embedding,\n                   prev_frame_labels,\n                   dis_bias=0.,\n                   multi_local_distance=[15],\n                   ori_size=None,\n                   atrous_rate=1,\n                   use_float16=True,\n                   allow_downsample=True,"
        },
        {
            "comment": "This code computes nearest neighbor features for local matching in video segmentation. It takes embedding vectors, class labels, and a list of maximum distances as input. The function allows downsampling and parallel processing.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/framework/segment/utils.py\":566-583",
            "content": "                   allow_parallel=True):\n    \"\"\"Computes nearest neighbor features while only allowing local matches.\n    Args:\n        prev_frame_embedding: [height, width, embedding_dim],\n          the embedding vectors for the last frame.\n        query_embedding: [height, width, embedding_dim],\n          the embedding vectors for the query frames.\n        prev_frame_labels: [height, width, n_objects],\n        the class labels of the previous frame.\n        multi_local_distance: A list of Integer,\n          a list of maximum distance allowed for local matching.\n        ori_size: (ori_height, ori_width),\n          the original spatial size. If \"None\", (ori_height, ori_width) = (height, width).\n        atrous_rate: Integer, the atrous rate of local matching.\n        use_float16: Bool, if \"True\", use float16 type for matching.\n        allow_downsample: Bool, if \"True\", downsample prev_frame_embedding and query_embedding\n          with a stride of 2.\n        allow_parallel: Bool, if \"True\", do matching in a parallel way. If \"False\", do matching in"
        },
        {
            "comment": "This function calculates nearest neighbor features by using local pairwise distances in a parallel manner, with options to cast data types and allow downsampling. It takes query and previous frame embeddings as input, and returns nearest neighbor features of shape [1, height, width, n_objects, 1].",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/framework/segment/utils.py\":584-608",
            "content": "          a for-loop way, which will save GPU memory.\n    Returns:\n        nn_features: A float32 np.array of nearest neighbor features of shape\n          [1, height, width, n_objects, 1].\n    \"\"\"\n    max_distance = multi_local_distance[-1]\n    if ori_size is None:\n        height, width = prev_frame_embedding.shape[:2]\n        ori_size = (height, width)\n    obj_num = prev_frame_labels.shape[2]\n    pad = paddle.ones([1]) * WRONG_LABEL_PADDING_DISTANCE\n    if use_float16:\n        query_embedding = paddle.cast(query_embedding, dtype=\"float16\")\n        prev_frame_embedding = paddle.cast(prev_frame_embedding,\n                                           dtype=\"float16\")\n        pad = paddle.cast(pad, dtype=\"float16\")\n    if allow_parallel:\n        d = local_pairwise_distances_parallel(query_embedding,\n                                              prev_frame_embedding,\n                                              max_distance=max_distance,\n                                              atrous_rate=atrous_rate,\n                                              allow_downsample=allow_downsample)"
        },
        {
            "comment": "This code calculates pairwise distances between query and previous frame embeddings. If the shape of the distances doesn't match the original size, it interpolates labels using nearest neighbor mode. The code then pads the labels with zeros to match the maximum distance considering the atrous rate.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/framework/segment/utils.py\":609-634",
            "content": "    else:\n        d = local_pairwise_distances(query_embedding,\n                                     prev_frame_embedding,\n                                     max_distance=max_distance,\n                                     atrous_rate=atrous_rate,\n                                     allow_downsample=allow_downsample)\n    height, width = d.shape[:2]\n    labels = paddle.unsqueeze(paddle.transpose(prev_frame_labels, [2, 0, 1]), 1)\n    labels = paddle.unsqueeze(paddle.transpose(prev_frame_labels, [2, 0, 1]),\n                              axis=1)\n    if (height, width) != ori_size:\n        labels = F.interpolate(labels, size=(height, width), mode='nearest')\n    pad_max_distance = max_distance - max_distance % atrous_rate\n    atrous_max_distance = pad_max_distance // atrous_rate\n    #no change pad\n    padded_labels = F.pad(labels, (\n        pad_max_distance,\n        pad_max_distance,\n        pad_max_distance,\n        pad_max_distance,\n    ),\n                          mode='constant',\n                          value=0)"
        },
        {
            "comment": "This code segment applies atrous spatial pyramid pooling in a PaddlePaddle implementation. It creates offset masks, performs element-wise masking, computes minimum distances, and reshapes the data for each local distance level to perform feature extraction at different scales.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/framework/segment/utils.py\":636-661",
            "content": "    offset_masks = paddle.transpose(\n        paddle.reshape(\n            F.unfold(padded_labels,\n                     kernel_sizes=[height, width],\n                     strides=[atrous_rate, atrous_rate]),\n            [obj_num, height, width, -1]), [1, 2, 3, 0]) > 0.9\n    d_tiled = paddle.expand(paddle.unsqueeze(\n        d, axis=-1), [-1, -1, -1, obj_num])  # h, w, num_local_pos, obj_num\n    d_masked = paddle.where(offset_masks, d_tiled, pad)\n    dists = paddle.min(d_masked, axis=2)\n    multi_dists = [\n        paddle.unsqueeze(paddle.transpose(dists, [2, 0, 1]), axis=1)\n    ]  # n_objects, num_multi_local, h, w\n    reshaped_d_masked = paddle.reshape(d_masked, [\n        height, width, 2 * atrous_max_distance + 1, 2 * atrous_max_distance + 1,\n        obj_num\n    ])\n    for local_dis in multi_local_distance[:-1]:\n        local_dis = local_dis // atrous_rate\n        start_idx = atrous_max_distance - local_dis\n        end_idx = atrous_max_distance + local_dis + 1\n        new_d_masked = paddle.reshape(\n            reshaped_d_masked[:, :, start_idx:end_idx, start_idx:end_idx, :],"
        },
        {
            "comment": "This code performs image segmentation by reshaping and resizing the distance matrix, calculating minimum distances, and applying sigmoid activation. It also handles cases where input size is not the original size.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/framework/segment/utils.py\":662-683",
            "content": "            reshaped_d_masked[:, :, start_idx:end_idx,\n                              start_idx:end_idx, :].shape)\n        new_d_masked = paddle.reshape(new_d_masked,\n                                      [height, width, -1, obj_num])\n        new_dists = paddle.min(new_d_masked, axis=2)\n        new_dists = paddle.unsqueeze(paddle.transpose(new_dists, [2, 0, 1]),\n                                     axis=1)\n        multi_dists.append(new_dists)\n    multi_dists = paddle.concat(multi_dists, axis=1)\n    multi_dists = (F.sigmoid(multi_dists +\n                             paddle.reshape(dis_bias, [-1, 1, 1, 1])) - 0.5) * 2\n    if use_float16:\n        multi_dists = paddle.cast(multi_dists, dtype=\"float32\")\n    if (height, width) != ori_size:\n        multi_dists = F.interpolate(multi_dists,\n                                    size=ori_size,\n                                    mode='bilinear',\n                                    align_corners=True)\n    multi_dists = paddle.transpose(multi_dists, perm=[2, 3, 0, 1])"
        },
        {
            "comment": "This function calculates the attention heads for each object in a given scene. It takes reference and previous embeddings and labels as input, along with an optional epsilon value. It then computes the positional and negative head values, divides them by their respective counts (positive and negative labels), and returns the resulting attention heads for each object. The epsilon is added to avoid division by zero in the calculations.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/framework/segment/utils.py\":684-708",
            "content": "    multi_dists = paddle.reshape(multi_dists,\n                                 [1, ori_size[0], ori_size[1], obj_num, -1])\n    return multi_dists\ndef calculate_attention_head(ref_embedding,\n                             ref_label,\n                             prev_embedding,\n                             prev_label,\n                             epsilon=1e-5):\n    ref_head = ref_embedding * ref_label\n    ref_head_pos = paddle.sum(ref_head, axis=(2, 3))\n    ref_head_neg = paddle.sum(ref_embedding, axis=(2, 3)) - ref_head_pos\n    ref_pos_num = paddle.sum(ref_label, axis=(2, 3))\n    ref_neg_num = paddle.sum(1. - ref_label, axis=(2, 3))\n    ref_head_pos = ref_head_pos / (ref_pos_num + epsilon)\n    ref_head_neg = ref_head_neg / (ref_neg_num + epsilon)\n    prev_head = prev_embedding * prev_label\n    prev_head_pos = paddle.sum(prev_head, axis=(2, 3))\n    prev_head_neg = paddle.sum(prev_embedding, axis=(2, 3)) - prev_head_pos\n    prev_pos_num = paddle.sum(prev_label, axis=(2, 3))\n    prev_neg_num = paddle.sum(1. - prev_label, axis=(2, 3))"
        },
        {
            "comment": "This code calculates the attention head values for evaluation, where it sums up reference embeddings multiplied by their corresponding labels. It also accounts for positive and negative instances of reference embeddings by subtracting them from total sums. The final total_head is returned as a concatenated matrix.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/framework/segment/utils.py\":709-735",
            "content": "    prev_head_pos = prev_head_pos / (prev_pos_num + epsilon)\n    prev_head_neg = prev_head_neg / (prev_neg_num + epsilon)\n    total_head = paddle.concat(\n        x=[ref_head_pos, ref_head_neg, prev_head_pos, prev_head_neg], axis=1)\n    return total_head\ndef calculate_attention_head_for_eval(ref_embeddings,\n                                      ref_labels,\n                                      prev_embedding,\n                                      prev_label,\n                                      epsilon=1e-5):\n    total_ref_head_pos = 0.\n    total_ref_head_neg = 0.\n    total_ref_pos_num = 0.\n    total_ref_neg_num = 0.\n    for idx in range(len(ref_embeddings)):\n        ref_embedding = ref_embeddings[idx]\n        ref_label = ref_labels[idx]\n        ref_head = ref_embedding * ref_label\n        ref_head_pos = paddle.sum(ref_head, axis=(2, 3))\n        ref_head_neg = paddle.sum(ref_embedding, axis=(2, 3)) - ref_head_pos\n        ref_pos_num = paddle.sum(ref_label, axis=(2, 3))\n        ref_neg_num = paddle.sum(1. - ref_label, axis=(2, 3))"
        },
        {
            "comment": "This code calculates and returns a total head value by accumulating reference (ref) head values and previous (prev) head values, then normalizing them. It handles potential zero-division cases with a small epsilon for stability. The resulting total head consists of reference positive (pos), reference negative (neg), previous positive (pos), and previous negative (neg) head components concatenated along axis 1.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/framework/segment/utils.py\":736-753",
            "content": "        total_ref_head_pos = total_ref_head_pos + ref_head_pos\n        total_ref_head_neg = total_ref_head_neg + ref_head_neg\n        total_ref_pos_num = total_ref_pos_num + ref_pos_num\n        total_ref_neg_num = total_ref_neg_num + ref_neg_num\n    ref_head_pos = total_ref_head_pos / (total_ref_pos_num + epsilon)\n    ref_head_neg = total_ref_head_neg / (total_ref_neg_num + epsilon)\n    prev_head = prev_embedding * prev_label\n    prev_head_pos = paddle.sum(prev_head, axis=(2, 3))\n    prev_head_neg = paddle.sum(prev_embedding, axis=(2, 3)) - prev_head_pos\n    prev_pos_num = paddle.sum(prev_label, axis=(2, 3))\n    prev_neg_num = paddle.sum(1. - prev_label, axis=(2, 3))\n    prev_head_pos = prev_head_pos / (prev_pos_num + epsilon)\n    prev_head_neg = prev_head_neg / (prev_neg_num + epsilon)\n    total_head = paddle.concat(\n        x=[ref_head_pos, ref_head_neg, prev_head_pos, prev_head_neg], axis=1)\n    return total_head"
        }
    ]
}