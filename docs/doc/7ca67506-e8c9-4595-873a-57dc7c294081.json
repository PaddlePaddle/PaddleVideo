{
    "summary": "The code provides instructions for training and testing ST-GCN, a skeleton-based action recognition model, on FSD and NTU-RGB+D datasets, with accuracy results given. It exports the model's architecture and parameters using `export_model.py` and allows inference with optional GPU usage via `predict.py`.",
    "details": [
        {
            "comment": "This code is a documentation for ST-GCN, a skeleton-based action recognition model. It explains the model's introduction, data requirements (FSD and NTU-RGBD), training instructions on both datasets, and how to perform inference.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/stgcn.md\":0-48",
            "content": "[\u7b80\u4f53\u4e2d\u6587](../../../zh-CN/model_zoo/recognition/stgcn.md) | English\n# ST-GCN\n---\n## Contents\n- [Introduction](#Introduction)\n- [Data](#Data)\n- [Train](#Train)\n- [Test](#Test)\n- [Inference](#Inference)\n- [Reference](#Reference)\n## Introduction\nST-GCN is skeleton-based action recognition model proposed in AAAI 2018.\n<div align=\"center\">\n<img src=\"../../../images/st-gcn.png\" height=200 width=950 hspace='10'/> <br />\n</div>\n## Data\nPlease refer to FSD data download and preparation doc [FSD](../../dataset/fsd.md)\nPlease refer to NTU-RGBD data download and preparation doc [NTU-RGBD](../../dataset/ntu-rgbd.md)\n## Train\n### Train on FSD\n- Train ST-GCN on FSD scripts:\n```bash\npython3.7 main.py -c configs/recognition/stgcn/stgcn_fsd.yaml\n```\n- Turn off `valid` when training, as validation dataset is not available for the competition.\n### Train on NTU-RGBD\n- Train ST-GCN on NTU-RGBD scripts:\n```bash\npython3.7 -B -m paddle.distributed.launch --gpus=\"0,1,2,3\"  --log_dir=log_stgcn  main.py  --validate -c configs/recognition/stgcn/stgcn_ntucs.yaml"
        },
        {
            "comment": "This code provides instructions for testing the ST-GCN model on two datasets: FSD and NTU-RGB+D. The user is directed to run specific test scripts with provided command lines, specifying the configuration file and weight path. Results are saved in a submission.csv file and the final scores can be obtained from the competition website. Accuracy results for both datasets are also included.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/stgcn.md\":49-88",
            "content": "```\n- config file `stgcn_ntucs.yaml` corresponding to the config of ST-GCN on NTU-RGB+D dataset with cross-subject splits.\n## Test\n### Test on FSD\n- Test scripts\uff1a\n```bash\npython3.7 main.py --test -c configs/recognition/stgcn/stgcn_fsd.yaml -w output/STGCN/STGCN_epoch_00090.pdparams\n```\n- Specify the config file with `-c`, specify the weight path with `-w`.\n- Evaluation results will be saved in `submission.csv` file, final score can be obtained in [competition website](https://aistudio.baidu.com/aistudio/competition/detail/115).\nAccuracy on FSD-10 dataset:\nTest_Data| Top-1 | checkpoints |\n| :----: | :----: | :---- |\n| Test_A | 59.07 | [STGCN_fsd.pdparams](https://videotag.bj.bcebos.com/PaddleVideo-release2.2/STGCN_fsd.pdparams) |\n### Test on NTU-RGB+D\n- Test scripts\uff1a\n```bash\npython3.7 main.py --test -c configs/recognition/stgcn/stgcn_ntucs.yaml -w output/STGCN/STGCN_best.pdparams\n```\n- Specify the config file with `-c`, specify the weight path with `-w`.\nAccuracy on NTU-RGB+D dataset:\n| split | Top-1 | checkpoints |"
        },
        {
            "comment": "This code provides the commands to export the model architecture and parameters for a STGCN model, as well as how to use the model to make inferences. The `export_model.py` script is used to generate the `STGCN.pdmodel` and `STGCN.pdiparams` files. The `predict.py` script is then used for making predictions using the exported model with optional GPU usage.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/stgcn.md\":89-114",
            "content": "| :----: | :----: | :---- |\n| cross-subject | 82.28 | [STGCN_ntucs.pdparams](https://videotag.bj.bcebos.com/PaddleVideo-release2.2/STGCN_ntucs.pdparams) |\n## Inference\n### export inference model\n To get model architecture file `STGCN.pdmodel` and parameters file `STGCN.pdiparams`, use:\n```bash\npython3.7 tools/export_model.py -c configs/recognition/stgcn/stgcn_fsd.yaml \\\n                                -p data/STGCN_fsd.pdparams \\\n                                -o inference/STGCN\n```\n- Args usage please refer to [Model Inference](https://github.com/PaddlePaddle/PaddleVideo/blob/release/2.0/docs/zh-CN/start.md#2-%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86).\n### infer\n```bash\npython3.7 tools/predict.py --input_file data/fsd10/example_skeleton.npy \\\n                           --config configs/recognition/stgcn/stgcn_fsd.yaml \\\n                           --model_file inference/STGCN/STGCN.pdmodel \\\n                           --params_file inference/STGCN/STGCN.pdiparams \\\n                           --use_gpu=True \\"
        },
        {
            "comment": "False",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/stgcn.md\":115-128",
            "content": "                           --use_tensorrt=False\n```\nexample of logs:\n```\nCurrent video file: data/fsd10/example_skeleton.npy\n        top-1 class: 27\n        top-1 score: 0.9912770986557007\n```\n## Reference\n- [Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition](https://arxiv.org/abs/1801.07455), Sijie Yan, Yuanjun Xiong, Dahua Lin"
        }
    ]
}