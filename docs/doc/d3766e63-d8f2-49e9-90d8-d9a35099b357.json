{
    "summary": "The code sets up libraries, initializes DALI and TSN model, creates a dataloader, builds solver, trains model with optimization steps, logs performance metrics, updates learning rates, supports resuming training/finetuning, and saves states at intervals.",
    "details": [
        {
            "comment": "This code imports necessary libraries and modules, sets up licenses, and imports functions from other files for model building, solver configuration, and additional utility functions. It also defines a loader for TSN-Dali dataset and functions for input data preparation.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/tasks/train_dali.py\":0-24",
            "content": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport time\nimport os.path as osp\nimport paddle\nfrom ..modeling.builder import build_model\nfrom ..solver import build_lr, build_optimizer\nfrom ..utils import do_preciseBN\nfrom paddlevideo.utils import get_logger, coloring\nfrom paddlevideo.utils import (AverageMeter, build_record, log_batch, log_epoch,\n                               save, load, mkdir)\nfrom paddlevideo.loader import TSN_Dali_loader, get_input_data"
        },
        {
            "comment": "This code snippet initializes and trains a DALI (Data Augmentation and Input Pipeline Library) for the TSN model. It first constructs the model, creates a Dali dataloader, builds a solver with specified optimizer and learning rate, and then resumes training from the last checkpoint if provided.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/tasks/train_dali.py\":25-62",
            "content": "\"\"\"\nWe only supported DALI training for TSN model now.\n\"\"\"\ndef train_dali(cfg, weights=None, parallel=True):\n    \"\"\"Train model entry\n    Args:\n    \tcfg (dict): configuration.\n        weights (str): weights path for finetuning.\n    \tparallel (bool): Whether multi-cards training. Default: True.\n    \"\"\"\n    logger = get_logger(\"paddlevideo\")\n    batch_size = cfg.DALI_LOADER.get('batch_size', 8)\n    places = paddle.set_device('gpu')\n    model_name = cfg.model_name\n    output_dir = cfg.get(\"output_dir\", f\"./output/{model_name}\")\n    mkdir(output_dir)\n    # 1. Construct model\n    model = build_model(cfg.MODEL)\n    if parallel:\n        model = paddle.DataParallel(model)\n    # 2. Construct dali dataloader\n    train_loader = TSN_Dali_loader(cfg.DALI_LOADER).build_dali_reader()\n    # 3. Construct solver.\n    lr = build_lr(cfg.OPTIMIZER.learning_rate, None)\n    optimizer = build_optimizer(cfg.OPTIMIZER, lr, model=model)\n    # Resume\n    resume_epoch = cfg.get(\"resume_epoch\", 0)\n    if resume_epoch:\n        filename = osp.join(output_dir,"
        },
        {
            "comment": "This code snippet is part of a model training pipeline. It first checks if the resume_epoch is 0 or if weights are provided for finetuning, then loads and sets the corresponding state dictionaries for the model and optimizer. The model is trained for specified epochs, with the option to continue from a previous epoch or start from scratch depending on the resume_epoch and weights inputs. It also records reader time during training loop iterations.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/tasks/train_dali.py\":63-87",
            "content": "                            model_name + f\"_epoch_{resume_epoch:05d}\")\n        resume_model_dict = load(filename + '.pdparams')\n        resume_opt_dict = load(filename + '.pdopt')\n        model.set_state_dict(resume_model_dict)\n        optimizer.set_state_dict(resume_opt_dict)\n    # Finetune:\n    if weights:\n        assert resume_epoch == 0, f\"Conflict occurs when finetuning, please switch resume function off by setting resume_epoch to 0 or not indicating it.\"\n        model_dict = load(weights)\n        model.set_state_dict(model_dict)\n    # 4. Train Model\n    for epoch in range(0, cfg.epochs):\n        if epoch < resume_epoch:\n            logger.info(\n                f\"| epoch: [{epoch+1}] <= resume_epoch: [{ resume_epoch}], continue... \"\n            )\n            continue\n        model.train()\n        record_list = build_record(cfg.MODEL)\n        tic = time.time()\n        for i, data in enumerate(train_loader):\n            data = get_input_data(data)\n            record_list['reader_time'].update(time.time() - tic)"
        },
        {
            "comment": "This code is training a model. It performs forward, backward pass, and optimization steps before logging performance metrics and updating learning rates. The model takes input data and calculates outputs in 'train' mode. Then, it calculates the average loss from the outputs. Next, it updates gradients using backward propagation, optimizes the model with step and clears gradients. It records log information such as learning rate and batch time for later analysis. The code also checks if there is an interval in the training to log current metrics and provides an instance per second rate (ips) as performance indicator. Lastly, it updates learning rates using both iteration steps and epoch steps, based on configuration settings.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/tasks/train_dali.py\":88-115",
            "content": "            # 4.1 forward\n            outputs = model(data, mode='train')\n            # 4.2 backward\n            avg_loss = outputs['loss']\n            avg_loss.backward()\n            # 4.3 minimize\n            optimizer.step()\n            optimizer.clear_grad()\n            # log record\n            record_list['lr'].update(optimizer._global_learning_rate(),\n                                     batch_size)\n            for name, value in outputs.items():\n                record_list[name].update(value, batch_size)\n            record_list['batch_time'].update(time.time() - tic)\n            tic = time.time()\n            if i % cfg.get(\"log_interval\", 10) == 0:\n                ips = \"ips: {:.5f} instance/sec.\".format(\n                    batch_size / record_list[\"batch_time\"].val)\n                log_batch(record_list, i, epoch + 1, cfg.epochs, \"train\", ips)\n            # learning rate iter step\n            if cfg.OPTIMIZER.learning_rate.get(\"iter_step\"):\n                lr.step()\n        # learning rate epoch step"
        },
        {
            "comment": "This code chunk performs the following actions:\n1. Checks if learning rate should be updated based on iteration count.\n2. Calculates and logs the training instance speed (ips).\n3. Optionally applies precise Batch Normalization (bn) to improve accuracy.\n4. Saves the model's and optimizer's state every 'save_interval' epochs.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/tasks/train_dali.py\":116-140",
            "content": "        if not cfg.OPTIMIZER.learning_rate.get(\"iter_step\"):\n            lr.step()\n        ips = \"ips: {:.5f} instance/sec.\".format(\n            batch_size * record_list[\"batch_time\"].count /\n            record_list[\"batch_time\"].sum)\n        log_epoch(record_list, epoch + 1, \"train\", ips)\n        # use precise bn to improve acc\n        if cfg.get(\"PRECISEBN\") and (epoch % cfg.PRECISEBN.preciseBN_interval\n                                     == 0 or epoch == cfg.epochs - 1):\n            do_preciseBN(\n                model, train_loader, parallel,\n                min(cfg.PRECISEBN.num_iters_preciseBN, len(train_loader)))\n        # 5. Save model and optimizer\n        if epoch % cfg.get(\"save_interval\", 1) == 0 or epoch == cfg.epochs - 1:\n            save(\n                optimizer.state_dict(),\n                osp.join(output_dir,\n                         model_name + f\"_epoch_{epoch+1:05d}.pdopt\"))\n            save(\n                model.state_dict(),\n                osp.join(output_dir,\n                         model_name + f\"_epoch_{epoch+1:05d}.pdparams\"))"
        },
        {
            "comment": "This line logs the completion of training a specific model using the \"logger.info\" function, indicating that the training process for the specified \"model_name\" has ended.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/tasks/train_dali.py\":142-142",
            "content": "    logger.info(f'training {model_name} finished')"
        }
    ]
}