{
    "summary": "The code introduces an AttentionLstmErnie class for ERNIE-based scenario classification, implementing an LSTM-based attention model for video tagging using text and audio data. It employs dropout, batch normalization, and Neural Machine Translation approach.",
    "details": [
        {
            "comment": "This code defines the AttentionLstmErnie class, which extends the functionality of scenario-classify by incorporating text information. It uses ERNIE to extract text features and operates in either 'train' or 'infer' mode.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/MultimodalVideoTag/scenario_lib/models/attention_lstm_ernie.py\":0-33",
            "content": "#!/usr/bin/env python\n# coding=utf-8\n\"\"\"\nattention lstm add ernie model\n\"\"\"\n#  Copyright (c) 2019 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\nimport paddle\nimport paddle.static as static\nfrom .ernie import ErnieConfig, ErnieModel\nclass AttentionLstmErnie(object):\n    \"\"\"\n    Base on scenario-classify (image + audio), add text information\n    use ERNIE to extract text feature\n    \"\"\"\n    def __init__(self, name, cfg, mode='train'):\n        self.cfg = cfg\n        self.name = name"
        },
        {
            "comment": "This code initializes a model by setting attributes from a configuration file and calling the `get_config` function. The `get_config` function retrieves the model's configurations, including feature numbers, dimensions, data types, and more. It also gets mode-specific settings like batch size.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/MultimodalVideoTag/scenario_lib/models/attention_lstm_ernie.py\":34-58",
            "content": "        self.mode = mode\n        self.py_reader = None\n        self.get_config()\n    def get_config(self):\n        \"\"\"get_config\n        \"\"\"\n        # get model configs\n        self.feature_num = self.cfg.MODEL.feature_num\n        self.feature_names = self.cfg.MODEL.feature_names\n        self.feature_dims = self.cfg.MODEL.feature_dims\n        self.feature_dtypes = self.cfg.MODEL.feature_dtypes\n        self.feature_lod_level = self.cfg.MODEL.feature_lod_level\n        self.num_classes = self.cfg.MODEL.num_classes\n        self.embedding_size = self.cfg.MODEL.embedding_size\n        self.lstm_size_img = self.cfg.MODEL.lstm_size_img\n        self.lstm_size_audio = self.cfg.MODEL.lstm_size_audio\n        self.ernie_freeze = self.cfg.MODEL.ernie_freeze\n        self.lstm_pool_mode = self.cfg.MODEL.lstm_pool_mode\n        self.drop_rate = self.cfg.MODEL.drop_rate\n        self.loss_type = self.cfg.TRAIN.loss_type\n        self.ernie_pretrain_dict_path = self.cfg.TRAIN.ernie_pretrain_dict_path\n        # get mode configs\n        self.batch_size = self.get_config_from_sec(self.mode, 'batch_size', 1)"
        },
        {
            "comment": "This code is a part of the AttentionLSTMERNIE model. It initializes the number of GPUs, learning rate, weight decay, and other parameters for training mode. The function get_config_from_sec retrieves values from a configuration file using section and item names. The build_input function constructs input data by iterating over feature names, dimensions, data types, and lod levels.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/MultimodalVideoTag/scenario_lib/models/attention_lstm_ernie.py\":59-84",
            "content": "        self.num_gpus = self.get_config_from_sec(self.mode, 'num_gpus', 1)\n        if self.mode == 'train':\n            self.learning_rate = self.get_config_from_sec(\n                'train', 'learning_rate', 1e-3)\n            self.weight_decay = self.get_config_from_sec(\n                'train', 'weight_decay', 8e-4)\n            self.num_samples = self.get_config_from_sec(\n                'train', 'num_samples', 5000000)\n            self.decay_epochs = self.get_config_from_sec(\n                'train', 'decay_epochs', [5])\n            self.decay_gamma = self.get_config_from_sec(\n                'train', 'decay_gamma', 0.1)\n    def get_config_from_sec(self, sec, item, default=None):\n        \"\"\"get_config_from_sec\"\"\"\n        if sec.upper() not in self.cfg:\n            return default\n        return self.cfg[sec.upper()].get(item, default)\n    def build_input(self, use_pyreader):\n        \"\"\"\n        build input\n        \"\"\"\n        self.feature_input = []\n        for name, dim, dtype, lod_level in zip(self.feature_names,"
        },
        {
            "comment": "Code initializes the reader for data feeding into the model, sets the label input, and defines a function \"ernie_encoder\" that extracts text features using the Ernie model.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/MultimodalVideoTag/scenario_lib/models/attention_lstm_ernie.py\":85-107",
            "content": "                                               self.feature_dims,\n                                               self.feature_dtypes,\n                                               self.feature_lod_level):\n            self.feature_input.append(\n                static.data(shape=dim,\n                                  lod_level=lod_level,\n                                  dtype=dtype,\n                                  name=name))\n        self.label_input = static.data(shape=[self.num_classes],\n                                             dtype='float32',\n                                             name='label')\n        self.py_reader = paddle.fluid.io.PyReader(feed_list=self.feature_input +\n                                           [self.label_input],\n                                           capacity=1024,\n                                           iterable=True)\n    def ernie_encoder(self):\n        \"\"\"\n        text feature extractor\n        \"\"\"\n        ernie_config = ErnieConfig(\n            os.path.join(self.ernie_pretrain_dict_path, 'ernie_config.json'))"
        },
        {
            "comment": "This code initializes an ErnieModel with features extracted from input data. If self.ernie_freeze is True, it freezes the ERNIE model's parameters to prevent further training. It then retrieves the sequence output and applies a dropout if in train mode.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/MultimodalVideoTag/scenario_lib/models/attention_lstm_ernie.py\":108-130",
            "content": "        if self.mode != 'train':\n            ernie_config['attention_probs_dropout_prob'] = 0.0\n            ernie_config['hidden_dropout_prob'] = 0.0\n        src_ids = self.feature_input[2][:, 0]\n        sent_ids = self.feature_input[2][:, 1]\n        position_ids = self.feature_input[2][:, 2]\n        task_ids = self.feature_input[2][:, 3]\n        input_mask = self.feature_input[2][:, 4].astype('float32')\n        ernie = ErnieModel(src_ids=src_ids,\n                           position_ids=position_ids,\n                           sentence_ids=sent_ids,\n                           task_ids=task_ids,\n                           input_mask=input_mask,\n                           config=ernie_config)\n        enc_out = ernie.get_sequence_output()\n        # to Freeze ERNIE param\n        if self.ernie_freeze is True:\n            enc_out.stop_gradient = True\n        # ernie cnn\n        enc_out_cnn = ernie.get_sequence_textcnn_output(enc_out, input_mask)\n        enc_out_cnn_drop = paddle.nn.functional.dropout(enc_out_cnn, p=self.drop_rate, training=(self.mode=='train'))"
        },
        {
            "comment": "This code defines a function called \"build_model\" that creates and returns the model for video tagging. The model takes image, audio, and text features as input to generate attention-based LSTM features from the image. It applies fully connected layers (fc) on the image features and then passes them through dynamic LSTMs to obtain the forward and backward LSTM outputs. These outputs are used in further processing for video tagging.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/MultimodalVideoTag/scenario_lib/models/attention_lstm_ernie.py\":131-153",
            "content": "        return enc_out_cnn_drop\n    def build_model(self):\n        \"\"\"build_model\n        \"\"\"\n        # ---------------- transfer from old paddle ---------------\n        # get image,audio,text feature\n        video_input_tensor = self.feature_input[0]\n        audio_input_tensor = self.feature_input[1]\n        self.ernie_feature = self.ernie_encoder()\n        # ------image------\n        lstm_forward_fc = static.nn.fc(x=video_input_tensor,\n                                          size=self.lstm_size_img * 4,\n                                          activation=None,\n                                          bias_attr=False)\n        lstm_forward, _ = paddle.fluid.layers.dynamic_lstm(input=lstm_forward_fc,\n                                                    size=self.lstm_size_img *\n                                                    4,\n                                                    is_reverse=False,\n                                                    use_peepholes=True)\n        lsmt_backward_fc = static.nn.fc(x=video_input_tensor,"
        },
        {
            "comment": "This code defines a dynamic LSTM layer for image features, concatenates it with the backward pass, and applies dropout if in training mode. If 'text_guide' pooling mode is selected, it computes attention weights between text features and LSTM output using seq2seq attention. Otherwise, it uses an FC layer to reduce the dimensions of the LSTM output.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/MultimodalVideoTag/scenario_lib/models/attention_lstm_ernie.py\":154-171",
            "content": "                                           size=self.lstm_size_img * 4,\n                                           activation=None,\n                                           bias_attr=None)\n        lstm_backward, _ = paddle.fluid.layers.dynamic_lstm(input=lsmt_backward_fc,\n                                                     size=self.lstm_size_img *\n                                                     4,\n                                                     is_reverse=True,\n                                                     use_peepholes=True)\n        lstm_forward_img = paddle.concat(\n            x=[lstm_forward, lstm_backward], axis=1)\n        lstm_dropout = paddle.nn.functional.dropout(lstm_forward_img, p=self.drop_rate, training=(self.mode=='train'))\n        if self.lstm_pool_mode == 'text_guide':\n            lstm_weight = self.attention_weight_by_feature_seq2seq_attention(\n                self.ernie_feature, lstm_dropout, self.lstm_size_img * 2)\n        else:\n            lstm_weight = static.nn.fc(x=lstm_dropout,"
        },
        {
            "comment": "This code snippet is defining a LSTM model for processing both visual and audio inputs. It initializes a LSTM layer with dropout, applies element-wise multiplication with weights, performs sequence pooling on the output, and defines FC layers followed by LSTM for processing audio input. Regularization is applied to the audio LSTM layer using an L2 decay regularizer.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/MultimodalVideoTag/scenario_lib/models/attention_lstm_ernie.py\":172-193",
            "content": "                                          size=1,\n                                          activation='sequence_softmax',\n                                          bias_attr=None)\n        scaled = paddle.multiply(x=lstm_dropout,\n                                              y=lstm_weight)\n        self.lstm_pool = paddle.static.nn.sequence_pool(input=scaled,\n                                                    pool_type='sum')\n        # ------audio------\n        lstm_forward_fc_audio = static.nn.fc(\n            x=audio_input_tensor,\n            size=self.lstm_size_audio * 4,\n            activation=None,\n            bias_attr=paddle.ParamAttr(\n                regularizer=paddle.regularizer.L2Decay(coeff=0.0),\n                initializer=paddle.nn.initializer.Normal(std=0.0)))\n        lstm_forward_audio, _ = paddle.fluid.layers.dynamic_lstm(\n            input=lstm_forward_fc_audio,\n            size=self.lstm_size_audio * 4,\n            is_reverse=False,\n            use_peepholes=True)\n        lsmt_backward_fc_audio = static.nn.fc(x=audio_input_tensor,"
        },
        {
            "comment": "This code is creating a dynamic LSTM for audio input, reversing it, concatenating the forward and backward outputs, applying dropout if in training mode, and then performing attention weight calculation based on the pooling mode.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/MultimodalVideoTag/scenario_lib/models/attention_lstm_ernie.py\":194-213",
            "content": "                                                 size=self.lstm_size_audio * 4,\n                                                 activation=None,\n                                                 bias_attr=False)\n        lstm_backward_audio, _ = paddle.fluid.layers.dynamic_lstm(\n            input=lsmt_backward_fc_audio,\n            size=self.lstm_size_audio * 4,\n            is_reverse=True,\n            use_peepholes=True)\n        lstm_forward_audio = paddle.concat(\n            x=[lstm_forward_audio, lstm_backward_audio], axis=1)\n        lstm_dropout_audio = paddle.nn.functional.dropout(lstm_forward_audio, p=self.drop_rate, training=(self.mode=='train'))\n        if self.lstm_pool_mode == 'text_guide':\n            lstm_weight_audio = self.attention_weight_by_feature_seq2seq_attention(\n                self.ernie_feature, lstm_dropout_audio,\n                self.lstm_size_audio * 2)\n        else:\n            lstm_weight_audio = static.nn.fc(x=lstm_dropout_audio,\n                                                size=1,"
        },
        {
            "comment": "This code implements a LSTM-based attention model that combines audio and text data for video tagging. It consists of three main parts: LSTM layers, attention mechanism, and fully connected (FC) layer. The LSTM layers process the text and audio inputs separately and then concatenate them with the ERNIE feature. The attention mechanism is applied to calculate the weight for each LSTM output sequence. The FC layer has a softmax activation function if loss type is set to 'softmax', otherwise, it uses no activation when loss type is 'sigmoid'.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/MultimodalVideoTag/scenario_lib/models/attention_lstm_ernie.py\":214-234",
            "content": "                                                activation='sequence_softmax',\n                                                bias_attr=None)\n        scaled_audio = paddle.multiply(x=lstm_dropout_audio,\n                                                    y=lstm_weight_audio)\n        self.lstm_pool_audio = paddle.static.nn.sequence_pool(input=scaled_audio,\n                                                          pool_type='sum')\n        lstm_concat = paddle.concat(\n            x=[self.lstm_pool, self.lstm_pool_audio, self.ernie_feature],\n            axis=1,\n            name='final_concat')\n        # lstm_concat = self.add_bn(lstm_concat)\n        if self.loss_type == 'softmax':\n            self.fc = static.nn.fc(x=lstm_concat,\n                                      size=self.num_classes,\n                                      activation='softmax')\n        elif self.loss_type == 'sigmoid':\n            self.fc = static.nn.fc(x=lstm_concat,\n                                      size=self.num_classes,\n                                      activation=None)"
        },
        {
            "comment": "This code calculates attention weights for a feature sequence using a Neural Machine Translation approach. It expands the text feature across the sequence, concatenates it with the original sequence feature, and passes it through an FC layer with 'tanh' activation to calculate energy values. The calculated energy is then used to determine attention weights by feature.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/MultimodalVideoTag/scenario_lib/models/attention_lstm_ernie.py\":235-259",
            "content": "            self.logit = self.fc\n            self.fc = paddle.nn.functional.sigmoid(self.fc)\n        self.network_outputs = [self.fc]\n    def attention_weight_by_feature_seq2seq_attention(\n            self,\n            text_feature,\n            sequence_feature,\n            sequence_feature_dim,\n            name_prefix=\"seq2seq_attention\"):\n        \"\"\"\n        caculate weight by feature\n        Neural Machine Translation by Jointly Learning to Align and Translate\n        \"\"\"\n        text_feature_expand = paddle.static.nn.sequence_expand(text_feature,\n                                                           sequence_feature,\n                                                           ref_level=0)\n        sequence_text_concat = paddle.concat(\n            x=[sequence_feature, text_feature_expand],\n            axis=-1,\n            name='video_text_concat')\n        energy = static.nn.fc(x=sequence_text_concat,\n                                 size=sequence_feature_dim,\n                                 activation='tanh',"
        },
        {
            "comment": "This function adds dropout and batch normalization to the LSTM concatenation. It projects the input to 8192 dimensions using an FC layer, applies batch normalization, and then applies a relu activation if in training mode. If not in training mode (is_test), it skips the batch normalization step. Finally, it applies dropout to the result.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/MultimodalVideoTag/scenario_lib/models/attention_lstm_ernie.py\":260-284",
            "content": "                                 name=name_prefix + \"_tanh_fc\")\n        weight_vector = static.nn.fc(x=energy,\n                                        size=1,\n                                        activation='sequence_softmax',\n                                        bias_attr=None,\n                                        name=name_prefix + \"_softmax_fc\")\n        return weight_vector\n    def add_bn(self, lstm_concat):\n        \"\"\"\n        v2.5 add drop out and batch norm\n        \"\"\"\n        input_fc_proj = static.nn.fc(\n            x=lstm_concat,\n            size=8192,\n            activation=None,\n            bias_attr=paddle.ParamAttr(\n                regularizer=paddle.regularizer.L2Decay(coeff=0.0),\n                initializer=paddle.nn.initializer.Normal(std=0.0)))\n        input_fc_proj_bn = paddle.static.nn.batch_norm(\n            input=input_fc_proj,\n            act=\"relu\",\n            is_test=(not self.mode == 'train'))\n        input_fc_proj_dropout = paddle.nn.functional.dropout(\n            input_fc_proj_bn,"
        },
        {
            "comment": "This code defines an attention LSTM model using Ernie. It applies dropout, fully connected layers with batch normalization and dropout again. The optimizer function sets a learning rate that decays over specified epochs.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/MultimodalVideoTag/scenario_lib/models/attention_lstm_ernie.py\":285-311",
            "content": "            p=self.drop_rate,\n            training=(self.mode=='train'))\n        input_fc_hidden = static.nn.fc(\n            x=input_fc_proj_dropout,\n            size=4096,\n            activation=None,\n            bias_attr=paddle.ParamAttr(\n                regularizer=paddle.regularizer.L2Decay(coeff=0.0),\n                initializer=paddle.nn.initializer.Normal(std=0.0)))\n        input_fc_hidden_bn = paddle.static.nn.batch_norm(\n            input=input_fc_hidden,\n            act=\"relu\",\n            is_test=(not self.mode == 'train'))\n        input_fc_hidden_dropout = paddle.nn.functional.dropout(\n            input_fc_hidden_bn,\n            p=self.drop_rate,\n            training=(self.mode=='train'))\n        return input_fc_hidden_dropout\n    def optimizer(self):\n        \"\"\"\n        optimizer\n        \"\"\"\n        assert self.mode == 'train', \"optimizer only can be get in train mode\"\n        values = [\n            self.learning_rate * (self.decay_gamma ** i)\n            for i in range(len(self.decay_epochs) + 1)"
        },
        {
            "comment": "This code defines a class that contains two methods: one for initializing an optimizer with piecewise decay learning rate and another for calculating the softlabel cross-entropy loss. The optimizer uses RMSProp algorithm and decays the learning rate based on defined epochs. The loss is calculated using the soft label version of cross entropy, suitable for certain types of neural networks.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/MultimodalVideoTag/scenario_lib/models/attention_lstm_ernie.py\":312-333",
            "content": "        ]\n        iter_per_epoch = self.num_samples / self.batch_size\n        boundaries = [e * iter_per_epoch for e in self.decay_epochs]\n        return paddle.optimizer.RMSProp(\n            learning_rate=paddle.optimizer.lr.PiecewiseDecay(values=values,\n                                                       boundaries=boundaries),\n            centered=True,\n            weight_decay=paddle.regularizer.L2Decay(\n                coeff=self.weight_decay))\n    def softlabel_cross_entropy_loss(self):\n        \"\"\"\n        softlabel_cross_entropy_loss\n        \"\"\"\n        assert self.mode != 'infer', \"invalid loss calculationg in infer mode\"\n        '''\n        cost = paddle.nn.functional.cross_entropy(input=self.network_outputs[0], \\\n                                          label=self.label_input)\n        '''\n        cost = paddle.nn.functional.cross_entropy(input=self.network_outputs[0], \\\n                                          label=self.label_input,\n                                          soft_label=True)"
        },
        {
            "comment": "The code defines a loss function that takes in a loss type ('sigmoid' or others) and returns the calculated loss value. It includes functions for computing the sum of losses (sum_cost) and scaling it based on the number of GPUs (self.num_gpus). For the 'sigmoid' loss type, it uses binary cross-entropy with a reduction to be none, and calculates the mean loss over all batch elements. The scale operation is used to adjust the loss value for distributed training across multiple GPUs.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/MultimodalVideoTag/scenario_lib/models/attention_lstm_ernie.py\":335-364",
            "content": "        cost = paddle.sum(x=cost, axis=-1)\n        sum_cost = paddle.sum(x=cost)\n        self.loss_ = paddle.scale(sum_cost,\n                                        scale=self.num_gpus,\n                                        bias_after_scale=False)\n        return self.loss_\n    def sigmoid_cross_entropy_loss(self):\n        \"\"\"\n        sigmoid_cross_entropy_loss\n        \"\"\"\n        assert self.mode != 'infer', \"invalid loss calculationg in infer mode\"\n        cost = paddle.nn.functional.binary_cross_entropy(input=self.logit,\\\n                                          label=self.label_input, reduction=None)\n        cost = paddle.sum(x=cost, axis=-1)\n        sum_cost = paddle.sum(x=cost)\n        self.loss_ = paddle.scale(sum_cost,\n                                        scale=self.num_gpus,\n                                        bias_after_scale=False)\n        return self.loss_\n    def loss(self):\n        \"\"\"\n        loss\n        \"\"\"\n        if self.loss_type == 'sigmoid':\n            return self.sigmoid_cross_entropy_loss()"
        },
        {
            "comment": "This code defines several methods for a model class. The `softlabel_cross_entropy_loss()` method returns the soft label cross-entropy loss. The `outputs()` method returns the network outputs. The `feeds()` method returns the feature and label inputs based on the current mode. The `pyreader()` method returns the PyReader object. Finally, the `epoch_num()` method returns the number of training epochs.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/MultimodalVideoTag/scenario_lib/models/attention_lstm_ernie.py\":365-399",
            "content": "        else:\n            return self.softlabel_cross_entropy_loss()\n    def outputs(self):\n        \"\"\"\n        get outputs\n        \"\"\"\n        return self.network_outputs\n    def feeds(self):\n        \"\"\"\n        get feeds\n        \"\"\"\n        return self.feature_input if self.mode == 'infer' else self.feature_input + [\n            self.label_input\n        ]\n    def pyreader(self):\n        \"\"\"pyreader\"\"\"\n        return self.py_reader\n    def epoch_num(self):\n        \"\"\"get train epoch num\"\"\"\n        return self.cfg.TRAIN.epoch\n    def load_test_weights_file(self, exe, weights, prog, place):\n        \"\"\"\n        load_test_weights_file\n        \"\"\"\n        load_vars = [x for x in prog.list_vars() \\\n                     if isinstance(x, paddle.framework.Parameter)]\n        static.load_vars(exe,\n                           dirname=weights,\n                           vars=load_vars,\n                           filename=\"param\")"
        }
    ]
}