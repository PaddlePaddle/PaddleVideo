{
    "summary": "Creates a data loader class with LRU caching, PaddlePaddle library, and MSRVTT dataset for efficient training data access. Supports refreshing, clearing caches, setting args, creating datasets, printing cache info, and storing dataloader in an instance variable.",
    "details": [
        {
            "comment": "This code is a function for loading datasets, using LRU caching and PaddlePaddle library. It takes parameters like `use_zeros_for_missing`, `eval_only`, `data_dir`, `text_agg`, `text_feat`, `split_name`, `dataset_name`, and `cls_partition`. It imports MSRVTT dataset for loading specific datasets.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/data_loader/data_loaders.py\":0-35",
            "content": "# Copyright (c) 2021  PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nimport paddle\nimport logging\nimport functools\nfrom pathlib import Path\nfrom typing import Dict, List\nfrom typeguard import typechecked\nfrom zsvision.zs_utils import memcache\nfrom data_loader.MSRVTT_dataset import MSRVTT\nfrom utils import HashableDict, HashableOrderedDict\n@functools.lru_cache(maxsize=64, typed=False)\ndef dataset_loader(\n        use_zeros_for_missing: bool,\n        eval_only: bool,\n        data_dir: str,\n        text_agg: str,\n        text_feat: str,\n        split_name: str,\n        dataset_name: str,\n        cls_partition: str,"
        },
        {
            "comment": "Function `create_dataset` takes parameters to create an instance of a specific dataset class (MSRVTT in this case) with specified options. The function returns the created dataset object.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/data_loader/data_loaders.py\":36-68",
            "content": "        root_feat_folder: str,\n        text_dim: int,\n        num_test_captions: int,\n        restrict_train_captions: int,\n        logger: logging.Logger,\n        max_tokens: Dict[str, int],\n        raw_input_dims: HashableOrderedDict,\n        feat_aggregation: HashableDict,\n):\n    print(f\"refreshing cache for {dataset_name} data loader [{split_name}]\")\n    kwargs = dict(\n        data_dir=Path(data_dir),\n        text_dim=text_dim,\n        logger=logger,\n        eval_only=eval_only,\n        text_agg=text_agg,\n        text_feat=text_feat,\n        max_tokens=max_tokens,\n        split_name=split_name,\n        cls_partition=cls_partition,\n        raw_input_dims=raw_input_dims,\n        root_feat_folder=root_feat_folder,\n        feat_aggregation=feat_aggregation,\n        num_test_captions=num_test_captions,\n        use_zeros_for_missing=use_zeros_for_missing,\n        restrict_train_captions=restrict_train_captions,\n    )\n    if dataset_name == \"MSRVTT\":\n        dataset = MSRVTT(**kwargs)\n    return dataset\nclass ExpertDataLoader:"
        },
        {
            "comment": "This code is a constructor for a data loader class that takes various parameters like eval_only, use_zeros_for_missing, text_dim, batch_size, etc. It initializes the object and ensures dictionaries are hashable to enable caching, and provides an optional refresh of dataloader and cuda cache.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/data_loader/data_loaders.py\":70-100",
            "content": "    @typechecked\n    def __init__(\n            self,\n            eval_only: bool,\n            use_zeros_for_missing: bool,\n            text_dim: int,\n            batch_size: int,\n            num_workers: int,\n            num_test_captions: int,\n            data_dir: str,\n            text_agg: str,\n            text_feat: str,\n            split_name: str,\n            dataset_name: str,\n            root_feat_folder: str,\n            max_tokens: Dict[str, int],\n            raw_input_dims: Dict[str, int],\n            feat_aggregation: Dict[str, Dict],\n            logger: logging.Logger,\n            restrict_train_captions: int = 0,\n            drop_last: bool = False,\n            refresh_lru_cache: bool = False,\n    ):\n        # Ensure that the dictionaries are hashable to allow use of caching\n        raw_input_dims = HashableOrderedDict(raw_input_dims)\n        feat_aggregation = HashableDict(feat_aggregation)\n        max_tokens = HashableDict(max_tokens)\n        if refresh_lru_cache:\n            logger.info(\"Explicitly refreshing dataloader and cuda cache\")"
        },
        {
            "comment": "This code clears dataset and memory caches, sets common arguments for a specific dataset loader function, creates the dataset with these args, prints cache information, and stores the created dataloader in an instance variable.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/data_loader/data_loaders.py\":101-126",
            "content": "            dataset_loader.cache_clear()\n            memcache.cache_clear()\n        common_kwargs = dict(\n            logger=logger,\n            data_dir=data_dir,\n            text_dim=text_dim,\n            text_agg=text_agg,\n            eval_only=eval_only,\n            text_feat=text_feat,\n            max_tokens=max_tokens,\n            dataset_name=dataset_name,\n            split_name=split_name,\n            root_feat_folder=root_feat_folder,\n            use_zeros_for_missing=use_zeros_for_missing,\n            num_test_captions=num_test_captions,\n            raw_input_dims=raw_input_dims,\n            feat_aggregation=feat_aggregation,\n            restrict_train_captions=restrict_train_captions,\n        )\n        dataset = dataset_loader(cls_partition=\"train\", **common_kwargs)\n        x = dataset_loader.cache_info()  # pylint: disable=no-value-for-parameter\n        logger.info(f\"cache info {x}\")\n        self.dataloaders = {\"dataset\": dataset}\n        self.dataloaders[\"retrieval\"] = dataset.get_retrieval_data()"
        },
        {
            "comment": "This function creates a DataLoader for training data with specified parameters and stores it in the self.dataloaders dictionary. It also logs the number of workers used and sets num_test_captions and dataset_name variables. The __getitem__ method returns the dataloader based on the provided key from the self.dataloaders dictionary.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/data_loader/data_loaders.py\":128-144",
            "content": "        if not eval_only:\n            train_loader = paddle.io.DataLoader(\n                dataset=dataset,\n                batch_size=batch_size,\n                num_workers=num_workers,\n                collate_fn=dataset.collate_data,\n                drop_last=drop_last,\n                shuffle=True,\n            )\n            self.dataloaders[\"train\"] = train_loader\n        logger.info(f\"Loading data loaders with {num_workers} workers\")\n        self.num_test_captions = num_test_captions\n        self.dataset_name = dataset_name\n    def __getitem__(self, key):\n        return self.dataloaders[key]"
        }
    ]
}