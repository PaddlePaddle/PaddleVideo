{
    "summary": "ASRF is an improved video action segmentation model built upon ms-tcn, utilizing PaddlePaddle framework for training and exporting inference models. It provides accuracy results and performance metrics, with examples for running inference on PaddleVideo.",
    "details": [
        {
            "comment": "ASRF is an improved video action segmentation model built upon ms-tcn, which was published in 2021. It utilizes the PaddlePaddle framework and can be trained on datasets such as 50salads, breakfast, or gtea. The model requires additional data construction using a specific script for preparation.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/segmentation/asrf.md\":0-34",
            "content": "[\u7b80\u4f53\u4e2d\u6587](../../../zh-CN/model_zoo/segmentation/asrf.md) | English\n# ASRF : Video Action Segmentation Model\n---\n## Contents\n- [Introduction](#Introduction)\n- [Data](#Data)\n- [Train](#Train)\n- [Test](#Test)\n- [Inference](#Inference)\n- [Reference](#Reference)\n## Introduction\nASRF model is an improvement on the video motion segmentation model ms-tcn, which was published on WACV in 2021. We reproduce the officially implemented pytorch code and obtain approximate results in paddlevideo.\n<p align=\"center\">\n<img src=\"../../../images/asrf.png\" height=300 width=400 hspace='10'/> <br />\nMS-TCN Overview\n</p>\n## Data\nASRF can choose 50salads, breakfast, gtea as trianing set. Please refer to Video Action Segmentation dataset download and preparation doc [Video Action Segmentation dataset](../../dataset/SegmentationDataset.md)\nUnlike MS-TCN, ASRF model requires additional data construction. The script process is as follows\n```bash\npython data/50salads/prepare_asrf_data.py --dataset_dir data/\n```\n## Train\nAfter prepare dataset, we can run sprits."
        },
        {
            "comment": "This code is running a training command for an action segmentation model (ASRF) on the GTEA dataset, specifically without using pre-training. It uses CUDA device 3 and a provided configuration file. The test command tests MS-TCN on a dataset using a previously trained model's weights. The index calculation in the test refers to an evaluation script provided by the original author of ms-tcn. The codebase is from the official ASRF repository in PyTorch.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/segmentation/asrf.md\":36-54",
            "content": "```bash\n# gtea dataset\nexport CUDA_VISIBLE_DEVICES=3\npython3.7 main.py  --validate -c configs/segmentation/asrf/asrf_gtea.yaml\n```\n- Start the training by using the above command line or script program. There is no need to use the pre training model. The video action segmentation model is usually a full convolution network. Due to the different lengths of videos, the `DATASET.batch_size` of the video action segmentation model is usually set to `1`, that is, batch training is not required. At present, only **single sample** training is supported.\n## Test\nTest MS-TCN on dataset scripts:\n```bash\npython main.py  --test -c configs/segmentation/asrf/asrf_gtea.yaml --weights=./output/ASRF/ASRF_split_1.pdparams\n```\n- The specific implementation of the index is to calculate ACC, edit and F1 scores by referring to the test script[evel.py](https://github.com/yabufarha/ms-tcn/blob/master/eval.py) provided by the author of ms-tcn.\nThe reproduction of pytorch comes from the official [code base](https://github.com/yiskw713/asrf)"
        },
        {
            "comment": "The code provides accuracy results for different models on three datasets, Breakfast, 50salads, and GTEA, using a 4 or 5-fold validation method as per the MS-TCN paper. The performance metrics include Accuracy (Acc), Edit Distance (Edit), and F1 scores at different thresholds (F1@0.1, F1@0.25, F1@0.5).",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/segmentation/asrf.md\":56-79",
            "content": "- The evaluation method of data set adopts the folding verification method in ms-tcn paper, and the division method of folding is the same as that in ms-tcn paper.\nAccuracy on Breakfast dataset(4 folding verification):\n| Model | Acc | Edit | F1@0.1 | F1@0.25 | F1@0.5 |\n| :---: | :---: | :---: | :---: | :---: | :---: |\n| paper | 67.6% | 72.4% | 74.3% | 68.9% | 56.1% |\n| pytorch | 65.8% | 71.0% | 72.3% | 66.5% | 54.9% |\n| paddle | 66.1% | 71.9% | 73.3% | 67.9% | 55.7% |\nAccuracy on 50salads dataset(5 folding verification):\n| Model | Acc | Edit | F1@0.1 | F1@0.25 | F1@0.5 |\n| :---: | :---: | :---: | :---: | :---: | :---: |\n| paper | 84.5% | 79.3% | 82.9% | 83.5% | 77.3% |\n| pytorch | 81.4% | 75.6% | 82.7% | 81.2% | 77.2% |\n| paddle | 81.6% | 75.8% | 83.0% | 81.5% | 74.8% |\nAccuracy on gtea dataset(4 folding verification):\n| Model | Acc | Edit | F1@0.1 | F1@0.25 | F1@0.5 |\n| :---: | :---: | :---: | :---: | :---: | :---: |\n| paper | 77.3% | 83.7% | 89.4% | 87.8% | 79.8% |\n| pytorch | 76.3% | 79.6% | 87.3% | 85.8% | 74.9% |"
        },
        {
            "comment": "Table showing model weight for gtea with corresponding F1@0.5 and checkpoint links, followed by command to export inference model for ASRF_gtea using given parameters.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/segmentation/asrf.md\":80-99",
            "content": "| paddle | 77.1% | 83.3% | 88.9% | 87.5% | 79.1% |\nModel weight for gtea\nTest_Data| F1@0.5 | checkpoints |\n| :----: | :----: | :---- |\n| gtea_split1 | 72.4409 | [ASRF_gtea_split_1.pdparams](https://videotag.bj.bcebos.com/PaddleVideo-release2.2/ASRF_gtea_split_1.pdparams) |\n| gtea_split2 | 76.6666 | [ASRF_gtea_split_2.pdparams](https://videotag.bj.bcebos.com/PaddleVideo-release2.2/ASRF_gtea_split_2.pdparams) |\n| gtea_split3 | 84.5528 | [ASRF_gtea_split_3.pdparams](https://videotag.bj.bcebos.com/PaddleVideo-release2.2/ASRF_gtea_split_3.pdparams) |\n| gtea_split4 | 82.6771 | [ASRF_gtea_split_4.pdparams](https://videotag.bj.bcebos.com/PaddleVideo-release2.2/ASRF_gtea_split_4.pdparams) |\n## Infer\n### export inference model\n```bash\npython3.7 tools/export_model.py -c configs/segmentation/asrf/asrf_gtea.yaml \\\n                                -p data/ASRF_gtea_split_1.pdparams \\\n                                -o inference/ASRF\n```\nTo get model architecture file `ASRF.pdmodel` and parameters file `ASRF.pdiparams`, use:"
        },
        {
            "comment": "This code provides an example of how to run model inference using the ASRF segmentation model from PaddleVideo. The input file should contain a list of .npy files, and the code demonstrates how to execute it with specific configuration, model, and parameter files. It also shows the location where the results will be written after inference is complete.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/segmentation/asrf.md\":101-130",
            "content": "- Args usage please refer to [Model Inference](https://github.com/PaddlePaddle/PaddleVideo/blob/release/2.0/docs/zh-CN/start.md#2-%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86).\n### infer\nInput file are the file list for infering, for example:\n```\nS1_Cheese_C1.npy\nS1_CofHoney_C1.npy\nS1_Coffee_C1.npy\nS1_Hotdog_C1.npy\n...\n```\n```bash\npython3.7 tools/predict.py --input_file data/gtea/splits/test.split1.bundle \\\n                           --config configs/segmentation/asrf/asrf_gtea.yaml \\\n                           --model_file inference/ASRF/ASRF.pdmodel \\\n                           --params_file inference/ASRF/ASRF.pdiparams \\\n                           --use_gpu=True \\\n                           --use_tensorrt=False\n```\nexample of logs:\n```bash\nresult write in : ./inference/infer_results/S1_Cheese_C1.txt\nresult write in : ./inference/infer_results/S1_CofHoney_C1.txt\nresult write in : ./inference/infer_results/S1_Coffee_C1.txt\nresult write in : ./inference/infer_results/S1_Hotdog_C1.txt\nresult write in : ./inference/infer_results/S1_Pealate_C1.txt"
        },
        {
            "comment": "Writes the inference results to separate text files for \"Peanut\" and \"Tea\" scenes.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/segmentation/asrf.md\":131-138",
            "content": "result write in : ./inference/infer_results/S1_Peanut_C1.txt\nresult write in : ./inference/infer_results/S1_Tea_C1.txt\n```\n## Reference\n- [Alleviating Over-segmentation Errors by Detecting Action Boundaries](https://arxiv.org/pdf/2007.06866v1.pdf), Yuchi Ishikawa, Seito Kasai, Yoshimitsu Aoki, Hirokatsu Kataoka"
        }
    ]
}