{
    "summary": "This code trains TSM model using ResNet-50, PaddlePaddle, and AMP on UCF-101 and Kinetics-400 datasets with Momentum optimization and L2_Decay. It supports three sampling methods, provides training details, and gives inference instructions.",
    "details": [
        {
            "comment": "This code implements the TSM (Temporal Shift Module) model for video understanding using a single RGB stream and ResNet-50 as the backbone. It follows the ICCV 2019 paper for details, and requires data from Kinetics-400 which can be downloaded and prepared according to the provided instructions.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/tsm.md\":0-32",
            "content": "[\u7b80\u4f53\u4e2d\u6587](../../../zh-CN/model_zoo/recognition/tsm.md) | English\n# TSM\n## Contents\n- [Introduction](#Introduction)\n- [Data](#Data)\n- [Train](#Train)\n- [Test](#Test)\n- [Inference](#Inference)\n- [Details](#Details)\n- [Reference](#Reference)\n## Introduction\nTemporal Shift Module (TSM) is a popular model that attracts more attention at present.\nThe method of moving through channels greatly improves the utilization ability of temporal information without increasing any\nadditional number of parameters and calculation amount.\nMoreover, due to its lightweight and efficient characteristics, it is very suitable for industrial landing.\n  <div align=\"center\">\n  <img src=\"../../../images/tsm_architecture.png\" height=250 width=700 hspace='10'/> <br />\n  </div>\nThis code implemented **single RGB stream** of TSM networks. Backbone is ResNet-50.\nPlease refer to the ICCV 2019 paper for details [TSM: Temporal Shift Module for Efficient Video Understanding](https://arxiv.org/pdf/1811.08383.pdf)\n## Data\nPlease refer to Kinetics-400 data download and preparation [k400 data preparation](../../dataset/k400.md)"
        },
        {
            "comment": "This code explains how to train a TSM (Temporal Shift Module) model on the Kinetics-400 dataset using the PaddleVideo framework. The user needs to download and replace the pretrained ResNet50_pretrain.pdparams model, then specify the new weight path in the tsm_k400_frames.yaml configuration file. Training can be started by running a specific command based on the desired configuration.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/tsm.md\":34-61",
            "content": "Please refer to UCF101 data download and preparation [ucf101 data preparation](../../dataset/ucf101.md)\n## Train\n### Train on the Kinetics-400 dataset\n#### download pretrain-model\n1. Please download [ResNet50_pretrain.pdparams](https://videotag.bj.bcebos.com/PaddleVideo/PretrainModel/ResNet50_pretrain.pdparams) as pretraind model:\n   ```bash\n   wget https://videotag.bj.bcebos.com/PaddleVideo/PretrainModel/ResNet50_pretrain.pdparams\n   ```\n2. Open `PaddleVideo/configs/recognition/tsm/tsm_k400_frames.yaml`, and fill in the downloaded weight path below `pretrained:`\n   ```bash\n   MODEL:\n   \tframework: \"Recognizer2D\"\n   \t\tbackbone:\n   \t\tname: \"ResNetTSM\"\n   \t\tpretrained: your weight path\n   ```\n#### Start training\n- By specifying different configuration files, different data formats/data sets can be used for training. Taking the training configuration of Kinetics-400 data set + 8 cards + frames format as an example, the startup command is as follows (more training commands can be viewed in `PaddleVideo/run.sh`)."
        },
        {
            "comment": "This code snippet is running a PaddlePaddle (a deep learning framework) script to train the TSM (Temporal Shift Module) model on the Kinetics-400 dataset. The model is trained for videos and frames formats separately, utilizing Automatic Mixed Precision (AMP) for faster training with some environment variable settings. AMP works better with the NHWC data format and needs specific environment variable configurations as well.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/tsm.md\":63-90",
            "content": "  ```bash\n  python3.7 -B -m paddle.distributed.launch --gpus=\"0,1,2,3,4,5,6,7\" --log_dir=log_tsm main.py  --validate -c configs/recognition/tsm/tsm_k400_frames.yaml\n  ```\n- Training Kinetics-400 dataset of videos format using scripts.\n  ```bash\n  python3.7 -B -m paddle.distributed.launch --gpus=\"0,1,2,3,4,5,6,7\" --log_dir=log_tsm main.py  --validate -c configs/recognition/tsm/tsm_k400_videos.yaml\n  ```\n- AMP is useful for speeding up training, scripts as follows:\n```bash\nexport FLAGS_conv_workspace_size_limit=800 #MB\nexport FLAGS_cudnn_exhaustive_search=1\nexport FLAGS_cudnn_batchnorm_spatial_persistent=1\npython3.7 -B -m paddle.distributed.launch --gpus=\"0,1,2,3,4,5,6,7\" --log_dir=log_tsm main.py  --amp --validate -c configs/recognition/tsm/tsm_k400_frames.yaml\n```\n- AMP works better with `NHWC` data format, scripts as follows:\n```bash\nexport FLAGS_conv_workspace_size_limit=800 #MB\nexport FLAGS_cudnn_exhaustive_search=1\nexport FLAGS_cudnn_batchnorm_spatial_persistent=1\npython3.7 -B -m paddle.distributed.l"
        },
        {
            "comment": "This code snippet is for training the TSM (Temporal Shift Module) model on the UCF-101 dataset. It involves loading a pre-trained model, specifying the configuration file, and using 8 GPUs for training. The command launches the model with amp (automatic mixed precision) and validation mode. The provided link shows how to download the pre-trained TSM_k400 model.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/tsm.md\":90-117",
            "content": "aunch --gpus=\"0,1,2,3,4,5,6,7\" --log_dir=log_tsm main.py  --amp --validate -c configs/recognition/tsm/tsm_k400_frames_nhwc.yaml\n```\n- For the config file usage\uff0cplease refer to [config](../../tutorials/config.md).\n### Train on UCF-101 dataset\n#### download pretrain-model\n- Load the TSM model we trained on Kinetics-400 [TSM_k400.pdparams](https://videotag.bj.bcebos.com/PaddleVideo-release2.1/TSM/TSM_k400.pdparams), or download it through the command line\n  ```bash\n  wget https://videotag.bj.bcebos.com/PaddleVideo-release2.1/TSM/TSM_k400.pdparams\n  ```\n- Open `PaddleVideo/configs/recognition/tsm/tsm_ucf101_frames.yaml`, and fill in the downloaded weight path below `pretrained:`\n  ```bash\n  MODEL:\n      framework: \"Recognizer2D\"\n      backbone:\n          name: \"ResNetTSM\"\n          pretrained: your weight path\n  ```\n#### Start training\n- By specifying different configuration files, different data formats/data sets can be used for training. Taking the training configuration of Kinetics-400 data set + 8 cards"
        },
        {
            "comment": "This code snippet provides commands to train the TSM (Temporal Shift Module) model on the UCF-101 dataset using PaddleVideo. It also demonstrates how to use AMP (Automatic Mixed Precision) for faster training and shows that it works better with `NHWC` data format. The provided commands can be executed in a terminal, specifying the required arguments like GPUs, log directory, and configuration file.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/tsm.md\":117-143",
            "content": " + frames format as an example, the startup command is as follows (more training commands can be viewed in `PaddleVideo/run.sh`).\n  ```bash\n  python3.7 -B -m paddle.distributed.launch --gpus=\"0,1,2,3\" --log_dir=log_tsm main.py  --validate -c configs/recognition/tsm/tsm_ucf101_frames.yaml\n  ```\n- Training UCF-101 dataset of videos format using scripts.\n  ```bash\n  python3.7 -B -m paddle.distributed.launch --gpus=\"0,1,2,3\" --log_dir=log_tsm main.py  --validate -c configs/recognition/tsm/tsm_ucf101_videos.yaml\n  ```\n- AMP is useful for speeding up training, scripts as follows:\n  ```bash\n  export FLAGS_conv_workspace_size_limit=800 #MB\n  export FLAGS_cudnn_exhaustive_search=1\n  export FLAGS_cudnn_batchnorm_spatial_persistent=1\n  python3.7 -B -m paddle.distributed.launch --gpus=\"0,1,2,3\" --log_dir=log_tsm main.py  --amp --validate -c configs/recognition/tsm/tsm_ucf101_frames.yaml\n  ```\n- AMP works better with `NHWC` data format, scripts as follows:\n  ```bash\n  export FLAGS_conv_workspace_size_limit=800 #MB\n  export FLAGS_cudnn_exhaustive_search=1"
        },
        {
            "comment": "This code exports the flag for CUDNN batch normalization spatial persistent and runs a Python script to test the model with specified configuration files. The testing command takes the best model weights from a directory and evaluates the accuracy on validation datasets of Kinetics-400 and UCF-101.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/tsm.md\":144-165",
            "content": "  export FLAGS_cudnn_batchnorm_spatial_persistent=1\n  python3.7 -B -m paddle.distributed.launch --gpus=\"0,1,2,3\" --log_dir=log_tsm main.py  --amp --validate -c configs/recognition/tsm/tsm_ucf101_frames_nhwc.yaml\n  ```\n## Test\nPut the weight of the model to be tested into the `output/TSM/` directory, the test command is as follows\n```bash\npython3 main.py --test -c configs/recognition/tsm/tsm.yaml -w output/TSM/TSM_best.pdparams\n```\n---\nWhen the test configuration uses the following parameters, the evaluation accuracy on the validation data set of Kinetics-400 is as follows:\n| backbone | Sampling method | Training Strategy | num_seg | target_size | Top-1 | checkpoints |\n| :--------: | :---------------: | :-------: | :-----------: | :-----: | :-----------: | :-----------: |\n| ResNet50 | Uniform         | NCHW | 8       | 224         | 71.06 | [TSM_k400.pdparams](https://videotag.bj.bcebos.com/PaddleVideo-release2.1/TSM/TSM_k400.pdparams)        |\nWhen the test configuration uses the following parameters, the evaluation accuracy on the validation data set of UCF-101 is as follows:"
        },
        {
            "comment": "This code provides information about different TSM (Temporal Shift Module) models trained using ResNet50 backbone with three sampling methods: Uniform, NCHW, NHWC+AMP. It shows the training strategy, number of segments, target size, and Top-1 accuracy for each model. It also mentions where to find the corresponding checkpoints and provides instructions on how to export the inference model using Python script.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/tsm.md\":167-180",
            "content": "| backbone | Sampling method | Training Strategy | num_seg | target_size | Top-1 | checkpoints |\n| :------: | :-------------: | :-----------------: | :-----: | :---------: | :---: | :---------: |\n| ResNet50 |     Uniform     | NCHW              |    8    |     224     | 94.42 |    [TSM_ucf101_nchw.pdparams](https://videotag.bj.bcebos.com/PaddleVideo-release2.1/TSM/TSM_ucf101_nchw.pdparams)     |\n| ResNet50 |     Uniform     | NCHW+AMP |    8    |     224     | 94.40 |   [TSM_ucf101_amp_nchw.pdparams](https://videotag.bj.bcebos.com/PaddleVideo-release2.1/TSM/TSM_ucf101_amp_nchw.pdparams)     |\n| ResNet50 |     Uniform     | NHWC+AMP |    8    |     224     | 94.55 |   [TSM_ucf101_amp_nhwc.pdparams](https://videotag.bj.bcebos.com/PaddleVideo-release2.1/TSM/TSM_ucf101_amp_nhwc.pdparams)     |\n## Inference\n### export inference model\nTo get model architecture file `TSM.pdmodel` and parameters file `TSM.pdiparams`, use:\n```bash\npython3.7 tools/export_model.py -c configs/recognition/tsm/tsm_k400_frames.yaml \\"
        },
        {
            "comment": "This code is running a model inference for TSM (Temporal Shift Module) on an input video file using PaddlePaddle framework. It specifies the necessary arguments including the input file, configuration file, and model files. The --use_gpu and --use_tensorrt options are set to True and False respectively. The data processing step involves dividing the video into segments, extracting frames randomly, and applying random data enhancement.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/tsm.md\":181-202",
            "content": "                                -p data/TSM_k400.pdparams \\\n                                -o inference/TSM\n```\n- Args usage please refer to [Model Inference](https://github.com/PaddlePaddle/PaddleVideo/blob/release/2.0/docs/zh-CN/start.md#2-%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86).\n### infer\n```bash\npython3.7 tools/predict.py --input_file data/example.avi \\\n                           --config configs/recognition/tsm/tsm_k400_frames.yaml \\\n                           --model_file inference/TSM/TSM.pdmodel \\\n                           --params_file inference/TSM/TSM.pdiparams \\\n                           --use_gpu=True \\\n                           --use_tensorrt=False\n```\n## Implementation details\n### data processing\n- The model reads the `mp4` data in the Kinetics-400 data set, first divides each piece of video data into `num_seg` segments, and then uniformly extracts 1 frame of image from each segment to obtain sparsely sampled `num_seg` video frames. Then do the same random data enhancement to this `n"
        },
        {
            "comment": "The code outlines the training strategy for TSM (Temporal Shift Module) model, which includes using Momentum optimization algorithm with L2_Decay, global gradient clipping, and attenuating the learning rate at certain epochs. It also specifies the total number of epochs, learning rates for FC layer weights and biases, Dropout ratio, and parameter initialization methods.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/tsm.md\":202-220",
            "content": "um_seg` frame image, including multi-scale random cropping, random left and right flips, data normalization, etc., and finally zoom to `target_size`.\n### Training strategy\n*  Use Momentum optimization algorithm training, momentum=0.9\n*  Using L2_Decay, the weight attenuation coefficient is 1e-4\n*  Using global gradient clipping, the clipping factor is 20.0\n*  The total number of epochs is 50, and the learning rate will be attenuated by 0.1 times when the epoch reaches 20 and 40\n*  The learning rate of the weight and bias of the FC layer are respectively 5 times and 10 times the overall learning rate, and the bias does not set L2_Decay\n*  Dropout_ratio=0.5\n### Parameter initialization\n- Initialize the weight of the FC layer with the normal distribution of Normal(mean=0, std=0.001), and initialize the bias of the FC layer with a constant of 0\n## Reference\n- [TSM: Temporal Shift Module for Efficient Video Understanding](https://arxiv.org/pdf/1811.08383.pdf), Ji Lin, Chuang Gan, Song Han"
        }
    ]
}