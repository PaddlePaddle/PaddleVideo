{
    "summary": "The code imports modules, defines Kaiming uniform initialization and SingleStageModel class. It initializes MSTCN backbone with DilatedResidualLayer stages and applies softmax to previous outputs, concatenating them together while initializing weights for convolutional layers with KaimingUniform_like_torch.",
    "details": [
        {
            "comment": "This code snippet appears to be part of a larger file and sets up some initial definitions, imports, and checks for necessary conditions. It includes license information, imports various modules, and defines a function to calculate fan-in and fan-out for tensor dimensions.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/ms_tcn.py\":0-31",
            "content": "# Copyright (c) 2021  PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport paddle\nimport paddle.nn as nn\nimport paddle.nn.functional as F\nimport numpy as np\nimport copy\nimport random\nimport math\nfrom paddle import ParamAttr\nfrom ..registry import BACKBONES\nfrom ..weight_init import weight_init_\ndef _calculate_fan_in_and_fan_out(tensor):\n    dimensions = len(tensor.shape)\n    if dimensions < 2:\n        raise ValueError(\"Fan in and fan out can not be computed \\\n        for tensor with fewer than 2 dimensions\")"
        },
        {
            "comment": "This code defines three functions: `_calculate_fan_in_and_fan_out`, `calculate_gain`, and `KaimingUniform_like_torch`. The first function calculates the fan-in and fan-out values based on the input tensor's dimensions. The second function determines the gain value depending on the nonlinearity used. The third function applies the Kaiming uniform initialization to the weight_npy parameter, utilizing the previous two functions.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/ms_tcn.py\":33-67",
            "content": "    if dimensions == 2:  # Linear\n        fan_in = tensor.shape[1]\n        fan_out = tensor.shape[0]\n    else:\n        num_input_fmaps = tensor.shape[1]\n        num_output_fmaps = tensor.shape[0]\n        receptive_field_size = 1\n        if tensor.dim() > 2:\n            receptive_field_size = tensor[0][0].numel()\n        fan_in = num_input_fmaps * receptive_field_size\n        fan_out = num_output_fmaps * receptive_field_size\n    return fan_in, fan_out\ndef calculate_gain(nonlinearity=None, a=None):\n    if nonlinearity == 'tanh':\n        return 5.0 / 3\n    elif nonlinearity == 'relu':\n        return math.sqrt(2.0)\n    elif nonlinearity == 'leaky_relu':\n        if a != None:\n            return math.sqrt(2.0 / (1 + a**2))\n        else:\n            return math.sqrt(2.0 / (1 + 0.01**2))\n    elif nonlinearity == 'selu':\n        return 3.0 / 4\n    else:\n        return 1\ndef KaimingUniform_like_torch(weight_npy,\n                              mode='fan_in',\n                              nonlinearity='leaky_relu'):\n    fan_in, fan_out = _calculate_fan_in_and_fan_out(weight_npy)"
        },
        {
            "comment": "This code defines a SingleStageModel class that inherits from nn.Layer and consists of a convolutional layer, multiple DilatedResidualLayers, and another convolutional layer. The model is initialized with specified parameters: number of layers, number of feature maps, input dimension, and number of output classes.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/ms_tcn.py\":68-99",
            "content": "    if mode == 'fan_in':\n        fan_mode = fan_in\n    else:\n        fan_mode = fan_out\n    a = math.sqrt(5.0)\n    gain = calculate_gain(nonlinearity=nonlinearity, a=a)\n    std = gain / math.sqrt(fan_mode)\n    bound = math.sqrt(3.0) * std\n    return np.random.uniform(-bound, bound, weight_npy.shape)\ndef init_bias(weight_npy, bias_npy):\n    # attention this weight is not bias\n    fan_in, fan_out = _calculate_fan_in_and_fan_out(weight_npy)\n    bound = 1.0 / math.sqrt(fan_in)\n    return np.random.uniform(-bound, bound, bias_npy.shape)\nclass SingleStageModel(nn.Layer):\n    def __init__(self, num_layers, num_f_maps, dim, num_classes):\n        super(SingleStageModel, self).__init__()\n        self.conv_in = nn.Conv1D(dim, num_f_maps, 1)\n        self.layers = nn.LayerList([\n            copy.deepcopy(DilatedResidualLayer(2**i, num_f_maps, num_f_maps))\n            for i in range(num_layers)\n        ])\n        self.conv_out = nn.Conv1D(num_f_maps, num_classes, 1)\n    def forward(self, x):\n        out = self.conv_in(x)\n        for layer in self.layers:"
        },
        {
            "comment": "The code defines a DilatedResidualLayer, which is a type of residual layer used in the MSTCN backbone. The MSTCN class initializes a SingleStageModel and a list of stages using the provided parameters. Each stage within the model is an instance of the DilatedResidualLayer.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/ms_tcn.py\":100-131",
            "content": "            out = layer(out)\n        out = self.conv_out(out)\n        return out\nclass DilatedResidualLayer(nn.Layer):\n    def __init__(self, dilation, in_channels, out_channels):\n        super(DilatedResidualLayer, self).__init__()\n        self.conv_dilated = nn.Conv1D(in_channels,\n                                      out_channels,\n                                      3,\n                                      padding=dilation,\n                                      dilation=dilation)\n        self.conv_in = nn.Conv1D(out_channels, out_channels, 1)\n        self.dropout = nn.Dropout()\n    def forward(self, x):\n        out = F.relu(self.conv_dilated(x))\n        out = self.conv_in(out)\n        out = self.dropout(out)\n        return (x + out)\n@BACKBONES.register()\nclass MSTCN(nn.Layer):\n    def __init__(self, num_stages, num_layers, num_f_maps, dim, num_classes):\n        super().__init__()\n        self.stage1 = SingleStageModel(num_layers, num_f_maps, dim, num_classes)\n        self.stages = nn.LayerList([\n            copy.deepcopy("
        },
        {
            "comment": "The code defines a forward function for MSTCN model and initializes the weights for convolutional layers. It iterates over stages, applying softmax to previous output and concatenating it to previous outputs. Weights are initialized with KaimingUniform_like_torch for conv1D layers and bias is set according to the layer's weight.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/ms_tcn.py\":132-153",
            "content": "                SingleStageModel(num_layers, num_f_maps, num_classes,\n                                 num_classes)) for s in range(num_stages - 1)\n        ])\n    def forward(self, x):\n        \"\"\" MSTCN forward\n        \"\"\"\n        out = self.stage1(x)\n        outputs = out.unsqueeze(0)\n        for s in self.stages:\n            out = s(F.softmax(out, axis=1))\n            outputs = paddle.concat((outputs, out.unsqueeze(0)), axis=0)\n        return outputs\n    def init_weights(self):\n        for layer in self.sublayers():\n            if isinstance(layer, nn.Conv1D):\n                layer.weight.set_value(\n                    KaimingUniform_like_torch(layer.weight).astype('float32'))\n                if layer.bias is not None:\n                    layer.bias.set_value(\n                        init_bias(layer.weight, layer.bias).astype('float32'))"
        }
    ]
}