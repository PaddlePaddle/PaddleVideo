{
    "summary": "This code initializes optimizer configurations, handles weight decay, grad clip, and excludes parameters for L2 decay. It sets learning rate with LRScheduler, supports multi-precision, and creates an optimizer based on inputs.",
    "details": [
        {
            "comment": "This code is from the \"optimizer.py\" file in the PaddleVideo library, and it's responsible for building an optimizer. It imports necessary modules, checks compatibility with Python versions, defines a function build_optimizer that takes parameters such as configuration (cfg), learning rate scheduler (lr_scheduler), model, and optional AMP usage (use_amp). This file also includes some license information and comments.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/solver/optimizer.py\":0-30",
            "content": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport inspect\n# for python3.11\nif not hasattr(inspect, 'getargspec'):\n    inspect.getargspec = inspect.getfullargspec\nfrom typing import Dict\nimport paddle\nfrom paddle.optimizer.lr import LRScheduler\nfrom paddle.regularizer import L1Decay, L2Decay\nfrom paddlevideo.utils import get_logger\ndef build_optimizer(cfg: Dict,\n                    lr_scheduler: LRScheduler,\n                    model: paddle.nn.Layer,\n                    use_amp: bool = False,"
        },
        {
            "comment": "Builds an optimizer and learning rate scheduler according to the OPTIMIZER field in the configuration. The Momentum or Adam optimizers are applied to optimize the network, and L1Decay or L2Decay regularizers are used to avoid overfitting. The function takes optimizer configuration (cfg) and learning rate scheduler (lr_scheduler) as arguments.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/solver/optimizer.py\":31-62",
            "content": "                    amp_level: str = None) -> paddle.optimizer.Optimizer:\n    \"\"\"Build an optimizer and learning rate scheduler to optimize parameters accroding to ```OPTIMIZER``` field in configuration.\n    In configuration:\n    OPTIMIZER:\n        name: Momentum\n        momentum: 0.9\n        weight_decay: 0.001\n    or\n    OPTIMIZER:\n        name: Momentum\n        momentum: 0.9\n        weight_decay:\n            name: \"L1\"\n            value: 0.001\n    Momentum optimizer will be applied to optimize network and L1Decay regularizer will be applied to avoid overfit.\n    OPTIMIZER:\n        name: Adam\n        weight_decay:\n            name: \"L2\"\n            value: 0.001\n    Adam optimizer will be applied to optimize network and L2Decay regularizer will applied to avoid overfit.\n    Refer to ```https://www.paddlepaddle.org.cn/documentation/docs/en/develop/api/paddle/regularizer/L2Decay_en.html``` for more details.\n    Args:\n        cfg (Dict): optimizer configuration.\n        lr_scheduler (LRScheduler): learning rate scheduler."
        },
        {
            "comment": "This code defines a function that creates an optimizer for a given model. It accepts parameters such as the model, whether to use AMP or not, and the AMP level. The function also handles weight decay by checking if a 'weight_decay' configuration is present and applying the appropriate settings (L1 or L2 decay).",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/solver/optimizer.py\":63-84",
            "content": "        model (paddle.nn.Layer, optional): model which contains parameters to be optimized. Defaults to None.\n        use_amp (bool, optional): Whether use amp. Defaults to False.\n        amp_level (str, optional): amp level when amp is enabled. Defaults to None.\n    Returns:\n        paddle.optimizer.Optimizer: an optimizer for the input model.\n    \"\"\"\n    logger = get_logger(\"paddlevideo\")\n    cfg_copy = cfg.copy()\n    # NOTE: check none and illegal cfg!!!\n    opt_name = cfg_copy.pop('name')\n    # deal with weight decay\n    if cfg_copy.get('weight_decay'):\n        if isinstance(cfg_copy.get('weight_decay'),\n                      float):  # just an float factor\n            cfg_copy['weight_decay'] = cfg_copy.get('weight_decay')\n        elif 'L1' in cfg_copy.get('weight_decay').get(\n                'name').upper():  # specify L2 wd and it's float factor\n            cfg_copy['weight_decay'] = L1Decay(\n                cfg_copy.get('weight_decay').get('value'))\n        elif 'L2' in cfg_copy.get('weight_decay').get("
        },
        {
            "comment": "This code is initializing the configuration for an optimizer, handling L1 and L2 weight decay, grad clip, and no_weight_decay parameters. If 'name' is specified for L1 wd, it sets the 'weight_decay' to the float factor. For grad clip, if a float value is given, it is set as the 'grad_clip', or if 'global' in name, creates a ClipGradByGlobalNorm object. If 'no_weight_decay_name' is specified, it extracts the list of parameters to exclude from L2 decay.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/solver/optimizer.py\":85-108",
            "content": "                'name').upper():  # specify L1 wd and it's float factor\n            cfg_copy['weight_decay'] = L2Decay(\n                cfg_copy.get('weight_decay').get('value'))\n        else:\n            raise ValueError\n    # deal with grad clip\n    if cfg_copy.get('grad_clip'):\n        if isinstance(cfg_copy.get('grad_clip'), float):\n            cfg_copy['grad_clip'] = cfg_copy.get('grad_clip').get('value')\n        elif 'global' in cfg_copy.get('grad_clip').get('name').lower():\n            cfg_copy['grad_clip'] = paddle.nn.ClipGradByGlobalNorm(\n                cfg_copy.get('grad_clip').get('value'))\n        else:\n            raise ValueError\n    # Set for optimizers that cannot be applied to l2decay, i.e. AdamW\n    if cfg_copy.get('no_weight_decay_name'):\n        no_weight_decay_name = cfg_copy.pop('no_weight_decay_name')\n        no_weight_decay_name_list = no_weight_decay_name.split(' ')\n        # NOTE: use param.name not name\n        no_weight_decay_param_list = [\n            param.name for name, param in model.named_parameters()"
        },
        {
            "comment": "This code checks if there are any parameters without weight decay, and sets the learning rate using a LRScheduler. It also handles multi-precision for optimizer when use_amp is True and amp_level is 'O2'. The code updates the optimizer_setting with no_weight_decay_param_list and \"multi_precision\" if required, logging relevant information throughout.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/solver/optimizer.py\":109-132",
            "content": "            if any(key_word in name for key_word in no_weight_decay_name_list)\n        ]  # get the full param name of no weight decay\n        _apply_decay_param_fun = lambda name: name not in no_weight_decay_param_list\n        cfg_copy['apply_decay_param_fun'] = _apply_decay_param_fun\n        logger.info(\n            f\"No weight Decay list :({len(no_weight_decay_param_list)})\",\n            no_weight_decay_param_list)\n    cfg_copy.pop('learning_rate')\n    # set multi_precision\n    optimizer_setting = {\n        'learning_rate': lr_scheduler,\n        'parameters': model.parameters(),\n        **cfg_copy\n    }\n    optimizer_init_args = inspect.getargspec(\n        getattr(paddle.optimizer, opt_name).__init__).args\n    if use_amp and amp_level == \"O2\" and \"multi_precision\" in optimizer_init_args:\n        # support \"multi_precision\" arg in optimizer's __init__ function.\n        optimizer_setting.update({\"multi_precision\": True})\n        logger.info(\n            \"Set multi_precision=True for optimizer when use_amp=True and amp_level='O2'\""
        },
        {
            "comment": "This code is creating and returning an optimizer based on the given \"opt_name\" and \"optimizer_setting\". The optimizer type is determined by using \"paddle.optimizer[opt_name]\" and the parameters are passed through **optimizer_settings** to initialize the optimizer object.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/solver/optimizer.py\":133-135",
            "content": "        )\n    return getattr(paddle.optimizer, opt_name)(**optimizer_setting)"
        }
    ]
}