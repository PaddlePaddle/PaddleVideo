{
    "summary": "This code shows deploying PaddleServing for deep learning model prediction via HTTP using PP-TSM models and Docker on Linux. Issues with proxy, no response; check log file for errors at \"./deploy/python_serving/PipelineServingLogs/pipeline.log\". Refer to Serving's GitHub for more deployment types like RPC prediction service.",
    "details": [
        {
            "comment": "This code provides an overview of deploying a model service using PaddleServing for deep learning predictions. It uses HTTP prediction service deployment as an example and suggests installing Serving through Docker on Linux platforms, while Windows is currently not supported.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/python_serving/readme_en.md\":0-15",
            "content": "English | [\u7b80\u4f53\u4e2d\u6587](./readme.md)\n# Model service deployment\n## Introduction\n[Paddle Serving](https://github.com/PaddlePaddle/Serving) aims to help deep learning developers easily deploy online prediction services, support one-click deployment of industrial-grade service capabilities, high concurrency between client and server Efficient communication and support for developing clients in multiple programming languages.\nThis section takes the HTTP prediction service deployment as an example to introduce how to use PaddleServing to deploy the model service in PaddleVideo. Currently, only Linux platform deployment is supported, and Windows platform is not currently supported.\n## Serving installation\nThe Serving official website recommends using docker to install and deploy the Serving environment. First, you need to pull the docker environment and create a Serving-based docker.\n```bash\n# start GPU docker\ndocker pull paddlepaddle/serving:0.7.0-cuda10.2-cudnn7-devel\nnvidia-docker run -p 9292:9292 --name test -dit paddlepaddle/serving:0.7.0-cuda10.2-cudnn7-devel bash"
        },
        {
            "comment": "Install PaddleServing server and client packages for CPU and GPU environments, depending on the deployment type.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/python_serving/readme_en.md\":16-40",
            "content": "nvidia-docker exec -it test bash\n# start CPU docker\ndocker pull paddlepaddle/serving:0.7.0-devel\ndocker run -p 9292:9292 --name test -dit paddlepaddle/serving:0.7.0-devel bash\ndocker exec -it test bash\n```\nAfter entering docker, you need to install Serving-related python packages.\n```bash\npython3.7 -m pip install paddle-serving-client==0.7.0\npython3.7 -m pip install paddle-serving-app==0.7.0\npython3.7 -m pip install faiss-cpu==1.7.1post2\n#If it is a CPU deployment environment:\npython3.7 -m pip install paddle-serving-server==0.7.0 #CPU\npython3.7 -m pip install paddlepaddle==2.2.0 # CPU\n#If it is a GPU deployment environment\npython3.7 -m pip install paddle-serving-server-gpu==0.7.0.post102 # GPU with CUDA10.2 + TensorRT6\npython3.7 -m pip install paddlepaddle-gpu==2.2.0 # GPU with CUDA10.2\n#Other GPU environments need to confirm the environment and then choose which one to execute\npython3.7 -m pip install paddle-serving-server-gpu==0.7.0.post101 # GPU with CUDA10.1 + TensorRT6\npython3.7 -m pip install paddle-serving-server-gpu==0.7.0.post112 # GPU with CUDA11.2 + TensorRT8"
        },
        {
            "comment": "This code snippet provides instructions on how to deploy a behavior recognition service using PaddleServing. It explains that the model must be converted into a Serving model and provides an example of PP-TSM model conversion process. The user is guided to enter the PaddleVideo directory, download the trained PP-TSM model, convert it into an inference model, and finally, provide an option to download a pre-converted inference model if desired.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/python_serving/readme_en.md\":41-62",
            "content": "```\n* If the installation speed is too slow, you can change the source through `-i https://pypi.tuna.tsinghua.edu.cn/simple` to speed up the installation process\n* For more environment and corresponding installation packages, see: https://github.com/PaddlePaddle/Serving/blob/v0.9.0/doc/Install_Linux_Env_CN.md\n## Behavior recognition service deployment\n### Model conversion\nWhen using PaddleServing for service deployment, you need to convert the saved inference model into a Serving model. The following uses the PP-TSM model as an example to introduce how to deploy the behavior recognition service.\n- Download the trained PP-TSM model and convert it into an inference model:\n  ```bash\n  # Enter PaddleVideo directory\n  cd PaddleVideo\n  wget -P data/ https://videotag.bj.bcebos.com/PaddleVideo-release2.1/PPTSM/ppTSM_k400_uniform.pdparams\n  python3.7 tools/export_model.py \\\n  -c configs/recognition/pptsm/pptsm_k400_frames_uniform.yaml \\\n  -p data/ppTSM_k400_uniform.pdparams \\\n  -o inference/ppTSM\n  ```\n- We also provide the converted inference model, download and unzip by the following command"
        },
        {
            "comment": "This code downloads a pre-trained model and converts it into a format suitable for server deployment using paddle_serving_client. The converted model is saved in the specified directory with the corresponding program and parameter files.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/python_serving/readme_en.md\":63-82",
            "content": "  ```bash\n  mkdir ./inference\n  wget -nc -P ./inference https://videotag.bj.bcebos.com/PaddleVideo-release2.3/ppTSM.zip --no-check-certificate\n  pushd ./inference\n  unzip ppTSM.zip\n  popd\n  ```\n- Use paddle_serving_client to convert the converted inference model into a model format that is easy for server deployment:\n  ```bash\n  python3.7 -m paddle_serving_client.convert \\\n  --dirname inference/ppTSM \\\n  --model_filename ppTSM.pdmodel \\\n  --params_filename ppTSM.pdiparams \\\n  --serving_server ./deploy/python_serving/ppTSM_serving_server/ \\\n  --serving_client ./deploy/python_serving/ppTSM_serving_client/\n  ```\n  | parameter | type | default value | description |\n  | ----------------- | ---- | ------------------ | ------- -------------------------------------------------- --- |\n  | `dirname` | str | - | The storage path of the model file to be converted. The program structure file and parameter file are saved in this directory. |\n  | `model_filename` | str | None | The name of the file storing the model In"
        },
        {
            "comment": "This code defines the required parameters for converting a PaddleVideo PP-TSM inference model. Upon successful conversion, it creates `ppTSM_serving_server` and `ppTSM_serving_client` folders with necessary files for the converted model's serving.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/python_serving/readme_en.md\":82-93",
            "content": "ference Program structure that needs to be converted. If set to None, use `__model__` as the default filename |\n  | `params_filename` | str | None | File name where all parameters of the model to be converted are stored. It needs to be specified if and only if all model parameters are stored in a single binary file. If the model parameters are stored in separate files, set it to None |\n  | `serving_server` | str | `\"serving_server\"` | The storage path of the converted model files and configuration files. Default is serving_server |\n  | `serving_client` | str | `\"serving_client\"` | The converted client configuration file storage path. Default is serving_client |\nAfter the PP-TSM inference model is converted, there will be additional folders of `ppTSM_serving_server` and `ppTSM_serving_client` in the current folder, with the following formats:\n  ```bash\n  PaddleVideo/deploy/python_serving\n  \u251c\u2500\u2500 ppTSM_serving_server\n      \u251c\u2500\u2500 ppTSM.pdiparams\n      \u251c\u2500\u2500 ppTSM.pdmodel\n      \u251c\u2500\u2500 serving_server_conf.prototxt"
        },
        {
            "comment": "This code snippet is modifying the model configuration files `serving_server_conf.prototxt` and `serving_client_conf.stream.prototxt`. It changes the `alias_name` under `fetch_var` to \"outputs\" in both files for compatibility with different models during deployment. This allows the inference and deployment of various models without modifying the code, simply by updating the configuration file's alias names.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/python_serving/readme_en.md\":94-118",
            "content": "      \u2514\u2500\u2500 serving_server_conf.stream.prototxt\n  \u251c\u2500\u2500 ppTSM_serving_client\n      \u251c\u2500\u2500 serving_client_conf.prototxt\n      \u2514\u2500\u2500 serving_client_conf.stream.prototxt\n  ```\nAfter getting the model files, you need to modify the files `serving_server_conf.prototxt` under `ppTSM_serving_server` and `ppTSM_serving_client` respectively, and change `alias_name` under `fetch_var` in both files to `outputs`\n**Remarks**: In order to be compatible with the deployment of different models, Serving provides the function of input and output renaming. In this way, when different models are inferred and deployed, they only need to modify the `alias_name` of the configuration file, and the inference deployment can be completed without modifying the code.\nThe modified `serving_server_conf.prototxt` looks like this:\n```yaml\nfeed_var {\n  name: \"data_batch_0\"\n  alias_name: \"data_batch_0\"\n  is_lod_tensor: false\n  feed_type: 1\n  shape: 8\n  shape: 3\n  shape: 224\n  shape: 224\n}\nfetch_var {\n  name: \"linear_2.tmp_1\"\n  alias_name: \"outputs\"\n  is_lod_tensor: false"
        },
        {
            "comment": "This code snippet is for starting the PaddleVideo pipeline service in Python using the recognition_web_service.py script. The `-n` flag specifies the name of the model, and the `-c` flag points to the configuration file for the pipeline service.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/python_serving/readme_en.md\":119-144",
            "content": "  fetch_type: 1\n  shape: 400\n}\n```\n### Service deployment and requests\nThe `python_serving` directory contains the code for starting the pipeline service, C++ serving service (TODO) and sending prediction requests, including:\n```bash\n__init__.py\nconfigs/xxx.yaml            # start the configuration file of the pipeline service\npipeline_http_client.py     # python script for sending pipeline prediction request via http\npipeline_rpc_client.py      # python script for sending pipeline prediction request in rpc mode\nrecognition_web_service.py  # python script that starts the pipeline server\nutils.py                    # common functions used in inference, such as parse_file_paths, numpy_to_base64, video_to_numpy\n```\n#### Python Serving\n- Go to the working directory:\n```bash\ncd deploy/python_serving\n```\n- Start the service:\n```bash\n# Start in the current command line window and stay in front\npython3.7 recognition_web_service.py -n PPTSM -c configs/PP-TSM.yaml\n# Start in the background, the logs printed during the process will be redirected and saved to log.txt"
        },
        {
            "comment": "This code is running a web service for model prediction and two client scripts to send prediction requests via HTTP and RPC, printing the results in the command line. The result shows an example output with probabilities and labels for a given input video file. If no result is returned or there's an output decoding error, it might be related to the proxy setting when starting the service.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/python_serving/readme_en.md\":145-174",
            "content": "python3.7 recognition_web_service.py -n PPTSM -c configs/PP-TSM.yaml &>log.txt &\n```\n- send request:\n```bash\n# Send a prediction request in http and receive the result\npython3.7 pipeline_http_client.py -i ../../data/example.avi\n# Send a prediction request in rpc and receive the result\npython3.7 pipeline_rpc_client.py -i ../../data/example.avi\n```\nAfter a successful run, the results of the model prediction will be printed in the cmd window, and the results are as follows:\n```bash\n# http method print result\n{'err_no': 0, 'err_msg': '', 'key': ['label', 'prob'], 'value': [\"['archery']\", '[0.9907388687133789]'], 'tensors ': []}\n# The result of printing in rpc mode\nPipelineClient::predict pack_data time:1645631086.764019\nPipelineClient::predict before time:1645631086.8485317\nkey: \"label\"\nkey: \"prob\"\nvalue: \"[\\'archery\\']\"\nvalue: \"[0.9907388687133789]\"\n```\n## FAQ\n**Q1**: No result is returned after the request is sent or an output decoding error is prompted\n**A1**: Do not set the proxy when starting the service an"
        },
        {
            "comment": "Closing the proxy before starting the service and sending request using \"unset https_proxy; unset http_proxy\". No response after server started, check log file for error message at \"./deploy/python_serving/PipelineServingLogs/pipeline.log\". For more deployment types like RPC prediction service, refer to Serving's GitHub official website.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/python_serving/readme_en.md\":174-184",
            "content": "d sending the request. You can close the proxy before starting the service and sending the request. The command to close the proxy is:\n```\nunset https_proxy\nunset http_proxy\n```\n**Q2**: There is no response after the server is started, and it has been stopped at `start proxy service`\n**A2**: It is likely that a problem was encountered during the startup process. You can view the detailed error message in the `./deploy/python_serving/PipelineServingLogs/pipeline.log` log file\nFor more service deployment types, such as `RPC prediction service`, you can refer to Serving's [github official website](https://github.com/PaddlePaddle/Serving/tree/v0.7.0/examples)"
        }
    ]
}