{
    "summary": "This code defines a PaddlePaddle transformer encoder layer for normalization and training, including residual connections, dropout, self-attention mechanism, and position-wise feed-forward networks. It creates a Transformer Encoder with Scaled Dot-Product Attention for NLP tasks.",
    "details": [
        {
            "comment": "This code defines a function called \"multi_head_attention\" which performs multi-head attention operations on queries, keys, and values. The function takes in additional parameters such as attn_bias, d_key, d_value, d_model. This is part of the Transformer encoder model implementation in PaddlePaddle framework.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/MultimodalVideoTag/scenario_lib/models/transformer_encoder.py\":0-31",
            "content": "#   Copyright (c) 2019 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Transformer encoder.\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom functools import partial\nimport paddle\nimport paddle.static as static\ndef multi_head_attention(queries,\n                         keys,\n                         values,\n                         attn_bias,\n                         d_key,\n                         d_value,\n                         d_model,"
        },
        {
            "comment": "This code snippet defines a Multi-Head Attention layer. It takes in queries, keys (optional), and values (optional) as inputs, and performs linear projections on the queries before computing the attention weights. The function __compute_qkv also handles the case when keys or values are None by setting them to be equal to queries if needed. The inputs should all be 3-D tensors.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/MultimodalVideoTag/scenario_lib/models/transformer_encoder.py\":32-56",
            "content": "                         n_head=1,\n                         dropout_rate=0.,\n                         cache=None,\n                         param_initializer=None,\n                         name='multi_head_att'):\n    \"\"\"\n    Multi-Head Attention. Note that attn_bias is added to the logit before\n    computing softmax activiation to mask certain selected positions so that\n    they will not considered in attention weights.\n    \"\"\"\n    keys = queries if keys is None else keys\n    values = keys if values is None else values\n    if not (len(queries.shape) == len(keys.shape) == len(values.shape) == 3):\n        raise ValueError(\n            \"Inputs: quries, keys and values should all be 3-D tensors.\")\n    def __compute_qkv(queries, keys, values, n_head, d_key, d_value):\n        \"\"\"\n        Add linear projection to queries, keys, and values.\n        \"\"\"\n        q = static.nn.fc(x=queries,\n                      size=d_key * n_head,\n                      num_flatten_dims=2,\n                      weight_attr=paddle.ParamAttr("
        },
        {
            "comment": "This code defines a function for the Transformer Encoder layer. It includes functions for multi-head attention, position-wise feed-forward network layers, and splits heads of input tensors. Parameters such as d_key, n_head, and param_initializer are used to define the dimensions and initialization methods for weights. The code uses Paddle's static nn library and defines the names for different FC layers within the function.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/MultimodalVideoTag/scenario_lib/models/transformer_encoder.py\":57-79",
            "content": "                          name=name + '_query_fc.w_0',\n                          initializer=param_initializer),\n                      bias_attr=name + '_query_fc.b_0')\n        k = static.nn.fc(x=keys,\n                      size=d_key * n_head,\n                      num_flatten_dims=2,\n                      weight_attr=paddle.ParamAttr(\n                          name=name + '_key_fc.w_0',\n                          initializer=param_initializer),\n                      bias_attr=name + '_key_fc.b_0')\n        v = static.nn.fc(x=values,\n                      size=d_value * n_head,\n                      num_flatten_dims=2,\n                      weight_attr=paddle.ParamAttr(\n                          name=name + '_value_fc.w_0',\n                          initializer=param_initializer),\n                      bias_attr=name + '_value_fc.b_0')\n        return q, k, v\n    def __split_heads(x, n_head):\n        \"\"\"\n        Reshape the last dimension of inpunt tensor x so that it becomes two\n        dimensions and then transpose. Specifically, input a tensor with shape"
        },
        {
            "comment": "This code is performing tensor reshaping and transposing operations to split the input tensor into multiple smaller tensors, representing different attention heads. The `__split_heads` function splits the tensor into a shape of [bs, n_head, max_sequence_length, hidden_dim], while the `__combine_heads` function reverses this process by transposing and reshaping the last two dimensions to combine the attention heads back into one dimension.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/MultimodalVideoTag/scenario_lib/models/transformer_encoder.py\":80-103",
            "content": "        [bs, max_sequence_length, n_head * hidden_dim] then output a tensor\n        with shape [bs, n_head, max_sequence_length, hidden_dim].\n        \"\"\"\n        hidden_size = x.shape[-1]\n        # The value 0 in shape attr means copying the corresponding dimension\n        # size of the input as the output dimension size.\n        reshaped = paddle.reshape(\n            x=x, shape=[0, 0, n_head, hidden_size // n_head])\n        # permuate the dimensions into:\n        # [batch_size, n_head, max_sequence_len, hidden_size_per_head]\n        return paddle.transpose(x=reshaped, perm=[0, 2, 1, 3])\n    def __combine_heads(x):\n        \"\"\"\n        Transpose and then reshape the last two dimensions of inpunt tensor x\n        so that it becomes one dimension, which is reverse to __split_heads.\n        \"\"\"\n        if len(x.shape) == 3: return x\n        if len(x.shape) != 4:\n            raise ValueError(\"Input(x) should be a 4-D Tensor.\")\n        trans_x = paddle.transpose(x, perm=[0, 2, 1, 3])\n        # The value 0 in shape attr means copying the corresponding dimension"
        },
        {
            "comment": "This code defines a function that performs Scaled Dot-Product Attention. It first scales the query vector by dividing it with the square root of the key dimension, then takes the dot product between scaled query and key matrices after transposing the key matrix. If attention bias is provided, it adds it to the product. It applies softmax activation on the result to get weights, which are optionally dropout masked if a dropout rate is specified. Finally, it computes the output vector by taking the weighted sum of value vectors. This function is used in the context of Transformer Encoder layers.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/MultimodalVideoTag/scenario_lib/models/transformer_encoder.py\":104-127",
            "content": "        # size of the input as the output dimension size.\n        return paddle.reshape(\n            x=trans_x,\n            shape=[0, 0, trans_x.shape[2] * trans_x.shape[3]])\n    def scaled_dot_product_attention(q, k, v, attn_bias, d_key, dropout_rate):\n        \"\"\"\n        Scaled Dot-Product Attention\n        \"\"\"\n        scaled_q = paddle.scale(x=q, scale=d_key**-0.5)\n        product = paddle.matmul(x=scaled_q, y=k, transpose_y=True)\n        if attn_bias:\n            # product += attn_bias\n            product = paddle.add(x=product, y=attn_bias)\n        weights = paddle.nn.functional.softmax(x=product)\n        if dropout_rate:\n            weights = paddle.nn.functional.dropout(weights, p=dropout_rate, mode=\"upscale_in_train\", training=True)\n        out = paddle.matmul(x=weights, y=v)\n        return out\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n    if cache is not None:  # use cache and concat time steps\n        # Since the inplace reshape in __split_heads changes the shape of k and"
        },
        {
            "comment": "This code is reshaping the cache input for the next time step and splitting the inputs into multiple heads. It performs scaled dot product attention, combines the outputs of each head, and projects the result back to the model size using a fully connected layer.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/MultimodalVideoTag/scenario_lib/models/transformer_encoder.py\":128-153",
            "content": "        # v, which is the cache input for next time step, reshape the cache\n        # input from the previous time step first.\n        k = cache[\"k\"] = paddle.concat(\n            x=[paddle.reshape(\n                x=cache[\"k\"], shape=[0, 0, d_model]), k], axis=1)\n        v = cache[\"v\"] = paddle.concat(\n            x=[paddle.reshape(\n                x=cache[\"v\"], shape=[0, 0, d_model]), v], axis=1)\n    q = __split_heads(q, n_head)\n    k = __split_heads(k, n_head)\n    v = __split_heads(v, n_head)\n    ctx_multiheads = scaled_dot_product_attention(q, k, v, attn_bias, d_key,\n                                                  dropout_rate)\n    out = __combine_heads(ctx_multiheads)\n    # Project back to the model size.\n    proj_out = static.nn.fc(x=out,\n                         size=d_model,\n                         num_flatten_dims=2,\n                         weight_attr=paddle.ParamAttr(\n                             name=name + '_output_fc.w_0',\n                             initializer=param_initializer),\n                         bias_attr=name + '_output_fc.b_0')"
        },
        {
            "comment": "This code defines the position-wise feed-forward network used in a transformer encoder. It consists of two linear transformations with a ReLU activation applied to each position separately and identically. The hidden layer is passed through a dropout if dropout_rate is specified.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/MultimodalVideoTag/scenario_lib/models/transformer_encoder.py\":154-181",
            "content": "    return proj_out\ndef positionwise_feed_forward(x,\n                              d_inner_hid,\n                              d_hid,\n                              dropout_rate,\n                              hidden_act,\n                              param_initializer=None,\n                              name='ffn'):\n    \"\"\"\n    Position-wise Feed-Forward Networks.\n    This module consists of two linear transformations with a ReLU activation\n    in between, which is applied to each position separately and identically.\n    \"\"\"\n    hidden = static.nn.fc(x=x,\n                       size=d_inner_hid,\n                       num_flatten_dims=2,\n                       activation=hidden_act,\n                       weight_attr=paddle.ParamAttr(\n                           name=name + '_fc_0.w_0',\n                           initializer=param_initializer),\n                       bias_attr=name + '_fc_0.b_0')\n    if dropout_rate:\n        hidden = paddle.nn.functional.dropout(\n            hidden,\n            p=dropout_rate,\n            mode=\"upscale_in_train\","
        },
        {
            "comment": "This code defines a function for a transformer encoder layer in the PaddleVideo MultimodalVideoTag application. The layer includes a multi-head attention mechanism and a position-wise feed-forward network, with residual connections and layer normalization added before or after these operations, as specified by the process_cmd argument.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/MultimodalVideoTag/scenario_lib/models/transformer_encoder.py\":182-207",
            "content": "            training=True)\n    out = static.nn.fc(x=hidden,\n                    size=d_hid,\n                    num_flatten_dims=2,\n                    weight_attr=paddle.ParamAttr(\n                        name=name + '_fc_1.w_0', initializer=param_initializer),\n                    bias_attr=name + '_fc_1.b_0')\n    return out\ndef pre_post_process_layer(prev_out, out, process_cmd, dropout_rate=0.,\n                           name=''):\n    \"\"\"\n    Add residual connection, layer normalization and droput to the out tensor\n    optionally according to the value of process_cmd.\n    This will be used before or after multi-head attention and position-wise\n    feed-forward networks.\n    \"\"\"\n    for cmd in process_cmd:\n        if cmd == \"a\":  # add residual connection\n            # out = out + prev_out if prev_out else out\n            out = paddle.add(x=out, y=prev_out) if prev_out else out\n        elif cmd == \"n\":  # add layer normalization\n            out_dtype = out.dtype\n            if out_dtype == \"float16\":\n                out = paddle.cast(x=out, dtype=\"float32\")"
        },
        {
            "comment": "This code is part of a transformer encoder layer implementation in PaddlePaddle. It applies layer normalization, optional float16 casting, and optionally dropout for training. The pre_process_layer and post_process_layer are partial functions used for data pre-processing and post-processing respectively. The encoder_layer function takes input, attention bias, and number of heads as inputs to create a transformer encoder layer.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/MultimodalVideoTag/scenario_lib/models/transformer_encoder.py\":208-235",
            "content": "            out = static.nn.layer_norm(\n                out,\n                begin_norm_axis=len(out.shape) - 1,\n                param_attr=paddle.ParamAttr(\n                    name=name + '_layer_norm_scale',\n                    initializer=paddle.nn.initializer.Constant(value=1.)),\n                bias_attr=paddle.ParamAttr(\n                    name=name + '_layer_norm_bias',\n                    initializer=paddle.nn.initializer.Constant(value=0.)))\n            if out_dtype == \"float16\":\n                out = paddle.cast(x=out, dtype=\"float16\")\n        elif cmd == \"d\":  # add dropout\n            if dropout_rate:\n                out = paddle.nn.functional.dropout(\n                    out,\n                    p=dropout_rate,\n                    dropout_implementation=\"upscale_in_train\",\n                    training=True)\n    return out\npre_process_layer = partial(pre_post_process_layer, None)\npost_process_layer = pre_post_process_layer\ndef encoder_layer(enc_input,\n                  attn_bias,\n                  n_head,"
        },
        {
            "comment": "This code defines a transformer encoder layer that stacks multiple layers to form a deep encoder. The encoder consists of a multi-head self-attention mechanism followed by position-wise feed-forward networks, all with residual connections and layer normalization to add dropout.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/MultimodalVideoTag/scenario_lib/models/transformer_encoder.py\":236-267",
            "content": "                  d_key,\n                  d_value,\n                  d_model,\n                  d_inner_hid,\n                  prepostprocess_dropout,\n                  attention_dropout,\n                  relu_dropout,\n                  hidden_act,\n                  preprocess_cmd=\"n\",\n                  postprocess_cmd=\"da\",\n                  param_initializer=None,\n                  name=''):\n    \"\"\"The encoder layers that can be stacked to form a deep encoder.\n    This module consits of a multi-head (self) attention followed by\n    position-wise feed-forward networks and both the two components companied\n    with the post_process_layer to add residual connection, layer normalization\n    and droput.\n    \"\"\"\n    attn_output = multi_head_attention(\n        pre_process_layer(\n            enc_input,\n            preprocess_cmd,\n            prepostprocess_dropout,\n            name=name + '_pre_att'),\n        None,\n        None,\n        attn_bias,\n        d_key,\n        d_value,\n        d_model,\n        n_head,\n        attention_dropout,"
        },
        {
            "comment": "This code defines a transformer encoder model. It utilizes an attention mechanism to process input sequences, followed by position-wise feed forward layers. The function takes input sequences, attention bias, number of layers, number of heads, and other parameters as inputs and returns the processed output.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/MultimodalVideoTag/scenario_lib/models/transformer_encoder.py\":268-307",
            "content": "        param_initializer=param_initializer,\n        name=name + '_multi_head_att')\n    attn_output = post_process_layer(\n        enc_input,\n        attn_output,\n        postprocess_cmd,\n        prepostprocess_dropout,\n        name=name + '_post_att')\n    ffd_output = positionwise_feed_forward(\n        pre_process_layer(\n            attn_output,\n            preprocess_cmd,\n            prepostprocess_dropout,\n            name=name + '_pre_ffn'),\n        d_inner_hid,\n        d_model,\n        relu_dropout,\n        hidden_act,\n        param_initializer=param_initializer,\n        name=name + '_ffn')\n    return post_process_layer(\n        attn_output,\n        ffd_output,\n        postprocess_cmd,\n        prepostprocess_dropout,\n        name=name + '_post_ffn')\ndef encoder(enc_input,\n            attn_bias,\n            n_layer,\n            n_head,\n            d_key,\n            d_value,\n            d_model,\n            d_inner_hid,\n            prepostprocess_dropout,\n            attention_dropout,\n            relu_dropout,\n            hidden_act,"
        },
        {
            "comment": "This code defines a function to create an encoder consisting of multiple layers, where each layer is generated by calling the \"encoder_layer\" function. The encoder takes in input, attention bias, number of heads, dimensionality of keys and values, model dimensions, inner hidden dimensions, and dropout rates for preprocessing and postprocessing. The function applies each layer to the input sequentially, updating the input with each iteration. Finally, it applies a pre-processing layer to the output using specified preprocessing command and prepostprocess_dropout.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/MultimodalVideoTag/scenario_lib/models/transformer_encoder.py\":308-337",
            "content": "            preprocess_cmd=\"n\",\n            postprocess_cmd=\"da\",\n            param_initializer=None,\n            name=''):\n    \"\"\"\n    The encoder is composed of a stack of identical layers returned by calling\n    encoder_layer.\n    \"\"\"\n    for i in range(n_layer):\n        enc_output = encoder_layer(\n            enc_input,\n            attn_bias,\n            n_head,\n            d_key,\n            d_value,\n            d_model,\n            d_inner_hid,\n            prepostprocess_dropout,\n            attention_dropout,\n            relu_dropout,\n            hidden_act,\n            preprocess_cmd,\n            postprocess_cmd,\n            param_initializer=param_initializer,\n            name=name + '_layer_' + str(i))\n        enc_input = enc_output\n    enc_output = pre_process_layer(\n        enc_output, preprocess_cmd, prepostprocess_dropout, name=\"post_encoder\")\n    return enc_output"
        }
    ]
}