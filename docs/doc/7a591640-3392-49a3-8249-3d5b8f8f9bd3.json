{
    "summary": "PaddleSlim is a library for model compression in PaddleVideo, offering quantization, pruning, distillation, and search for enhanced inference performance and reduced computational complexity. It can be installed via pip install and demonstrates PP-TSM offline quantization with deployment options in Python and C++ using PaddleLite's opt tool.",
    "details": [
        {
            "comment": "This code introduces PaddleSlim, a model compression library for compressing PaddleVideo models. It includes functions for model quantization (reducing full precision to fixed-point numbers) and model pruning (cutting unimportant convolution kernels). This improves inference performance and reduces computational complexity while preserving accuracy.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/slim/readme_en.md\":0-8",
            "content": "## Slim function introduction\nA complex model is beneficial to improve the performance of the model, but it also leads to some redundancy in the model. This part provides the function of reducing the model, including two parts: model quantization (quantization training, offline quantization), model pruning.\nAmong them, model quantization reduces the full precision to fixed-point numbers to reduce this redundancy, so as to reduce the computational complexity of the model and improve the inference performance of the model.\nModel quantization can convert FP32-precision model parameters to Int8-precision without losing the accuracy of the model, reducing the size of model parameters and speeding up the calculation. Using the quantized model has a speed advantage when deploying on mobile terminals.\nModel pruning cuts out the unimportant convolution kernels in the CNN, reduces the amount of model parameters, and thus reduces the computational complexity of the model.\nThis tutorial will introduce how to use PaddleSlim, a paddle model compression library, to compress PaddleVideo models."
        },
        {
            "comment": "This code snippet provides a brief introduction to PaddleSlim, which offers model pruning, quantization, distillation, and neural network search for model compression. It highlights the quick start process, explaining that after training a model, quantization or pruning can be used to further compress it while speeding up predictions. The code then provides instructions on how to install PaddleSlim via pip install.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/slim/readme_en.md\":9-29",
            "content": "[PaddleSlim](https://github.com/PaddlePaddle/PaddleSlim) integrates model pruning, quantization (including quantization training and offline quantization), distillation and neural network search and other commonly used and leading model compression functions in the industry. If you are interested, you can follow and understand.\nBefore starting this tutorial, it is recommended to understand [PaddleVideo model training method](../../docs/zh-CN/usage.md) and [PaddleSlim](https://paddleslim.readthedocs.io/zh_CN/ latest/index.html)\n## quick start\nAfter training a model, if you want to further compress the model size and speed up prediction, you can use quantization or pruning to compress the model.\nModel compression mainly includes five steps:\n1. Install PaddleSlim\n2. Prepare the trained model\n3. Model Compression\n4. Export the quantitative inference model\n5. Quantitative Model Prediction Deployment\n### 1. Install PaddleSlim\n* It can be installed by pip install.\n```bash\npython3.7 -m pip install paddleslim -i https://pypi.tuna.tsinghua.edu.cn/simple"
        },
        {
            "comment": "In this code, it explains how to install the latest features of PaddleSlim, prepare a trained model for quantization (either using provided models or regular training), and perform model compression including offline quantization. The offline quantization process requires pre-training model loading and defining the quantization strategy.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/slim/readme_en.md\":30-63",
            "content": "```\n* If you get the latest features of PaddleSlim, you can install it from source.\n```bash\ngit clone https://github.com/PaddlePaddle/PaddleSlim.git\ncd Paddleslim\npython3.7 setup.py install\n```\n### 2. Prepare the trained model\nPaddleVideo provides a series of trained [models](../../docs/zh-CN/model_zoo/README.md). If the model to be quantized is not in the list, you need to follow the [regular training](../ ../docs/zh-CN/usage.md) method to get the trained model.\n### 3. Model Compression\nGo to PaddleVideo root directory\n```bash\ncd PaddleVideo\n```\nThe offline quantization code is located in `deploy/slim/quant_post_static.py`.\n#### 3.1 Model Quantization\nQuantization training includes offline quantization training and online quantization training (TODO). The effect of online quantization training is better. The pre-training model needs to be loaded, and the model can be quantized after the quantization strategy is defined.\n##### 3.1.1 Online quantitative training\nTODO\n##### 3.1.2 Offline Quantization\n**Note"
        },
        {
            "comment": "The code explains the process of offline quantization for a trained model using the PaddleVideo framework. The user must first export an inference model from the trained model and download calibration data before executing the quantization script with specific parameters. The configuration file, `pptsm_k400_frames_uniform_quantization.yaml`, contains all quantization environment parameters except for `use_gpu`.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/slim/readme_en.md\":63-86",
            "content": "**: For offline quantization, you must use the `inference model` exported from the trained model for quantization. For general model export `inference model`, please refer to [Tutorial](../../docs/zh-CN/usage.md#5-Model Inference).\nGenerally speaking, the offline quantization loss model has more accuracy.\nTaking the PP-TSM model as an example, after generating the `inference model`, the offline quantization operation is as follows\n```bash\n# download a small amount of data for calibration\npushd ./data/k400\nwget -nc https://videotag.bj.bcebos.com/Data/k400_rawframes_small.tar\ntar -xf k400_rawframes_small.tar\npopd\n# then switch to deploy/slim\ncd deploy/slim\n# execute quantization script\npython3.7 quant_post_static.py \\\n-c ../../configs/recognition/pptsm/pptsm_k400_frames_uniform_quantization.yaml \\\n--use_gpu=True\n```\nAll quantization environment parameters except `use_gpu` are configured in `pptsm_k400_frames_uniform_quantization.yaml` file\nWhere `inference_model_dir` represents the directory path of the "
        },
        {
            "comment": "This code demonstrates how to use the PP-TSM offline quantization model for prediction. After exporting the inference model, the __model__ and __params__ files are generated in the specified output directory (quant_output_dir). These files can be used directly for prediction deployment without re-exporting the model. The provided example uses Python's tools/predict.py script to make predictions on a video file (data/example.avi), using the specified configuration (configs/recognition/pptsm/pptsm_k400_frames_uniform.yaml). The results include the top-1 class and score.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/slim/readme_en.md\":86-110",
            "content": "`inference model` exported in the previous step, and `quant_output_dir` represents the output directory path of the quantization model\nAfter successful execution, the `__model__` file and the `__params__` file are generated in the `quant_output_dir` directory, which are used to store the generated offline quantization model\nSimilar to the usage of `inference model`, you can directly use these two files for prediction deployment without re-exporting the model.\n```bash\n# Use PP-TSM offline quantization model for prediction\n# Go back to the PaddleVideo directory\ncd ../../\n# Use the quantized model to make predictions\npython3.7 tools/predict.py \\\n--input_file data/example.avi \\\n--config configs/recognition/pptsm/pptsm_k400_frames_uniform.yaml \\\n--model_file ./inference/ppTSM/quant_model/__model__ \\\n--params_file ./inference/ppTSM/quant_model/__params__ \\\n--use_gpu=True \\\n--use_tensorrt=False\n```\nThe output is as follows:\n```bash\nCurrent video file: data/example.avi\n        top-1 class: 5\n        top-1 score: 0.9997928738594055"
        },
        {
            "comment": "This code provides an overview of model pruning, exporting the model, and deployment. It mentions using PaddleLite's opt model conversion tool for deployment and refers to two serving deployments: Python and C++. For quantitative training, it suggests loading pre-trained models, adjusting learning rates, and modifying the number of epochs while maintaining other configuration settings unchanged.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/slim/readme_en.md\":111-131",
            "content": "```\n#### 3.2 Model pruning\nTODO\n### 4. Export the model\nTODO\n### 5. Model Deployment\nThe model exported in the above steps can be converted through the opt model conversion tool of PaddleLite.\nReference for model deployment\n[Serving Python Deployment](../python_serving/readme.md)\n[Serving C++ Deployment](../cpp_serving/readme.md)\n## Training hyperparameter suggestions\n* During quantitative training, it is recommended to load the pre-trained model obtained from regular training to accelerate the convergence of quantitative training.\n* During quantitative training, it is recommended to modify the initial learning rate to `1/20~1/10` of conventional training, and modify the number of training epochs to `1/5~1/2` of conventional training. In terms of learning rate strategy, add On Warmup, other configuration information is not recommended to be modified."
        }
    ]
}