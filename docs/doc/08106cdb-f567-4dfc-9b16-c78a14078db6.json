{
    "summary": "This code implements AGCN model for improved ST-GCN accuracy on FSD-10 and NTU-RGBD datasets, achieving high Top-1 accuracies. It provides instructions for data preparation, training, testing, inference, evaluation, and exports an AGCN model for video recognition using Multi-stream Adaptive Graph Convolutional Networks.",
    "details": [
        {
            "comment": "This code describes the Adaptive Graph Convolution Network (AGCN) implementation for improving the accuracy of ST-GCN, trained on FSD-10 and NTU-RGBD datasets. It provides instructions for data preparation, training, testing, and inference.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/agcn.md\":0-45",
            "content": "[\u7b80\u4f53\u4e2d\u6587](../../../zh-CN/model_zoo/recognition/agcn.md) | English\n# AGCN\n---\n## Contents\n- [Introduction](#Introduction)\n- [Data](#Data)\n- [Train](#Train)\n- [Test](#Test)\n- [Inference](#Inference)\n- [Reference](#Reference)\n## Introduction\nWe implemented Adaptive Graph Convolution Network to improve the accuracy of [ST-GCN](./stgcn.md).\n## Data\nPlease refer to FSD-10 data download and preparation doc [FSD](../../dataset/fsd.md)\nPlease refer to NTU-RGBD data download and preparation doc [NTU-RGBD](../../dataset/ntu-rgbd.md)\n## Train\n### Train on FSD\n- Train AGCN on FSD scripts:\n```bash\npython3.7 main.py -c configs/recognition/agcn/agcn_fsd.yaml\n```\n- Turn off `valid` when training, as validation dataset is not available for the competition.\n### Train on NTU-RGBD\n- Train AGCN on NTU-RGBD scripts:\n```bash\npython3.7 -B -m paddle.distributed.launch --gpus=\"0,1,2,3\"  --log_dir=log_agcn  main.py  --validate -c configs/recognition/agcn/agcn_ntucs.yaml\n```\n- config file `agcn_ntucs.yaml` corresponding to the config of AGCN on NTU-RGB+D dataset with cross-subject splits."
        },
        {
            "comment": "This code provides test scripts to evaluate the performance of the AGCN model on two datasets: FSD and NTU-RGB+D. The test scripts require specifying a configuration file (-c) and a weight path (-w). Evaluation results are saved in submission.csv, with final scores available on the competition website. Testing on FSD dataset returns a Top-1 accuracy of 62.29, while testing on NTU-RGB+D dataset (cross-subject split) returns a Top-1 accuracy of 83.27. The respective model checkpoints are also provided as links for further exploration.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/agcn.md\":48-83",
            "content": "## Test\n### Test onf FSD\n- Test scripts\uff1a\n```bash\npython3.7 main.py --test -c configs/recognition/agcn/agcn_fsd.yaml  -w output/AGCN/AGCN_epoch_00100.pdparams\n```\n- Specify the config file with `-c`, specify the weight path with `-w`.\n- Evaluation results will be saved in `submission.csv` file, final score can be obtained in [competition website](https://aistudio.baidu.com/aistudio/competition/detail/115).\nAccuracy on FSD dataset:\n| Test_Data | Top-1 | checkpoints |\n| :----: | :----: | :---- |\n| Test_A | 62.29 | [AGCN_fsd.pdparams](https://videotag.bj.bcebos.com/PaddleVideo-release2.2/AGCN_fsd.pdparams)|\n### Test on NTU-RGB+D\n- Test scripts\uff1a\n```bash\npython3.7 main.py --test -c configs/recognition/agcn/agcn_ntucs.yaml -w output/AGCN/AGCN_best.pdparams\n```\n- Specify the config file with `-c`, specify the weight path with `-w`.\nAccuracy on NTU-RGB+D dataset:\n| split | Top-1 | checkpoints |\n| :----: | :----: | :---- |\n| cross-subject | 83.27 | [AGCN_ntucs.pdparams](https://videotag.bj.bcebos.com/PaddleVideo-release2.2/AGCN_ntucs.pdparams)|"
        },
        {
            "comment": "This code provides instructions on how to export and use an inference model called AGCN for video recognition. It shows the command to obtain the architecture file (AGCN.pdmodel) and parameter file (AGCN.pdiparams), as well as an example of how to run prediction using the provided files, specifying input data, configuration, and whether to use GPU or not. The output includes the top-1 class and its corresponding score.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/agcn.md\":86-116",
            "content": "## Inference\n### export inference model\n To get model architecture file `AGCN.pdmodel` and parameters file `AGCN.pdiparams`, use:\n```bash\npython3.7 tools/export_model.py -c configs/recognition/agcn/agcn_fsd.yaml \\\n                                -p data/AGCN_fsd.pdparams \\\n                                -o inference/AGCN\n```\n- Args usage please refer to [Model Inference](https://github.com/PaddlePaddle/PaddleVideo/blob/release/2.0/docs/zh-CN/start.md#2-%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86).\n### infer\n```bash\npython3.7 tools/predict.py --input_file data/fsd10/example_skeleton.npy \\\n                           --config configs/recognition/agcn/agcn_fsd.yaml \\\n                           --model_file inference/AGCN/AGCN.pdmodel \\\n                           --params_file inference/AGCN/AGCN.pdiparams \\\n                           --use_gpu=True \\\n                           --use_tensorrt=False\n```\nexample of logs:\n```\nCurrent video file: data/fsd10/example_skeleton.npy\n        top-1 class: 27\n        top-1 score: 0.8965644240379333"
        },
        {
            "comment": "This code snippet implements a Multi-stream Adaptive Graph Convolutional Network for skeleton-based action recognition. It utilizes two input streams (spatial and temporal) to process the data and applies adaptive graph convolution on each stream separately, followed by concatenation of the two streams before being passed through MLP and softmax layers.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/agcn.md\":117-128",
            "content": "```\n## Reference\n- [Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition](https://arxiv.org/abs/1801.07455), Sijie Yan, Yuanjun Xiong, Dahua Lin\n- [Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition](https://arxiv.org/abs/1805.07694), Lei Shi, Yifan Zhang, Jian Cheng, Hanqing Lu\n- [Skeleton-Based Action Recognition with Multi-Stream Adaptive Graph Convolutional Networks](https://arxiv.org/abs/1912.06971), Lei Shi, Yifan Zhang, Jian Cheng, Hanqing Lu\n- Many thanks to [li7819559](https://github.com/li7819559) and [ZhaoJingjing713](https://github.com/ZhaoJingjing713) for contributing the code."
        }
    ]
}