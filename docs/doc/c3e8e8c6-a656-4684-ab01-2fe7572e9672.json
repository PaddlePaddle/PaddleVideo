{
    "summary": "This code calculates retrieval metrics, offers sorting and visualization options, handles tie-breaking efficiently, and computes ranking metrics for input data using NumPy, SciPy, and Matplotlib.",
    "details": [
        {
            "comment": "This code is computing retrieval metrics from a similarity matrix. It takes two tensors as inputs, sims and query_masks. The sims tensor contains NxM matrix of similarities between embeddings, where x_{i,j} = <text_embd[i], vid_embed[j]>. The query_masks tensor is optional and is used to mask any missing queries from the dataset. It then calculates various retrieval metrics such as average precision score, mean average precision, and other related statistics.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/model/metric.py\":0-29",
            "content": "# Copyright 2021 Antoine Miech All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport math\nimport paddle\nimport numbers\nimport scipy.stats\nimport numpy as np\nfrom pathlib import Path\nfrom sklearn.metrics import average_precision_score\ndef t2v_metrics(sims, query_masks=None):\n    \"\"\"Compute retrieval metrics from a similiarity matrix.\n    Args:\n        sims (th.Tensor): N x M matrix of similarities between embeddings, where\n             x_{i,j} = <text_embd[i], vid_embed[j]>\n        query_masks (th.Tensor): mask any missing queries from the dataset (two videos"
        },
        {
            "comment": "This function calculates retrieval metrics for a given similarity matrix, and it ensures the matrix has two dimensions. It sorts the distances in the matrix and provides an option to visualize it using matplotlib. The code also computes the ground truth indices for each video, given the number of queries and videos.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/model/metric.py\":30-57",
            "content": "             in MSRVTT only have 19, rather than 20 captions)\n    Returns:\n        (dict[str:float]): retrieval metrics\n    \"\"\"\n    assert sims.ndim == 2, \"expected a matrix\"\n    num_queries, num_vids = sims.shape\n    dists = -sims\n    sorted_dists = np.sort(dists, axis=1)\n    if False:\n        import sys\n        import matplotlib\n        from pathlib import Path\n        matplotlib.use(\"Agg\")\n        import matplotlib.pyplot as plt\n        sys.path.insert(0, str(Path.home() / \"coding/src/zsvision/python\"))\n        from zsvision.zs_iterm import zs_dispFig # NOQA\n        plt.matshow(dists)\n        zs_dispFig()\n        import ipdb; ipdb.set_trace()\n    # The indices are computed such that they slice out the ground truth distances\n    # from the psuedo-rectangular dist matrix\n    queries_per_video = num_queries // num_vids\n    gt_idx = [[np.ravel_multi_index([ii, jj], (num_queries, num_vids))\n              for ii in range(jj * queries_per_video, (jj + 1) * queries_per_video)]\n              for jj in range(num_vids)]"
        },
        {
            "comment": "This section is handling tie-breaking in the similarity matrix, ensuring that it evaluates correctly even when there are ties. It averages over all possible partial orderings implied by the ties for a principled approach. This should occur extremely rarely but can distort scores if not handled properly.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/model/metric.py\":58-74",
            "content": "    gt_idx = np.array(gt_idx)\n    gt_dists = dists.reshape(-1)[gt_idx.reshape(-1)]\n    gt_dists = gt_dists[:, np.newaxis]\n    rows, cols = np.where((sorted_dists - gt_dists) == 0)  # find column position of GT\n    # --------------------------------\n    # NOTE: Breaking ties\n    # --------------------------------\n    # We sometimes need to break ties (in general, these should occur extremely rarely,\n    # but there are pathological cases when they can distort the scores, such as when\n    # the similarity matrix is all zeros). Previous implementations (e.g. the t2i\n    # evaluation function used\n    # here: https://github.com/niluthpol/multimodal_vtt/blob/master/evaluation.py and\n    # here: https://github.com/linxd5/VSE_Pytorch/blob/master/evaluation.py#L87) generally\n    # break ties \"optimistically\".  However, if the similarity matrix is constant this\n    # can evaluate to a perfect ranking. A principled option is to average over all\n    # possible partial orderings implied by the ties. See # this paper for a discussion:"
        },
        {
            "comment": "This code is computing information retrieval performance measures efficiently in the presence of tied scores, following McSherry et al. (2008). It handles ties optimistically or by averaging, and checks if the number of unique rows matches the number of queries.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/model/metric.py\":75-97",
            "content": "    #    McSherry, Frank, and Marc Najork,\n    #    \"Computing information retrieval performance measures efficiently in the presence\n    #    of tied scores.\" European conference on information retrieval. Springer, Berlin, \n    #    Heidelberg, 2008.\n    # http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.8892&rep=rep1&type=pdf\n    # break_ties = \"optimistically\"\n    break_ties = \"averaging\"\n    if rows.size > num_queries:\n        assert np.unique(rows).size == num_queries, \"issue in metric evaluation\"\n        if break_ties == \"optimistically\":\n            _, idx = np.unique(rows, return_index=True)\n            cols = cols[idx]\n        elif break_ties == \"averaging\":\n            # fast implementation, based on this code:\n            # https://stackoverflow.com/a/49239335\n            locs = np.argwhere((sorted_dists - gt_dists) == 0)\n            # Find the split indices\n            steps = np.diff(locs[:, 0])\n            splits = np.nonzero(steps)[0] + 1\n            splits = np.insert(splits, 0, 0)"
        },
        {
            "comment": "This code calculates the average rank of each query by dividing the summed ranks by their respective counts. It also provides a slower, more interpretable version for testing and asserts that the size of the calculated results matches the expected number of queries. The code includes optional debugging features to verify rank averaging across ties and recover single-query scores.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/model/metric.py\":99-121",
            "content": "            # Compute the result columns\n            summed_cols = np.add.reduceat(locs[:, 1], splits)\n            counts = np.diff(np.append(splits, locs.shape[0]))\n            avg_cols = summed_cols / counts\n            if False:\n                print(\"Running slower code to verify rank averaging across ties\")\n                # slow, but more interpretable version, used for testing\n                avg_cols_slow = [np.mean(cols[rows == idx]) for idx in range(num_queries)]\n                assert np.array_equal(avg_cols, avg_cols_slow), \"slow vs fast difference\"\n                print(\"passed num check\")\n            cols = avg_cols\n    msg = \"expected ranks to match queries ({} vs {}) \"\n    if cols.size != num_queries:\n        import ipdb; ipdb.set_trace()\n    assert cols.size == num_queries, msg\n    if False:\n        # overload mask to check that we can recover the scores for single-query\n        # retrieval\n        print(\"DEBUGGING MODE\")\n        query_masks = np.zeros_like(query_masks)\n        query_masks[:, 0] = 1  # recover single query score"
        },
        {
            "comment": "This function computes retrieval metrics from a similarity matrix and handles invalid queries by checking if query_masks are not None, removing invalid queries, updating the number of queries, and returning the results. It also includes a sanity check against old logic for square matrices.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/model/metric.py\":123-147",
            "content": "    if query_masks is not None:\n        # remove invalid queries\n        assert query_masks.size == num_queries, \"invalid query mask shape\"\n        cols = cols[query_masks.reshape(-1).astype(np.bool)]\n        assert cols.size == query_masks.sum(), \"masking was not applied correctly\"\n        # update number of queries to account for those that were missing\n        num_queries = query_masks.sum()\n    if False:\n        # sanity check against old logic for square matrices\n        gt_dists_old = np.diag(dists)\n        gt_dists_old = gt_dists_old[:, np.newaxis]\n        _, cols_old = np.where((sorted_dists - gt_dists_old) == 0)\n        assert np.array_equal(cols_old, cols), \"new metric doesn't match\"\n    return cols2metrics(cols, num_queries)\ndef v2t_metrics(sims, query_masks=None):\n    \"\"\"Compute retrieval metrics from a similiarity matrix.\n    Args:\n        sims (th.Tensor): N x M matrix of similarities between embeddings, where\n             x_{i,j} = <text_embd[i], vid_embed[j]>\n        query_masks (th.Tensor): mask any missing captions from the dataset"
        },
        {
            "comment": "This code calculates retrieval metrics for finding the closest \"GT caption\" in embedding space. It first switches axes of text and video, then applies various operations to compute distances between queries and captions. The code handles missing values by setting them to have a distance of infinity. The result is a dictionary of retrieval metrics.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/model/metric.py\":149-179",
            "content": "    Returns:\n        (dict[str:float]): retrieval metrics\n    NOTES: We find the closest \"GT caption\" in the style of VSE, which corresponds\n    to finding the rank of the closest relevant caption in embedding space:\n    github.com/ryankiros/visual-semantic-embedding/blob/master/evaluation.py#L52-L56\n    \"\"\"\n    # switch axes of text and video\n    sims = sims.T\n    if False:\n        # experiment with toy example\n        sims = np.ones((3, 3))\n        sims[0, 0] = 2\n        sims[1, 1:2] = 2\n        sims[2, :] = 2\n        query_masks = None\n    assert sims.ndim == 2, \"expected a matrix\"\n    num_queries, num_caps = sims.shape\n    dists = -sims\n    caps_per_video = num_caps // num_queries\n    break_ties = \"averaging\"\n    MISSING_VAL = 1E8\n    query_ranks = []\n    for ii in range(num_queries):\n        row_dists = dists[ii, :]\n        if query_masks is not None:\n            # Set missing queries to have a distance of infinity.  A missing query\n            # refers to a query position `n` for a video that had less than `n`"
        },
        {
            "comment": "The code performs ranking of captions based on distances and handles missing values. It uses distance subtraction instead of argsort for better deterministic results. The code skips rankings of missing captions, and when ties occur, it provides options to break them optimistically or by averaging.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/model/metric.py\":180-198",
            "content": "            # captions (for example, a few MSRVTT videos only have 19 queries)\n            row_dists[np.logical_not(query_masks.reshape(-1))] = MISSING_VAL\n        # NOTE: Using distance subtraction to perform the ranking is easier to make\n        # deterministic than using argsort, which suffers from the issue of defining\n        # \"stability\" for equal distances.  Example of distance subtraction code:\n        # github.com/antoine77340/Mixture-of-Embedding-Experts/blob/master/train.py\n        sorted_dists = np.sort(row_dists)\n        min_rank = np.inf\n        for jj in range(ii * caps_per_video, (ii + 1) * caps_per_video):\n            if row_dists[jj] == MISSING_VAL:\n                # skip rankings of missing captions\n                continue\n            ranks = np.where((sorted_dists - row_dists[jj]) == 0)[0]\n            if break_ties == \"optimistically\":\n                rank = ranks[0]\n            elif break_ties == \"averaging\":\n                # NOTE: If there is more than one caption per video, its possible for the"
        },
        {
            "comment": "This code snippet calculates the average rank of similarities in a matrix and checks if it's lower than the minimum rank. It also includes a sanity check against an older version of the code by comparing the calculated ranks with the diagonal elements of the distance matrix and asserts that they are equal using NumPy's array_equal function. If the assertion fails, it prints a message with the number of differences and uses matplotlib to visualize the distance matrix for debugging purposes.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/model/metric.py\":199-223",
            "content": "                # method to do \"worse than chance\" in the degenerate case when all\n                # similarities are tied.  TODO(Samuel): Address this case.\n                rank = ranks.mean()\n            if rank < min_rank:\n                min_rank = rank\n        query_ranks.append(min_rank)\n    query_ranks = np.array(query_ranks)\n    # sanity check against old version of code\n    if False:\n        sorted_dists = np.sort(dists, axis=1)\n        gt_dists_old = np.diag(dists)\n        gt_dists_old = gt_dists_old[:, np.newaxis]\n        rows_old, cols_old = np.where((sorted_dists - gt_dists_old) == 0)\n        if rows_old.size > num_queries:\n            _, idx = np.unique(rows_old, return_index=True)\n            cols_old = cols_old[idx]\n        num_diffs = (1 - (cols_old == query_ranks)).sum()\n        msg = f\"new metric doesn't match in {num_diffs} places\"\n        assert np.array_equal(cols_old, query_ranks), msg\n        # visualise the distance matrix\n        import sys\n        import matplotlib\n        matplotlib.use(\"Agg\")"
        },
        {
            "comment": "This code is using matplotlib to display a matrix of distances and then calculates various ranking metrics such as R1, R5, R10, MedR and MeanR for the input data. The function cols2metrics takes in two parameters: 'cols', which represents the input data, and 'num_queries', representing the total number of queries. It computes these ranking metrics using numpy and scipy libraries. Finally, it returns a dictionary containing all calculated metrics.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/model/metric.py\":224-242",
            "content": "        import matplotlib.pyplot as plt\n        sys.path.insert(0, str(Path.home() / \"coding/src/zsvision/python\"))\n        from zsvision.zs_iterm import zs_dispFig # NOQA\n        plt.matshow(dists)\n        zs_dispFig()\n    return cols2metrics(query_ranks, num_queries)\ndef cols2metrics(cols, num_queries):\n    metrics = {}\n    metrics[\"R1\"] = 100 * float(np.sum(cols == 0)) / num_queries\n    metrics[\"R5\"] = 100 * float(np.sum(cols < 5)) / num_queries\n    metrics[\"R10\"] = 100 * float(np.sum(cols < 10)) / num_queries\n    metrics[\"R50\"] = 100 * float(np.sum(cols < 50)) / num_queries\n    metrics[\"MedR\"] = np.median(cols) + 1\n    metrics[\"MeanR\"] = np.mean(cols) + 1\n    stats = [metrics[x] for x in (\"R1\", \"R5\", \"R10\")]\n    metrics[\"geometric_mean_R1-R5-R10\"] = scipy.stats.mstats.gmean(stats)\n    return metrics"
        }
    ]
}