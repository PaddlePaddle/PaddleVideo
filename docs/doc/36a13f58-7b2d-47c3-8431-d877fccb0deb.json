{
    "summary": "This code defines a Python class for the UCF101 Skeleton Dataset in PaddleVideo, loading skeleton features and normalizing data for action recognition tasks. The dataset includes train and test methods for preparing frames with `prepare_train` and `prepare_test` functions.",
    "details": [
        {
            "comment": "This code snippet is a Python class for UCF101 Skeleton Dataset in PaddleVideo. It loads skeleton features and applies normalization operations, registering the dataset for action recognition tasks.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/dataset/ucf101_skeleton.py\":0-34",
            "content": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os.path as osp\nimport copy\nimport random\nimport numpy as np\nimport pickle\nimport paddle\nfrom paddle.io import Dataset\nfrom ..registry import DATASETS\nfrom .base import BaseDataset\nfrom ...utils import get_logger\nlogger = get_logger(\"paddlevideo\")\n@DATASETS.register()\nclass UCF101SkeletonDataset(BaseDataset):\n    \"\"\"\n    Skeleton dataset for action recognition.\n    The dataset loads skeleton feature, and apply norm operatations."
        },
        {
            "comment": "This code defines a class that loads annotation data from a file, specifically for the UCF101 dataset's skeleton information. It takes arguments such as the file path, pipeline object, and whether it's building a test dataset. The load_file method checks if the file is a .pkl file and calls load_pkl_annotations to get video information. If the split argument is provided, it only uses the specified part of the data.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/dataset/ucf101_skeleton.py\":35-65",
            "content": "    Args:\n        file_path (str): Path to the index file.\n        pipeline(obj): Define the pipeline of data preprocessing.\n        test_mode (bool): Whether to bulid the test dataset. Default: False.\n    \"\"\"\n    def __init__(self,\n                 file_path,\n                 pipeline,\n                 split,\n                 repeat_times,\n                 test_mode=False):\n        self.split = split\n        self.repeat_times = repeat_times\n        super().__init__(file_path, pipeline, test_mode=test_mode)\n        self._ori_len = len(self.info)\n        self.start_index = 0\n        self.modality = \"Pose\"\n    def load_file(self):\n        \"\"\"Load annotation file to get video information.\"\"\"\n        assert self.file_path.endswith('.pkl')\n        return self.load_pkl_annotations()\n    def load_pkl_annotations(self):\n        with open(self.file_path, \"rb\") as f:\n            data = pickle.load(f)\n        if self.split:\n            split, data = data['split'], data['annotations']\n            identifier = 'filename' if 'filename' in data[0] else 'frame_dir'"
        },
        {
            "comment": "This code defines a dataset for PaddleVideo, containing train and test methods for preparing frames. The `prepare_train` and `prepare_test` functions create new results by copying the original information, setting modality and start index based on the given index. The `__len__` function returns the size of the dataset by multiplying the number of info items with repeat times.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/dataset/ucf101_skeleton.py\":66-88",
            "content": "            data = [x for x in data if x[identifier] in split[self.split]]\n        return data\n    def prepare_train(self, idx):\n        \"\"\"Prepare the frames for training given the index.\"\"\"\n        results = copy.deepcopy(self.info[idx % self._ori_len])\n        results['modality'] = self.modality\n        results['start_index'] = self.start_index\n        return self.pipeline(results)\n    def prepare_test(self, idx):\n        \"\"\"Prepare the frames for testing given the index.\"\"\"\n        results = copy.deepcopy(self.info[idx % self._ori_len])\n        results['modality'] = self.modality\n        results['start_index'] = self.start_index\n        return self.pipeline(results)\n    def __len__(self):\n        \"\"\"get the size of the dataset.\"\"\"\n        return len(self.info) * self.repeat_times"
        }
    ]
}