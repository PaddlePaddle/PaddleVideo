{
    "summary": "The code imports modules, transfers model parameters, adjusts positional embeddings, and provides save/load functions for Resnet18, VisionTransformer (TimeSformer), SwinTransformer3D models using PaddlePaddle library.",
    "details": [
        {
            "comment": "This code is from the PaddleVideo library and it imports necessary modules, defines a function for transferring pre-trained Swin model parameters, and deletes the classifier's weights from state_dicts.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/utils/save_load.py\":0-29",
            "content": "# Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\nimport os.path as osp\nimport time\nimport paddle\nimport paddle.nn.functional as F\nfrom paddlevideo.utils import get_logger, main_only\nfrom tqdm import tqdm\nimport numpy as np\nfrom scipy import ndimage\ndef pretrain_swin_param_trans(model, state_dicts):\n    # delete classifier's params\n    if 'head.fc' + '.weight' in state_dicts:\n        del state_dicts['head.fc' + '.weight']\n    if 'head.fc' + '.bias' in state_dicts:"
        },
        {
            "comment": "This code checks if the loaded state dictionaries match the model's state dictionaries and handles any inconsistencies. It removes unnecessary keys, adjusts certain weights, and bicubically interpolates relative position bias tables if they don't match to ensure proper loading of 2D or 3D weights.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/utils/save_load.py\":30-60",
            "content": "        del state_dicts['head.fc' + '.bias']\n    state_dicts = {\n        k.replace('backbone.', ''): v\n        for k, v in state_dicts.items()\n    }\n    if len(state_dicts) == len(model.state_dict()):\n        print(\"Load 3D weights\")\n        return state_dicts\n    print(\"Load 2D weights\")\n    relative_position_index_keys = [\n        k for k in state_dicts.keys() if \"relative_position_index\" in k\n    ]\n    for k in relative_position_index_keys:\n        del state_dicts[k]\n    # delete attn_mask since we always re-init it\n    attn_mask_keys = [k for k in state_dicts.keys() if \"attn_mask\" in k]\n    for k in attn_mask_keys:\n        del state_dicts[k]\n    state_dicts['patch_embed.proj.weight'] = state_dicts[\n        'patch_embed.proj.weight'].unsqueeze(2).tile(\n            [1, 1, model.patch_size[0], 1, 1]) / model.patch_size[0]\n    # bicubic interpolate relative_position_bias_table if not match\n    relative_position_bias_table_keys = [\n        k for k in state_dicts.keys() if \"relative_position_bias_table\" in k\n    ]"
        },
        {
            "comment": "Loading weights for relative position bias tables from pretrained and current model state dictionaries.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/utils/save_load.py\":61-81",
            "content": "    total_len = len(relative_position_bias_table_keys)\n    with tqdm(total=total_len,\n              position=1,\n              bar_format='{desc}',\n              desc=\"Loading weights\") as desc:\n        for key in tqdm(relative_position_bias_table_keys,\n                        total=total_len,\n                        position=0):\n            relative_position_bias_table_pretrained = state_dicts[key]\n            relative_position_bias_table_current = model.state_dict()[key]\n            L1, nH1 = relative_position_bias_table_pretrained.shape\n            L2, nH2 = relative_position_bias_table_current.shape\n            L2 = (2 * model.window_size[1] - 1) * (2 * model.window_size[2] - 1)\n            wd = model.window_size[0]\n            if nH1 != nH2:\n                desc.set_description(f\"Error in loading {key}, skip\")\n            else:\n                if L1 != L2:\n                    S1 = int(L1**0.5)\n                    relative_position_bias_table_pretrained_resized = paddle.nn.functional.interpolate(\n                        relative_position_bias_table_pretrained.transpose("
        },
        {
            "comment": "Function is loading pre-trained model parameters, resizing a table, and setting the description.\nThe code is performing model parameter transformation for ViT models, deleting unnecessary weights.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/utils/save_load.py\":82-104",
            "content": "                            [1, 0]).reshape([1, nH1, S1, S1]),\n                        size=(2 * model.window_size[1] - 1,\n                              2 * model.window_size[2] - 1),\n                        mode='bicubic')\n                    relative_position_bias_table_pretrained = relative_position_bias_table_pretrained_resized.reshape(\n                        [nH2, L2]).transpose([1, 0])\n                desc.set_description(f\"Loading {key}\")\n            state_dicts[key] = relative_position_bias_table_pretrained.tile(\n                [2 * wd - 1, 1])\n            time.sleep(0.01)\n    ret_str = \"loading {:<20d} weights completed.\".format(\n        len(model.state_dict()))\n    desc.set_description(ret_str)\n    return state_dicts\ndef pretrain_vit_param_trans(model, state_dicts, num_patches, num_seg,\n                             attention_type):\n    \"\"\"\n    Convert ViT's pre-trained model parameters to a parameter dictionary that matches the existing model\n    \"\"\"\n    if 'head' + '.weight' in state_dicts:\n        del state_dicts['head' + '.weight']"
        },
        {
            "comment": "This code block checks the shape of the 'pos_embed' tensor and adjusts it based on the number of patches provided. It resizes the tensor using Paddle's ndimage.zoom function and then reconstructs the updated positional embedding for the model. This is necessary when the number of patches changes, ensuring the positional embeddings are consistent with the new patch count.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/utils/save_load.py\":105-125",
            "content": "    if 'head' + '.bias' in state_dicts:\n        del state_dicts['head' + '.bias']\n    total_len = len(model.state_dict())\n    if num_patches + 1 != state_dicts['pos_embed'].shape[1]:  # when\n        pos_embed = state_dicts['pos_embed']\n        cls_pos_embed = paddle.to_tensor(\n            pos_embed[0, 0, :]).unsqueeze(0).unsqueeze(1)\n        other_pos_embed = paddle.to_tensor(pos_embed[0, 1:, :])\n        gs_new = int(np.sqrt(num_patches))\n        gs_old = int(np.sqrt(other_pos_embed.shape[0]))\n        zoom = (gs_new / gs_old, gs_new / gs_old, 1)\n        other_pos_embed = paddle.reshape(other_pos_embed, [gs_old, gs_old, -1])\n        other_pos_embed = ndimage.zoom(other_pos_embed, zoom, order=1)\n        other_pos_embed = paddle.to_tensor(other_pos_embed)\n        new_pos_embed = paddle.reshape(other_pos_embed, [1, num_patches, -1])\n        new_pos_embed = paddle.concat((cls_pos_embed, new_pos_embed), axis=1)\n        state_dicts['pos_embed'] = new_pos_embed\n        time.sleep(0.01)\n    if 'time_embed' in state_dicts and num_seg != state_dicts["
        },
        {
            "comment": "This code block is part of a larger program that loads pre-trained model weights. It first checks if the shape of 'time_embed' matches a specific condition, and if not, it performs some transformations on it. Afterwards, it starts a progress bar with the description \"Loading weights\" to show the progress of loading these weights. If the attention type is 'divided_space_time', it makes a copy of state_dicts and iterates over its keys, replacing 'attn' keys with 'temporal_attn' if not already present.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/utils/save_load.py\":126-146",
            "content": "            'time_embed'].shape[1]:\n        time_embed = state_dicts['time_embed'].transpose((0, 2, 1)).unsqueeze(0)\n        new_time_embed = F.interpolate(time_embed,\n                                       size=(time_embed.shape[-2], num_seg),\n                                       mode='nearest')\n        state_dicts['time_embed'] = new_time_embed.squeeze(0).transpose(\n            (0, 2, 1))\n        time.sleep(0.01)\n    with tqdm(total=total_len,\n              position=1,\n              bar_format='{desc}',\n              desc=\"Loading weights\") as desc:\n        if attention_type == 'divided_space_time':\n            new_state_dicts = state_dicts.copy()\n            for key in tqdm(state_dicts):\n                if 'blocks' in key and 'attn' in key:\n                    desc.set_description(\"Loading %s\" % key)\n                    new_key = key.replace('attn', 'temporal_attn')\n                    if not new_key in state_dicts:\n                        new_state_dicts[new_key] = state_dicts[key]\n                    else:"
        },
        {
            "comment": "This code appears to be related to model weight loading and adaptation for a pre-trained ResNet18 in a specific context. It modifies the state_dicts of certain keys, like replacing 'norm1' with 'temporal_norm1', possibly to adapt the weights to fit the new model structure. The code also checks if a certain key exists and copies it if not, ensuring the new model has all necessary parameters. Finally, it updates the description for the loading process.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/utils/save_load.py\":147-171",
            "content": "                        new_state_dicts[new_key] = state_dicts[new_key]\n                if 'blocks' in key and 'norm1' in key:\n                    desc.set_description(\"Loading %s\" % key)\n                    new_key = key.replace('norm1', 'temporal_norm1')\n                    if not new_key in state_dicts:\n                        new_state_dicts[new_key] = state_dicts[key]\n                    else:\n                        new_state_dicts[new_key] = state_dicts[new_key]\n                time.sleep(0.01)\n        elif attention_type == 'space_only':  # tokenshift raw vit\n            new_state_dicts = state_dicts.copy()\n    ret_str = \"loading {:<20d} weights completed.\".format(\n        len(model.state_dict()))\n    desc.set_description(ret_str)\n    return new_state_dicts\ndef pretrain_resnet18_param_trans(model, loaded_dict):\n    encoder_dict = model.encoder.state_dict()\n    pose_encoder_dict = model.pose_encoder.state_dict()\n    names = ['encoder.', 'encoder_day.', 'encoder_night.']\n    for name in names:\n        total_len = len(loaded_dict.items())"
        },
        {
            "comment": "This code is loading weights from a dictionary, updating the encoder_dict if the key already exists. It also updates loaded_dict for a specific convolution layer based on the number of input images and uses tqdm to provide progress updates.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/utils/save_load.py\":172-196",
            "content": "        with tqdm(total=total_len,\n                  position=1,\n                  bar_format='{desc}',\n                  desc=\"Loading weights\") as desc:\n            for key, value in tqdm(loaded_dict.items(),\n                                   total=total_len,\n                                   position=0):\n                key = str(name + key)\n                if key in encoder_dict:\n                    encoder_dict[key] = value\n                    desc.set_description('Loading %s' % key)\n                time.sleep(0.01)\n    num_input_images = 2\n    loaded_dict['conv1.weight'] = paddle.concat(\n        [loaded_dict['conv1.weight']] * num_input_images, 1) / num_input_images\n    total_len = len(loaded_dict.items())\n    with tqdm(total=total_len,\n              position=1,\n              bar_format='{desc}',\n              desc=\"Loading weights\") as desc:\n        for name, value in tqdm(loaded_dict.items(),\n                                total=total_len,\n                                position=0):\n            name = str('encoder.' + name)"
        },
        {
            "comment": "This code loads pre-trained model parameters from a specified file path and converts them for use in the existing model. If the weight_path is not a valid checkpoint file, it raises an IOError. The code also utilizes Paddle's `paddle.load()` function to load state_dicts from the specified file path. It handles loading of Resnet18 parameters specifically with the `pretrain_resnet18_param_trans()` function.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/utils/save_load.py\":197-225",
            "content": "            if name in pose_encoder_dict:\n                pose_encoder_dict[name] = value\n                desc.set_description('Loading %s' % key)\n            time.sleep(0.01)\n        ret_str = \"loading {:<20d} weights completed.\".format(\n            len(model.state_dict()))\n        desc.set_description(ret_str)\n    return encoder_dict, pose_encoder_dict\n#XXX(shipping): maybe need load N times because of different cards have different params.\n@main_only\ndef load_ckpt(model, weight_path, **kargs):\n    \"\"\"\n    1. Load pre-trained model parameters\n    2. Extract and convert from the pre-trained model to the parameters\n    required by the existing model\n    3. Load the converted parameters of the existing model\n    \"\"\"\n    #model.set_state_dict(state_dict)\n    if not osp.isfile(weight_path):\n        raise IOError(f'{weight_path} is not a checkpoint file')\n    #state_dicts = load(weight_path)\n    logger = get_logger(\"paddlevideo\")\n    state_dicts = paddle.load(weight_path)\n    if 'ResnetEncoder' in str(model):\n        encoder_dict, pose_encoder_dict = pretrain_resnet18_param_trans("
        },
        {
            "comment": "This code is loading the model's weights and dictionary entries. It checks the type of the model and then either loads or transposes the parameters accordingly, handling cases such as VisionTransformer (TimeSformer) and SwinTransformer3D. For other models, it simply initializes an empty dictionary and starts loading each item from the state_dict in a tqdm progress bar.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/utils/save_load.py\":226-247",
            "content": "            model, state_dicts)\n        model.encoder.load_dict(encoder_dict)\n        model.pose_encoder.load_dict(pose_encoder_dict)\n        tmp = model.state_dict()\n    elif \"VisionTransformer\" in str(model):  # For TimeSformer case\n        tmp = pretrain_vit_param_trans(model, state_dicts, kargs['num_patches'],\n                                       kargs['num_seg'],\n                                       kargs['attention_type'])\n    elif 'SwinTransformer3D' in str(model):\n        tmp = pretrain_swin_param_trans(model, state_dicts)\n    else:\n        tmp = {}\n        total_len = len(model.state_dict())\n        with tqdm(total=total_len,\n                  position=1,\n                  bar_format='{desc}',\n                  desc=\"Loading weights\") as desc:\n            for item in tqdm(model.state_dict(), total=total_len, position=0):\n                name = item\n                desc.set_description('Loading %s' % name)\n                if name not in state_dicts:  # Convert from non-parallel model\n                    if str('backbone.' + name) in state_dicts:"
        },
        {
            "comment": "This code saves a PaddlePaddle model's state dictionary and optionally the student model's state dictionary to separate files. It also has functionality for handling parallel models, converting them into separate state dictionaries. The `mkdir` function is used to create directories if they don't exist already. If the `save_student_model` flag is set to True, it will save both the main and student model weights in separate files.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/utils/save_load.py\":248-281",
            "content": "                        tmp[name] = state_dicts['backbone.' + name]\n                else:  # Convert from parallel model\n                    tmp[name] = state_dicts[name]\n                time.sleep(0.01)\n        ret_str = \"loading {:<20d} weights completed.\".format(\n            len(model.state_dict()))\n        desc.set_description(ret_str)\n    model.set_state_dict(tmp)\ndef mkdir(dir):\n    if not os.path.exists(dir):\n        # avoid error when train with multiple gpus\n        try:\n            os.makedirs(dir)\n        except:\n            pass\ndef _extract_student_weights(all_params, student_prefix=\"Student.\"):\n    s_params = {\n        key[len(student_prefix):]: all_params[key]\n        for key in all_params if student_prefix in key\n    }\n    return s_params\n@main_only\ndef save(obj, path, save_student_model=False):\n    if save_student_model:\n        s_params = _extract_student_weights(obj)\n        student_path = path.replace(\".pdparams\", \"_student.pdparams\")\n        if len(s_params) > 0:\n            paddle.save(s_params, student_path)"
        },
        {
            "comment": "This code defines two functions: \"save\" and \"load\". The \"save\" function uses the Paddle library to save an object (obj) at a specified path. The \"load\" function checks if a file exists, raises an IOError if it does not, and then loads the object from the file using the Paddle library's load function.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/utils/save_load.py\":282-288",
            "content": "    paddle.save(obj, path)\ndef load(file_name):\n    if not osp.isfile(file_name):\n        raise IOError(f'{file_name} not exist')\n    return paddle.load(file_name)"
        }
    ]
}