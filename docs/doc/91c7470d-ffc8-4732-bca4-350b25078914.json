{
    "summary": "DistributedShortSampler streamlines distributed data loading, dynamic batch sizes, and GPU support for PaddleVideo's multigrid. It efficiently calculates average batch size and offers sample dropping options.",
    "details": [
        {
            "comment": "The code defines a DistributedShortSampler class which is a sampler for restricting data loading to a subset of the dataset in distributed training. It allows each process to load exclusive subsets by passing the DistributedBatchSampler as a DataLoader sampler and supports dynamic batch size changes following short cycle schedules. The class takes in a dataset, batch_sizes list, and optionally num_replicas (process number in distributed training).",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/utils/multigrid/short_sampler.py\":0-27",
            "content": "from __future__ import print_function\nfrom __future__ import division\nimport numpy as np\nimport math\nimport paddle\n__all__ = [\"DistributedShortSampler\"]\nclass DistributedShortSampler(paddle.io.BatchSampler):\n    \"\"\"Sampler that restricts data loading to a subset of the dataset.\n    In such case, each process can pass a DistributedBatchSampler instance\n    as a DataLoader sampler, and load a subset of the original dataset that\n    is exclusive to it.\n    .. note::\n        Batch size is dynamic changed following short cycle schedule.\n    Args:\n        dataset(paddle.io.Dataset): this could be a `paddle.io.Dataset` implement\n                     or other python object which implemented\n                     `__len__` for BatchSampler to get sample\n                     number of data source.\n        batch_sizes(list): batch size list of one cycle.\n        num_replicas(int, optional): porcess number in distributed training.\n            If :attr:`num_replicas` is None, :attr:`num_replicas` will be\n            retrieved from :code:`paddle.fluid.dygraph.parallel.ParallenEnv`."
        },
        {
            "comment": "The `__init__` method initializes an instance of the class with a dataset, batch sizes, number of replicas (optional), rank (optional), whether to shuffle indices (optional), and whether to drop last incomplete batch (optional). The batch_sizes should be positive integers. The method performs assertions on the inputs to ensure validity.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/utils/multigrid/short_sampler.py\":28-50",
            "content": "            Default None.\n        rank(int, optional): the rank of the current process among :attr:`num_replicas`\n            processes. If :attr:`rank` is None, :attr:`rank` is retrieved from\n            :code:`paddle.fluid.dygraph.parallel.ParallenEnv`. Default None.\n        shuffle(bool): whther to shuffle indices order before genrating\n            batch indices. Default False.\n        drop_last(bool): whether drop the last incomplete batch dataset size\n            is not divisible by the batch size. Default False\n    \"\"\"\n    def __init__(self,\n                 dataset,\n                 batch_sizes,\n                 num_replicas=None,\n                 rank=None,\n                 shuffle=False,\n                 drop_last=False):\n        self.dataset = dataset\n        assert any(isinstance(batch_size, int) and batch_size > 0 for batch_size in batch_sizes), \\\n            \"batch_size should be a positive integer\"\n        self.batch_sizes = batch_sizes\n        self.len_batch_sizes = len(self.batch_sizes)\n        assert isinstance(shuffle, bool), \\"
        },
        {
            "comment": "The code initializes a MultigridSampler object, which manages the sampling of data across multiple ranks in distributed training. It checks for valid input values (boolean for shuffle and drop_last) and ensures positive integer for num_replicas. It determines the number of ranks and local rank based on provided values or environment. The total number of samples is calculated based on the dataset size and number of ranks, and an array of indices is created.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/utils/multigrid/short_sampler.py\":51-78",
            "content": "            \"shuffle should be a boolean value\"\n        self.shuffle = shuffle\n        assert isinstance(drop_last, bool), \\\n            \"drop_last should be a boolean number\"\n        if num_replicas is not None:\n            assert isinstance(num_replicas, int) and num_replicas > 0, \\\n                \"num_replicas should be a positive integer\"\n            self.nranks = num_replicas\n        else:\n            self.nranks = paddle.distributed.ParallelEnv().nranks\n        if rank is not None:\n            assert isinstance(rank, int) and rank >= 0, \\\n                \"rank should be a non-negative integer\"\n            self.local_rank = rank\n        else:\n            self.local_rank = paddle.distributed.ParallelEnv().local_rank\n        self.drop_last = drop_last\n        self.epoch = 0\n        self.num_samples = int(math.ceil(len(self.dataset) * 1.0 / self.nranks))\n        self.total_size = self.num_samples * self.nranks\n    def __iter__(self):\n        num_samples = len(self.dataset)\n        indices = np.arange(num_samples).tolist()"
        },
        {
            "comment": "This code ensures that the number of samples selected is equal to the total size, and then subsamples them by batch sizes. It handles the last batch with potentially fewer samples due to modulo operations and shuffles the indices if desired.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/utils/multigrid/short_sampler.py\":79-101",
            "content": "        indices += indices[:(self.total_size -\n                             len(indices))]  #completion last iter\n        assert len(indices) == self.total_size\n        if self.shuffle:\n            np.random.RandomState(self.epoch).shuffle(indices)\n            self.epoch += 1\n        # subsample\n        def _get_indices_by_batch_size(indices):\n            total_batch_size = sum(self.batch_sizes)\n            subsampled_indices = []\n            last_batch_size = self.total_size % (\n                total_batch_size * self.nranks)  #number samples of last batch\n            assert last_batch_size % self.nranks == 0\n            last_local_batch_size = last_batch_size // self.nranks\n            for i in range(self.local_rank * total_batch_size,\n                           len(indices) - last_batch_size,\n                           total_batch_size * self.nranks):\n                subsampled_indices.extend(indices[i:i + total_batch_size])\n            indices = indices[len(indices) - last_batch_size:]\n            subsampled_indices.extend("
        },
        {
            "comment": "This code is responsible for creating a sampler that supports dynamic batch sizes. It first sub-samples the input indices based on the local rank and local batch size. Then, it handles cases with multiple GPUs (ranks > 1), dividing the indices into batches of uniform size. Finally, it yields these batches until all samples have been used, or if the drop_last flag is set to False, it yields remaining samples even if they don't form a full batch. The average batch size is also calculated and stored in the class variable avg_batch_size.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/utils/multigrid/short_sampler.py\":102-129",
            "content": "                indices[self.local_rank *\n                        last_local_batch_size:(self.local_rank + 1) *\n                        last_local_batch_size])\n            return subsampled_indices\n        if self.nranks > 1:\n            indices = _get_indices_by_batch_size(indices)\n        assert len(indices) == self.num_samples  #index length in each card\n        _sample_iter = iter(indices)\n        batch_indices = []\n        counter = 0\n        batch_size = self.batch_sizes[0]\n        for idx in _sample_iter:\n            batch_indices.append(\n                (idx, counter %\n                 self.len_batch_sizes))  #to be used in dataloader get_item\n            if len(batch_indices) == batch_size:\n                yield batch_indices\n                counter += 1\n                batch_size = self.batch_sizes[counter % self.len_batch_sizes]\n                batch_indices = []\n        if not self.drop_last and len(batch_indices) > 0:\n            yield batch_indices\n    def __len__(self):\n        avg_batch_size = sum(self.batch_sizes) / float(self.len_batch_sizes)"
        },
        {
            "comment": "This code defines a class for a sampler that can be used with PaddleVideo's multigrid. It calculates the number of samples to return based on batch size and either rounds down or up depending on whether drop_last is set. The set_epoch method sets the epoch number and, when shuffle is True, uses it as seeds for random numbers. This can result in the same ordering being yielded at all epochs if the same number is set each time.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/utils/multigrid/short_sampler.py\":130-145",
            "content": "        if self.drop_last:\n            return int(np.floor(self.num_samples / avg_batch_size))\n        else:\n            return int(np.ceil(self.num_samples / avg_batch_size))\n    def set_epoch(self, epoch):\n        \"\"\"\n        Sets the epoch number. When :attr:`shuffle=True`, this number is used\n        as seeds of random numbers. By default, users may not set this, all\n        replicas (workers) use a different random ordering for each epoch.\n        If set same number at each epoch, this sampler will yield the same\n        ordering at all epoches.\n        Arguments:\n            epoch (int): Epoch number.\n        \"\"\"\n        self.epoch = epoch"
        }
    ]
}