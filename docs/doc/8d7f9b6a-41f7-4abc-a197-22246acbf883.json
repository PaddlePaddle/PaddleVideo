{
    "summary": "The PaddleVideo library evaluates video classification models using GAP, hit@one, precision error, and loss metrics. The `EvaluationMetrics` class accumulates these metrics per mini-batch or epoch using AveragePrecisionCalculator for GAP calculation.",
    "details": [
        {
            "comment": "Code snippet is a part of the PaddleVideo library, providing functions to help evaluate video classification models. It includes flattening list functionality and calculates hit at one for predictions and actuals using numpy operations.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/VideoTag/metrics/youtube8m/eval_util.py\":0-27",
            "content": "# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Provides functions to help with evaluating models.\"\"\"\nimport datetime\nimport numpy\nfrom . import mean_average_precision_calculator as map_calculator\nfrom . import average_precision_calculator as ap_calculator\ndef flatten(l):\n    \"\"\" Merges a list of lists into a single list. \"\"\"\n    return [item for sublist in l for item in sublist]\ndef calculate_hit_at_one(predictions, actuals):\n    \"\"\"Performs a local (numpy) calculation of the hit at one."
        },
        {
            "comment": "This code calculates the average hit at one and precision at equal recall rate for a batch of predictions and corresponding actuals. These are metrics commonly used in evaluation of machine learning models, particularly in video classification tasks. The functions take as input two matrices: 'predictions' containing model outputs and 'actuals' with ground truth labels. They return the average hit at one and precision at equal recall rate across the entire batch respectively.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/VideoTag/metrics/youtube8m/eval_util.py\":29-58",
            "content": "  Args:\n    predictions: Matrix containing the outputs of the model.\n      Dimensions are 'batch' x 'num_classes'.\n    actuals: Matrix containing the ground truth labels.\n      Dimensions are 'batch' x 'num_classes'.\n  Returns:\n    float: The average hit at one across the entire batch.\n  \"\"\"\n    top_prediction = numpy.argmax(predictions, 1)\n    hits = actuals[numpy.arange(actuals.shape[0]), top_prediction]\n    return numpy.average(hits)\ndef calculate_precision_at_equal_recall_rate(predictions, actuals):\n    \"\"\"Performs a local (numpy) calculation of the PERR.\n  Args:\n    predictions: Matrix containing the outputs of the model.\n      Dimensions are 'batch' x 'num_classes'.\n    actuals: Matrix containing the ground truth labels.\n      Dimensions are 'batch' x 'num_classes'.\n  Returns:\n    float: The average precision at equal recall rate across the entire batch.\n  \"\"\"\n    aggregated_precision = 0.0\n    num_videos = actuals.shape[0]\n    for row in numpy.arange(num_videos):\n        num_labels = int(numpy.sum(actuals[row]))"
        },
        {
            "comment": "The code calculates the global average precision (GAP) using the top_k predictions and actuals for each video. It uses a function called AveragePrecisionCalculator to calculate the metric. The function first partitions the predictions based on their values, then iterates through the top indices, adding up the correct ones to calculate item precision. Finally, it averages the item precisions across all videos to get the GAP.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/VideoTag/metrics/youtube8m/eval_util.py\":59-86",
            "content": "        top_indices = numpy.argpartition(predictions[row],\n                                         -num_labels)[-num_labels:]\n        item_precision = 0.0\n        for label_index in top_indices:\n            if predictions[row][label_index] > 0:\n                item_precision += actuals[row][label_index]\n        item_precision /= top_indices.size\n        aggregated_precision += item_precision\n    aggregated_precision /= num_videos\n    return aggregated_precision\ndef calculate_gap(predictions, actuals, top_k=20):\n    \"\"\"Performs a local (numpy) calculation of the global average precision.\n  Only the top_k predictions are taken for each of the videos.\n  Args:\n    predictions: Matrix containing the outputs of the model.\n      Dimensions are 'batch' x 'num_classes'.\n    actuals: Matrix containing the ground truth labels.\n      Dimensions are 'batch' x 'num_classes'.\n    top_k: How many predictions to use per video.\n  Returns:\n    float: The global average precision.\n  \"\"\"\n    gap_calculator = ap_calculator.AveragePrecisionCalculator()"
        },
        {
            "comment": "This code extracts the top k predictions for each video, sorted by class. It returns a tuple containing the sparse_predictions, sparse_labels, and num_positives. The gap_calculator accumulates the flattened sparse_predictions, flattened sparse_labels, and sum of num_positives. Finally, it returns the average precision at n using peek_ap_at_n() from the gap_calculator.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/VideoTag/metrics/youtube8m/eval_util.py\":87-108",
            "content": "    sparse_predictions, sparse_labels, num_positives = top_k_by_class(\n        predictions, actuals, top_k)\n    gap_calculator.accumulate(flatten(sparse_predictions),\n                              flatten(sparse_labels), sum(num_positives))\n    return gap_calculator.peek_ap_at_n()\ndef top_k_by_class(predictions, labels, k=20):\n    \"\"\"Extracts the top k predictions for each video, sorted by class.\n  Args:\n    predictions: A numpy matrix containing the outputs of the model.\n      Dimensions are 'batch' x 'num_classes'.\n    k: the top k non-zero entries to preserve in each prediction.\n  Returns:\n    A tuple (predictions,labels, true_positives). 'predictions' and 'labels'\n    are lists of lists of floats. 'true_positives' is a list of scalars. The\n    length of the lists are equal to the number of classes. The entries in the\n    predictions variable are probability predictions, and\n    the corresponding entries in the labels variable are the ground truth for\n    those predictions. The entries in 'true_positives' are the number of true"
        },
        {
            "comment": "Function evaluates predictions and labels for each video and calculates top-k triplets (prediction, class) for each class. If k is not a positive integer, it raises ValueError. It returns out_predictions, out_labels, and out_true_positives for further analysis.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/VideoTag/metrics/youtube8m/eval_util.py\":109-134",
            "content": "    positives for each class in the ground truth.\n  Raises:\n    ValueError: An error occurred when the k is not a positive integer.\n  \"\"\"\n    if k <= 0:\n        raise ValueError(\"k must be a positive integer.\")\n    k = min(k, predictions.shape[1])\n    num_classes = predictions.shape[1]\n    prediction_triplets = []\n    for video_index in range(predictions.shape[0]):\n        prediction_triplets.extend(\n            top_k_triplets(predictions[video_index], labels[video_index], k))\n    out_predictions = [[] for v in range(num_classes)]\n    out_labels = [[] for v in range(num_classes)]\n    for triplet in prediction_triplets:\n        out_predictions[triplet[0]].append(triplet[1])\n        out_labels[triplet[0]].append(triplet[2])\n    out_true_positives = [numpy.sum(labels[:, i]) for i in range(num_classes)]\n    return out_predictions, out_labels, out_true_positives\ndef top_k_triplets(predictions, labels, k=20):\n    \"\"\"Get the top_k for a 1-d numpy array. Returns a sparse list of tuples in\n  (prediction, class) format\"\"\""
        },
        {
            "comment": "The code defines a class `EvaluationMetrics` to store various evaluation metrics for video classification. The `__init__` method initializes the metrics such as hit@one, precision error (perr), and loss. It also initializes two calculators: MeanAveragePrecisionCalculator and AveragePrecisionCalculator. The `accumulate` method updates these metrics based on predictions, labels, and loss values.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/VideoTag/metrics/youtube8m/eval_util.py\":135-163",
            "content": "    m = len(predictions)\n    k = min(k, m)\n    indices = numpy.argpartition(predictions, -k)[-k:]\n    return [(index, predictions[index], labels[index]) for index in indices]\nclass EvaluationMetrics(object):\n    \"\"\"A class to store the evaluation metrics.\"\"\"\n    def __init__(self, num_class, top_k):\n        \"\"\"Construct an EvaluationMetrics object to store the evaluation metrics.\n    Args:\n      num_class: A positive integer specifying the number of classes.\n      top_k: A positive integer specifying how many predictions are considered per video.\n    Raises:\n      ValueError: An error occurred when MeanAveragePrecisionCalculator cannot\n        not be constructed.\n    \"\"\"\n        self.sum_hit_at_one = 0.0\n        self.sum_perr = 0.0\n        self.sum_loss = 0.0\n        self.map_calculator = map_calculator.MeanAveragePrecisionCalculator(\n            num_class)\n        self.global_ap_calculator = ap_calculator.AveragePrecisionCalculator()\n        self.top_k = top_k\n        self.num_examples = 0\n    #def accumulate(self, predictions, labels, loss):"
        },
        {
            "comment": "The code defines a function \"accumulate\" that takes in predictions, labels and loss from a mini-batch. It calculates three metrics: mean_hit_at_one, mean_perr, and mean_loss. The function then returns a dictionary containing these metrics for the batch.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/VideoTag/metrics/youtube8m/eval_util.py\":164-189",
            "content": "    def accumulate(self, loss, predictions, labels):\n        \"\"\"Accumulate the metrics calculated locally for this mini-batch.\n    Args:\n      predictions: A numpy matrix containing the outputs of the model.\n        Dimensions are 'batch' x 'num_classes'.\n      labels: A numpy matrix containing the ground truth labels.\n        Dimensions are 'batch' x 'num_classes'.\n      loss: A numpy array containing the loss for each sample.\n    Returns:\n      dictionary: A dictionary storing the metrics for the mini-batch.\n    Raises:\n      ValueError: An error occurred when the shape of predictions and actuals\n        does not match.\n    \"\"\"\n        batch_size = labels.shape[0]\n        mean_hit_at_one = calculate_hit_at_one(predictions, labels)\n        mean_perr = calculate_precision_at_equal_recall_rate(\n            predictions, labels)\n        mean_loss = numpy.mean(loss)\n        # Take the top 20 predictions.\n        sparse_predictions, sparse_labels, num_positives = top_k_by_class(\n            predictions, labels, self.top_k)"
        },
        {
            "comment": "This code calculates and accumulates various evaluation metrics during the epoch, including hit_at_one, perr, and loss. It then returns a dictionary with these metrics after an entire epoch of training. If no examples were accumulated during the epoch, it raises a ValueError.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/VideoTag/metrics/youtube8m/eval_util.py\":190-218",
            "content": "        self.map_calculator.accumulate(sparse_predictions, sparse_labels,\n                                       num_positives)\n        self.global_ap_calculator.accumulate(flatten(sparse_predictions),\n                                             flatten(sparse_labels),\n                                             sum(num_positives))\n        self.num_examples += batch_size\n        self.sum_hit_at_one += mean_hit_at_one * batch_size\n        self.sum_perr += mean_perr * batch_size\n        self.sum_loss += mean_loss * batch_size\n        return {\n            \"hit_at_one\": mean_hit_at_one,\n            \"perr\": mean_perr,\n            \"loss\": mean_loss\n        }\n    def get(self):\n        \"\"\"Calculate the evaluation metrics for the whole epoch.\n    Raises:\n      ValueError: If no examples were accumulated.\n    Returns:\n      dictionary: a dictionary storing the evaluation metrics for the epoch. The\n        dictionary has the fields: avg_hit_at_one, avg_perr, avg_loss, and\n        aps (default nan).\n    \"\"\"\n        if self.num_examples <= 0:"
        },
        {
            "comment": "This code defines a class for evaluating metrics in video tagging. It calculates average hit at one, perr, and loss, as well as maps and global APs. The clear method resets the metrics to zero.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/VideoTag/metrics/youtube8m/eval_util.py\":219-243",
            "content": "            raise ValueError(\"total_sample must be positive.\")\n        avg_hit_at_one = self.sum_hit_at_one / self.num_examples\n        avg_perr = self.sum_perr / self.num_examples\n        avg_loss = self.sum_loss / self.num_examples\n        aps = self.map_calculator.peek_map_at_n()\n        gap = self.global_ap_calculator.peek_ap_at_n()\n        epoch_info_dict = {}\n        return {\n            \"avg_hit_at_one\": avg_hit_at_one,\n            \"avg_perr\": avg_perr,\n            \"avg_loss\": avg_loss,\n            \"aps\": aps,\n            \"gap\": gap\n        }\n    def clear(self):\n        \"\"\"Clear the evaluation metrics and reset the EvaluationMetrics object.\"\"\"\n        self.sum_hit_at_one = 0.0\n        self.sum_perr = 0.0\n        self.sum_loss = 0.0\n        self.map_calculator.clear()\n        self.global_ap_calculator.clear()\n        self.num_examples = 0"
        }
    ]
}