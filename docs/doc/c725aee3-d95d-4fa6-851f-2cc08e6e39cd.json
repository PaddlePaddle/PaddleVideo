{
    "summary": "This section provides Linux setup for deploying PaddleVideo models, offers Windows support, and recommends Docker. It installs OpenCV 3.4.7, sets paths, compiles Video inference code, builds prediction library with simple commands, and defines model parameters/configuration options but may encounter errors due to missing libcudnn or incorrect CUDNN_LIB_DIR setting.",
    "details": [
        {
            "comment": "This section introduces the C++ deployment method of PaddleVideo model, which offers better performance compared to Python. It provides instructions for setting up a Linux environment with docker recommendation and mentions that Windows support is under development. Additionally, it requires installing extra dependencies like paddledet using pip.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/cpp_infer/readme_en.md\":0-19",
            "content": "English | [\u7b80\u4f53\u4e2d\u6587](./readme.md)\n# Server-side C++ prediction\nThis chapter introduces the C++ deployment method of the PaddleVideo model. For the python prediction deployment method, please refer to the **Model Reasoning** chapter of the respective model.\nC++ is better than python in terms of performance calculation. Therefore, in most CPU and GPU deployment scenarios, C++ deployment methods are mostly used. This section will introduce how to configure the C++ environment in the Linux (CPU/GPU) environment and complete it.\nPaddleVideo model deployment.\nBefore getting started, you need to install additional dependencies as follows:\n```bash\npython -m pip install [paddledet](git+https://github.com/LDOUBLEV/AutoLog)\n```\n## 1. Prepare the environment\n- For Linux environment, docker is recommended.\n- Windows environment, currently supports compilation based on `Visual Studio 2019 Community` (TODO)\n* This document mainly introduces the PaddleVideo C++ prediction process based on the Linux environment. If yo"
        },
        {
            "comment": "This code provides instructions for compiling the OpenCV library and installing FFmpeg to enable normal video reading under Linux. It also mentions a Windows Compilation Tutorial that needs to be completed (TODO).",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/cpp_infer/readme_en.md\":19-36",
            "content": "u need to perform C++ prediction based on the prediction library under Windows, please refer to [Windows Compilation Tutorial](./docs/windows_vs2019_build.md)(TODO) for the specific compilation method\n* **The purpose of preparing the environment is to get the compiled opencv library and paddle prediction library**.\n### 1.1 Compile opencv library\n* First, you need to download the compressed package compiled from the source code in the Linux environment from the opencv official website, and unzip it into a folder. Take opencv3.4.7 as an example, the download command is as follows:\n    ```bash\n    cd deploy/cpp_infer\n    wget https://github.com/opencv/opencv/archive/3.4.7.tar.gz\n    tar -xf 3.4.7.tar.gz\n    ```\n    After decompression, you can get the decompressed folder of `opencv-3.4.7` in the `deploy/cpp_infer` directory.\n* Install ffmpeg\n    Opencv and ffmpeg can read the video normally under linux, otherwise it may encounter the situation that the number of video frames returns to 0 or no video frame can be read"
        },
        {
            "comment": "This code installs necessary libraries for compiling OpenCV 3.4.7 on Linux, sets the source and installation paths, removes existing build folder, creates a new one, runs cmake to configure build options and libraries to use, and then proceeds with the compilation process.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/cpp_infer/readme_en.md\":38-75",
            "content": "    Using a relatively simple apt installation, the installation command is as follows:\n    ```bash\n    apt-get update\n    apt install libavformat-dev\n    apt install libavcodec-dev\n    apt install libswresample-dev\n    apt install libswscale-dev\n    apt install libavutil-dev\n    apt install libsdl1.2-dev\n    apt-get install ffmpeg\n    ```\n* To prepare to compile opencv, first enter the `opencv-3.4.7` folder, and then set the opencv source path `root_path` and the installation path `install_path`. The execution command is as follows:\n    ```bash\n    cd opencv-3.4.7\n    root_path=$PWD  # That is the absolute path of opencv-3.4.7\n    install_path=${root_path}/opencv3\n    rm -rf build\n    mkdir build\n    cd build\n    cmake .. \\\n        -DCMAKE_INSTALL_PREFIX=${install_path} \\\n        -DCMAKE_BUILD_TYPE=Release \\\n        -DBUILD_SHARED_LIBS=OFF \\\n        -DWITH_IPP=OFF \\\n        -DBUILD_IPP_IW=OFF \\\n        -DWITH_LAPACK=OFF \\\n        -DWITH_EIGEN=OFF \\\n        -DCMAKE_INSTALL_LIBDIR=lib64 \\\n        -DWITH_ZLIB=ON \\\n        -DBUILD_ZLIB=ON \\"
        },
        {
            "comment": "Configuring and installing OpenCV library with specified options and building the Video inference C++ code using it.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/cpp_infer/readme_en.md\":76-109",
            "content": "        -DWITH_JPEG=ON \\\n        -DBUILD_JPEG=ON \\\n        -DWITH_PNG=ON \\\n        -DBUILD_PNG=ON \\\n        -DWITH_TIFF=ON \\\n        -DBUILD_TIFF=ON \\\n        -DWITH_FFMPEG=ON\n    make -j\n    make install\n    ```\n    After the completion of `make install`, opencv header files and library files will be generated in this folder, which will be used to compile the Video inference C++ code later.\n    Finally, the installation path `install_path` will be used as the specified path, and a folder of `opencv3` will be obtained. The file structure is shown below.\n    ```shell\n    opencv-3.4.7/\n    \u251c\u2500\u2500 opencv3/\n    \u2502   \u251c\u2500\u2500 bin/\n    \u2502   \u251c\u2500\u2500 include/\n    \u2502   \u251c\u2500\u2500 lib/\n    \u2502   \u251c\u2500\u2500 lib64/\n    \u2502   \u2514\u2500\u2500 share/\n    ```\n### 1.2 Download or compile Paddle prediction library\nThere are two ways to obtain the Paddle prediction library, which will be described in detail below.\n#### 1.2.1 Download and install directly\n* [Paddle prediction library official website](https://paddleinference.paddlepaddle.org.cn/v2.2/user_guides/download_li"
        },
        {
            "comment": "This code provides instructions for downloading and unzipping the prediction library, or compiling it from source code if you want the latest features. It specifies the appropriate version selection on the official website (paddle version>=2.0.1, 2.2.2 recommended) and the required environment (gcc8.2). The wget command downloads the tgz package, tar extracts it into a subfolder of paddle_inference in the current folder. Alternatively, cloning the latest code from Paddle GitHub and compiling from source can be done for accessing the latest prediction library features.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/cpp_infer/readme_en.md\":109-122",
            "content": "b.html) provides different cuda versions of Linux prediction libraries, you can Check and **select the appropriate prediction library version** on the official website (it is recommended to select the prediction library with paddle version>=2.0.1, and the prediction library of 2.2.2 is recommended).\n* Download and get a `paddle_inference.tgz` compressed package, and then unzip it into a folder, the command is as follows (taking the machine environment as gcc8.2 as an example):\n    ```bash\n    wget https://paddle-inference-lib.bj.bcebos.com/2.2.2/cxx_c/Linux/GPU/x86-64_gcc8.2_avx_mkl_cuda10.1_cudnn7.6.5_trt6.0.1.5/paddle_inference.tgz\n    tar -xf paddle_inference.tgz\n    ```\n    Eventually, a subfolder of `paddle_inference/` will be generated in the current folder.\n#### 1.2.2 Prediction library source code compilation\n* If you want to get the latest prediction library features, you can clone the latest code from Paddle github and compile the prediction library from source code.\n* You can refer t"
        },
        {
            "comment": "This code provides the installation and compilation instructions for Paddle prediction library. The steps involve cloning the Paddle repository, checking out a specific release branch, configuring and building the project with CMake, and finally generating the prediction library by making and making inference_lib_dist. This process is done to ensure that users can obtain the latest and most optimized version of the prediction library for their needs.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/cpp_infer/readme_en.md\":122-149",
            "content": "o [Paddle prediction library installation and compilation instructions](https://paddleinference.paddlepaddle.org.cn/user_guides/source_compile.html) instructions from github Obtain the Paddle code, and then compile it to generate the latest prediction library. The method of using git to get the code is as follows.\n    ```shell\n    git clone https://github.com/PaddlePaddle/Paddle.git\n    git checkout release/2.2\n    ```\n* After entering the Paddle directory, the compilation method is as follows.\n    ```shell\n    rm -rf build\n    mkdir build\n    cd build\n    cmake .. \\\n        -DWITH_CONTRIB=OFF \\\n        -DWITH_MKL=ON \\\n        -DWITH_MKLDNN=ON \\\n        -DWITH_TESTING=OFF \\\n        -DCMAKE_BUILD_TYPE=Release \\\n        -DWITH_INFERENCE_API_TEST=OFF \\\n        -DON_INFER=ON \\\n        -DWITH_PYTHON=ON\n    make -j\n    make inference_lib_dist -j4 # 4\u4e3a\u7f16\u8bd1\u65f6\u4f7f\u7528\u6838\u6570\uff0c\u53ef\u6839\u636e\u673a\u5668\u60c5\u51b5\u81ea\u884c\u4fee\u6539\n    ```\n    You can refer to [documentation](https://www.paddlepaddle.org.cn/documentation/docs/zh/2.0/guides/05_inference_deployment/inference/b"
        },
        {
            "comment": "Step 1: The code describes the generation of several files and folders after a successful compilation process. These include `CMakeCache.txt`, `paddle/`, `third_party/`, and `version.txt`.\n\nStep 2: Explains that among these, `paddle` is the C++ library required for prediction, while `version.txt` contains version information of the current prediction library.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/cpp_infer/readme_en.md\":149-172",
            "content": "uild_and_install_lib_cn.html#congyuanmabianyi) for more introduction of compilation parameter options.\n* After the compilation is complete, you can see the following files and folders are generated under the file `build/paddle_inference_install_dir/`.\n    ```\n    build/\n    \u2514\u2500\u2500 paddle_inference_install_dir/\n        \u251c\u2500\u2500 CMakeCache.txt\n        \u251c\u2500\u2500 paddle/\n        \u251c\u2500\u2500 third_party/\n        \u2514\u2500\u2500 version.txt\n    ```\n    Among them, `paddle` is the Paddle library required for C++ prediction, and `version.txt` contains the version information of the current prediction library.\n## 2. Compile and run the prediction demo\n### 2.1 Export the model as an inference model\n* This step is the same as the export prediction model under the python deployment mode. You can refer to the model prediction chapter of the respective model. Several related inference model files exported are used for model prediction. **Taking PP-TSM as an example**, the directory structure of the derived prediction model is as follows.\n    ```\n    inference/"
        },
        {
            "comment": "This code snippet provides instructions for compiling the PaddleVideo C++ prediction demo. First, navigate to the `deploy/cpp_infer` directory. Then, execute the compile command `bash tools/build.sh`. Modify environment paths in `tools/build.sh`, such as OPENCV_DIR, LIB_DIR, CUDA_LIB_DIR, and CUDNN_LIB_DIR to match your system's configuration.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/cpp_infer/readme_en.md\":173-202",
            "content": "    \u2514\u2500\u2500 ppTSM/\n        \u251c\u2500\u2500 ppTSM.pdiparams\n        \u251c\u2500\u2500 ppTSM.pdiparamsinfo\n        \u2514\u2500\u2500 ppTSM.pdmodel\n    ```\n### 2.2 Compile PaddleVideo C++ prediction demo\n* Enter the `deploy/cpp_infer` directory and execute the following compile command\n    ```shell\n    bash tools/build.sh\n    ```\n    The addresses of the Paddle C++ prediction library, opencv and other dependent libraries in `tools/build.sh` need to be replaced with the actual addresses on your own machine.\n* Specifically, you need to modify the environment path in `tools/build.sh`, the relevant content is as follows:\n    ```shell\n    OPENCV_DIR=your_opencv_dir\n    LIB_DIR=your_paddle_inference_dir\n    CUDA_LIB_DIR=/usr/local/cuda/lib64\n    CUDNN_LIB_DIR=/usr/lib/x86_64-linux-gnu/\n    ```\n    The above parameters are as follows (the following path users can modify according to their own machine conditions)\n    `OPENCV_DIR` is the address where opencv is compiled and installed\n     `LIB_DIR` is the download (`paddle_inference` folder) or the generated Paddle prediction library address (`build/paddle_inference_install_dir` folder)"
        },
        {
            "comment": "To enable TensorRT acceleration during prediction, modify the code in `tools/build.sh` by setting `DWITH_GPU=ON`, `DWITH_TENSORRT=ON`, and providing the absolute path to TensorRT using `TENSORRT_DIR`. This allows for GPU-accelerated predictions with PaddleVideo's C++ implementation.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/cpp_infer/readme_en.md\":203-230",
            "content": "     `CUDA_LIB_DIR` is the address of the cuda library file, which is `/usr/local/cuda/lib64` in docker\n     `CUDNN_LIB_DIR` is the cudnn library file address, which is `/usr/lib/x86_64-linux-gnu/` in docker.\n     **If you want to enable TensorRT acceleration during prediction, you need to modify the code at `tools/build.sh`3**\n     1. Set `DWITH_GPU=ON`\n     2. Set `DWITH_TENSORRT=ON`\n     3. Set `TENSORRT_DIR=/path/to/TensorRT-x.x.x.x`\n    **The above paths are all absolute paths, do not use relative paths**\n* After the compilation is complete, an executable file named `ppvideo` will be generated in the `cpp_infer/build` folder.\n### 2.3 Run PaddleVideo C++ prediction demo\nOperation mode:\n```bash\n./build/ppvideo <mode> [--param1] [--param2] [...]\n```\nAmong them, `mode` is a required parameter, which means the selected function, and the value range is ['rec'], which means **video recognition** (more functions will be added in succession).\n##### 1. Call video recognition:\n```bash\n# run PP-TSM inference\n./build/ppvideo rec \\"
        },
        {
            "comment": "This code sets the model directory, inference model name, video directory, number of segments, and segment length for PaddleVideo's PP-TSM and PP-TSN inference. Additional parameters include use_gpu, gpu_id, gpu_mem, cpu_threads, enable_mkldnn, use_tensorrt, and precision for customizing the inference process.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/cpp_infer/readme_en.md\":231-257",
            "content": "--rec_model_dir=../../inference/ppTSM \\\n--inference_model_name=ppTSM \\\n--video_dir=./example_video_dir \\\n--num_seg=8 \\\n--seg_len=1\n# run PP-TSN inference\n./build/ppvideo rec \\\n--rec_model_dir=../../inference/ppTSN \\\n--inference_model_name=ppTSN \\\n--video_dir=./example_video_dir \\\n--num_seg=25 \\\n--seg_len=1\n```\nMore parameters are as follows:\n- General parameters\n    | Parameter name | Type | Default parameter | Meaning |\n    | ------------- | ---- | --------------- | ------------------------------------------------------------ |\n    | use_gpu | bool | false | Whether to use GPU |\n    | gpu_id | int | 0 | GPU id, valid when using GPU |\n    | gpu_mem | int | 4000 | GPU memory requested |\n    | cpu_threads | int | 10 | The number of threads for CPU prediction. When the number of machine cores is sufficient, the larger the value, the faster the prediction speed |\n    | enable_mkldnn | bool | false | Whether to use mkldnn library |\n    | use_tensorrt | bool | false | Whether to use the tensorrt library |\n    | precision | str | \"fp32\" | Use fp32/fp16/uint8 precision to predict |"
        },
        {
            "comment": "This code provides configuration parameters for video recognition models. The `video_dir` specifies the folder path containing the video to be recognized, while `rec_model_dir` points to the exported prediction model's location. The `inference_model_name` refers to the name of the model used in prediction. `num_seg` and `seg_len` determine the number of video segments and frames per segment respectively. `rec_batch_num` indicates the batch size during model prediction, and `char_list_file` stores category labels and names.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/cpp_infer/readme_en.md\":258-270",
            "content": "    | benchmark | bool | true | Whether to enable benchmark during prediction, after enabling it, the configuration, model, time-consuming and other information will be output at the end. |\n- Video recognition model related\n    | Parameter name | Type | Default parameter | Meaning |\n    | -------------- | ------ | --------------------------------------------- | ------------------------------------ |\n    | video_dir | string | \"../example_video_dir\" | The path of the folder where the video to be recognized is stored |\n    | rec_model_dir | string | \"\" | The folder path where the exported prediction model is stored |\n    | inference_model_name | string | \"ppTSM\" | The name of the model used in the prediction |\n    | num_seg | int | 8 | Number of video segments |\n    | seg_len | int | 1 | The number of frames extracted in each segment of the video |\n    | rec_batch_num | int | 1 | Batch size during model prediction |\n    | char_list_file | str | \"../../data/k400/Kinetics-400_label_list.txt\" | The text path for storing all category labels and corresponding names |"
        },
        {
            "comment": "This code snippet demonstrates the output of the inference process for a sample video. It shows the input video, its class and score. Additionally, it provides information about the runtime device, IR optimization, memory optimization, whether TensorRT is enabled or not, the number of CPU math library threads, and data information such as batch size and input shape.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/cpp_infer/readme_en.md\":272-288",
            "content": "\u200b\tTake the sample video `example01.avi` under example_video_dir as the input video as an example, the final \tscreen will output the detection results as follows.\n```bash\n[./inference/ppTSM]\n[./deploy/cpp_infer/example_video_dir]\ntotal videos num: 1\n./example_video_dir/example01.avi   class: 5 archery       score: 0.999556\nI1125 08:10:45.834288 13955 autolog.h:50] ----------------------- Config info -----------------------\nI1125 08:10:45.834458 13955 autolog.h:51] runtime_device: cpu\nI1125 08:10:45.834467 13955 autolog.h:52] ir_optim: True\nI1125 08:10:45.834475 13955 autolog.h:53] enable_memory_optim: True\nI1125 08:10:45.834483 13955 autolog.h:54] enable_tensorrt: 0\nI1125 08:10:45.834518 13955 autolog.h:55] enable_mkldnn: False\nI1125 08:10:45.834525 13955 autolog.h:56] cpu_math_library_num_threads: 10\nI1125 08:10:45.834532 13955 autolog.h:57] ----------------------- Data info -----------------------\nI1125 08:10:45.834540 13955 autolog.h:58] batch_size: 1\nI1125 08:10:45.834547 13955 autolog.h:59] input_shape: dynamic"
        },
        {
            "comment": "The code is displaying information about the model used for inference. It mentions the model name, precision type, and total time spent on inference. Additionally, it provides a breakdown of preprocessing, inference, and post-processing times. The error message indicates that the CUDA Deep Neural Network library (libcudnn) is missing or not found during compilation.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/cpp_infer/readme_en.md\":289-307",
            "content": "I1125 08:10:45.834556 13955 autolog.h:60] data_num: 1\nI1125 08:10:45.834564 13955 autolog.h:61] ----------------------- Model info -----------------------\nI1125 08:10:45.834573 13955 autolog.h:62] model_name: rec\nI1125 08:10:45.834579 13955 autolog.h:63] precision: fp32\nI1125 08:10:45.834586 13955 autolog.h:64] ----------------------- Perf info ------------------------\nI1125 08:10:45.834594 13955 autolog.h:65] Total time spent(ms): 2739\nI1125 08:10:45.834602 13955 autolog.h:67] preprocess_time(ms): 10.6524, inference_time(ms): 1269.55, postprocess_time(ms): 0.009118\n```\n### 3 FAQ\n1. The following error occurred during the compilation of the demo\n     ```shell\n     make[2]: *** No rule to make target '/usr/lib/x86_64-linux-gn/libcudnn.so', needed by 'ppvideo'. Stop.\n     make[2]: *** Waiting for unfinished jobs....\n     [ 16%] Building CXX object CMakeFiles/ppvideo.dir/src/main.cpp.o\n     [ 50%] Building CXX object CMakeFiles/ppvideo.dir/src/preprocess_op.cpp.o\n     [ 50%] Building CXX object CMakeFiles/ppvideo.dir/src/postprocess_op.cpp.o"
        },
        {
            "comment": "Error: CMakeFiles/ppvideo.dir/all and all target failed with Error 2 due to missing libcudnn.so, possibly caused by incorrect CUDNN_LIB_DIR setting.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/cpp_infer/readme_en.md\":308-315",
            "content": "     [83%] Building CXX object CMakeFiles/ppvideo.dir/src/utility.cpp.o\n     [ 83%] Building CXX object CMakeFiles/ppvideo.dir/src/video_rec.cpp.o\n     CMakeFiles/Makefile2:95: recipe for target 'CMakeFiles/ppvideo.dir/all' failed\n     make[1]: *** [CMakeFiles/ppvideo.dir/all] Error 2\n     Makefile:83: recipe for target 'all' failed\n     make: *** [all] Error 2\n     ````\n     It may be that `CUDNN_LIB_DIR` is set incorrectly, resulting in that `libcudnn.so` in this directory cannot be found."
        }
    ]
}