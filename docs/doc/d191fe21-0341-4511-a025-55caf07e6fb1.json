{
    "summary": "This code enhances PaddleVideo's loader with resize operation and augmentation pipeline, enabling diverse data preprocessing. It calculates crop offsets and performs object detection image augmentation using uniform sampling, resizing, and flipping techniques, resizes images, scales by 255.0, concatenates frames, transposes array dimensions, stores results in 'results', and returns arrays.",
    "details": [
        {
            "comment": "This code is from the PaddleVideo library, specifically the loader module's augmentations pipeline. It defines a Scale class that scales images based on their short side to the given short_size parameter. The fixed_ratio parameter determines whether or not the image should be resized while maintaining its aspect ratio. This class is then registered in PIPELINES for later use.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":0-33",
            "content": "#  Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport math\nimport random\nfrom collections.abc import Sequence\nimport cv2\nimport numpy as np\nimport paddle\nimport paddle.nn.functional as F\nfrom PIL import Image\nfrom ..registry import PIPELINES\n@PIPELINES.register()\nclass Scale(object):\n    \"\"\"\n    Scale images.\n    Args:\n        short_size(float | int): Short size of an image will be scaled to the short_size.\n        fixed_ratio(bool): Set whether to zoom according to a fixed ratio. default: True"
        },
        {
            "comment": "The code defines a class for resize operations. It takes parameters for short size, fixed ratio (defaults to True), keep ratio, do_round (default False), and backend (default 'pillow'). The class checks if fixed_ratio and keep_ratio can't be true at the same time. It also ensures the backend is either 'pillow' or 'cv2'. The __call__ method performs resize operations on images, taking a Sequence of PIL.Image as input.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":34-60",
            "content": "        do_round(bool): Whether to round up when calculating the zoom ratio. default: False\n        backend(str): Choose pillow or cv2 as the graphics processing backend. default: 'pillow'\n    \"\"\"\n    def __init__(self,\n                 short_size,\n                 fixed_ratio=True,\n                 keep_ratio=None,\n                 do_round=False,\n                 backend='pillow'):\n        self.short_size = short_size\n        assert (fixed_ratio and not keep_ratio) or (not fixed_ratio), \\\n            f\"fixed_ratio and keep_ratio cannot be true at the same time\"\n        self.fixed_ratio = fixed_ratio\n        self.keep_ratio = keep_ratio\n        self.do_round = do_round\n        assert backend in [\n            'pillow', 'cv2'\n        ], f\"Scale's backend must be pillow or cv2, but get {backend}\"\n        self.backend = backend\n    def __call__(self, results):\n        \"\"\"\n        Performs resize operations.\n        Args:\n            imgs (Sequence[PIL.Image]): List where each item is a PIL.Image.\n            For example, [PIL.Image0, PIL.Image1, PIL.Image2, ...]"
        },
        {
            "comment": "This code is responsible for resizing images to a specified size in a PaddleVideo pipeline. It iterates through each image, checks the aspect ratio, and resizes them accordingly before appending to the resized_imgs list. If the image is already the correct size, it is directly added to the list without further processing.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":61-87",
            "content": "        return:\n            resized_imgs: List where each item is a PIL.Image after scaling.\n        \"\"\"\n        imgs = results['imgs']\n        resized_imgs = []\n        for i in range(len(imgs)):\n            img = imgs[i]\n            if isinstance(img, np.ndarray):\n                h, w, _ = img.shape\n            elif isinstance(img, Image.Image):\n                w, h = img.size\n            else:\n                raise NotImplementedError\n            if (w <= h and w == self.short_size) or (h <= w\n                                                     and h == self.short_size):\n                if self.backend == 'pillow' and not isinstance(\n                        img, Image.Image):\n                    img = Image.fromarray(img)\n                resized_imgs.append(img)\n                continue\n            if w <= h:\n                ow = self.short_size\n                if self.fixed_ratio:\n                    oh = int(self.short_size * 4.0 / 3.0)\n                elif self.keep_ratio is False:\n                    oh = self.short_size"
        },
        {
            "comment": "This code calculates the output image size for resizing and maintains aspect ratio if specified. It uses scale_factor to calculate the output height (oh) and width (ow), considering do_round, fixed_ratio, keep_ratio flags and short_size.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":88-107",
            "content": "                else:\n                    scale_factor = self.short_size / w\n                    oh = int(h * float(scale_factor) +\n                             0.5) if self.do_round else int(h *\n                                                            self.short_size / w)\n                    ow = int(w * float(scale_factor) +\n                             0.5) if self.do_round else self.short_size\n            else:\n                oh = self.short_size\n                if self.fixed_ratio:\n                    ow = int(self.short_size * 4.0 / 3.0)\n                elif self.keep_ratio is False:\n                    ow = self.short_size\n                else:\n                    scale_factor = self.short_size / h\n                    oh = int(h * float(scale_factor) +\n                             0.5) if self.do_round else self.short_size\n                    ow = int(w * float(scale_factor) +\n                             0.5) if self.do_round else int(w *\n                                                            self.short_size / h)"
        },
        {
            "comment": "This code defines an augmentation pipeline for image processing. It resizes images using different backends based on the backend specified and whether the ratio should be preserved or not. The results are then returned as a dictionary with 'imgs' key containing the resized images. Additionally, there is a RandomCrop class which performs random crop operations on images of the specified target size.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":108-137",
            "content": "            if self.backend == 'pillow':\n                resized_imgs.append(img.resize((ow, oh), Image.BILINEAR))\n            elif self.backend == 'cv2' and (self.keep_ratio is not None):\n                resized_imgs.append(\n                    cv2.resize(img, (ow, oh), interpolation=cv2.INTER_LINEAR))\n            else:\n                resized_imgs.append(\n                    Image.fromarray(\n                        cv2.resize(np.asarray(img), (ow, oh),\n                                   interpolation=cv2.INTER_LINEAR)))\n        results['imgs'] = resized_imgs\n        return results\n@PIPELINES.register()\nclass RandomCrop(object):\n    \"\"\"\n    Random crop images.\n    Args:\n        target_size(int): Random crop a square with the target_size from an image.\n    \"\"\"\n    def __init__(self, target_size):\n        self.target_size = target_size\n    def __call__(self, results):\n        \"\"\"\n        Performs random crop operations.\n        Args:\n            imgs: List where each item is a PIL.Image.\n            For example, [PIL.Image0, PIL.Image1, PIL.Image2, ...]"
        },
        {
            "comment": "This code is a part of PaddleVideo's augmentations.py, which applies random cropping to images. It checks if the backend used is 'pyav', and if so, extracts the image dimensions. If not, it gets the image size from the first image in the list. Then, it asserts that the image dimensions are larger than the target size. Finally, it generates a random crop position and crops each image in the list using these positions. The cropped images are stored in the 'crop_images' list which is returned at the end.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":138-163",
            "content": "        return:\n            crop_imgs: List where each item is a PIL.Image after random crop.\n        \"\"\"\n        imgs = results['imgs']\n        if 'backend' in results and results['backend'] == 'pyav':  # [c,t,h,w]\n            h, w = imgs.shape[2:]\n        else:\n            w, h = imgs[0].size\n        th, tw = self.target_size, self.target_size\n        assert (w >= self.target_size) and (h >= self.target_size), \\\n            \"image width({}) and height({}) should be larger than crop size\".format(\n                w, h, self.target_size)\n        crop_images = []\n        if 'backend' in results and results['backend'] == 'pyav':\n            x1 = np.random.randint(0, w - tw)\n            y1 = np.random.randint(0, h - th)\n            crop_images = imgs[:, :, y1:y1 + th, x1:x1 + tw]  # [C, T, th, tw]\n        else:\n            x1 = random.randint(0, w - tw)\n            y1 = random.randint(0, h - th)\n            for img in imgs:\n                if w == tw and h == th:\n                    crop_images.append(img)\n                else:"
        },
        {
            "comment": "RandomResizedCrop is a pipeline that resizes and crops images randomly with specified area, aspect ratio range, target size, and backend. The method get_crop_bbox takes image shape, area and aspect ratio ranges as input and returns the crop bounding box within the specified range of area and aspect ratio.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":164-196",
            "content": "                    crop_images.append(img.crop((x1, y1, x1 + tw, y1 + th)))\n        results['imgs'] = crop_images\n        return results\n@PIPELINES.register()\nclass RandomResizedCrop(RandomCrop):\n    def __init__(self,\n                 area_range=(0.08, 1.0),\n                 aspect_ratio_range=(3 / 4, 4 / 3),\n                 target_size=224,\n                 backend='cv2'):\n        self.area_range = area_range\n        self.aspect_ratio_range = aspect_ratio_range\n        self.target_size = target_size\n        self.backend = backend\n    @staticmethod\n    def get_crop_bbox(img_shape,\n                      area_range,\n                      aspect_ratio_range,\n                      max_attempts=10):\n        assert 0 < area_range[0] <= area_range[1] <= 1\n        assert 0 < aspect_ratio_range[0] <= aspect_ratio_range[1]\n        img_h, img_w = img_shape\n        area = img_h * img_w\n        min_ar, max_ar = aspect_ratio_range\n        aspect_ratios = np.exp(\n            np.random.uniform(np.log(min_ar), np.log(max_ar),"
        },
        {
            "comment": "This function generates a random crop size based on the aspect ratios and target areas. It then iterates through candidate crop sizes, selecting one that fits within the image bounds. If no suitable crop is found, it falls back to centering a smaller crop. The function returns the offset coordinates and crop dimensions for the selected crop.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":197-218",
            "content": "                              size=max_attempts))\n        target_areas = np.random.uniform(*area_range, size=max_attempts) * area\n        candidate_crop_w = np.round(np.sqrt(target_areas *\n                                            aspect_ratios)).astype(np.int32)\n        candidate_crop_h = np.round(np.sqrt(target_areas /\n                                            aspect_ratios)).astype(np.int32)\n        for i in range(max_attempts):\n            crop_w = candidate_crop_w[i]\n            crop_h = candidate_crop_h[i]\n            if crop_h <= img_h and crop_w <= img_w:\n                x_offset = random.randint(0, img_w - crop_w)\n                y_offset = random.randint(0, img_h - crop_h)\n                return x_offset, y_offset, x_offset + crop_w, y_offset + crop_h\n        # Fallback\n        crop_size = min(img_h, img_w)\n        x_offset = (img_w - crop_size) // 2\n        y_offset = (img_h - crop_size) // 2\n        return x_offset, y_offset, x_offset + crop_size, y_offset + crop_size\n    def __call__(self, results):"
        },
        {
            "comment": "This code is a part of PaddleVideo library and performs image cropping based on the specified backend. It first retrieves the image dimensions, then applies a crop box to each image according to the defined area range and aspect ratio range. The code handles different backends such as Pillow, OpenCV (cv2), and PyAV. If an unsupported backend is encountered, it raises a NotImplementedError.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":219-248",
            "content": "        imgs = results['imgs']\n        if self.backend == 'pillow':\n            img_w, img_h = imgs[0].size\n        elif self.backend == 'cv2':\n            img_h, img_w, _ = imgs[0].shape\n        elif self.backend == 'pyav':\n            img_h, img_w = imgs.shape[2:]  # [cthw]\n        else:\n            raise NotImplementedError\n        left, top, right, bottom = self.get_crop_bbox(\n            (img_h, img_w), self.area_range, self.aspect_ratio_range)\n        if self.backend == 'pillow':\n            img_w, img_h = imgs[0].size\n            imgs = [img.crop(left, top, right, bottom) for img in imgs]\n        elif self.backend == 'cv2':\n            img_h, img_w, _ = imgs[0].shape\n            imgs = [img[top:bottom, left:right] for img in imgs]\n        elif self.backend == 'pyav':\n            img_h, img_w = imgs.shape[2:]  # [cthw]\n            imgs = imgs[:, :, top:bottom, left:right]\n        else:\n            raise NotImplementedError\n        results['imgs'] = imgs\n        return results\n@PIPELINES.register()\nclass CenterCrop(object):"
        },
        {
            "comment": "This code defines a class for center cropping images. The constructor takes the target size, whether to round the coordinates (True by default), and the backend (default is Pillow). The `__call__` method applies the center crop operation on a list of PIL Image objects, returning a new list with the cropped images.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":249-275",
            "content": "    \"\"\"\n    Center crop images.\n    Args:\n        target_size(int): Center crop a square with the target_size from an image.\n        do_round(bool): Whether to round up the coordinates of the upper left corner of the cropping area. default: True\n    \"\"\"\n    def __init__(self, target_size, do_round=True, backend='pillow'):\n        self.target_size = target_size\n        self.do_round = do_round\n        self.backend = backend\n    def __call__(self, results):\n        \"\"\"\n        Performs Center crop operations.\n        Args:\n            imgs: List where each item is a PIL.Image.\n            For example, [PIL.Image0, PIL.Image1, PIL.Image2, ...]\n        return:\n            ccrop_imgs: List where each item is a PIL.Image after Center crop.\n        \"\"\"\n        imgs = results['imgs']\n        ccrop_imgs = []\n        th, tw = self.target_size, self.target_size\n        if isinstance(imgs, paddle.Tensor):\n            h, w = imgs.shape[-2:]\n            x1 = int(round((w - tw) / 2.0)) if self.do_round else (w - tw) // 2\n            y1 = int(round((h - th) / 2.0)) if self.do_round else (h - th) // 2"
        },
        {
            "comment": "This function performs center crop on images based on the given target size. It first checks if the image dimensions are larger than the crop size, and then calculates the starting coordinates for cropping. If the backend is Pillow, it uses the crop() method to perform the cropping operation; if the backend is OpenCV (cv2), it slices the image array accordingly. The resulting cropped images are stored in 'ccrop_imgs' list and returned in the 'results' dictionary under the key 'imgs'.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":276-296",
            "content": "            ccrop_imgs = imgs[:, :, y1:y1 + th, x1:x1 + tw]\n        else:\n            for img in imgs:\n                if self.backend == 'pillow':\n                    w, h = img.size\n                elif self.backend == 'cv2':\n                    h, w, _ = img.shape\n                else:\n                    raise NotImplementedError\n                assert (w >= self.target_size) and (h >= self.target_size), \\\n                    \"image width({}) and height({}) should be larger than crop size\".format(\n                        w, h, self.target_size)\n                x1 = int(round(\n                    (w - tw) / 2.0)) if self.do_round else (w - tw) // 2\n                y1 = int(round(\n                    (h - th) / 2.0)) if self.do_round else (h - th) // 2\n                if self.backend == 'cv2':\n                    ccrop_imgs.append(img[y1:y1 + th, x1:x1 + tw])\n                elif self.backend == 'pillow':\n                    ccrop_imgs.append(img.crop((x1, y1, x1 + tw, y1 + th)))\n        results['imgs'] = ccrop_imgs"
        },
        {
            "comment": "The MultiScaleCrop class is a pipeline module that randomly crops images with multiple scales, targeting a specific size. It allows adjustable parameters like maximum distortion, fix crop start point, and duplicate candidate crop points for flexibility. This module is useful in image processing tasks where random cropping can provide more data augmentation and improve model performance.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":297-324",
            "content": "        return results\n@PIPELINES.register()\nclass MultiScaleCrop(object):\n    \"\"\"\n    Random crop images in with multiscale sizes\n    Args:\n        target_size(int): Random crop a square with the target_size from an image.\n        scales(int): List of candidate cropping scales.\n        max_distort(int): Maximum allowable deformation combination distance.\n        fix_crop(int): Whether to fix the cutting start point.\n        allow_duplication(int): Whether to allow duplicate candidate crop starting points.\n        more_fix_crop(int): Whether to allow more cutting starting points.\n    \"\"\"\n    def __init__(\n            self,\n            target_size,  # NOTE: named target size now, but still pass short size in it!\n            scales=None,\n            max_distort=1,\n            fix_crop=True,\n            allow_duplication=False,\n            more_fix_crop=True,\n            backend='pillow'):\n        self.target_size = target_size\n        self.scales = scales if scales else [1, .875, .75, .66]\n        self.max_distort = max_distort"
        },
        {
            "comment": "This code defines a class for multi-scale cropping of images with Pillow or OpenCV backend. The `__init__` method initializes the instance variables and checks if the provided backend is either 'pillow' or 'cv2'. The `__call__` method performs the actual multi-scale cropping operation on a given list of images, applying random crop offsets to each image with the specified target size.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":325-359",
            "content": "        self.fix_crop = fix_crop\n        self.allow_duplication = allow_duplication\n        self.more_fix_crop = more_fix_crop\n        assert backend in [\n            'pillow', 'cv2'\n        ], f\"MultiScaleCrop's backend must be pillow or cv2, but get {backend}\"\n        self.backend = backend\n    def __call__(self, results):\n        \"\"\"\n        Performs MultiScaleCrop operations.\n        Args:\n            imgs: List where wach item is a PIL.Image.\n            XXX:\n        results:\n        \"\"\"\n        imgs = results['imgs']\n        input_size = [self.target_size, self.target_size]\n        im_size = imgs[0].size\n        # get random crop offset\n        def _sample_crop_size(im_size):\n            image_w, image_h = im_size[0], im_size[1]\n            base_size = min(image_w, image_h)\n            crop_sizes = [int(base_size * x) for x in self.scales]\n            crop_h = [\n                input_size[1] if abs(x - input_size[1]) < 3 else x\n                for x in crop_sizes\n            ]\n            crop_w = [\n                input_size[0] if abs(x - input_size[0]) < 3 else x"
        },
        {
            "comment": "This code generates a random crop pair from provided sizes, and then applies different cropping locations to the image. If fix_crop is False, it randomly selects an offset for the crop pair within the image boundaries. If fix_crop is True, it calculates four different offsets in a grid pattern using step values based on the image size. The resulting crops are stored in 'ret'.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":360-383",
            "content": "                for x in crop_sizes\n            ]\n            pairs = []\n            for i, h in enumerate(crop_h):\n                for j, w in enumerate(crop_w):\n                    if abs(i - j) <= self.max_distort:\n                        pairs.append((w, h))\n            crop_pair = random.choice(pairs)\n            if not self.fix_crop:\n                w_offset = random.randint(0, image_w - crop_pair[0])\n                h_offset = random.randint(0, image_h - crop_pair[1])\n            else:\n                w_step = (image_w - crop_pair[0]) / 4\n                h_step = (image_h - crop_pair[1]) / 4\n                ret = list()\n                ret.append((0, 0))  # upper left\n                if self.allow_duplication or w_step != 0:\n                    ret.append((4 * w_step, 0))  # upper right\n                if self.allow_duplication or h_step != 0:\n                    ret.append((0, 4 * h_step))  # lower left\n                if self.allow_duplication or (h_step != 0 and w_step != 0):\n                    ret.append((4 * w_step, 4 * h_step))  # lower right"
        },
        {
            "comment": "This code samples random crop sizes and offsets for image augmentation. It appends different cropping positions based on user allowance or specific flag settings, then randomly selects one of these positions. Finally, it crops the image using the selected position and size.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":384-404",
            "content": "                if self.allow_duplication or (h_step != 0 or w_step != 0):\n                    ret.append((2 * w_step, 2 * h_step))  # center\n                if self.more_fix_crop:\n                    ret.append((0, 2 * h_step))  # center left\n                    ret.append((4 * w_step, 2 * h_step))  # center right\n                    ret.append((2 * w_step, 4 * h_step))  # lower center\n                    ret.append((2 * w_step, 0 * h_step))  # upper center\n                    ret.append((1 * w_step, 1 * h_step))  # upper left quarter\n                    ret.append((3 * w_step, 1 * h_step))  # upper right quarter\n                    ret.append((1 * w_step, 3 * h_step))  # lower left quarter\n                    ret.append((3 * w_step, 3 * h_step))  # lower righ quarter\n                w_offset, h_offset = random.choice(ret)\n            return crop_pair[0], crop_pair[1], w_offset, h_offset\n        crop_w, crop_h, offset_w, offset_h = _sample_crop_size(im_size)\n        crop_img_group = [\n            img.crop((offset_w, offset_h, offset_w + crop_w, offset_h + crop_h))"
        },
        {
            "comment": "This code is a PaddleVideo pipeline for image augmentation, specifically performing random flips with a given probability. It resizes and crops images according to the provided input size. If the backend is set to 'pillow', it uses PIL library's resize function; otherwise, it uses OpenCV's resize function. The results are stored in the 'imgs' key of the 'results' dictionary, which is then returned.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":405-439",
            "content": "            for img in imgs\n        ]\n        if self.backend == 'pillow':\n            ret_img_group = [\n                img.resize((input_size[0], input_size[1]), Image.BILINEAR)\n                for img in crop_img_group\n            ]\n        else:\n            ret_img_group = [\n                Image.fromarray(\n                    cv2.resize(np.asarray(img),\n                               dsize=(input_size[0], input_size[1]),\n                               interpolation=cv2.INTER_LINEAR))\n                for img in crop_img_group\n            ]\n        results['imgs'] = ret_img_group\n        return results\n@PIPELINES.register()\nclass RandomFlip(object):\n    \"\"\"\n    Random Flip images.\n    Args:\n        p(float): Random flip images with the probability p.\n    \"\"\"\n    def __init__(self, p=0.5):\n        self.p = p\n    def __call__(self, results):\n        \"\"\"\n        Performs random flip operations.\n        Args:\n            imgs: List where each item is a PIL.Image.\n            For example, [PIL.Image0, PIL.Image1, PIL.Image2, ...]"
        },
        {
            "comment": "This code implements a random image flipping and brightness adjustment in PaddleVideo's pipeline. It takes an image as input, randomly decides whether to flip or keep it intact with probability 'p', and adjusts the brightness if applied. The result is then returned.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":440-472",
            "content": "        return:\n            flip_imgs: List where each item is a PIL.Image after random flip.\n        \"\"\"\n        imgs = results['imgs']\n        v = random.random()\n        if v < self.p:\n            if isinstance(imgs, paddle.Tensor):\n                results['imgs'] = paddle.flip(imgs, axis=[3])\n            elif isinstance(imgs[0], np.ndarray):\n                results['imgs'] = [cv2.flip(img, 1, img) for img in imgs\n                                   ]  # [[h,w,c], [h,w,c], ..., [h,w,c]]\n            else:\n                results['imgs'] = [\n                    img.transpose(Image.FLIP_LEFT_RIGHT) for img in imgs\n                ]\n        else:\n            results['imgs'] = imgs\n        return results\n@PIPELINES.register()\nclass RandomBrightness(object):\n    \"\"\"\n    Random Brightness images.\n    Args:\n        p(float): Random brightness images with the probability p.\n    \"\"\"\n    def __init__(self, p=0.1, brightness=1):\n        self.p = p\n        self.brightness = brightness\n    def __call__(self, results):\n        \"\"\""
        },
        {
            "comment": "The code defines two classes, RandomBrightness and RandomSaturation, which perform random operations on image brightness and saturation respectively. The RandomBrightness class applies ColorJitter with a specified brightness level to each image in the list with a certain probability, while the RandomSaturation class adjusts the saturation of images with another probability. Both classes are registered as Pipelines for data augmentation in PaddleVideo.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":473-507",
            "content": "        Performs random brightness operations.\n        Args:\n            imgs: List where each item is a PIL.Image.\n            For example, [PIL.Image0, PIL.Image1, PIL.Image2, ...]\n        return:\n            brightness_imgs: List where each item is a PIL.Image after random brightness.\n        \"\"\"\n        imgs = results['imgs']\n        v = random.random()\n        if v < self.p:\n            transform = ColorJitter(brightness=self.brightness)\n            results['imgs'] = [transform(img) for img in imgs]\n        else:\n            results['imgs'] = imgs\n        return results\n@PIPELINES.register()\nclass RandomSaturation(object):\n    \"\"\"\n    Random Saturation images.\n    Args:\n        p(float): Random saturation images with the probability p.\n    \"\"\"\n    def __init__(self, p=0.1, saturation=2):\n        self.p = p\n        self.saturation = saturation\n    def __call__(self, results):\n        \"\"\"\n        Performs random saturation operations.\n        Args:\n            imgs: List where each item is a PIL.Image.\n            For example, [PIL.Image0, PIL.Image1, PIL.Image2, ...]"
        },
        {
            "comment": "This code snippet contains two classes: RandomSaturation and RandomHue. Both classes are pipeline transforms for image processing in the PaddleVideo framework. The RandomSaturation class applies random saturation adjustments to images with a certain probability, while the RandomHue class randomly alters hue values of images with another probability. These transforms can be used to augment and enhance the dataset for better model training.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":508-545",
            "content": "        return:\n            saturation_imgs: List where each item is a PIL.Image after random saturation.\n        \"\"\"\n        imgs = results['imgs']\n        v = random.random()\n        if v < self.p:\n            transform = ColorJitter(saturation=self.saturation)\n            results['imgs'] = [transform(img) for img in imgs]\n        else:\n            results['imgs'] = imgs\n        return results\n@PIPELINES.register()\nclass RandomHue(object):\n    \"\"\"\n    Random Hue images.\n    Args:\n        p(float): Random hue images with the probability p.\n    \"\"\"\n    def __init__(self, p=0.1, hue=0.5):\n        self.p = p\n        self.hue = hue\n    def __call__(self, results):\n        \"\"\"\n        Performs random hue operations.\n        Args:\n            imgs: List where each item is a PIL.Image.\n            For example, [PIL.Image0, PIL.Image1, PIL.Image2, ...]\n        return:\n            hue_imgs: List where each item is a PIL.Image after random hue.\n        \"\"\"\n        imgs = results['imgs']\n        v = random.random()\n        if v < self.p:"
        },
        {
            "comment": "The code defines a pipeline module for data augmentation in PaddleVideo. It includes a RandomGamma class that randomly applies gamma correction to images with a specified probability and gamma value range. The ColorJitter transform is used to apply random changes to the hue of images. The results are stored in a dictionary under the 'imgs' key, either after applying transformations or as is if no transformation is needed. The code also handles adjusting gamma for both numpy arrays and PIL Image objects.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":546-576",
            "content": "            transform = ColorJitter(hue=self.hue)\n            results['imgs'] = [transform(img) for img in imgs]\n        else:\n            results['imgs'] = imgs\n        return results\n@PIPELINES.register()\nclass RandomGamma(object):\n    \"\"\"\n    Random Gamma images.\n    Args:\n        p(float): Random gamma images with the probability p.\n        gamma (float): Non negative real number, same as `\\\\gamma` in the equation.\n                       gamma larger than 1 make the shadows darker,\n                      while gamma smaller than 1 make dark regions lighter.\n    \"\"\"\n    def __init__(self, p=0.1, gamma=0.2):\n        self.p = p\n        self.value = [1 - gamma, 1 + gamma]\n        self.value[0] = max(self.value[0], 0)\n    def _adust_gamma(self, img, gamma, gain=1.0):\n        flag = False\n        if isinstance(img, np.ndarray):\n            flag = True\n            img = Image.fromarray(img)\n        input_mode = img.mode\n        img = img.convert(\"RGB\")\n        gamma_map = [\n            int((255 + 1 - 1e-3) * gain * pow(ele / 255.0, gamma))"
        },
        {
            "comment": "This code is defining a pipeline for image augmentation, specifically adjusting gamma values randomly. It checks if a random number falls below the threshold and applies a random gamma adjustment to each image in the input list. If not, it leaves the images unchanged. Finally, it registers an Image2Array class that converts PIL.Image to Numpy array with transposed dimensions from 'dhwc' to 'dchw'.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":577-610",
            "content": "            for ele in range(256)\n        ] * 3\n        img = img.point(\n            gamma_map)  # use PIL's point-function to accelerate this part\n        img = img.convert(input_mode)\n        if flag:\n            img = np.array(img)\n        return img\n    def __call__(self, results):\n        \"\"\"\n        Performs random gamma operations.\n        Args:\n            imgs: List where each item is a PIL.Image.\n            For example, [PIL.Image0, PIL.Image1, PIL.Image2, ...]\n        return:\n            gamma_imgs: List where each item is a PIL.Image after random gamma.\n        \"\"\"\n        imgs = results['imgs']\n        v = random.random()\n        if v < self.p:\n            gamma = random.uniform(self.value[0], self.value[1])\n            results['imgs'] = [self._adust_gamma(img, gamma) for img in imgs]\n        else:\n            results['imgs'] = imgs\n        return results\n@PIPELINES.register()\nclass Image2Array(object):\n    \"\"\"\n    transfer PIL.Image to Numpy array and transpose dimensions from 'dhwc' to 'dchw'.\n    Args:"
        },
        {
            "comment": "This code is part of a class that performs Image to NumpyArray operations. It initializes with the option to transpose or not, and specifies the data format as either 'tchw' or 'cthw'. The class checks if the backend is 'pyav', then transposes the images accordingly. If 'transpose' is True and 'data_format' is 'tchw', it transposes the images from (0, 3, 1, 2) to (0, 3, 1, 2), resulting in 'tchw'. Otherwise, if 'transpose' is True and 'data_format' is 'cthw', it transposes the images from (3, 0, 1, 2) to (3, 0, 1, 2), resulting in 'cthw'. The transposed images are then stored back into 'imgs'.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":611-637",
            "content": "        transpose: whether to transpose or not, default True, False for slowfast.\n    \"\"\"\n    def __init__(self, transpose=True, data_format='tchw'):\n        assert data_format in [\n            'tchw', 'cthw'\n        ], f\"Target format must in ['tchw', 'cthw'], but got {data_format}\"\n        self.transpose = transpose\n        self.data_format = data_format\n    def __call__(self, results):\n        \"\"\"\n        Performs Image to NumpyArray operations.\n        Args:\n            imgs: List where each item is a PIL.Image.\n            For example, [PIL.Image0, PIL.Image1, PIL.Image2, ...]\n        return:\n            np_imgs: Numpy array.\n        \"\"\"\n        imgs = results['imgs']\n        if 'backend' in results and results[\n                'backend'] == 'pyav':  # [T,H,W,C] in [0, 1]\n            if self.transpose:\n                if self.data_format == 'tchw':\n                    t_imgs = imgs.transpose((0, 3, 1, 2))  # tchw\n                else:\n                    t_imgs = imgs.transpose((3, 0, 1, 2))  # cthw\n            results['imgs'] = t_imgs"
        },
        {
            "comment": "This code is defining a class for normalization in PaddleVideo's loader pipelines. It takes mean and std values as arguments to normalize the image data, and allows for transpose operation depending on data_format. The tensor_shape parameter is optional with default value [3,1,1] for standard usage or [1,1,1,3] for slowfast support. Inplace flag can be set to True to perform in-place operations if desired.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":638-664",
            "content": "        else:\n            t_imgs = np.stack(imgs).astype('float32')\n            if self.transpose:\n                if self.data_format == 'tchw':\n                    t_imgs = t_imgs.transpose(0, 3, 1, 2)  # tchw\n                else:\n                    t_imgs = t_imgs.transpose(3, 0, 1, 2)  # cthw\n            results['imgs'] = t_imgs\n        return results\n@PIPELINES.register()\nclass Normalization(object):\n    \"\"\"\n    Normalization.\n    Args:\n        mean(Sequence[float]): mean values of different channels.\n        std(Sequence[float]): std values of different channels.\n        tensor_shape(list): size of mean, default [3,1,1]. For slowfast, [1,1,1,3]\n    \"\"\"\n    def __init__(self, mean, std, tensor_shape=[3, 1, 1], inplace=False):\n        if not isinstance(mean, Sequence):\n            raise TypeError(\n                f'Mean must be list, tuple or np.ndarray, but got {type(mean)}')\n        if not isinstance(std, Sequence):\n            raise TypeError(\n                f'Std must be list, tuple or np.ndarray, but got {type(std)}')"
        },
        {
            "comment": "This code defines a class for normalizing images. It takes mean and std values as inputs, which are used for image normalization. If inplace is set to False, it converts the input into numpy arrays with appropriate shapes and data types. The __call__ method performs normalization on the given results. If inplace is True, it uses the existing array and avoids making copies. The method calculates mean and std values for normalization and applies them to each image in the results using cv2.subtract.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":666-692",
            "content": "        self.inplace = inplace\n        if not inplace:\n            self.mean = np.array(mean).reshape(tensor_shape).astype(np.float32)\n            self.std = np.array(std).reshape(tensor_shape).astype(np.float32)\n        else:\n            self.mean = np.array(mean, dtype=np.float32)\n            self.std = np.array(std, dtype=np.float32)\n    def __call__(self, results):\n        \"\"\"\n        Performs normalization operations.\n        Args:\n            imgs: Numpy array.\n        return:\n            np_imgs: Numpy array after normalization.\n        \"\"\"\n        if self.inplace:\n            n = len(results['imgs'])\n            h, w, c = results['imgs'][0].shape\n            norm_imgs = np.empty((n, h, w, c), dtype=np.float32)\n            for i, img in enumerate(results['imgs']):\n                norm_imgs[i] = img\n            for img in norm_imgs:  # [n,h,w,c]\n                mean = np.float64(self.mean.reshape(1, -1))  # [1, 3]\n                stdinv = 1 / np.float64(self.std.reshape(1, -1))  # [1, 3]\n                cv2.subtract(img, mean, img)"
        },
        {
            "comment": "This code applies image normalization and potentially scales the images while preserving aspect ratio, with options for random scaling. This is part of a PaddleVideo pipeline, likely for preprocessing input data before feeding it to a model for training or inference. It can be used with different backends such as \"cv2\" or \"pyav\", and returns the processed image results.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":693-721",
            "content": "                cv2.multiply(img, stdinv, img)\n        else:\n            imgs = results['imgs']\n            norm_imgs = imgs / 255.0\n            norm_imgs -= self.mean\n            norm_imgs /= self.std\n            if 'backend' in results and results['backend'] == 'pyav':\n                norm_imgs = paddle.to_tensor(norm_imgs, dtype=paddle.float32)\n        results['imgs'] = norm_imgs\n        return results\n@PIPELINES.register()\nclass JitterScale(object):\n    \"\"\"\n    Scale image, while the target short size is randomly select between min_size and max_size.\n    Args:\n        min_size: Lower bound for random sampler.\n        max_size: Higher bound for random sampler.\n    \"\"\"\n    def __init__(self,\n                 min_size,\n                 max_size,\n                 short_cycle_factors=[0.5, 0.7071],\n                 default_min_size=256):\n        self.default_min_size = default_min_size\n        self.orig_min_size = self.min_size = min_size\n        self.max_size = max_size\n        self.short_cycle_factors = short_cycle_factors"
        },
        {
            "comment": "This code defines a function that performs jitter resize operations. It takes in an image sequence and scales each image based on a random size between min_size and max_size, considering short cycle factors and asserting the minimum length of images. If the backend is pyav, it retrieves height and width separately; otherwise, it gets the size from the first image.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":723-748",
            "content": "    def __call__(self, results):\n        \"\"\"\n        Performs jitter resize operations.\n        Args:\n            imgs (Sequence[PIL.Image]): List where each item is a PIL.Image.\n            For example, [PIL.Image0, PIL.Image1, PIL.Image2, ...]\n        return:\n            resized_imgs: List where each item is a PIL.Image after scaling.\n        \"\"\"\n        short_cycle_idx = results.get('short_cycle_idx')\n        if short_cycle_idx in [0, 1]:\n            self.min_size = int(\n                round(self.short_cycle_factors[short_cycle_idx] *\n                      self.default_min_size))\n        else:\n            self.min_size = self.orig_min_size\n        imgs = results['imgs']\n        size = int(round(np.random.uniform(self.min_size, self.max_size)))\n        assert (len(imgs) >= 1), \\\n            \"len(imgs):{} should be larger than 1\".format(len(imgs))\n        if 'backend' in results and results['backend'] == 'pyav':\n            height, width = imgs.shape[2:]\n        else:\n            width, height = imgs[0].size"
        },
        {
            "comment": "This code resizes images to a specified size (width or height equals size). It checks if the image is loaded by PyAV and performs the resize operation using F.interpolate for PyAV-loaded images, otherwise it uses PIL's Image.resize function for other images. The resized images are added to 'imgs' in the results dictionary and returned.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":749-773",
            "content": "        if (width <= height and width == size) or (height <= width\n                                                   and height == size):\n            return results\n        new_width = size\n        new_height = size\n        if width < height:\n            new_height = int(math.floor((float(height) / width) * size))\n        else:\n            new_width = int(math.floor((float(width) / height) * size))\n        if 'backend' in results and results['backend'] == 'pyav':\n            frames_resize = F.interpolate(imgs,\n                                          size=(new_height, new_width),\n                                          mode=\"bilinear\",\n                                          align_corners=False)  # [c,t,h,w]\n        else:\n            frames_resize = []\n            for j in range(len(imgs)):\n                img = imgs[j]\n                scale_img = img.resize((new_width, new_height), Image.BILINEAR)\n                frames_resize.append(scale_img)\n        results['imgs'] = frames_resize\n        return results"
        },
        {
            "comment": "This code defines a MultiCenterCrop class that performs center crop, left center crop, and right center crop operations on images. It takes a target size as input and returns the cropped images. The function checks if the image size is larger than the target size before performing the operation. If the image size is smaller, it throws an assertion error.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":776-804",
            "content": "@PIPELINES.register()\nclass MultiCenterCrop(object):\n    \"\"\"\n    center crop, left center crop right center crop\n    Args:\n        target_size(int): Random crop a square with the target_size from an image.\n    \"\"\"\n    def __init__(self, target_size):\n        self.target_size = target_size\n    def __call__(self, results):\n        \"\"\"\n        Performs random crop operations.\n        Args:\n            imgs: List where each item is a PIL.Image.\n            For example, [PIL.Image0, PIL.Image1, PIL.Image2, ...]\n        return:\n            crop_imgs: List where each item is a PIL.Image after random crop.\n        \"\"\"\n        imgs = results['imgs']\n        if 'backend' in results and results['backend'] == 'pyav':  # [c,t,h,w]\n            h, w = imgs.shape[2:]\n        else:\n            w, h = imgs[0].size\n        th, tw = self.target_size, self.target_size\n        assert (w >= self.target_size) and (h >= self.target_size), \\\n            \"image width({}) and height({}) should be larger than crop size\".format(\n                w, h, self.target_size)"
        },
        {
            "comment": "This code is performing image cropping for a specific backend (pyav) and storing the results in three separate lists: crop_imgs_center, crop_imgs_left, and crop_imgs_right. The cropping is done based on the size of the original image compared to the target size, with different crops for center, left, and right areas.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":806-833",
            "content": "        crop_images = []\n        #just for tensor\n        crop_imgs_center = []\n        crop_imgs_left = []\n        crop_imgs_right = []\n        if 'backend' in results and results['backend'] == 'pyav':\n            #center_corp\n            x1 = 0\n            if w > self.target_size:\n                x1 = int((w - self.target_size) / 2.0)\n            y1 = 0\n            if h > self.target_size:\n                y1 = int((h - self.target_size) / 2.0)\n            crop_imgs_center = imgs[:, :, y1:y1 + th,\n                                    x1:x1 + tw].numpy()  # [C, T, th, tw]\n            #left_crop\n            x1 = 0\n            y1 = 0\n            if h > self.target_size:\n                y1 = int((h - self.target_size) / 2.0)\n            crop_imgs_left = imgs[:, :, y1:y1 + th, x1:x1 + tw].numpy()\n            #right_crop\n            x1 = 0\n            y1 = 0\n            if w > self.target_size:\n                x1 = w - self.target_size\n            if h > self.target_size:\n                y1 = int((h - self.target_size) / 2.0)"
        },
        {
            "comment": "This code defines a MultiCrop pipeline that randomly crops an image into three parts: center, left, and right. The cropped images are concatenated horizontally and converted to Paddle Tensor before returning the results.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":834-864",
            "content": "            crop_imgs_right = imgs[:, :, y1:y1 + th, x1:x1 + tw].numpy()\n            crop_imgs = np.concatenate(\n                (crop_imgs_center, crop_imgs_left, crop_imgs_right), axis=1)\n            crop_images = paddle.to_tensor(crop_imgs)\n        else:\n            x1 = 0\n            if w > self.target_size:\n                x1 = random.randint(0, w - tw)\n            y1 = 0\n            if h > self.target_size:\n                y1 = random.randint(0, h - th)\n            for img in imgs:\n                if w == tw and h == th:\n                    crop_images.append(img)\n                else:\n                    crop_images.append(img.crop((x1, y1, x1 + tw, y1 + th)))\n        results['imgs'] = crop_images\n        return results\n@PIPELINES.register()\nclass MultiCrop(object):\n    \"\"\"\n    Random crop image.\n    This operation can perform multi-crop during multi-clip test, as in slowfast model.\n    Args:\n        target_size(int): Random crop a square with the target_size from an image.\n    \"\"\"\n    def __init__(self,\n                 target_size,"
        },
        {
            "comment": "The code initializes an augmentation class with parameters for target size, short cycle factors, default crop size, and test mode. It then defines a __call__ method that performs random cropping operations on images based on the provided parameters.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":865-890",
            "content": "                 default_crop_size=224,\n                 short_cycle_factors=[0.5, 0.7071],\n                 test_mode=False):\n        self.orig_target_size = self.target_size = target_size\n        self.short_cycle_factors = short_cycle_factors\n        self.default_crop_size = default_crop_size\n        self.test_mode = test_mode\n    def __call__(self, results):\n        \"\"\"\n        Performs random crop operations.\n        Args:\n            imgs: List where each item is a PIL.Image.\n            For example, [PIL.Image0, PIL.Image1, PIL.Image2, ...]\n        return:\n            crop_imgs: List where each item is a PIL.Image after random crop.\n        \"\"\"\n        imgs = results['imgs']\n        spatial_sample_index = results['spatial_sample_index']\n        spatial_num_clips = results['spatial_num_clips']\n        short_cycle_idx = results.get('short_cycle_idx')\n        if short_cycle_idx in [0, 1]:\n            self.target_size = int(\n                round(self.short_cycle_factors[short_cycle_idx] *\n                      self.default_crop_size))"
        },
        {
            "comment": "This code checks if the image size matches the target size. If it does, it returns the results. If not, it generates crops for multi-crop testing mode or a single crop for non-testing mode based on random offsets. The code also handles the case where the target size is determined from a saved value before the call.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":891-913",
            "content": "        else:\n            self.target_size = self.orig_target_size  # use saved value before call\n        w, h = imgs[0].size\n        if w == self.target_size and h == self.target_size:\n            return results\n        assert (w >= self.target_size) and (h >= self.target_size), \\\n            \"image width({}) and height({}) should be larger than crop size({},{})\".format(w, h, self.target_size, self.target_size)\n        frames_crop = []\n        if not self.test_mode:\n            x_offset = random.randint(0, w - self.target_size)\n            y_offset = random.randint(0, h - self.target_size)\n        else:  # multi-crop\n            x_gap = int(\n                math.ceil((w - self.target_size) / (spatial_num_clips - 1)))\n            y_gap = int(\n                math.ceil((h - self.target_size) / (spatial_num_clips - 1)))\n            if h > w:\n                x_offset = int(math.ceil((w - self.target_size) / 2))\n                if spatial_sample_index == 0:\n                    y_offset = 0\n                elif spatial_sample_index == spatial_num_clips - 1:"
        },
        {
            "comment": "This code calculates the crop offsets for a set of images based on their size and target size. If the aspect ratio is preserved, it determines the y_offset, otherwise, it calculates the x and y offsets separately for each spatial sample index. The resulting cropped images are stored in frames\\_crop and added to results['imgs']. PackOutput is a pipeline register that takes an alpha argument and is used in slowfast model to get slow pathway from fast pathway based on alpha factor.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":914-943",
            "content": "                    y_offset = h - self.target_size\n                else:\n                    y_offset = y_gap * spatial_sample_index\n            else:\n                y_offset = int(math.ceil((h - self.target_size) / 2))\n                if spatial_sample_index == 0:\n                    x_offset = 0\n                elif spatial_sample_index == spatial_num_clips - 1:\n                    x_offset = w - self.target_size\n                else:\n                    x_offset = x_gap * spatial_sample_index\n        for img in imgs:\n            nimg = img.crop((x_offset, y_offset, x_offset + self.target_size,\n                             y_offset + self.target_size))\n            frames_crop.append(nimg)\n        results['imgs'] = frames_crop\n        return results\n@PIPELINES.register()\nclass PackOutput(object):\n    \"\"\"\n    In slowfast model, we want to get slow pathway from fast pathway based on\n    alpha factor.\n    Args:\n        alpha(int): temporal length of fast/slow\n    \"\"\"\n    def __init__(self, alpha):\n        self.alpha = alpha"
        },
        {
            "comment": "The code defines a GroupFullResSample pipeline that selects and groups slow and fast pathways from input images. It resizes the pathways to the specified crop_size, performs horizontal flips if flip is True, and stores them in frames_list before updating results['imgs'].",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":945-974",
            "content": "    def __call__(self, results):\n        fast_pathway = results['imgs']\n        # sample num points between start and end\n        slow_idx_start = 0\n        slow_idx_end = fast_pathway.shape[0] - 1\n        slow_idx_num = fast_pathway.shape[0] // self.alpha\n        slow_idxs_select = np.linspace(slow_idx_start, slow_idx_end,\n                                       slow_idx_num).astype(\"int64\")\n        slow_pathway = fast_pathway[slow_idxs_select]\n        # T H W C -> C T H W.\n        slow_pathway = slow_pathway.transpose(3, 0, 1, 2)\n        fast_pathway = fast_pathway.transpose(3, 0, 1, 2)\n        # slow + fast\n        frames_list = [slow_pathway, fast_pathway]\n        results['imgs'] = frames_list\n        return results\n@PIPELINES.register()\nclass GroupFullResSample(object):\n    def __init__(self, crop_size, flip=False):\n        self.crop_size = crop_size if not isinstance(crop_size, int) else (\n            crop_size, crop_size)\n        self.flip = flip\n    def __call__(self, results):\n        img_group = results['imgs']"
        },
        {
            "comment": "This code performs image augmentation by creating a list of different crops and flips from the input image group. It calculates the crop size and step sizes, creates offsets for each crop position, iterates over the input images to create normal and flipped crops, and stores them in separate groups before combining them into the oversample_group. Finally, it adds the oversample_group to the results dictionary and returns the results.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":976-1006",
            "content": "        image_w, image_h = img_group[0].size\n        crop_w, crop_h = self.crop_size\n        w_step = (image_w - crop_w) // 4\n        h_step = (image_h - crop_h) // 4\n        offsets = list()\n        offsets.append((0 * w_step, 2 * h_step))  # left\n        offsets.append((4 * w_step, 2 * h_step))  # right\n        offsets.append((2 * w_step, 2 * h_step))  # center\n        oversample_group = list()\n        for o_w, o_h in offsets:\n            normal_group = list()\n            flip_group = list()\n            for i, img in enumerate(img_group):\n                crop = img.crop((o_w, o_h, o_w + crop_w, o_h + crop_h))\n                normal_group.append(crop)\n                if self.flip:\n                    flip_crop = crop.copy().transpose(Image.FLIP_LEFT_RIGHT)\n                    flip_group.append(flip_crop)\n            oversample_group.extend(normal_group)\n            if self.flip:\n                oversample_group.extend(flip_group)\n        results['imgs'] = oversample_group\n        return results\n@PIPELINES.register()"
        },
        {
            "comment": "This code defines a class \"TenCrop\" which crops a given image into 10 cropped images, taking the top-left corner, bottom-left corner, top-right corner, bottom-right corner, and center of the image. It achieves this by using the target size for crop and calculating the width and height steps based on the original image's dimensions. The class also includes a __call__ method which takes a results dictionary as input and returns an array of cropped images.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":1007-1036",
            "content": "class TenCrop:\n    \"\"\"\n    Crop out 5 regions (4 corner points + 1 center point) from the picture,\n    and then flip the cropping result to get 10 cropped images, which can make the prediction result more robust.\n    Args:\n        target_size(int | tuple[int]): (w, h) of target size for crop.\n    \"\"\"\n    def __init__(self, target_size):\n        self.target_size = (target_size, target_size)\n    def __call__(self, results):\n        imgs = results['imgs']\n        img_w, img_h = imgs[0].size\n        crop_w, crop_h = self.target_size\n        w_step = (img_w - crop_w) // 4\n        h_step = (img_h - crop_h) // 4\n        offsets = [\n            (0, 0),\n            (4 * w_step, 0),\n            (0, 4 * h_step),\n            (4 * w_step, 4 * h_step),\n            (2 * w_step, 2 * h_step),\n        ]\n        img_crops = list()\n        for x_offset, y_offset in offsets:\n            crop = [\n                img.crop(\n                    (x_offset, y_offset, x_offset + crop_w, y_offset + crop_h))\n                for img in imgs\n            ]"
        },
        {
            "comment": "This code is for the \"UniformCrop\" pipeline, which performs uniform spatial sampling on images by selecting three regions: two ends of the long side and the middle position (left/right or top/bottom). The target size can be provided as an integer for square crop or a tuple for specific width and height. It uses either OpenCV or PIL for image manipulation based on the 'backend' argument, which defaults to OpenCV.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":1037-1068",
            "content": "            crop_fliped = [\n                timg.transpose(Image.FLIP_LEFT_RIGHT) for timg in crop\n            ]\n            img_crops.extend(crop)\n            img_crops.extend(crop_fliped)\n        results['imgs'] = img_crops\n        return results\n@PIPELINES.register()\nclass UniformCrop:\n    \"\"\"\n    Perform uniform spatial sampling on the images,\n    select the two ends of the long side and the middle position (left middle right or top middle bottom) 3 regions.\n    Args:\n        target_size(int | tuple[int]): (w, h) of target size for crop.\n    \"\"\"\n    def __init__(self, target_size, backend='cv2'):\n        if isinstance(target_size, tuple):\n            self.target_size = target_size\n        elif isinstance(target_size, int):\n            self.target_size = (target_size, target_size)\n        else:\n            raise TypeError(\n                f'target_size must be int or tuple[int], but got {type(target_size)}'\n            )\n        self.backend = backend\n    def __call__(self, results):\n        imgs = results['imgs']"
        },
        {
            "comment": "This code is determining the image offsets for cropping based on the target size and the original image dimensions. It supports two backends: 'pyav' and 'pillow'. If the backend is 'pyav', it extracts the height and width of the image. If the backend is 'pillow', it retrieves the width and height from the first image. The code then calculates the step size for cropping based on whether the target size matches the image dimensions or not, and finally constructs a list of offsets to crop the images.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":1069-1098",
            "content": "        if 'backend' in results and results['backend'] == 'pyav':  # [c,t,h,w]\n            img_h, img_w = imgs.shape[2:]\n        elif self.backend == 'pillow':\n            img_w, img_h = imgs[0].size\n        else:\n            img_h, img_w = imgs[0].shape[:2]\n        crop_w, crop_h = self.target_size\n        if crop_h == img_h:\n            w_step = (img_w - crop_w) // 2\n            offsets = [\n                (0, 0),\n                (w_step * 2, 0),\n                (w_step, 0),\n            ]\n        elif crop_w == img_w:\n            h_step = (img_h - crop_h) // 2\n            offsets = [\n                (0, 0),\n                (0, h_step * 2),\n                (0, h_step),\n            ]\n        else:\n            raise ValueError(\n                f\"img_w({img_w}) == crop_w({crop_w}) or img_h({img_h}) == crop_h({crop_h})\"\n            )\n        img_crops = []\n        if 'backend' in results and results['backend'] == 'pyav':  # [c,t,h,w]\n            for x_offset, y_offset in offsets:\n                crop = imgs[:, :, y_offset:y_offset + crop_h,"
        },
        {
            "comment": "The code is defining a pipeline for image augmentation, including cropping and resizing operations. If the backend is 'pillow', it performs cropping using pixel coordinates; otherwise, it uses slice notation to crop images. The results are stored in 'img_crops' and returned as 'results['imgs']'.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":1099-1126",
            "content": "                            x_offset:x_offset + crop_w]\n                img_crops.append(crop)\n            img_crops = paddle.concat(img_crops, axis=1)\n        else:\n            if self.backend == 'pillow':\n                for x_offset, y_offset in offsets:\n                    crop = [\n                        img.crop((x_offset, y_offset, x_offset + crop_w,\n                                  y_offset + crop_h)) for img in imgs\n                    ]\n                    img_crops.extend(crop)\n            else:\n                for x_offset, y_offset in offsets:\n                    crop = [\n                        img[y_offset:y_offset + crop_h,\n                            x_offset:x_offset + crop_w] for img in imgs\n                    ]\n                    img_crops.extend(crop)\n        results['imgs'] = img_crops\n        return results\n@PIPELINES.register()\nclass GroupResize(object):\n    def __init__(self, height, width, scale, K, mode='train'):\n        self.height = height\n        self.width = width\n        self.scale = scale"
        },
        {
            "comment": "This code initializes a resize transformation for image augmentation in PaddleVideo. The transformations are applied based on the scale and mode ('infer' or 'train') specified. For infer mode, it processes color images by applying resizing to each scale level. In train mode, it calculates the K matrix and its inverse for each scale level and stores them in the results dictionary.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":1127-1155",
            "content": "        self.resize = {}\n        self.K = np.array(K, dtype=np.float32)\n        self.mode = mode\n        for i in range(self.scale):\n            s = 2**i\n            self.resize[i] = paddle.vision.transforms.Resize(\n                (self.height // s, self.width // s), interpolation='lanczos')\n    def __call__(self, results):\n        if self.mode == 'infer':\n            imgs = results['imgs']\n            for k in list(imgs):  # (\"color\", 0, -1)\n                if \"color\" in k or \"color_n\" in k:\n                    n, im, _ = k\n                    for i in range(self.scale):\n                        imgs[(n, im, i)] = self.resize[i](imgs[(n, im, i - 1)])\n        else:\n            imgs = results['imgs']\n            for scale in range(self.scale):\n                K = self.K.copy()\n                K[0, :] *= self.width // (2**scale)\n                K[1, :] *= self.height // (2**scale)\n                inv_K = np.linalg.pinv(K)\n                imgs[(\"K\", scale)] = K\n                imgs[(\"inv_K\", scale)] = inv_K\n            for k in list(imgs):"
        },
        {
            "comment": "This code applies color jitter augmentation to the images by randomly adjusting brightness, contrast, saturation, and hue. The ColorJitter class initializes a colorjitter transform with specified parameters for train mode or test mode. The __call__ method is called on each image in the results dictionary and checks if the color augmentation should be applied. If true, the color jittered image is returned.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":1156-1192",
            "content": "                if \"color\" in k or \"color_n\" in k:\n                    n, im, i = k\n                    for i in range(self.scale):\n                        imgs[(n, im, i)] = self.resize[i](imgs[(n, im, i - 1)])\n            results['imgs'] = imgs\n        return results\n@PIPELINES.register()\nclass ColorJitter(object):\n    \"\"\"Randomly change the brightness, contrast, saturation and hue of an image.\n    \"\"\"\n    def __init__(self,\n                 brightness=0,\n                 contrast=0,\n                 saturation=0,\n                 hue=0,\n                 mode='train',\n                 p=0.5,\n                 keys=None):\n        self.mode = mode\n        self.colorjitter = paddle.vision.transforms.ColorJitter(\n            brightness, contrast, saturation, hue)\n        self.p = p\n    def __call__(self, results):\n        \"\"\"\n        Args:\n            results (PIL Image): Input image.\n        Returns:\n            PIL Image: Color jittered image.\n        \"\"\"\n        do_color_aug = random.random() > self.p\n        imgs = results['imgs']"
        },
        {
            "comment": "This code is part of a data augmentation pipeline, specifically handling color and flip transformations for images. It iterates over the 'imgs' dictionary to find and organize color images, applying color jitter if required. Then it removes specific color images based on the mode (\"train\" or \"test\"). Finally, it returns the updated results dictionary with the modified image groupings. The GroupRandomFlip class performs random flipping of images with a specified probability.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":1193-1226",
            "content": "        for k in list(imgs):\n            f = imgs[k]\n            if \"color\" in k or \"color_n\" in k:\n                n, im, i = k\n                imgs[(n, im, i)] = f\n                if do_color_aug:\n                    imgs[(n + \"_aug\", im, i)] = self.colorjitter(f)\n                else:\n                    imgs[(n + \"_aug\", im, i)] = f\n        if self.mode == \"train\":\n            for i in results['frame_idxs']:\n                del imgs[(\"color\", i, -1)]\n                del imgs[(\"color_aug\", i, -1)]\n                del imgs[(\"color_n\", i, -1)]\n                del imgs[(\"color_n_aug\", i, -1)]\n        else:\n            for i in results['frame_idxs']:\n                del imgs[(\"color\", i, -1)]\n                del imgs[(\"color_aug\", i, -1)]\n        results['img'] = imgs\n        return results\n@PIPELINES.register()\nclass GroupRandomFlip(object):\n    def __init__(self, p=0.5):\n        self.p = p\n    def __call__(self, results):\n        imgs = results['imgs']\n        do_flip = random.random() > self.p\n        if do_flip:"
        },
        {
            "comment": "This code is part of a machine learning pipeline that processes image data. It first flips left-right some images marked with \"color\" or \"color_n\". Then, it converts certain color and depth images to floats and normalizes them to [0,1] for training. Finally, it returns the updated image dictionary as part of the results.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":1227-1256",
            "content": "            for k in list(imgs):\n                if \"color\" in k or \"color_n\" in k:\n                    n, im, i = k\n                    imgs[(n, im,\n                          i)] = imgs[(n, im,\n                                      i)].transpose(Image.FLIP_LEFT_RIGHT)\n            if \"depth_gt\" in imgs:\n                imgs['depth_gt'] = np.array(np.fliplr(imgs['depth_gt']))\n        results['imgs'] = imgs\n        return results\n@PIPELINES.register()\nclass ToArray(object):\n    def __init__(self):\n        pass\n    def __call__(self, results):\n        imgs = results['imgs']\n        for k in list(imgs):\n            if \"color\" in k or \"color_n\" in k or \"color_aug\" in k or \"color_n_aug\" in k:\n                n, im, i = k\n                imgs[(n, im,\n                      i)] = np.array(imgs[(n, im, i)]).astype('float32') / 255.0\n                imgs[(n, im, i)] = imgs[(n, im, i)].transpose((2, 0, 1))\n        if \"depth_gt\" in imgs:\n            imgs['depth_gt'] = np.array(imgs['depth_gt']).astype('float32')\n        results['imgs'] = imgs"
        },
        {
            "comment": "This code defines a class called YowoAug for image augmentation. It takes in parameters such as target size, jitter, hue, saturation, exposure, and valid mode. The class has methods to randomly scale the image, distort the image by changing hue, saturation, and exposure levels, and returns the augmented results.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":1257-1293",
            "content": "        return results\n@PIPELINES.register()\nclass YowoAug(object):\n    def __init__(self, target_size=224, jitter=0.2, hue=0.1, saturation=1.5, exposure=1.5, valid_mode=False):\n        self.shape = (target_size, target_size)\n        self.jitter = jitter\n        self.hue = hue\n        self.saturation = saturation\n        self.exposure = exposure\n        self.valid_mode = valid_mode\n    def _rand_scale(self, s):\n        scale = random.uniform(1, s)\n        if (random.randint(1, 10000) % 2):\n            return scale\n        return 1. / scale\n    def _distort_image(self, im, hue, sat, val):\n        im = im.convert('HSV')\n        cs = list(im.split())\n        cs[1] = cs[1].point(lambda i: i * sat)\n        cs[2] = cs[2].point(lambda i: i * val)\n        def _change_hue(x):\n            x += hue * 255\n            if x > 255:\n                x -= 255\n            if x < 0:\n                x += 255\n            return x\n        cs[0] = cs[0].point(_change_hue)\n        im = Image.merge(im.mode, tuple(cs))\n        im = im.convert('RGB')"
        },
        {
            "comment": "The code snippet defines several functions related to image augmentation and truth detection in an object detection task. The \"constrain_image\" function ensures the image is within a specific range of values. \"random_distort_image\" applies distortion to the input image randomly. \"read_truths_args\" reads the ground truth boxes from a file, scales and transforms them accordingly, and checks if the box scale is smaller than the minimum required scale before adding it to new_truths. Lastly, \"_fill_truth_detection\" fills in the ground truth detection with additional parameters like flip, dx, dy, sx, and sy.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":1294-1321",
            "content": "        # constrain_image(im)\n        return im\n    def _random_distort_image(self, im, dhue, dsat, dexp):\n        res = self._distort_image(im, dhue, dsat, dexp)\n        return res\n    def _read_truths_args(self, lab_path, min_box_scale):\n        truths = np.loadtxt(lab_path)\n        truths = np.reshape(truths, (truths.size // 5, 5))\n        new_truths = []\n        for i in range(truths.shape[0]):\n            cx = (truths[i][1] + truths[i][3]) / (2 * 320)\n            cy = (truths[i][2] + truths[i][4]) / (2 * 240)\n            imgw = (truths[i][3] - truths[i][1]) / 320\n            imgh = (truths[i][4] - truths[i][2]) / 240\n            truths[i][0] = truths[i][0] - 1\n            truths[i][1] = cx\n            truths[i][2] = cy\n            truths[i][3] = imgw\n            truths[i][4] = imgh\n            if truths[i][3] < min_box_scale:\n                continue\n            new_truths.append([truths[i][0], truths[i][1], truths[i][2], truths[i][3], truths[i][4]])\n        return np.array(new_truths)\n    def _fill_truth_detection(self, labpath, flip, dx, dy, sx, sy):"
        },
        {
            "comment": "The code resizes and normalizes bounding box coordinates from a loaded label file, adjusts them based on image size scaling factors and offsets, and updates the bounding boxes accordingly.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":1322-1352",
            "content": "        max_boxes = 50\n        label = np.zeros((max_boxes, 5))\n        bs = np.loadtxt(labpath)\n        bs = np.reshape(bs, (-1, 5))\n        for i in range(bs.shape[0]):\n            cx = (bs[i][1] + bs[i][3]) / (2 * 320)\n            cy = (bs[i][2] + bs[i][4]) / (2 * 240)\n            imgw = (bs[i][3] - bs[i][1]) / 320\n            imgh = (bs[i][4] - bs[i][2]) / 240\n            bs[i][0] = bs[i][0] - 1\n            bs[i][1] = cx\n            bs[i][2] = cy\n            bs[i][3] = imgw\n            bs[i][4] = imgh\n        cc = 0\n        for i in range(bs.shape[0]):\n            x1 = bs[i][1] - bs[i][3] / 2\n            y1 = bs[i][2] - bs[i][4] / 2\n            x2 = bs[i][1] + bs[i][3] / 2\n            y2 = bs[i][2] + bs[i][4] / 2\n            x1 = min(0.999, max(0, x1 * sx - dx))\n            y1 = min(0.999, max(0, y1 * sy - dy))\n            x2 = min(0.999, max(0, x2 * sx - dx))\n            y2 = min(0.999, max(0, y2 * sy - dy))\n            bs[i][1] = (x1 + x2) / 2\n            bs[i][2] = (y1 + y2) / 2\n            bs[i][3] = (x2 - x1)"
        },
        {
            "comment": "This code initializes a list of bounding boxes, applies jitter if not in valid mode (randomly adjusts image size and position), reshapes the list into a single array of bounding boxes, and returns it.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":1353-1389",
            "content": "            bs[i][4] = (y2 - y1)\n            if flip:\n                bs[i][1] = 0.999 - bs[i][1]\n            if bs[i][3] < 0.001 or bs[i][4] < 0.001:\n                continue\n            label[cc] = bs[i]\n            cc += 1\n            if cc >= 50:\n                break\n        label = np.reshape(label, (-1))\n        return label\n    def __call__(self, results):\n        clip = results['imgs']\n        frame_num = len(clip)\n        oh = clip[0].height\n        ow = clip[0].width\n        labpath = results['filename'].replace('jpg', 'txt').replace('rgb-images', 'labels')\n        if not self.valid_mode:\n            dw = int(ow * self.jitter)\n            dh = int(oh * self.jitter)\n            pleft = random.randint(-dw, dw)\n            pright = random.randint(-dw, dw)\n            ptop = random.randint(-dh, dh)\n            pbot = random.randint(-dh, dh)\n            swidth = ow - pleft - pright\n            sheight = oh - ptop - pbot\n            sx = float(swidth) / ow\n            sy = float(sheight) / oh\n            dx = (float(pleft) / ow) / sx"
        },
        {
            "comment": "This code performs image augmentations and label manipulations. It applies random crop, resize, flip (horizontally), and distortion to the image(s) with a certain probability. The label is either filled from the truth detection or set to zero vector depending on the size of the extracted truth data.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":1390-1418",
            "content": "            dy = (float(ptop) / oh) / sy\n            flip = random.randint(1, 10000) % 2\n            dhue = random.uniform(-self.hue, self.hue)\n            dsat = self._rand_scale(self.saturation)\n            dexp = self._rand_scale(self.exposure)\n            # Augment\n            cropped = [img.crop((pleft, ptop, pleft + swidth - 1, ptop + sheight - 1)) for img in clip]\n            sized = [img.resize(self.shape) for img in cropped]\n            if flip:\n                sized = [img.transpose(Image.FLIP_LEFT_RIGHT) for img in sized]\n            clip = [self._random_distort_image(img, dhue, dsat, dexp) for img in sized]\n            label = self._fill_truth_detection(labpath, flip, dx, dy, 1. / sx, 1. / sy)\n        else:\n            label = np.zeros([50 * 5])\n            tmp = self._read_truths_args(labpath, 8.0 / clip[0].width).astype('float32')\n            tmp = np.reshape(tmp, [-1])\n            tsz = tmp.size\n            if tsz > 50 * 5:\n                label = tmp[0:50 * 5]\n            elif tsz > 0:\n                label[0:tsz] = tmp"
        },
        {
            "comment": "Resizes images to a specific shape, converts them to float32 type and scales by 255.0, concatenates frames into a single array, transposes array dimensions, stores image and label arrays in 'results', returns results",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/loader/pipelines/augmentations.py\":1419-1426",
            "content": "            clip = [img.resize(self.shape) for img in clip]\n        clip = [np.asarray(img).astype('float32') / 255.0 for img in clip]\n        clip = np.concatenate(clip, 0).reshape([frame_num, 224, 224, 3])\n        clip = np.transpose(clip, [3, 0, 1, 2])\n        results['imgs'] = clip\n        results['labels'] = label\n        return results"
        }
    ]
}