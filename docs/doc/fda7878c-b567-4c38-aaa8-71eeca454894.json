{
    "summary": "PaddleVideo's \"object_detection_evaluation\" module offers `ObjectDetectionEvaluator` for evaluating object detection outcomes, including metrics like mAP and mean correct localization, considering IOU threshold. It handles AVA dataset and computes AVA metrics for average precision and mean average precision.",
    "details": [
        {
            "comment": "The provided code is a part of the \"object_detection_evaluation\" module in the PaddleVideo library. This module provides a class called \"ObjectDetectionEvaluation\" that manages ground truth information for object detection datasets, computes frequently used metrics like Precision, Recall, and CorLoc from detection results. The class supports adding ground truth information sequentially and various operations for evaluation.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/ava_evaluation/object_detection_evaluation.py\":0-20",
            "content": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n\"\"\"object_detection_evaluation module.\nObjectDetectionEvaluation is a class which manages ground truth information of\na object detection dataset, and computes frequently used detection metrics such\nas Precision, Recall, CorLoc of the provided detection results.\nIt supports the following operations:\n1) Add ground truth information of images sequentially."
        },
        {
            "comment": "This code defines an abstract class `DetectionEvaluator` for evaluating object detection results. It takes categories as input and allows adding single ground truth and detected image information. After adding all the data, it can be evaluated to get a metrics dictionary. This evaluation is done on numpy boxes and box lists.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/ava_evaluation/object_detection_evaluation.py\":21-57",
            "content": "2) Add detection result of images sequentially.\n3) Evaluate detection metrics on already inserted detection results.\n4) Write evaluation result into a pickle file for future processing or\n   visualization.\nNote: This module operates on numpy boxes and box lists.\n\"\"\"\nimport collections\nimport logging\nfrom abc import ABCMeta, abstractmethod\nimport numpy as np\nfrom . import metrics, per_image_evaluation, standard_fields\nclass DetectionEvaluator:\n    \"\"\"Interface for object detection evalution classes.\n    Example usage of the Evaluator:\n    ------------------------------\n    evaluator = DetectionEvaluator(categories)\n    # Detections and groundtruth for image 1.\n    evaluator.add_single_groundtruth_image_info(...)\n    evaluator.add_single_detected_image_info(...)\n    # Detections and groundtruth for image 2.\n    evaluator.add_single_groundtruth_image_info(...)\n    evaluator.add_single_detected_image_info(...)\n    metrics_dict = evaluator.evaluate()\n    \"\"\"\n    __metaclass__ = ABCMeta\n    def __init__(self, categories):"
        },
        {
            "comment": "This code defines a class constructor that takes categories as input and provides two abstract methods for adding ground truth and detected image information for evaluation. The categories are used to uniquely identify different objects in the images.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/ava_evaluation/object_detection_evaluation.py\":58-85",
            "content": "        \"\"\"Constructor.\n        Args:\n            categories: A list of dicts, each of which has the following keys -\n                'id': (required) an integer id uniquely identifying this\n                    category.\n                'name': (required) string representing category name e.g.,\n                    'cat', 'dog'.\n        \"\"\"\n        self._categories = categories\n    @abstractmethod\n    def add_single_ground_truth_image_info(self, image_id, groundtruth_dict):\n        \"\"\"Adds groundtruth for a single image to be used for evaluation.\n        Args:\n            image_id: A unique string/integer identifier for the image.\n            groundtruth_dict: A dictionary of groundtruth numpy arrays required\n                for evaluations.\n        \"\"\"\n    @abstractmethod\n    def add_single_detected_image_info(self, image_id, detections_dict):\n        \"\"\"Adds detections for a single image to be used for evaluation.\n        Args:\n            image_id: A unique string/integer identifier for the image.\n            detections_dict: A dictionary of detection numpy arrays required"
        },
        {
            "comment": "This code defines an `ObjectDetectionEvaluator` class that evaluates object detection results. It takes categories, IOU threshold, options for evaluating corner localizations and masks. The `evaluate()` method returns a dictionary of metrics, while the `clear()` method clears the state for a new evaluation.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/ava_evaluation/object_detection_evaluation.py\":86-119",
            "content": "                for evaluation.\n        \"\"\"\n    @abstractmethod\n    def evaluate(self):\n        \"\"\"Evaluates detections and returns a dictionary of metrics.\"\"\"\n    @abstractmethod\n    def clear(self):\n        \"\"\"Clears the state to prepare for a fresh evaluation.\"\"\"\nclass ObjectDetectionEvaluator(DetectionEvaluator):\n    \"\"\"A class to evaluate detections.\"\"\"\n    def __init__(\n        self,\n        categories,\n        matching_iou_threshold=0.5,\n        evaluate_corlocs=False,\n        metric_prefix=None,\n        use_weighted_mean_ap=False,\n        evaluate_masks=False,\n    ):\n        \"\"\"Constructor.\n        Args:\n            categories: A list of dicts, each of which has the following keys -\n                'id': (required) an integer id uniquely identifying this\n                    category.\n                'name': (required) string representing category name e.g.,\n                    'cat', 'dog'.\n            matching_iou_threshold: IOU threshold to use for matching\n                groundtruth boxes to detection boxes."
        },
        {
            "comment": "This code is initializing the ObjectDetectionEvaluator class, which evaluates object detection performance. It takes in optional parameters for corloc scores, metric prefix, and weighted mean AP computation. It checks if category IDs are 1-indexed and raises a ValueError if not.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/ava_evaluation/object_detection_evaluation.py\":120-138",
            "content": "            evaluate_corlocs: (optional) boolean which determines if corloc\n                scores are to be returned or not.\n            metric_prefix: (optional) string prefix for metric name; if None,\n                no prefix is used.\n            use_weighted_mean_ap: (optional) boolean which determines if the\n                mean average precision is computed directly from the scores and\n                tp_fp_labels of all classes.\n            evaluate_masks: If False, evaluation will be performed based on\n                boxes. If True, mask evaluation will be performed instead.\n        Raises:\n            ValueError: If the category ids are not 1-indexed.\n        \"\"\"\n        super(ObjectDetectionEvaluator, self).__init__(categories)\n        self._num_classes = max([cat['id'] for cat in categories])\n        if min(cat['id'] for cat in categories) < 1:\n            raise ValueError('Classes should be 1-indexed.')\n        self._matching_iou_threshold = matching_iou_threshold\n        self._use_weighted_mean_ap = use_weighted_mean_ap"
        },
        {
            "comment": "This code is initializing an object detection evaluation module, specifically for the Aggregated Average Precision metric. The module takes in parameters such as the number of ground truth classes, matching IOU threshold, and a label offset. It also adds a single image's ground truth information for evaluation purposes. The function expects an image ID and a dictionary containing ground truth boxes information.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/ava_evaluation/object_detection_evaluation.py\":139-159",
            "content": "        self._label_id_offset = 1\n        self._evaluate_masks = evaluate_masks\n        self._evaluation = ObjectDetectionEvaluation(\n            num_groundtruth_classes=self._num_classes,\n            matching_iou_threshold=self._matching_iou_threshold,\n            use_weighted_mean_ap=self._use_weighted_mean_ap,\n            label_id_offset=self._label_id_offset,\n        )\n        self._image_ids = set([])\n        self._evaluate_corlocs = evaluate_corlocs\n        self._metric_prefix = (metric_prefix + '_') if metric_prefix else ''\n    def add_single_ground_truth_image_info(self, image_id, groundtruth_dict):\n        \"\"\"Adds groundtruth for a single image to be used for evaluation.\n        Args:\n            image_id: A unique string/integer identifier for the image.\n            groundtruth_dict: A dictionary containing -\n                standard_fields.InputDataFields.groundtruth_boxes: float32\n                    numpy array of shape [num_boxes, 4] containing `num_boxes`\n                    groundtruth boxes of the format [ymin, xmin, ymax, xmax] in"
        },
        {
            "comment": "The code is defining the `add_groundtruth` function that takes in an image ID and a groundtruth dictionary. It checks if the image ID has been added before, and raises a ValueError if it has. The groundtruth dictionary should contain 'boxes', 'groundtruth_classes', 'groundtruth_difficult' (optional), and 'groundtruth_instance_masks' (if difficult instances). If the groundtruth is valid, it adds the information to the _image_ids set and initializes corresponding arrays for that image ID. If instance masks are not in the groundtruth dictionary, it raises a ValueError.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/ava_evaluation/object_detection_evaluation.py\":160-178",
            "content": "                    absolute image coordinates.\n                standard_fields.InputDataFields.groundtruth_classes: integer\n                    numpy array of shape [num_boxes] containing 1-indexed\n                    groundtruth classes for the boxes.\n                standard_fields.InputDataFields.groundtruth_difficult: Optional\n                    length M numpy boolean array denoting whether a ground\n                    truth box is a difficult instance or not. This field is\n                    optional to support the case that no boxes are difficult.\n                standard_fields.InputDataFields.groundtruth_instance_masks:\n                    Optional numpy array of shape [num_boxes, height, width]\n                    with values in {0, 1}.\n        Raises:\n            ValueError: On adding groundtruth for an image more than once. Will\n                also raise error if instance masks are not in groundtruth\n                dictionary.\n        \"\"\"\n        if image_id in self._image_ids:\n            raise ValueError("
        },
        {
            "comment": "This code is checking if an image with a specific id already exists. If not, it retrieves the groundtruth classes and difficult labels from the dictionary, either from existing keys or by setting them to None if not present or empty. It also checks if the image_id is already added to avoid duplicates.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/ava_evaluation/object_detection_evaluation.py\":179-197",
            "content": "                'Image with id {} already added.'.format(image_id))\n        groundtruth_classes = (\n            groundtruth_dict[\n                standard_fields.InputDataFields.groundtruth_classes] -\n            self._label_id_offset)\n        # If the key is not present in the groundtruth_dict or the array is\n        # empty (unless there are no annotations for the groundtruth on this\n        # image) use values from the dictionary or insert None otherwise.\n        if (standard_fields.InputDataFields.groundtruth_difficult\n                in groundtruth_dict.keys()) and (groundtruth_dict[\n                    standard_fields.InputDataFields.groundtruth_difficult].size\n                                                 or\n                                                 not groundtruth_classes.size):\n            groundtruth_difficult = groundtruth_dict[\n                standard_fields.InputDataFields.groundtruth_difficult]\n        else:\n            groundtruth_difficult = None\n            if not len(self._image_ids) % 1000:"
        },
        {
            "comment": "This code block checks if the ground truth difficult flag is specified for an image and raises a warning if not. It then adds single ground truth image information, including bounding boxes, class labels, and mask (if available), to the evaluation object. This allows for evaluating the performance of the object detection model on the given image.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/ava_evaluation/object_detection_evaluation.py\":198-218",
            "content": "                logging.warn(('image %s does not have groundtruth difficult '\n                              'flag specified'), image_id)\n        groundtruth_masks = None\n        if self._evaluate_masks:\n            if (standard_fields.InputDataFields.groundtruth_instance_masks\n                    not in groundtruth_dict):\n                raise ValueError(\n                    'Instance masks not in groundtruth dictionary.')\n            groundtruth_masks = groundtruth_dict[\n                standard_fields.InputDataFields.groundtruth_instance_masks]\n        self._evaluation.add_single_ground_truth_image_info(\n            image_key=image_id,\n            groundtruth_boxes=groundtruth_dict[\n                standard_fields.InputDataFields.groundtruth_boxes],\n            groundtruth_class_labels=groundtruth_classes,\n            groundtruth_is_difficult_list=groundtruth_difficult,\n            groundtruth_masks=groundtruth_masks,\n        )\n        self._image_ids.update([image_id])\n    def add_single_detected_image_info(self, image_id, detections_dict):"
        },
        {
            "comment": "This code snippet adds detections for a single image to be used in evaluation. It takes in image_id and a dictionary containing detection boxes, scores, classes, and masks as input. The detection boxes are represented by a float32 numpy array of shape [num_boxes, 4] with the format [ymin, xmin, ymax, xmax] in absolute image coordinates. Detection scores and classes are integer numpy arrays representing the scores and classes for each box respectively, while detection masks are represented by a uint8 numpy array of shape [num_boxes, height, width].",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/ava_evaluation/object_detection_evaluation.py\":219-235",
            "content": "        \"\"\"Adds detections for a single image to be used for evaluation.\n        Args:\n            image_id: A unique string/integer identifier for the image.\n            detections_dict: A dictionary containing -\n                standard_fields.DetectionResultFields.detection_boxes: float32\n                    numpy array of shape [num_boxes, 4] containing `num_boxes`\n                    detection boxes of the format [ymin, xmin, ymax, xmax] in\n                    absolute image coordinates.\n                standard_fields.DetectionResultFields.detection_scores: float32\n                    numpy array of shape [num_boxes] containing detection\n                    scores for the boxes.\n                standard_fields.DetectionResultFields.detection_classes:\n                    integer numpy array of shape [num_boxes] containing\n                    1-indexed detection classes for the boxes.\n                standard_fields.DetectionResultFields.detection_masks: uint8\n                    numpy array of shape [num_boxes, height, width] containing"
        },
        {
            "comment": "This code block retrieves detection classes and masks from the \"detections_dict\" dictionary. If _evaluate_Masks is True, it checks if detection masks are present in detections_dict. If not, it raises a ValueError. Then, it adds single detected image information to _evaluation using detected boxes, scores, and (optionally) detection masks.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/ava_evaluation/object_detection_evaluation.py\":236-258",
            "content": "                    `num_boxes` masks of values ranging between 0 and 1.\n        Raises:\n            ValueError: If detection masks are not in detections dictionary.\n        \"\"\"\n        detection_classes = (\n            detections_dict[\n                standard_fields.DetectionResultFields.detection_classes] -\n            self._label_id_offset)\n        detection_masks = None\n        if self._evaluate_masks:\n            if (standard_fields.DetectionResultFields.detection_masks\n                    not in detections_dict):\n                raise ValueError(\n                    'Detection masks not in detections dictionary.')\n            detection_masks = detections_dict[\n                standard_fields.DetectionResultFields.detection_masks]\n        self._evaluation.add_single_detected_image_info(\n            image_key=image_id,\n            detected_boxes=detections_dict[\n                standard_fields.DetectionResultFields.detection_boxes],\n            detected_scores=detections_dict[\n                standard_fields.DetectionResultFields.detection_scores],"
        },
        {
            "comment": "This code is related to object detection evaluation in the AVA dataset. The `create_category_index` function creates a dictionary of COCO compatible categories, keyed by category id. The `evaluate` function computes the evaluation results, returning a dictionary of metrics including summary_metrics. The code also uses `detection_classes` and `detection_masks` for evaluation.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/ava_evaluation/object_detection_evaluation.py\":259-289",
            "content": "            detected_class_labels=detection_classes,\n            detected_masks=detection_masks,\n        )\n    def create_category_index(self, categories):\n        \"\"\"Creates dictionary of COCO compatible categories keyed by category\n        id.\n        Args:\n            categories: a list of dicts, each of which has the following keys:\n                'id': (required) an integer id uniquely identifying this\n                    category.\n                'name': (required) string representing category name\n                    e.g., 'cat', 'dog', 'pizza'.\n        Returns:\n            category_index: a dict containing the same entries as categories,\n                but keyed by the 'id' field of each category.\n        \"\"\"\n        category_index = {}\n        for cat in categories:\n            category_index[cat['id']] = cat\n        return category_index\n    def evaluate(self):\n        \"\"\"Compute evaluation result.\n        Returns:\n            A dictionary of metrics with the following fields -\n            1. summary_metrics:"
        },
        {
            "comment": "This code calculates the mean average precision (mAP) and optionally, the mean correct localization score (meanCorLoc), at a specified IOU threshold for object detection evaluation. It creates metrics under different categories using category-specific results. The calculated values are then stored in the pascal_metrics dictionary.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/ava_evaluation/object_detection_evaluation.py\":290-314",
            "content": "                'Precision/mAP@<matching_iou_threshold>IOU': mean average\n                precision at the specified IOU threshold\n            2. per_category_ap: category specific results with keys of the form\n               'PerformanceByCategory/mAP@<matching_iou_threshold>IOU/category'\n        \"\"\"\n        (\n            per_class_ap,\n            mean_ap,\n            _,\n            _,\n            per_class_corloc,\n            mean_corloc,\n        ) = self._evaluation.evaluate()\n        metric = f'mAP@{self._matching_iou_threshold}IOU'\n        pascal_metrics = {self._metric_prefix + metric: mean_ap}\n        if self._evaluate_corlocs:\n            pascal_metrics[self._metric_prefix +\n                           'Precision/meanCorLoc@{}IOU'.format(\n                               self._matching_iou_threshold)] = mean_corloc\n        category_index = self.create_category_index(self._categories)\n        for idx in range(per_class_ap.size):\n            if idx + self._label_id_offset in category_index:\n                display_name = ("
        },
        {
            "comment": "This code calculates average precision (AP) and optional correct localization (CorLoc) metrics for object detection by category. It appends these metrics to the pascal_metrics dictionary based on the matching IOU threshold and category names from the category_index. The clear() function resets the evaluation state for a new evaluation.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/ava_evaluation/object_detection_evaluation.py\":315-337",
            "content": "                    self._metric_prefix +\n                    'PerformanceByCategory/AP@{}IOU/{}'.format(\n                        self._matching_iou_threshold,\n                        category_index[idx + self._label_id_offset]['name'],\n                    ))\n                pascal_metrics[display_name] = per_class_ap[idx]\n                # Optionally add CorLoc metrics.classes\n                if self._evaluate_corlocs: #False\n                    display_name = (\n                        self._metric_prefix +\n                        'PerformanceByCategory/CorLoc@{}IOU/{}'.format(\n                            self._matching_iou_threshold,\n                            category_index[idx +\n                                           self._label_id_offset]['name'],\n                        ))\n                    pascal_metrics[display_name] = per_class_corloc[idx]\n        return pascal_metrics\n    def clear(self):\n        \"\"\"Clears the state to prepare for a fresh evaluation.\"\"\"\n        self._evaluation = ObjectDetectionEvaluation("
        },
        {
            "comment": "The given code is a part of an object detection evaluation module in PaddleVideo. It defines classes and functions to evaluate detections using PASCAL metrics. The ObjectDetectionEvaluator class initializes with categories, matching IoU threshold, evaluating corlocs flag, and use weighted mean AP flag. PascalDetectionEvaluator is a subclass of ObjectDetectionEvaluator specifically for PASCAL evaluation. The code also defines the ObjectDetectionEvalMetrics namedtuple which includes average_precisions, mean_ap, precisions, recalls, corlocs, and mean_corloc attributes.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/ava_evaluation/object_detection_evaluation.py\":338-374",
            "content": "            num_groundtruth_classes=self._num_classes,\n            matching_iou_threshold=self._matching_iou_threshold,\n            use_weighted_mean_ap=self._use_weighted_mean_ap,\n            label_id_offset=self._label_id_offset,\n        )\n        self._image_ids.clear()\nclass PascalDetectionEvaluator(ObjectDetectionEvaluator):\n    \"\"\"A class to evaluate detections using PASCAL metrics.\"\"\"\n    def __init__(self, categories, matching_iou_threshold=0.5):\n        super(PascalDetectionEvaluator, self).__init__(\n            categories,\n            matching_iou_threshold=matching_iou_threshold,\n            evaluate_corlocs=False,\n            use_weighted_mean_ap=False,\n        )\nObjectDetectionEvalMetrics = collections.namedtuple(\n    'ObjectDetectionEvalMetrics',\n    [\n        'average_precisions',\n        'mean_ap',\n        'precisions',\n        'recalls',\n        'corlocs',\n        'mean_corloc',\n    ],\n)\nclass ObjectDetectionEvaluation:\n    \"\"\"Internal implementation of Pascal object detection metrics.\"\"\"\n    def __init__("
        },
        {
            "comment": "This function initializes the necessary attributes for object detection evaluation. It requires 'self', number of ground truth classes, matching and nms iou thresholds, maximum output boxes, whether to use weighted mean AP, label offset, and sets up dictionaries to store ground truth information. It also initializes counters for the number of instances and images per class.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/ava_evaluation/object_detection_evaluation.py\":375-401",
            "content": "        self,\n        num_groundtruth_classes,\n        matching_iou_threshold=0.5,\n        nms_iou_threshold=1.0,\n        nms_max_output_boxes=10000,\n        use_weighted_mean_ap=False,\n        label_id_offset=0,\n    ):\n        if num_groundtruth_classes < 1:\n            raise ValueError(\n                'Need at least 1 groundtruth class for evaluation.')\n        self.per_image_eval = per_image_evaluation.PerImageEvaluation(\n            num_groundtruth_classes=num_groundtruth_classes,\n            matching_iou_threshold=matching_iou_threshold,\n        )\n        self.num_class = num_groundtruth_classes\n        self.use_weighted_mean_ap = use_weighted_mean_ap\n        self.label_id_offset = label_id_offset\n        self.groundtruth_boxes = {}\n        self.groundtruth_class_labels = {}\n        self.groundtruth_masks = {}\n        self.groundtruth_is_difficult_list = {}\n        self.groundtruth_is_group_of_list = {}\n        self.num_gt_instances_per_class = np.zeros(self.num_class, dtype=int)\n        self.num_gt_imgs_per_class = np.zeros(self.num_class, dtype=int)"
        },
        {
            "comment": "This code initializes the detection variables and provides functions to clear detections, add single ground truth image info, and perform evaluation. The average precision per class is initialized with nan values, and these functions manage the data for object detection evaluation.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/ava_evaluation/object_detection_evaluation.py\":403-429",
            "content": "        self._initialize_detections()\n    def _initialize_detections(self):\n        self.detection_keys = set()\n        self.scores_per_class = [[] for _ in range(self.num_class)]\n        self.tp_fp_labels_per_class = [[] for _ in range(self.num_class)]\n        self.num_images_correctly_detected_per_class = np.zeros(self.num_class)\n        self.average_precision_per_class = np.empty(\n            self.num_class, dtype=float)\n        self.average_precision_per_class.fill(np.nan)\n        self.precisions_per_class = []\n        self.recalls_per_class = []\n        self.corloc_per_class = np.ones(self.num_class, dtype=float)\n    def clear_detections(self):\n        self._initialize_detections()\n    def add_single_ground_truth_image_info(\n        self,\n        image_key,\n        groundtruth_boxes,\n        groundtruth_class_labels,\n        groundtruth_is_difficult_list=None,\n        groundtruth_is_group_of_list=None,\n        groundtruth_masks=None,\n    ):\n        \"\"\"Adds groundtruth for a single image to be used for evaluation."
        },
        {
            "comment": "The function takes in an image identifier, ground truth boxes coordinates, class labels for the boxes, a boolean array denoting difficult instances, and another boolean array for group-of boxes. It calculates average precision and recall for object detection using these inputs. The function also supports cases where no boxes are difficult or groups-of.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/ava_evaluation/object_detection_evaluation.py\":431-446",
            "content": "        Args:\n            image_key: A unique string/integer identifier for the image.\n            groundtruth_boxes: float32 numpy array of shape [num_boxes, 4]\n                containing `num_boxes` groundtruth boxes of the format\n                [ymin, xmin, ymax, xmax] in absolute image coordinates.\n            groundtruth_class_labels: integer numpy array of shape [num_boxes]\n                containing 0-indexed groundtruth classes for the boxes.\n            groundtruth_is_difficult_list: A length M numpy boolean array\n                denoting whether a ground truth box is a difficult instance or\n                not. To support the case that no boxes are difficult, it is by\n                default set as None.\n            groundtruth_is_group_of_list: A length M numpy boolean array\n                denoting whether a ground truth box is a group-of box or not.\n                To support the case that no boxes are groups-of, it is by\n                default set as None.\n            groundtruth_masks: uint8 numpy array of shape"
        },
        {
            "comment": "This function adds the ground truth boxes, class labels, and masks to the database for a given image key. If the groundtruth_is_difficult_list or groundtruth_is_group_of_list are None, it creates them with default values. It stores these lists as well in the database for the specified image key.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/ava_evaluation/object_detection_evaluation.py\":447-466",
            "content": "                [num_boxes, height, width] containing `num_boxes` groundtruth\n                masks. The mask values range from 0 to 1.\n        \"\"\"\n        if image_key in self.groundtruth_boxes:\n            logging.warn(('image %s has already been added to the ground '\n                          'truth database.'), image_key)\n            return\n        self.groundtruth_boxes[image_key] = groundtruth_boxes\n        self.groundtruth_class_labels[image_key] = groundtruth_class_labels\n        self.groundtruth_masks[image_key] = groundtruth_masks\n        if groundtruth_is_difficult_list is None:\n            num_boxes = groundtruth_boxes.shape[0]\n            groundtruth_is_difficult_list = np.zeros(num_boxes, dtype=bool)\n        self.groundtruth_is_difficult_list[\n            image_key] = groundtruth_is_difficult_list.astype(dtype=bool)\n        if groundtruth_is_group_of_list is None:\n            num_boxes = groundtruth_boxes.shape[0]\n            groundtruth_is_group_of_list = np.zeros(num_boxes, dtype=bool)\n        self.groundtruth_is_group_of_list["
        },
        {
            "comment": "This function adds detections for a single image to be used for evaluation. It requires an image key, detected boxes, detected scores, and detected class labels as input. The detected boxes should be in the format [ymin, xmin, ymax, xmax] and the detected scores and detected class labels should be numpy arrays of the specified shapes. The function calls a _update_ground_truth_statistics method with groundtruth class labels, difficult list, and group of list as input. This method updates the ground truth statistics for evaluation.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/ava_evaluation/object_detection_evaluation.py\":467-492",
            "content": "            image_key] = groundtruth_is_group_of_list.astype(dtype=bool)\n        self._update_ground_truth_statistics(\n            groundtruth_class_labels,\n            groundtruth_is_difficult_list.astype(dtype=bool),\n            groundtruth_is_group_of_list.astype(dtype=bool),\n        )\n    def add_single_detected_image_info(\n        self,\n        image_key,\n        detected_boxes,\n        detected_scores,\n        detected_class_labels,\n        detected_masks=None,\n    ):\n        \"\"\"Adds detections for a single image to be used for evaluation.\n        Args:\n            image_key: A unique string/integer identifier for the image.\n            detected_boxes: float32 numpy array of shape [num_boxes, 4]\n                containing `num_boxes` detection boxes of the format\n                [ymin, xmin, ymax, xmax] in absolute image coordinates.\n            detected_scores: float32 numpy array of shape [num_boxes]\n                containing detection scores for the boxes.\n            detected_class_labels: integer numpy array of shape [num_boxes]"
        },
        {
            "comment": "This function creates a numpy array of detection masks based on detected boxes, scores and class labels. It raises a ValueError if the lengths of these lists are not equal. If an image key already exists in the detection keys list, it logs a warning message and returns without adding the image to the database.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/ava_evaluation/object_detection_evaluation.py\":493-515",
            "content": "                containing 0-indexed detection classes for the boxes.\n            detected_masks: np.uint8 numpy array of shape\n                [num_boxes, height, width] containing `num_boxes` detection\n                masks with values ranging between 0 and 1.\n        Raises:\n            ValueError: if the number of boxes, scores and class labels differ\n                in length.\n        \"\"\"\n        if len(detected_boxes) != len(detected_scores) or len(\n                detected_boxes) != len(detected_class_labels):\n            raise ValueError(\n                'detected_boxes, detected_scores and '\n                'detected_class_labels should all have same lengths. Got'\n                '[%d, %d, %d]' % len(detected_boxes),\n                len(detected_scores),\n                len(detected_class_labels),\n            )\n        if image_key in self.detection_keys:\n            logging.warn(('image %s has already been added to the ground '\n                          'truth database.'), image_key)\n            return"
        },
        {
            "comment": "This code is initializing ground truth values for object detection evaluation. If an image key exists in the ground truth boxes dictionary, it retrieves the corresponding ground truth values (boxes, class labels, masks) and removes them from their respective dictionaries to avoid memory overflow. If no image key exists, it initializes empty arrays or None values for the ground truth values.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/ava_evaluation/object_detection_evaluation.py\":517-535",
            "content": "        self.detection_keys.add(image_key)\n        if image_key in self.groundtruth_boxes:\n            groundtruth_boxes = self.groundtruth_boxes[image_key]\n            groundtruth_class_labels = self.groundtruth_class_labels[image_key]\n            # Masks are popped instead of look up. The reason is that we do not\n            # want to keep all masks in memory which can cause memory overflow.\n            groundtruth_masks = self.groundtruth_masks.pop(image_key)\n            groundtruth_is_difficult_list = self.groundtruth_is_difficult_list[\n                image_key]\n            groundtruth_is_group_of_list = self.groundtruth_is_group_of_list[\n                image_key]\n        else:\n            groundtruth_boxes = np.empty(shape=[0, 4], dtype=float)\n            groundtruth_class_labels = np.array([], dtype=int)\n            if detected_masks is None:\n                groundtruth_masks = None\n            else:\n                groundtruth_masks = np.empty(shape=[0, 1, 1], dtype=float)\n            groundtruth_is_difficult_list = np.array([], dtype=bool)"
        },
        {
            "comment": "This code is part of the PaddleVideo library and it computes object detection metrics. It takes in detected boxes, scores, class labels, ground truth boxes, class labels, masks etc., and calculates true positive and false positive labels for each image. The computed values are then stored per class in separate lists (scores_per_class and tp_fp_labels_per_class). Additionally, the function updates ground truth statistics by appending new ground truth class labels and difficult list to existing ones.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/ava_evaluation/object_detection_evaluation.py\":536-560",
            "content": "            groundtruth_is_group_of_list = np.array([], dtype=bool)\n        (\n            scores,\n            tp_fp_labels,\n        ) = self.per_image_eval.compute_object_detection_metrics(\n            detected_boxes=detected_boxes,\n            detected_scores=detected_scores,\n            detected_class_labels=detected_class_labels,\n            groundtruth_boxes=groundtruth_boxes,\n            groundtruth_class_labels=groundtruth_class_labels,\n            groundtruth_is_difficult_list=groundtruth_is_difficult_list,\n            groundtruth_is_group_of_list=groundtruth_is_group_of_list,\n            detected_masks=detected_masks,\n            groundtruth_masks=groundtruth_masks,\n        )\n        for i in range(self.num_class):\n            if scores[i].shape[0] > 0:\n                self.scores_per_class[i].append(scores[i])\n                self.tp_fp_labels_per_class[i].append(tp_fp_labels[i])\n    def _update_ground_truth_statistics(\n        self,\n        groundtruth_class_labels,\n        groundtruth_is_difficult_list,"
        },
        {
            "comment": "This function updates ground truth statistics for object detection by counting instances, excluding difficult boxes and treating them as normal ones for CorLoc computations. It iterates through class indices to determine the number of instances for each class label, excluding difficult or group-of boxes.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/ava_evaluation/object_detection_evaluation.py\":561-582",
            "content": "        groundtruth_is_group_of_list,\n    ):\n        \"\"\"Update grouth truth statitistics.\n        1. Difficult boxes are ignored when counting the number of ground truth\n        instances as done in Pascal VOC devkit.\n        2. Difficult boxes are treated as normal boxes when computing CorLoc\n        related statitistics.\n        Args:\n            groundtruth_class_labels: An integer numpy array of length M,\n                representing M class labels of object instances in ground truth\n            groundtruth_is_difficult_list: A boolean numpy array of length M\n                denoting whether a ground truth box is a difficult instance or\n                not\n            groundtruth_is_group_of_list: A boolean numpy array of length M\n                denoting whether a ground truth box is a group-of box or not\n        \"\"\"\n        for class_index in range(self.num_class):\n            num_gt_instances = np.sum(groundtruth_class_labels[\n                ~groundtruth_is_difficult_list\n                & ~groundtruth_is_group_of_list] == class_index)"
        },
        {
            "comment": "The code calculates average precision, mean average precision, precisions, recalls, and CorLoc scores for object detection evaluation. It checks if any ground truth instances exist for each class and returns a named tuple with evaluation results. If there are classes with no ground truth examples, it prints a warning message.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/ava_evaluation/object_detection_evaluation.py\":583-604",
            "content": "            self.num_gt_instances_per_class[class_index] += num_gt_instances\n            if np.any(groundtruth_class_labels == class_index):\n                self.num_gt_imgs_per_class[class_index] += 1\n    def evaluate(self):\n        \"\"\"Compute evaluation result.\n        Returns:\n            A named tuple with the following fields -\n                average_precision: float numpy array of average precision for\n                    each class.\n                mean_ap: mean average precision of all classes, float scalar\n                precisions: List of precisions, each precision is a float numpy\n                    array\n                recalls: List of recalls, each recall is a float numpy array\n                corloc: numpy float array\n                mean_corloc: Mean CorLoc score for each class, float scalar\n        \"\"\"\n        if (self.num_gt_instances_per_class == 0).any():\n            print(\n                'The following classes have no ground truth examples: %s',\n                np.squeeze(np.argwhere(self.num_gt_instances_per_class == 0)) +"
        },
        {
            "comment": "This code is part of a class that performs object detection evaluation using Average Vehicle Accuracy (AVA) metrics. It checks for the number of ground truth instances per class and concatenates scores and true positive/false positive labels per class. If weighted mean average precision (AP) calculation is enabled, it appends the scores and labels to the total arrays. The code uses the compute_precision_recall function from the metrics module to calculate precision and recall values.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/ava_evaluation/object_detection_evaluation.py\":605-628",
            "content": "                self.label_id_offset, \"self.detection_keys:\",self.detection_keys\n            )\n        if self.use_weighted_mean_ap:\n            all_scores = np.array([], dtype=float)\n            all_tp_fp_labels = np.array([], dtype=bool)\n        for class_index in range(self.num_class):\n            if self.num_gt_instances_per_class[class_index] == 0:\n                continue\n            if not self.scores_per_class[class_index]:\n                scores = np.array([], dtype=float)\n                tp_fp_labels = np.array([], dtype=bool)\n            else:\n                scores = np.concatenate(self.scores_per_class[class_index])\n                tp_fp_labels = np.concatenate(\n                    self.tp_fp_labels_per_class[class_index])\n            if self.use_weighted_mean_ap:\n                all_scores = np.append(all_scores, scores)\n                all_tp_fp_labels = np.append(all_tp_fp_labels, tp_fp_labels)\n            precision, recall = metrics.compute_precision_recall(\n                scores,\n                tp_fp_labels,"
        },
        {
            "comment": "This function calculates average precision and correlation localization for object detection evaluation. It stores the precision, recall, average precision per class, and correlation localization per class. If weighted mean AP is enabled, it computes precision, recall, mean AP, and mean correlation localization.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/ava_evaluation/object_detection_evaluation.py\":629-650",
            "content": "                self.num_gt_instances_per_class[class_index],\n            )\n            self.precisions_per_class.append(precision)\n            self.recalls_per_class.append(recall)\n            average_precision = metrics.compute_average_precision(\n                precision, recall)\n            self.average_precision_per_class[class_index] = average_precision\n        self.corloc_per_class = metrics.compute_cor_loc(\n            self.num_gt_imgs_per_class,\n            self.num_images_correctly_detected_per_class,\n        )\n        if self.use_weighted_mean_ap:\n            num_gt_instances = np.sum(self.num_gt_instances_per_class)\n            precision, recall = metrics.compute_precision_recall(\n                all_scores, all_tp_fp_labels, num_gt_instances)\n            mean_ap = metrics.compute_average_precision(precision, recall)\n        else:\n            mean_ap = np.nanmean(self.average_precision_per_class)\n        mean_corloc = np.nanmean(self.corloc_per_class)\n        return ObjectDetectionEvalMetrics("
        },
        {
            "comment": "This code snippet appears to be part of a class function that returns several evaluation metrics for object detection. The metrics include average precision per class, mean average precision, precisions and recalls per class, and mean corloc values. These metrics are commonly used in evaluating object detection models' performance.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/ava_evaluation/object_detection_evaluation.py\":651-657",
            "content": "            self.average_precision_per_class,\n            mean_ap,\n            self.precisions_per_class,\n            self.recalls_per_class,\n            self.corloc_per_class,\n            mean_corloc,\n        )"
        }
    ]
}