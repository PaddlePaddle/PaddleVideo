{
    "summary": "This code sets up PaddleVideo model inference, performs tests with MKLDNN or float point precision, iterates through thread settings and precisions, logs results, configures and builds PaddleVideo, sets OpenCV, CUDA, CUDNN directories, checks GPUID, runs inference tests on a list of model directories.",
    "details": [
        {
            "comment": "This script uses Bash to parse input file lines, extracting model information and inference parameters for C++ models. It sources a common function script and then proceeds to parse each line of the input file into various variables like model name, OpenCV usage, C++ inference model directory list, inference command, GPU/MKLDNN/CPU thread settings, etc. These parsed values are stored in different variables for further use.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/test_tipc/test_inference_cpp.sh\":0-28",
            "content": "#!/bin/bash\nsource test_tipc/common_func.sh\nFILENAME=$1\nMODE=$2\ndataline=$(awk 'NR==1, NR==18{print}'  $FILENAME)\n# parser params\nIFS=$'\\n'\nlines=(${dataline})\n# parser cpp inference model\nmodel_name=$(func_parser_value \"${lines[1]}\")\nuse_opencv=$(func_parser_value \"${lines[2]}\")\ncpp_infer_model_dir_list=$(func_parser_value \"${lines[3]}\")\ncpp_infer_is_quant=$(func_parser_value \"${lines[4]}\")\n# parser cpp inference\ninference_cmd=$(func_parser_value \"${lines[5]}\")\ncpp_use_gpu_key=$(func_parser_key \"${lines[6]}\")\ncpp_use_gpu_list=$(func_parser_value \"${lines[6]}\")\ncpp_use_mkldnn_key=$(func_parser_key \"${lines[7]}\")\ncpp_use_mkldnn_list=$(func_parser_value \"${lines[7]}\")\ncpp_cpu_threads_key=$(func_parser_key \"${lines[8]}\")\ncpp_cpu_threads_list=$(func_parser_value \"${lines[8]}\")\ncpp_batch_size_key=$(func_parser_key \"${lines[9]}\")\ncpp_batch_size_list=$(func_parser_value \"${lines[9]}\")\ncpp_use_trt_key=$(func_parser_key \"${lines[10]}\")\ncpp_use_trt_list=$(func_parser_value \"${lines[10]}\")\ncpp_precision_key=$(func_parser_key \"${lines[11]}\")"
        },
        {
            "comment": "This code is setting up variables for running a PaddleVideo model inference using C++. It sets the precision list, infer model key, image directory key and value, and other keys and values required for the benchmarking process. The code also creates a log path for storing results of the C++ inference and prepares to loop through possible GPU usage and MKLDNN configurations.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/test_tipc/test_inference_cpp.sh\":29-57",
            "content": "cpp_precision_list=$(func_parser_value \"${lines[11]}\")\ncpp_infer_model_key=$(func_parser_key \"${lines[12]}\")\ncpp_image_dir_key=$(func_parser_key \"${lines[13]}\")\ncpp_infer_img_dir=$(func_parser_value \"${lines[13]}\")\ncpp_infer_key1=$(func_parser_key \"${lines[14]}\")\ncpp_infer_value1=$(func_parser_value \"${lines[14]}\")\ncpp_benchmark_key=$(func_parser_key \"${lines[15]}\")\ncpp_benchmark_value=$(func_parser_value \"${lines[15]}\")\ncpp_infer_key2=$(func_parser_key \"${lines[16]}\")\ncpp_infer_value2=$(func_parser_value \"${lines[16]}\")\ncpp_infer_key3=$(func_parser_key \"${lines[17]}\")\ncpp_infer_value3=$(func_parser_value \"${lines[17]}\")\nLOG_PATH=\"./test_tipc/output/${model_name}/${MODE}\"\nmkdir -p ${LOG_PATH}\nstatus_log=\"${LOG_PATH}/results_cpp.log\"\nfunction func_cpp_inference(){\n    IFS='|'\n    _script=$1\n    _model_dir=$2\n    _log_path=$3\n    _img_dir=$4\n    _flag_quant=$5\n    # inference\n    for use_gpu in ${cpp_use_gpu_list[*]}; do\n        if [ ${use_gpu} = \"False\" ] || [ ${use_gpu} = \"cpu\" ]; then\n            for use_mkldnn in ${cpp_use_mkldnn_list[*]}; do"
        },
        {
            "comment": "This code checks if MKLDNN is not being used and quantized precision is true. If so, it continues without executing the loop. Otherwise, it iterates through different thread settings, batch sizes, and precisions to execute inference tests on CPU using MKLDNN (if enabled) or float point precision (default). Logs are saved with details of parameters used for each run.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/test_tipc/test_inference_cpp.sh\":58-71",
            "content": "                if [ ${use_mkldnn} = \"False\" ] && [ ${_flag_quant} = \"True\" ]; then\n                    continue\n                fi\n                for threads in ${cpp_cpu_threads_list[*]}; do\n                    for batch_size in ${cpp_batch_size_list[*]}; do\n                        precision=\"fp32\"\n                        if [ ${use_mkldnn} = \"False\" ] && [ ${_flag_quant} = \"True\" ]; then\n                            precison=\"int8\"\n                        fi\n                        _save_log_path=\"${_log_path}/cpp_infer_cpu_usemkldnn_${use_mkldnn}_threads_${threads}_precision_${precision}_batchsize_${batch_size}.log\"\n                        set_infer_data=$(func_set_params \"${cpp_image_dir_key}\" \"${_img_dir}\")\n                        set_benchmark=$(func_set_params \"${cpp_benchmark_key}\" \"${cpp_benchmark_value}\")\n                        set_batchsize=$(func_set_params \"${cpp_batch_size_key}\" \"${batch_size}\")\n                        set_cpu_threads=$(func_set_params \"${cpp_cpu_threads_key}\" \"${threads}\")"
        },
        {
            "comment": "This code is iterating over different model names and configurations, setting various parameters such as GPU usage and thread count. It then executes a command to run inference on the model and saves the log file. The script checks the status of the execution and logs any errors or warnings for debugging purposes.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/test_tipc/test_inference_cpp.sh\":72-84",
            "content": "                        set_model_dir=$(func_set_params \"${cpp_infer_model_key}\" \"${_model_dir}\")\n                        set_infer_params1=$(func_set_params \"${cpp_infer_key1}\" \"${cpp_infer_value1}\")\n                        set_infer_params2=$(func_set_params \"${cpp_infer_key2}\" \"${cpp_infer_value2}\")\n                        set_infer_params3=$(func_set_params \"${cpp_infer_key3}\" \"${cpp_infer_value3}\")\n                        command=\"${_script} ${cpp_use_gpu_key}=${use_gpu} ${cpp_use_mkldnn_key}=${use_mkldnn} ${set_cpu_threads} ${set_model_dir} ${set_batchsize} ${set_infer_data} ${set_benchmark} ${set_infer_params1} ${set_infer_params2} ${set_infer_params3} > ${_save_log_path} 2>&1 \"\n                        eval $command\n                        last_status=${PIPESTATUS[0]}\n                        eval \"cat ${_save_log_path}\"\n                        status_check $last_status \"${command}\" \"${status_log}\" \"${model_name}\"\n                    done\n                done\n            done\n        elif [ ${use_gpu} = \"True\" ] || [ ${use_gpu} = \"gpu\" ]; then"
        },
        {
            "comment": "The code snippet is performing nested loops to iterate over different combinations of TensorRT (TRT) usage and precision options. It checks specific conditions using if statements, such as avoiding quantized precision with non-quantized flag set or excluding certain combinations based on TRT and precision values. Finally, it sets variables for the log path, input data parameters, benchmark value, and batch size before potentially executing further code within these loops.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/test_tipc/test_inference_cpp.sh\":85-100",
            "content": "            for use_trt in ${cpp_use_trt_list[*]}; do\n                for precision in ${cpp_precision_list[*]}; do\n                    if [[ ${_flag_quant} = \"False\" ]] && [[ ${precision} =~ \"int8\" ]]; then\n                        continue\n                    fi\n                    if [[ ${precision} =~ \"fp16\" || ${precision} =~ \"int8\" ]] && [ ${use_trt} = \"False\" ]; then\n                        continue\n                    fi\n                    if [[ ${use_trt} = \"False\" || ${precision} =~ \"int8\" ]] && [ ${_flag_quant} = \"True\" ]; then\n                        continue\n                    fi\n                    for batch_size in ${cpp_batch_size_list[*]}; do\n                        _save_log_path=\"${_log_path}/cpp_infer_gpu_usetrt_${use_trt}_precision_${precision}_batchsize_${batch_size}.log\"\n                        set_infer_data=$(func_set_params \"${cpp_image_dir_key}\" \"${_img_dir}\")\n                        set_benchmark=$(func_set_params \"${cpp_benchmark_key}\" \"${cpp_benchmark_value}\")\n                        set_batchsize=$(func_set_params \"${cpp_batch_size_key}\" \"${batch_size}\")"
        },
        {
            "comment": "The code is setting parameters for a TensorRT inference script. It assigns values to various keys and directories before executing the script and saving the output log file. The last status of the command execution is checked, and the log file is displayed if no issues occurred.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/test_tipc/test_inference_cpp.sh\":101-111",
            "content": "                        set_tensorrt=$(func_set_params \"${cpp_use_trt_key}\" \"${use_trt}\")\n                        set_precision=$(func_set_params \"${cpp_precision_key}\" \"${precision}\")\n                        set_model_dir=$(func_set_params \"${cpp_infer_model_key}\" \"${_model_dir}\")\n                        set_infer_params1=$(func_set_params \"${cpp_infer_key1}\" \"${cpp_infer_value1}\")\n                        set_infer_params2=$(func_set_params \"${cpp_infer_key2}\" \"${cpp_infer_value2}\")\n                        set_infer_params3=$(func_set_params \"${cpp_infer_key3}\" \"${cpp_infer_value3}\")\n                        command=\"${_script} ${cpp_use_gpu_key}=${use_gpu} ${set_tensorrt} ${set_precision} ${set_model_dir} ${set_batchsize} ${set_infer_data} ${set_benchmark} ${set_infer_params1} ${set_infer_params2} ${set_infer_params3} > ${_save_log_path} 2>&1 \"\n                        eval $command\n                        last_status=${PIPESTATUS[0]}\n                        eval \"cat ${_save_log_path}\"\n                        status_check $last_status \"${command}\" \"${status_log}\" \"${model_name}\""
        },
        {
            "comment": "The code checks if the current hardware supports CPU and GPU, and if not, it prints a message. If the OpenCV library is missing or outdated, it downloads the latest version and builds it. It then sets up the installation path for the built OpenCV library.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/test_tipc/test_inference_cpp.sh\":113-145",
            "content": "                    done\n                done\n            done\n        else\n            echo \"Does not support hardware other than CPU and GPU Currently!\"\n        fi\n    done\n}\ncd deploy/cpp_infer\nif [ ${use_opencv} = \"True\" ]; then\n    if [ -d \"opencv-3.4.7/opencv3/\" ] && [ $(md5sum opencv-3.4.7.tar.gz | awk -F ' ' '{print $1}') = \"faa2b5950f8bee3f03118e600c74746a\" ];then\n        echo \"################### build opencv skipped ###################\"\n    else\n        echo \"################### building opencv ###################\"\n        rm -rf opencv-3.4.7.tar.gz opencv-3.4.7/\n        wget https://paddleocr.bj.bcebos.com/dygraph_v2.0/test/opencv-3.4.7.tar.gz\n        tar -xf opencv-3.4.7.tar.gz\n        cd opencv-3.4.7/\n        install_path=$(pwd)/opencv3\n        rm -rf build\n        mkdir build\n        cd build\n        cmake .. \\\n            -DCMAKE_INSTALL_PREFIX=${install_path} \\\n            -DCMAKE_BUILD_TYPE=Release \\\n            -DBUILD_SHARED_LIBS=OFF \\\n            -DWITH_IPP=OFF \\\n            -DBUILD_IPP_IW=OFF \\"
        },
        {
            "comment": "This code sets various CMake flags to configure the build process, then proceeds with making and installing the required libraries. It checks if a directory exists, downloads necessary files if needed, and finally starts building the PaddleVideo demo.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/test_tipc/test_inference_cpp.sh\":146-177",
            "content": "            -DWITH_LAPACK=OFF \\\n            -DWITH_EIGEN=OFF \\\n            -DCMAKE_INSTALL_LIBDIR=lib64 \\\n            -DWITH_ZLIB=ON \\\n            -DBUILD_ZLIB=ON \\\n            -DWITH_JPEG=ON \\\n            -DBUILD_JPEG=ON \\\n            -DWITH_PNG=ON \\\n            -DBUILD_PNG=ON \\\n            -DWITH_TIFF=ON \\\n            -DBUILD_TIFF=ON \\\n            -DWITH_FFMPEG=ON\n        make -j\n        make install\n        cd ../\n        echo \"################### building opencv finished ###################\"\n    fi\nfi\nif [ !-d \"paddle_inference\" ]; then\n    echo \"################### download inference lib skipped ###################\"\nelse\n    echo \"################### downloading inference lib ###################\"\n    wget -nc https://paddle-inference-lib.bj.bcebos.com/2.1.1-gpu-cuda10.1-cudnn7-mkl-gcc8.2/paddle_inference.tgz\n    tar -xf paddle_inference.tgz\n    echo \"################### downloading inference lib finished ###################\"\nfi\necho \"################### building PaddleVideo demo ####################\"\nif [ ${use_opencv} = \"True\" ]; then"
        },
        {
            "comment": "This code is configuring and building PaddleVideo, setting up OpenCV, CUDA, and CUDNN directories, and preparing for running test inference. It also checks if GPUID is set and sets the CUDA_VISIBLE_DEVICES environment variable accordingly. Finally, it loops through a list of model directories to run inference tests.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/test_tipc/test_inference_cpp.sh\":178-224",
            "content": "    OPENCV_DIR=$(pwd)/opencv-3.4.7/opencv3\nelse\n    OPENCV_DIR=''\nfi\nLIB_DIR=$(pwd)/paddle_inference\nCUDA_LIB_DIR=$(dirname `find /usr -name libcudart.so`)\nCUDNN_LIB_DIR=$(dirname `find /usr -name libcudnn.so`)\nBUILD_DIR=build\nrm -rf ${BUILD_DIR}\nmkdir ${BUILD_DIR}\ncd ${BUILD_DIR}\ncmake .. \\\n    -DPADDLE_LIB=${LIB_DIR} \\\n    -DWITH_MKL=ON \\\n    -DWITH_GPU=OFF \\\n    -DWITH_STATIC_LIB=OFF \\\n    -DWITH_TENSORRT=OFF \\\n    -DOPENCV_DIR=${OPENCV_DIR} \\\n    -DCUDNN_LIB=${CUDNN_LIB_DIR} \\\n    -DCUDA_LIB=${CUDA_LIB_DIR} \\\n    -DTENSORRT_DIR=${TENSORRT_DIR} \\\nmake -j\ncd ../../../\necho \"################### building PaddleVideo demo finished ###################\"\n# set cuda device\nGPUID=$2\nif [ ${#GPUID} -le 0 ];then\n    env=\" \"\nelse\n    env=\"export CUDA_VISIBLE_DEVICES=${GPUID}\"\nfi\nset CUDA_VISIBLE_DEVICES\neval $env\necho \"################### running test ###################\"\nexport Count=0\nIFS=\"|\"\ninfer_quant_flag=(${cpp_infer_is_quant})\nfor infer_model in ${cpp_infer_model_dir_list[*]}; do\n    #run inference\n    is_quant=${infer_quant_flag[Count]}"
        },
        {
            "comment": "This code snippet is calling a function \"func_cpp_inference\" to execute inference commands, incrementing the Count variable on each iteration of a loop. The function is called with input parameters for the command, model path, log path, image directory, and a quantization flag.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/test_tipc/test_inference_cpp.sh\":225-227",
            "content": "    func_cpp_inference \"${inference_cmd}\" \"${infer_model}\" \"${LOG_PATH}\" \"${cpp_infer_img_dir}\" ${is_quant}\n    Count=$(($Count + 1))\ndone"
        }
    ]
}