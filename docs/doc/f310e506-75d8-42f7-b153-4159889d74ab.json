{
    "summary": "The code defines a VisionTransformer class, ToShiftVIT model and TokenShiftVisionTransformer for image processing with attention blocks, positional embeddings, dropout and normalization layers. It also supports pretrained checkpoints.",
    "details": [
        {
            "comment": "The code defines a class for VisionTransformer backbones and imports necessary libraries. It includes functions like `to_2tuple` and `drop_path` for processing input data and implementing drop path operation, respectively. The code also handles initialization of zero and one constants.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/toshift_vit.py\":0-36",
            "content": "# Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom collections.abc import Callable\nimport numpy as np\nimport paddle\nimport paddle.nn as nn\nimport paddle.nn.functional as F\nfrom paddle.nn.initializer import Constant\nfrom ...utils import load_ckpt\nfrom ..registry import BACKBONES\nfrom ..weight_init import trunc_normal_\n__all__ = ['VisionTransformer']\nzeros_ = Constant(value=0.)\nones_ = Constant(value=1.)\ndef to_2tuple(x):\n    return tuple([x] * 2)\ndef drop_path(x, drop_prob=0., training=False):"
        },
        {
            "comment": "Code implements Drop Paths (Stochastic Depth) for residual blocks. The function applies dropout probabilistically, and the class `DropPath` handles it during forward pass. `Identity` class serves as an identity mapping.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/toshift_vit.py\":37-64",
            "content": "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n    # issuecomment-532968956 ...\n    See discussion: https://github.com/tensorflow/tpu/issues/494\n    \"\"\"\n    if drop_prob == 0. or not training:\n        return x\n    keep_prob = paddle.to_tensor(1 - drop_prob)\n    shape = (paddle.shape(x)[0], ) + (1, ) * (x.ndim - 1)\n    random_tensor = keep_prob + paddle.rand(shape).astype(x.dtype)\n    random_tensor = paddle.floor(random_tensor)  # binarize\n    output = x.divide(keep_prob) * random_tensor\n    return output\nclass DropPath(nn.Layer):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n    \"\"\"\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\nclass Identity(nn.Layer):"
        },
        {
            "comment": "This code defines three classes: Identity, Mlp, and Attention. Identity is a simple class that returns its input unchanged. Mlp stands for Multi-Layer Perceptron and defines a feedforward neural network layer with optional hidden layers. The Attention class implements a self-attention mechanism commonly used in transformer models. It initializes the necessary parameters and applies dropout to the input and output of the attention operation.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/toshift_vit.py\":65-103",
            "content": "    def __init__(self):\n        super(Identity, self).__init__()\n    def forward(self, input):\n        return input\nclass Mlp(nn.Layer):\n    def __init__(self,\n                 in_features,\n                 hidden_features=None,\n                 out_features=None,\n                 act_layer=nn.GELU,\n                 drop=0.0):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\nclass Attention(nn.Layer):\n    def __init__(self,\n                 dim,\n                 num_heads=8,\n                 qkv_bias=False,\n                 qk_scale=None,\n                 attn_drop=0.0,\n                 proj_drop=0.0):"
        },
        {
            "comment": "The code defines a class representing a self-attention module, with parameters like dimension (dim), number of heads (num_heads), and optional bias for the QKV linear layer (qkv_bias). The class initializes these attributes and defines its forward function to compute attention scores and output.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/toshift_vit.py\":104-137",
            "content": "        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim**-0.5\n        self.qkv = nn.Linear(dim, dim * 3, bias_attr=qkv_bias)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n        self.attn_drop = nn.Dropout(attn_drop)\n    def forward(self, x):\n        N, C = x.shape[1:]\n        qkv = self.qkv(x).reshape(\n            (-1, N, 3, self.num_heads, C // self.num_heads)).transpose(\n                (2, 0, 3, 1, 4))\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        attn = (q.matmul(k.transpose((0, 1, 3, 2)))) * self.scale\n        attn = nn.functional.softmax(attn, axis=-1)\n        attn = self.attn_drop(attn)\n        x = (attn.matmul(v)).transpose((0, 2, 1, 3)).reshape((-1, N, C))\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\nclass Block(nn.Layer):\n    def __init__(self,\n                 dim,\n                 num_heads,\n                 mlp_ratio=4.0,\n                 qkv_bias=False,"
        },
        {
            "comment": "The code above initializes an object with multiple parameters such as num_segments, fold_div, norm_layer, and attention_type. It also creates a norm1 layer based on the type of norm_layer provided (either a string or a Callable). If norm_layer is a string, it uses eval() to call the specified class, otherwise if it's a Callable, it directly initializes the layer with that function.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/toshift_vit.py\":138-163",
            "content": "                 qk_scale=None,\n                 drop=0.0,\n                 attn_drop=0.0,\n                 drop_path=0.1,\n                 act_layer=nn.GELU,\n                 norm_layer='nn.LayerNorm',\n                 epsilon=1e-5,\n                 num_segments = 8,\n                 fold_div = 4):\n                #attention_type='divided_space_time',\n        super().__init__()\n        self.n_seg = num_segments       #ckk\n        self.foldP_div = fold_div       #ckk\n        #self.attention_type = attention_type\n        if isinstance(norm_layer, str):\n            self.norm1 = eval(norm_layer)(dim, epsilon=epsilon)\n        elif isinstance(norm_layer, Callable):\n            self.norm1 = norm_layer(dim, epsilon=epsilon)\n        else:\n            raise TypeError(\n                \"The norm_layer must be str or paddle.nn.layer.Layer class\")\n        self.attn = Attention(dim,\n                              num_heads=num_heads,\n                              qkv_bias=qkv_bias,\n                              qk_scale=qk_scale,"
        },
        {
            "comment": "This code initializes the Temporal Attention parameters for the model. If the attention_type is 'divided_space_time', it creates a temporal normalization layer and an attention layer, as well as a fully connected layer for the temporal branch of the model. Drop paths are used for stochastic depth to reduce overfitting.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/toshift_vit.py\":164-185",
            "content": "                              attn_drop=attn_drop,\n                              proj_drop=drop)\n        # Temporal Attention Parameters\n        '''\n        if self.attention_type == 'divided_space_time':\n            if isinstance(norm_layer, str):\n                self.temporal_norm1 = eval(norm_layer)(dim, epsilon=epsilon)\n            elif isinstance(norm_layer, Callable):\n                self.temporal_norm1 = norm_layer(dim, epsilon=epsilon)\n            else:\n                raise TypeError(\n                    \"The norm_layer must be str or paddle.nn.layer.Layer class\")\n            self.temporal_attn = Attention(dim,\n                                           num_heads=num_heads,\n                                           qkv_bias=qkv_bias,\n                                           qk_scale=qk_scale,\n                                           attn_drop=attn_drop,\n                                           proj_drop=drop)\n            self.temporal_fc = nn.Linear(dim, dim)\n        '''\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here"
        },
        {
            "comment": "This code initializes a ToShift ViT model. It creates a drop path layer, normalization layer, and MLP based on the given parameters. The `shuift_tk` function performs token shifting by reshaping the input, creating a mask with stop gradient attribute, and element-wise adding it to the original input. This helps in improving the model's performance by reducing the effect of irrelevant tokens during training.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/toshift_vit.py\":186-212",
            "content": "        self.drop_path = DropPath(drop_path) if drop_path > 0. else Identity()\n        if isinstance(norm_layer, str):\n            self.norm2 = eval(norm_layer)(dim, epsilon=epsilon)\n        elif isinstance(norm_layer, Callable):\n            self.norm2 = norm_layer(dim, epsilon=epsilon)\n        else:\n            raise TypeError(\n                \"The norm_layer must be str or paddle.nn.layer.Layer class\")\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim,\n                       hidden_features=mlp_hidden_dim,\n                       act_layer=act_layer,\n                       drop=drop)\n    # token_shift\n    def shuift_tk(self, x):\n        t = self.n_seg\n        bt, n, c = x.shape\n        b = bt // t\n        x = x.reshape([b, t, n, c]) #B T N C\n        fold = c // self.foldP_div\n        out = paddle.zeros_like(x)\n        out.stop_gradient = True\n        # print(\"#### fold \", fold)\n        # print(out.shape)\n        # print(x[:, 1:, 0, :fold].unsqueeze(2).shape)\n        # print(out[:, :-1, 0:1, :fold].shape)"
        },
        {
            "comment": "This code defines a \"ToshiftVIT\" class, which appears to be a custom backbone for a Vision Transformer model. It includes a forward function that applies shift and drop path operations on the input, as well as a PatchEmbed class for image-to-patch embedding. The ToshiftVIT class also has an unknown \"shuift_tk\" function that seems to be used in the forward pass.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/toshift_vit.py\":213-244",
            "content": "        # exit(0)\n        out[:, :-1, 0, :fold] = x[:, 1:, 0, :fold] # shift left\n        out[:, 1:,  0, fold:2*fold] = x[:,:-1:, 0, fold:2*fold]\n        out[:, :, 1:, :2*fold] = x[:, :, 1:, :2*fold]\n        out[:, :, :, 2*fold:] = x[:, :, :, 2*fold:]\n        return out.reshape([bt, n, c])\n    def forward(self, x):\n        x = self.shuift_tk(x)\n        x = x + self.drop_path(self.attn(self.norm1(x)))\n        x = self.shuift_tk(x)\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\nclass PatchEmbed(nn.Layer):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n    def __init__(self,\n                 img_size=224,\n                 patch_size=16,\n                 in_channels=3,\n                 embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] //\n                                                        patch_size[0])\n        self.img_size = img_size\n        self.patch_size = patch_size"
        },
        {
            "comment": "This code defines a TokenShiftVisionTransformer class, which is a type of Vision Transformer model that supports patch input. The class has an initialization function where it sets the number of patches and creates a projection layer. It also includes a forward function for processing input data through the model. The assert statement ensures the input image size matches the expected model dimensions. The @BACKBONES.register() decorator registers the model with other backbones in the codebase.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/toshift_vit.py\":245-277",
            "content": "        self.num_patches = num_patches\n        self.proj = nn.Conv2D(in_channels,\n                              embed_dim,\n                              kernel_size=patch_size,\n                              stride=patch_size)\n    def forward(self, x):\n        B, C, T, H, W = x.shape\n        assert H == self.img_size[0] and W == self.img_size[1], \\\n            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = x.transpose((0, 2, 1, 3, 4))\n        x = x.reshape([-1, C, H, W])\n        x = self.proj(x)\n        W = x.shape[-1]\n        x = x.flatten(2).transpose((0, 2, 1))\n        return x, T, W\n@BACKBONES.register()\nclass TokenShiftVisionTransformer(nn.Layer):\n    \"\"\" Vision Transformer with support for patch input\n    \"\"\"\n    def __init__(self,\n                 pretrained=None,\n                 img_size=224,\n                 patch_size=16,\n                 in_channels=3,\n                 embed_dim=768,\n                 depth=12,\n                 num_heads=12,\n                 mlp_ratio=4,"
        },
        {
            "comment": "This code is initializing a class for the Toshift ViT backbone. It sets parameters such as pretrained, num_seg, attention_type, embed_dim, and others. It creates PatchEmbed and positional embeddings (cls_token and pos_embed). The code also calculates the number of patches.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/toshift_vit.py\":278-304",
            "content": "                 qkv_bias=False,\n                 qk_scale=None,\n                 drop_rate=0,\n                 attn_drop_rate=0.,\n                 drop_path_rate=0.1,\n                 norm_layer='nn.LayerNorm',\n                 epsilon=1e-5,\n                 num_seg=8,\n                 attention_type='divided_space_time',\n                 **args):\n        super().__init__()\n        self.pretrained = pretrained\n        self.num_seg = num_seg\n        self.attention_type = attention_type\n        self.num_features = self.embed_dim = embed_dim\n        self.patch_embed = PatchEmbed(img_size=img_size,\n                                      patch_size=patch_size,\n                                      in_channels=in_channels,\n                                      embed_dim=embed_dim)\n        num_patches = self.patch_embed.num_patches\n        # Positional Embeddings\n        self.cls_token = self.create_parameter(shape=(1, 1, embed_dim),\n                                               default_initializer=zeros_)\n        self.pos_embed = self.create_parameter(shape=(1, num_patches + 1,"
        },
        {
            "comment": "This code initializes and sets up the parameters for a Transformer-based backbone model. It creates positional embeddings, dropout layers, and a list of transformer blocks with specified dimensions, numbers of heads, ratios, biases, scale factors, drop rates, attn drop rates, and drop path rates. These parameters are used to build the network architecture for processing data in downstream tasks.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/toshift_vit.py\":305-329",
            "content": "                                                      embed_dim),\n                                               default_initializer=zeros_)\n        self.pos_drop = nn.Dropout(p=drop_rate)\n        if self.attention_type != 'space_only':\n            self.time_embed = self.create_parameter(shape=(1, num_seg,\n                                                           embed_dim),\n                                                    default_initializer=zeros_)\n            self.time_drop = nn.Dropout(p=drop_rate)\n        self.add_parameter(\"pos_embed\", self.pos_embed)\n        self.add_parameter(\"cls_token\", self.cls_token)\n        dpr = np.linspace(0, drop_path_rate, depth)\n        self.blocks = nn.LayerList([\n            Block(dim=embed_dim,\n                  num_heads=num_heads,\n                  mlp_ratio=mlp_ratio,\n                  qkv_bias=qkv_bias,\n                  qk_scale=qk_scale,\n                  drop=drop_rate,\n                  attn_drop=attn_drop_rate,\n                  drop_path=dpr[i],\n                  norm_layer=norm_layer,"
        },
        {
            "comment": "The code initializes a Toshift_VIT model with the specified number of segments and depth. It sets the attention type to 'divided_space_time' for certain blocks. The model's weights are then initialized using truncated normal distribution and the provided function, and any temporal FC layers in the respective block are set to zero. If a pretrained checkpoint is provided, it will be loaded.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/toshift_vit.py\":330-359",
            "content": "                  epsilon=epsilon,\n                  num_segments= self.num_seg\n                  ) for i in range(depth)\n                #attention_type=self.attention_type\n        ])\n        self.norm = eval(norm_layer)(embed_dim, epsilon=epsilon)\n    def init_weights(self):\n        \"\"\"First init model's weight\"\"\"\n        trunc_normal_(self.pos_embed, std=0.02)\n        trunc_normal_(self.cls_token, std=0.02)\n        self.apply(self._init_fn)\n        if self.attention_type == 'divided_space_time':\n            i = 0\n            for m in self.blocks.sublayers(include_self=True):\n                m_str = str(m)\n                if 'Block' in m_str:\n                    if i > 0:\n                        zeros_(m.temporal_fc.weight)\n                        zeros_(m.temporal_fc.bias)\n                    i += 1\n        \"\"\"Second, if provide pretrained ckpt, load it\"\"\"\n        if isinstance(\n                self.pretrained, str\n        ) and self.pretrained.strip() != \"\":  # load pretrained weights\n            load_ckpt(self,"
        },
        {
            "comment": "The code initializes a TOShiftViT model, defines an initialization function for the layers, and defines a forward_features function to process input feature maps. The function takes the number of patches from the patch embedding layer, expands the class token, concatenates it with the features, and applies positional embeddings if needed.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/toshift_vit.py\":360-385",
            "content": "                      self.pretrained,\n                      num_patches=self.patch_embed.num_patches,\n                      num_seg=self.num_seg,\n                      attention_type=self.attention_type)\n    def _init_fn(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight)\n            if m.bias is not None:\n                zeros_(m.bias)\n        elif isinstance(m, nn.LayerNorm):\n            ones_(m.weight)\n            zeros_(m.bias)\n    def forward_features(self, x):\n        # B = x.shape[0]\n        B = paddle.shape(x)[0]\n        x, T, W = self.patch_embed(x)  # [BT,nH*nW,F]\n        cls_tokens = self.cls_token.expand((B * T, -1, -1))  # [1,1,F]->[BT,1,F]\n        x = paddle.concat((cls_tokens, x), axis=1)\n        pos_interp = (x.shape[1] != self.pos_embed.shape[1])\n        if pos_interp:\n            pos_embed = self.pos_embed\n            cls_pos_embed = pos_embed[0, 0, :].unsqueeze(0).unsqueeze(1)\n            other_pos_embed = pos_embed[0, 1:, :].unsqueeze(0).transpose(\n                (0, 2, 1))"
        },
        {
            "comment": "This function reshapes and interpolates a positional embedding, adding it to the input if specified. It then performs positional dropout before passing through attention blocks and normalization layers, finally returning the forward pass of features.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/toshift_vit.py\":386-412",
            "content": "            P = int(other_pos_embed.shape[2]**0.5)\n            H = x.shape[1] // W\n            other_pos_embed = other_pos_embed.reshape([1, x.shape[2], P, P])\n            new_pos_embed = F.interpolate(other_pos_embed,\n                                          size=(H, W),\n                                          mode='nearest')\n            new_pos_embed = new_pos_embed.flatten(2)\n            new_pos_embed = new_pos_embed.transpose((0, 2, 1))\n            new_pos_embed = paddle.concat((cls_pos_embed, new_pos_embed),\n                                          axis=1)\n            x = x + new_pos_embed\n        else:\n            x = x + self.pos_embed\n        x = self.pos_drop(x)\n        # Attention blocks\n        for blk in self.blocks:\n            x = blk(x)\n        x = self.norm(x)\n        return x[:, 0]  # [B,  embed_dim]  -> [B*T, embed_dim]\n    def forward(self, x):\n        x = self.forward_features(x)\n        return x"
        }
    ]
}