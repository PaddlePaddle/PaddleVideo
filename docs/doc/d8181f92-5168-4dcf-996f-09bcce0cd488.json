{
    "summary": "This code performs data augmentation using resizing, cropping, and scaling/rotating transformations with cv2 libraries, offering fixed or random options. It also initializes segmentation variables, computes dilated areas, generates edge masks, and handles various data types for PaddlePaddle's video object detection task.",
    "details": [
        {
            "comment": "The code defines a Resize class that rescales images in a sample to the given output size. It accepts either an integer for uniform resizing or a tuple for specific dimensions. If the output size is not compatible with the image aspect ratio, it will be scaled proportionally. This class can process samples with one or more images (e.g., 'img1' and 'img2').",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/dataloaders/custom_transforms_f.py\":0-34",
            "content": "import os\nimport random\nimport cv2\nimport numpy as np\nimport paddle\nfrom PIL import Image\nimport dataloaders.helpers as helpers\nfrom davisinteractive.utils.operations import bresenham\nfrom paddle.vision.transforms import functional as F\ncv2.setNumThreads(0)\nNEW_BRANCH = True\nclass Resize(object):\n    \"\"\"Rescale the image in a sample to a given size.\n    Args:\n        output_size (tuple or int): Desired output size. If tuple, output is\n            matched to output_size. If int, smaller of image edges is matched\n            to output_size keeping aspect ratio the same.\n    \"\"\"\n    def __init__(self, output_size):\n        assert isinstance(output_size, (int, tuple))\n        if isinstance(output_size, int):\n            self.output_size = (output_size, output_size)\n        else:\n            self.output_size = output_size\n    #        self.seg_interpolation = cv2.INTER_CUBIC if is_continuous else cv2.INTER_NEAREST\n    #        self.fix = fix\n    def __call__(self, sample):\n        img1 = sample['img1']\n        # img2 = sample['img2']"
        },
        {
            "comment": "This code is a custom transform that resizes images in a sample to a specific output size. It checks if the current image size matches the desired output size, and if not, it uses cv2.resize() function to resize each image in the sample while maintaining aspect ratio for specified elements (img1, img2, ref_img) using INTER_CUBIC interpolation and others using INTER_NEAREST. It returns the modified sample with images resized according to the output size specified. The RandomCrop class is used to crop an image randomly to a given output size.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/dataloaders/custom_transforms_f.py\":35-68",
            "content": "        # ref_img=sample['ref_img']\n        h, w = img1.shape[:2]\n        if self.output_size == (h, w):\n            return sample\n        else:\n            new_h, new_w = self.output_size\n        new_h, new_w = int(new_h), int(new_w)\n        for elem in sample.keys():\n            if 'meta' in elem:\n                continue\n            tmp = sample[elem]\n            if elem == 'img1' or elem == 'img2' or elem == 'ref_img':\n                flagval = cv2.INTER_CUBIC\n            else:\n                flagval = cv2.INTER_NEAREST\n            tmp = cv2.resize(tmp, dsize=(new_w, new_h), interpolation=flagval)\n            sample[elem] = tmp\n        return sample\nclass RandomCrop(object):\n    \"\"\"Crop randomly the image in a sample.\n    Args:\n        output_size (tuple or int): Desired output size. If int, square crop\n            is made.\n    \"\"\"\n    def __init__(self, output_size, step=None):\n        assert isinstance(output_size, (int, tuple))\n        if isinstance(output_size, int):\n            self.output_size = (output_size, output_size)"
        },
        {
            "comment": "This code is part of a custom transform for image cropping. It takes an input sample, selects a random top and left position to crop the image, and checks if the corresponding reference scribble label has enough unique elements to proceed. If not, it continues selecting new positions until it finds one with enough unique elements in the scribble label. The step variable keeps track of how many times this loop has been executed.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/dataloaders/custom_transforms_f.py\":69-97",
            "content": "        else:\n            assert len(output_size) == 2\n            self.output_size = output_size\n        self.step = step\n    def __call__(self, sample):\n        image = sample['img1']\n        h, w = image.shape[:2]\n        new_h, new_w = self.output_size\n        new_h = h if new_h >= h else new_h\n        new_w = w if new_w >= w else new_w\n        is_contain_obj = False\n        if self.step is None:\n            while not is_contain_obj:\n                #                step += 1\n                top = np.random.randint(0, h - new_h + 1)\n                left = np.random.randint(0, w - new_w + 1)\n                ref_scribble_label = sample['ref_scribble_label']\n                new_ref_scribble_label = ref_scribble_label[top:top + new_h,\n                                                            left:left + new_w]\n                if len(np.unique(new_ref_scribble_label)) == 1:\n                    continue\n                else:\n                    for elem in sample.keys():\n                        if 'meta' in elem:"
        },
        {
            "comment": "This code is randomly selecting a region in the image and adjusting it to the new size while ensuring that there is at least one object present in the cropped region. It then updates the corresponding image and label based on this new crop.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/dataloaders/custom_transforms_f.py\":98-123",
            "content": "                            continue\n                        tmp = sample[elem]\n                        tmp = tmp[top:top + new_h, left:left + new_w]\n                        sample[elem] = tmp\n                    break\n        else:\n            st = 0\n            while not is_contain_obj and st < self.step:\n                st += 1\n                top = np.random.randint(0, h - new_h + 1)\n                left = np.random.randint(0, w - new_w + 1)\n                ref_scribble_label = sample['ref_scribble_label']\n                new_ref_scribble_label = ref_scribble_label[top:top + new_h,\n                                                            left:left + new_w]\n                if len(np.unique(\n                        new_ref_scribble_label)) == 1 or st < self.step - 1:\n                    continue\n                else:\n                    for elem in sample.keys():\n                        if 'meta' in elem:\n                            continue\n                        tmp = sample[elem]\n                        tmp = tmp[top:top + new_h, left:left + new_w]"
        },
        {
            "comment": "The code defines a class called \"ScaleNRotate\" which applies scaling and rotation transformations to images and their corresponding ground truth. It takes two possible arguments for rotations and scales, either as tuples or lists. If the argument is a tuple, it randomly selects a rotation and scale within the defined range. If the argument is a list, it applies one of the fixed possible rotations and scales from the provided list. The code also initializes the instance variables \"rots\" and \"scales\" based on the input arguments.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/dataloaders/custom_transforms_f.py\":124-153",
            "content": "                        sample[elem] = tmp\n                    break\n        return sample\nclass ScaleNRotate(object):\n    \"\"\"Scale (zoom-in, zoom-out) and Rotate the image and the ground truth.\n    Args:\n        two possibilities:\n        1.  rots (tuple): (minimum, maximum) rotation angle\n            scales (tuple): (minimum, maximum) scale\n        2.  rots [list]: list of fixed possible rotation angles\n            scales [list]: list of fixed possible scales\n    \"\"\"\n    def __init__(self, rots=(-30, 30), scales=(.75, 1.25)):\n        assert (isinstance(rots, type(scales)))\n        self.rots = rots\n        self.scales = scales\n    def __call__(self, sample):\n        if type(self.rots) == tuple:\n            # Continuous range of scales and rotations\n            rot = (self.rots[1] - self.rots[0]) * random.random() - \\\n                  (self.rots[1] - self.rots[0]) / 2\n            sc = (self.scales[1] - self.scales[0]) * random.random() - \\\n                 (self.scales[1] - self.scales[0]) / 2 + 1\n        elif type(self.rots) == list:"
        },
        {
            "comment": "This code applies random scaling, rotation, and warping to an image and its corresponding metadata. It selects a random scale and rotation from predefined ranges for each element in the sample, adjusting the shape of the image and preserving its center point. The cv2.warpAffine function is used to perform the transformation, using interpolation flags based on whether the original image contains only 0s and 1s or not. Finally, it returns the transformed sample.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/dataloaders/custom_transforms_f.py\":154-188",
            "content": "            # Fixed range of scales and rotations\n            rot = self.rots[random.randint(0, len(self.rots))]\n            sc = self.scales[random.randint(0, len(self.scales))]\n        for elem in sample.keys():\n            if 'meta' in elem:\n                continue\n            tmp = sample[elem]\n            h, w = tmp.shape[:2]\n            center = (w / 2, h / 2)\n            assert (center != 0)  # Strange behaviour warpAffine\n            M = cv2.getRotationMatrix2D(center, rot, sc)\n            if ((tmp == 0) | (tmp == 1)).all():\n                flagval = cv2.INTER_NEAREST\n            else:\n                flagval = cv2.INTER_CUBIC\n            tmp = cv2.warpAffine(tmp, M, (w, h), flags=flagval)\n            sample[elem] = tmp\n        return sample\nclass RandomScale(object):\n    \"\"\"Randomly resize the image and the ground truth to specified scales.\n    Args:\n        scales (list): the list of scales\n    \"\"\"\n    def __init__(self, scales=[0.75, 1, 1.25]):\n        self.scales = scales\n    def __call__(self, sample):"
        },
        {
            "comment": "The code includes classes for resizing, horizontally flipping, and subtracting the mean image from input data. The resizing function adjusts image size based on a randomly chosen scale from a fixed range. The RandomHorizontalFlip class flips images with a probability of 0.5. The SubtractMeanImage class subtracts a pre-calculated mean image from input images, presumably to normalize pixel values.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/dataloaders/custom_transforms_f.py\":190-228",
            "content": "        # Fixed range of scales\n        sc = self.scales[random.randint(0, len(self.scales) - 1)]\n        for elem in sample.keys():\n            if 'meta' in elem:\n                continue\n            tmp = sample[elem]\n            if elem == 'img1' or elem == 'img2' or elem == 'ref_img':\n                flagval = cv2.INTER_CUBIC\n            else:\n                flagval = cv2.INTER_NEAREST\n            tmp = cv2.resize(tmp, None, fx=sc, fy=sc, interpolation=flagval)\n            sample[elem] = tmp\n        return sample\nclass RandomHorizontalFlip(object):\n    \"\"\"Horizontally flip the given image and ground truth randomly with a probability of 0.5.\"\"\"\n    def __init__(self, prob):\n        self.p = prob\n    def __call__(self, sample):\n        if random.random() < self.p:\n            for elem in sample.keys():\n                if 'meta' in elem:\n                    continue\n                tmp = sample[elem]\n                tmp = cv2.flip(tmp, flipCode=1)\n                sample[elem] = tmp\n        return sample\nclass SubtractMeanImage(object):"
        },
        {
            "comment": "This code defines two classes, 'SubtractMeanImage' and 'CustomScribbleInteractive'. The former subtracts the mean from each image in a sample to normalize them. The latter initializes an object for custom scribble interactive functionality with parameters like scribbles, first frame, dilation, nocare_area, bresenham, use_previous_mask, and previous_mask_path.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/dataloaders/custom_transforms_f.py\":229-260",
            "content": "    def __init__(self, mean, change_channels=False):\n        self.mean = mean\n        self.change_channels = change_channels\n    def __call__(self, sample):\n        for elem in sample.keys():\n            if 'image' in elem:\n                if self.change_channels:\n                    sample[elem] = sample[elem][:, :, [2, 1, 0]]\n                sample[elem] = np.subtract(\n                    sample[elem], np.array(self.mean, dtype=np.float32))\n        return sample\n    def __str__(self):\n        return 'SubtractMeanImage' + str(self.mean)\nclass CustomScribbleInteractive(object):\n    def __init__(self,\n                 scribbles,\n                 first_frame,\n                 dilation=9,\n                 nocare_area=None,\n                 bresenham=True,\n                 use_previous_mask=False,\n                 previous_mask_path=None):\n        self.scribbles = scribbles\n        self.dilation = dilation\n        self.nocare_area = nocare_area\n        self.bresenham = bresenham\n        self.first_frame = first_frame"
        },
        {
            "comment": "This code initializes variables for segmentation mask, no-care area, and scribbles. It iterates over the scribbles of a specific frame and determines whether the scribble is foreground or background based on the object ID. The Bresenham algorithm is applied if specified in the configuration to generate all points for each scribble.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/dataloaders/custom_transforms_f.py\":261-287",
            "content": "        self.use_previous_mask = use_previous_mask\n        self.previous_mask_path = previous_mask_path\n    def __call__(self, sample):\n        meta = sample['meta']\n        frame_num = int(meta['frame_id'])\n        im_size = meta['im_size']\n        # Initialize gt to zeros, no-care areas to ones\n        scr_gt = np.zeros(im_size)\n        scr_nocare = np.ones(im_size)\n        mask = np.zeros(im_size)\n        mask_neg = np.zeros(im_size)\n        # Get all the scribbles for the current frame\n        for scribble in self.scribbles[frame_num]:\n            points_scribble = np.round(\n                np.array(scribble['path']) * np.array(\n                    (im_size[1], im_size[0]))).astype(int)\n            if self.bresenham and len(points_scribble) > 1:\n                all_points = bresenham(points_scribble)\n            else:\n                all_points = points_scribble\n            # Check if scribble is of same id to mark as foreground, otherwise as background\n            if scribble['object_id'] == meta['obj_id']:"
        },
        {
            "comment": "This code segment appears to be responsible for generating ground truth (GT) masks from human-drawn scribbles. If the first frame is encountered, it computes dilated foreground and background masks along with a no-care area. It also excludes negative examples from the training set. The mask and nocare_area are computed based on the conditions in the code snippet.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/dataloaders/custom_transforms_f.py\":288-309",
            "content": "                mask[all_points[:, 1] - 1, all_points[:, 0] - 1] = 1\n            else:\n                mask_neg[all_points[:, 1] - 1, all_points[:, 0] - 1] = 1\n        if self.nocare_area is None:\n            nz = np.where(mask > 0)\n            nocare_area = int(.5 * np.sqrt(\n                (nz[0].max() - nz[0].min()) * (nz[1].max() - nz[1].min())))\n        else:\n            nocare_area = 100\n        # In case we are reading the first human annotation round\n        if frame_num == self.first_frame:\n            # Compute dilated foreground, background, and no-care area\n            scr_gt, scr_nocare = helpers.gt_from_scribble(\n                mask, dilation=self.dilation, nocare_area=nocare_area)\n            scr_gt_neg, _ = helpers.gt_from_scribble(mask_neg,\n                                                     dilation=self.dilation,\n                                                     nocare_area=None)\n            # Negative examples included in the training\n            scr_gt[scr_gt_neg > 0] = 0\n            scr_nocare[scr_gt_neg > 0] = 0"
        },
        {
            "comment": "This code computes dilated foreground, background, and no-care area for annotation rounds generated by the robot. It first generates scr_gt_extra and scr_gt_neg using the gt_from_scribble function from helpers module. Then it ignores pixels that are not foreground if use_previous_mask is False. Else, it reads a previous mask image, converts it into float32 format and assigns pixel values greater than 0.8*255 to 1. These computations will be used in the subsequent operations of the code.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/dataloaders/custom_transforms_f.py\":311-329",
            "content": "        # For annotation rounds generated by the robot\n        else:\n            # Compute dilated foreground, background, and no-care area\n            scr_gt_extra, _ = helpers.gt_from_scribble(mask,\n                                                       dilation=self.dilation,\n                                                       nocare_area=None)\n            scr_gt_neg, _ = helpers.gt_from_scribble(mask_neg,\n                                                     dilation=self.dilation,\n                                                     nocare_area=None)\n            # Ignore pixels that are not foreground\n            if not self.use_previous_mask:\n                scr_nocare_extra = 1. - scr_gt_extra\n            else:\n                scr_nocare_extra = \\\n                    (cv2.imread(os.path.join(self.previous_mask_path, meta['seq_name'], str(meta['obj_id']),\n                                             meta['frame_id'] + '.png'), 0) > 0.8 * 255).astype(np.float32)\n            # Negative examples included in training"
        },
        {
            "comment": "This code is part of a data loader in the Ma-Net application. It transforms image and mask data for PaddlePaddle's video object detection task. The code handles scribble ground truth (scribble_gt) and scribble void pixels (scribble_void_pixels), applying necessary adjustments to ensure correct formatting and values. It then uses the ToTensor class to convert ndarrays in samples to tensors, handling color axis swapping due to differences between numpy and PaddlePaddle image formats.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/dataloaders/custom_transforms_f.py\":330-365",
            "content": "            scr_gt_extra[scr_gt_neg > 0] = 0\n            scr_nocare_extra[scr_gt_neg > 0] = 0\n            scr_gt = np.maximum(scr_gt, scr_gt_extra)\n            scr_nocare_extra[scr_gt > 0] = 0\n            scr_nocare = np.minimum(scr_nocare, scr_nocare_extra)\n        sample['scribble_gt'] = scr_gt\n        sample['scribble_void_pixels'] = scr_nocare\n        return sample\nclass ToTensor(object):\n    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n    def __call__(self, sample):\n        for elem in sample.keys():\n            if 'meta' in elem:\n                continue\n            tmp = sample[elem]\n            if tmp.ndim == 2:\n                tmp = tmp[:, :, np.newaxis]\n            else:\n                tmp = tmp / 255.\n                tmp -= (0.485, 0.456, 0.406)\n                tmp /= (0.229, 0.224, 0.225)\n            # swap color axis because\n            # numpy image: H x W x C\n            # paddle image: C X H X W\n            tmp = tmp.transpose([2, 0, 1])\n            sample[elem] = paddle.to_tensor(tmp)\n        return sample"
        },
        {
            "comment": "The code defines two classes, `GenerateEdge` and `GenerateEdge_2`, which generate edge masks from the input samples. The edge masks are generated based on whether there is a \"label2\" or \"ref_frame_gt\" present in the sample. If these labels are present, a kernel is applied to create an edge mask, which is then added to the sample. If they are not present, a RuntimeError is raised.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/dataloaders/custom_transforms_f.py\":368-404",
            "content": "class GenerateEdge(object):\n    \"\"\"\n    \"\"\"\n    def __init__(self, edgesize=1):\n        self.edgesize = edgesize\n    def __call__(self, sample):\n        \"\"\"\n        \"\"\"\n        if \"label2\" in sample:\n            label2 = sample['label2']\n            kernel_size = 2 * self.edgesize + 1\n            maskedge = np.zeros_like(label2)\n            maskedge[np.where(label2[:, 1:] != label2[:, :-1])] = 1\n            maskedge[np.where(label2[1:, :] != label2[:-1, :])] = 1\n            maskedge = cv2.dilate(\n                maskedge, np.ones((kernel_size, kernel_size), dtype=np.uint8))\n            sample[\"edge_mask\"] = maskedge\n        else:\n            raise RuntimeError(\n                \"We need parsing mask to generate the edge mask.\")\n        return sample\nclass GenerateEdge_2(object):\n    \"\"\"\n    \"\"\"\n    def __init__(self, edgesize=1):\n        self.edgesize = edgesize\n    def __call__(self, sample):\n        \"\"\"\n        \"\"\"\n        if \"ref_frame_gt\" in sample:\n            label2 = sample['ref_frame_gt']\n            kernel_size = 2 * self.edgesize + 1"
        },
        {
            "comment": "This code checks if a parsing mask is provided. If it is, it creates an edge mask by comparing the labels horizontally and vertically. It then dilates the resulting mask using cv2.dilate and assigns it to sample[\"edge_mask\"]. If no parsing mask is provided, it raises a RuntimeError.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/Ma-Net/dataloaders/custom_transforms_f.py\":405-415",
            "content": "            maskedge = np.zeros_like(label2)\n            maskedge[np.where(label2[:, 1:] != label2[:, :-1])] = 1\n            maskedge[np.where(label2[1:, :] != label2[:-1, :])] = 1\n            maskedge = cv2.dilate(\n                maskedge, np.ones((kernel_size, kernel_size), dtype=np.uint8))\n            sample[\"edge_mask\"] = maskedge\n        else:\n            raise RuntimeError(\n                \"We need parsing mask to generate the edge mask.\")\n        return sample"
        }
    ]
}