{
    "summary": "The Video-Swin-Transformer model achieves SOTA accuracy on Kinetics-400, offering multi-scale modeling, efficient local attention features, and mixed-precision training. Code provides data prep, training, testing, and inference instructions for 8 GPUs, with pre-trained Swin-Transformer models available in PaddleVideo.",
    "details": [
        {
            "comment": "This is a model card for the Video-Swin-Transformer video classification model, based on Swin Transformer. It utilizes multi-scale modeling and efficient local attention features to achieve SOTA accuracy on Kinetics-400 dataset. The code provides information about data preparation, training, testing, and inference.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/videoswin.md\":0-32",
            "content": "[\u7b80\u4f53\u4e2d\u6587](../../../zh-CN/model_zoo/recognition/videoswin.md) | English\n# Video-Swin-Transformer Video Classification Model\n## content\n- [Introduction](#Introduction)\n- [Data](#DATA)\n- [Train](#Train)\n- [Test](#Test)\n- [Inference](#Inference)\n- [Reference](#Reference)\n## Introduction\nVideo-Swin-Transformer is a video classification model based on Swin Transformer. It utilizes Swin Transformer's multi-scale modeling and efficient local attention characteristics. It currently achieves SOTA accuracy on the Kinetics-400 data set, surpassing the same transformer structure. The TimeSformer model.\n![VideoSwin](../../../images/videoswin.jpg)\n## DATA\nK400 data download and preparation please refer to [Kinetics-400 data preparation](../../dataset/k400.md)\n## Train\n### Kinetics-400 data set training\n#### Download and add pre-trained models\n1. Download the image pre-training model [swin_base_patch4_window7_224.pdparams](https://videotag.bj.bcebos.com/PaddleVideo-release2.2/swin_base_patch4_window7_224.pdparams) as the Backbone initialization parameter, or download it through the wget command"
        },
        {
            "comment": "This code provides the steps to download a pretrained VideoSwin model, update its configuration file with the downloaded path, and finally start training it on the Kinetics400 dataset using 8 GPUs.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/videoswin.md\":34-59",
            "content": "   ```bash\n   wget https://videotag.bj.bcebos.com/PaddleVideo-release2.2/swin_base_patch4_window7_224.pdparams # ImageNet pretrained model for VideoSwin_base\n   # wget https://videotag.bj.bcebos.com/PaddleVideorelease2.2/swin_small_patch4_window7_224.pdparams # Imagenet pretrained model for VideoSwin_small\n   ```\n2. Open `configs/recognition/videoswin/videoswin_base_k400_videos.yaml`, and fill in the downloaded weight storage path below `pretrained:`\n    ```yaml\n    MODEL:\n        framework: \"RecognizerTransformer\"\n        backbone:\n            name: \"SwinTransformer3D\"\n            pretrained: fill in the path here\n    ```\n#### Start training\n- The Kinetics400 data set uses 8 cards for training, and the start command of the training method is as follows:\n    ```bash\n    # videos data format\n    python3.7 -u -B -m paddle.distributed.launch --gpus=\"0,1,2,3,4,5,6,7\" --log_dir=log_videoswin_base main.py --validate -c configs/recognition/video_swin_transformer/videoswin_base_k400_videos.yaml\n    ```\n- Turn o"
        },
        {
            "comment": "The code sets up mixed-precision training with specific flags for faster processing. It also provides command for running the PaddleVideo model, specifically Video-Swin-Transformer, on GPUs and customizable configuration files. The accuracy is verified during training by checking for the \"best\" keyword in the log.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/videoswin.md\":59-74",
            "content": "n amp mixed-precision training to speed up the training process. The training start command is as follows:\n    ```bash\n    export FLAGS_conv_workspace_size_limit=800 # MB\n    export FLAGS_cudnn_exhaustive_search=1\n    export FLAGS_cudnn_batchnorm_spatial_persistent=1\n    # videos data format\n    python3.7 -u -B -m paddle.distributed.launch --gpus=\"0,1,2,3,4,5,6,7\" --log_dir=log_videoswin_base main.py --amp --validate -c configs/recognition/videoswin/videoswin_base_k400_videos.yaml\n    ```\n- In addition, you can customize and modify the parameter configuration to achieve the purpose of training/testing on different data sets. It is recommended that the naming method of the configuration file is `model_dataset name_file format_data format_sampling method.yaml` , Please refer to [config](../../../zh-CN/contribute/config.md) for parameter usage.\n## Test\n- The Video-Swin-Transformer model is verified during training. You can find the keyword `best` in the training log to obtain the model test accuracy. The log example is as follows:"
        },
        {
            "comment": "Code snippet shows how to test the best Video-Swin-Transformer model after training, using a different sampling method (UniformCrop) for improved accuracy. The command provided demonstrates how to execute the test with specific configuration settings and input files, resulting in evaluation metrics on the Kinetics-400 validation dataset.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/videoswin.md\":76-88",
            "content": "  ```log\n  Already save the best model (top1 acc)0.7258\n  ```\n- Since the sampling method of the Video-Swin-Transformer model test mode is a bit slower but more accurate **UniformCrop**, which is different from the **CenterCrop** used in the verification mode during the training process, so the verification recorded in the training log The index `topk Acc` does not represent the final test score, so after the training is completed, you can use the test mode to test the best model to obtain the final index. The command is as follows:\n  ```bash\n  python3.7 -B -m paddle.distributed.launch --gpus=\"0,1,2,3,4,5,6,7\" --log_dir=log_videoswin_base main.py --test -c configs/recognition/video_swin_transformer/videoswin_base_k400_videos.yaml -w \"output/VideoSwin_base/VideoSwin_base_best.pdparams\"\n  ```\n  When the test configuration uses the following parameters, the test indicators on the validation data set of Kinetics-400 are as follows:\n   |        backbone        | Sampling method | num_seg | target_s"
        },
        {
            "comment": "The table displays pre-trained model checkpoints for Swin-Transformer in PaddleVideo's model zoo, including the model size, input image size, top-1 accuracy, and corresponding URLs for downloading the pdparams files.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/videoswin.md\":88-91",
            "content": "ize | Top-1 |                                                        checkpoints                                                         | pretrain model |\n   | :--------------------: | :-------------: | :-----: | :---------: | :---- | :------------------------------------------------------------------------------------------------------------------------: | :----: |\n   | Swin-Transformer_base  |   UniformCrop   |   32    |     224     | 82.40 |  [SwinTransformer_k400_base.pdparams](https://videotag.bj.bcebos.com/PaddleVideo-release2.2/VideoSwin_base_k400.pdparams)  | [swin_base_patch4_window7_224.pdparams](https://videotag.bj.bcebos.com/PaddleVideo-release2.2/swin_base_patch4_window7_224.pdparams) |\n   | Swin-Transformer_small |   UniformCrop   |   32    |     224     | 80.18 | [SwinTransformer_k400_small.pdparams](https://videotag.bj.bcebos.com/PaddleVideo-release2.2/VideoSwin_small_k400.pdparams) | [swin_small_patch4_window7_224.pdparams](https://videotag.bj.bcebos.com/PaddleVideo-release2.2/swin_small_patch4_window7_224.pdparams) |"
        },
        {
            "comment": "This code snippet provides instructions for exporting an inference model and using the predictive engine inference in PaddleVideo. The first command generates the necessary files (`.pdmodel` and `.pdiparams`) required for prediction, while the second command performs the actual prediction on a given input video file with specified configuration and model files.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/videoswin.md\":93-116",
            "content": "## Inference\n### Export inference model\n```bash\npython3.7 tools/export_model.py -c configs/recognition/videoswin/videoswin_base_k400_videos.yaml \\\n                                -p data/VideoSwin_base_k400.pdparams \\\n                                -o inference/VideoSwin_base\n```\nThe above command will generate the model structure file `VideoSwin_base.pdmodel` and the model weight file `VideoSwin_base.pdiparams` required for prediction.\n- For the meaning of each parameter, please refer to [Model Inference](../../usage.md#2-infer)\n### Use predictive engine inference\n```bash\npython3.7 tools/predict.py --input_file data/example.avi \\\n                           --config configs/recognition/videoswin/videoswin_base_k400_videos.yaml \\\n                           --model_file inference/VideoSwin_base/VideoSwin_base.pdmodel \\\n                           --params_file inference/VideoSwin_base/VideoSwin_base.pdiparams \\\n                           --use_gpu=True \\\n                           --use_tensorrt=False\n```"
        },
        {
            "comment": "This code showcases an example of using the Video-Swin-Transformer model trained on Kinetics-400 to predict a video file. The output includes the top-1 class and score, and referring to the category id and name correspondence table allows for identifying the predicted category name.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/videoswin.md\":118-130",
            "content": "The output example is as follows:\n```log\nCurrent video file: data/example.avi\n        top-1 class: 5\n        top-1 score: 0.9999829530715942\n```\nIt can be seen that using the Video-Swin-Transformer model trained on Kinetics-400 to predict `data/example.avi`, the output top1 category id is `5`, and the confidence is 0.99. By referring to the category id and name correspondence table `data/k400/Kinetics-400_label_list.txt`, it can be known that the predicted category name is `archery`.\n## Reference\n- [Video Swin Transformer](https://arxiv.org/pdf/2106.13230.pdf), Ze Liu, Jia Ning, Yue Cao, Yixuan Wei"
        }
    ]
}