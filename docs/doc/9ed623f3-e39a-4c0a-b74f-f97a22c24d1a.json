{
    "summary": "This code explains how to convert dygraph models to static models for inference and deployment using PaddleInference, and provides examples on video inference testing with predict.py and benchmarking. Support for C++ infer and PaddleHub Serving deploy are coming soon.",
    "details": [
        {
            "comment": "The code provides instructions on how to convert a dygraph model to a static model for inference and deployment, as well as testing the exported model using PaddleVideo's test script. The conversion is done using the \"export_model.py\" script with appropriate arguments, and some additional parameters are added for TSM. Refer to official documents for more information.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/tutorials/deployment.md\":0-23",
            "content": "[\u7b80\u4f53\u4e2d\u6587](../../zh-CN/tutorials/deployment.md) | English\n# Inference\n## How to convert dygraph model to static model?\nTo infer and deploy a model, we need export an inference model, or called to_static: `convert dygraph model to static model`, at first.\n```python\npython3.7 tools/export_model.py -c config_file -o output_path -p params_file\n```\nNote: In `export_model.py`, It will build a model again, and then loading the prarams. But some init params in the infer phase is different from the train phase.\nwe add `num_seg` for TSM in advanced, please add more params or modify them if it is necessary.\nplease refer to [official documents](https://www.paddlepaddle.org.cn/documentation/docs/zh/develop/guides/04_dygraph_to_static/index_cn.html) for more information.\n## How to test the export model?\nPaddleVideo supports a test script to test the exported model.\n```python\npython3.7 tools/test_export_model.py -p params_file -i inference_folder -c config_file\n```\nWe just print the output shape, please feel free to ex"
        },
        {
            "comment": "This code explains how to use the PaddleInference tool for testing video inference, providing examples on using predict.py and enabling benchmarking. It also mentions that support for C++ infer is coming soon, as well as instructions on using PaddleHub Serving deploy and PaddleLite deploy, which will be added later.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/tutorials/deployment.md\":23-47",
            "content": "tend it. Avtually, only test a video file by PaddleInference can make sure the exported model is right.\n## How to use PaddleInference?\nPaddleVideo supports ```tools/predict.py``` to infer\n```python\npython3.7 tools/predict.py -v example.avi --model_file \"./inference/example.pdmodel\" --params_file \"./inference/example.pdiparams\" --enable_benchmark=False --model=\"example\" --num_seg=8\n ```\n## How to test inference speed?\nPaddleVideo support a script to test inference speed\n```python\npython3.7 tools/predict.py --enable_benchmark=True --model_file=\u6a21\u578b\u6587\u4ef6 --params_file=\u53c2\u6570\u6587\u4ef6\n```\n## How to use C++ infer?\n<sup> coming soon</sup>\n# Deployment\n## How to use PaddleHub Serving deploy?\n<sup> coming soon</sup>\n## How to use PaddleLite deploy?\n<sup> coming soon</sup>"
        }
    ]
}