{
    "summary": "The 2s-AGCN model, an enhanced ST-GCN version for motion recognition, utilizes dual-flow adaptive convolutional networks and focuses on second-order bone data. Code offers test scripts, accuracy results, and download links for models trained on different datasets, with PaddleVideo exporting an action recognition model using AGCN2s.",
    "details": [
        {
            "comment": "This code provides an introduction to the 2s-AGCN model, an improved version of ST-GCN published in CVPR2019. It uses a dual-flow adaptive convolutional network and focuses on the second-order information of bone data for motion recognition.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/agcn2s.md\":0-19",
            "content": "[\u7b80\u4f53\u4e2d\u6587](../../../zh-CN/model_zoo/recognition/2sAGCN.md) | English\n# CTR-GCN\n---\n## Contents\n- [Introduction](#Introduction)\n- [Data](#Data)\n- [Train](#Train)\n- [Test](#Test)\n- [Inference](#Inference)\n- [Reference](#Reference)\n## Introduction\n![\u6a21\u578b\u7ed3\u6784\u56fe](../../../images/agcn2s.png)\n[2s-AGCN](https://openaccess.thecvf.com/content_CVPR_2019/papers/Shi_Two-Stream_Adaptive_Graph_Convolutional_Networks_for_Skeleton-Based_Action_Recognition_CVPR_2019_paper.pdf) is an improved article on ST-GCN published in CVPR2019. It proposes a dual-flow adaptive convolutional network, which improves the shortcomings of the original ST-GCN. In the existing GCN based approach, the topology of the graph is set manually and fixed to all layers and input samples. In addition, the second-order information of bone data (bone length and orientation) is naturally more beneficial and discriminating for motion recognition, which was rarely studied in the methods at that time. Therefore, this paper puts forward a node and bones of tw"
        },
        {
            "comment": "This code provides information on the AGCN2S model, a skeleton-based gesture recognition network. It uses data from NTU-RGBD, with details of its preparation found in another file. The code also outlines how to train the CTR-GCN model on various configurations such as cross-subject and cross-view training scenarios, using bone or joint data. This serves as a guide for running the model's training scripts.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/agcn2s.md\":19-39",
            "content": "o kinds of information fusion based on skeleton shuangliu network, and join in figure convolution adjacency matrix adaptive matrix, a sharp rise in the bones of gesture recognition accuracy, also has laid the foundation for subsequent work (the subsequent basic skeleton gesture recognition are based on the flow of network framework).\n## Data\nData download and processing are consistent with CTR-GCN. For details, please refer to [NTU-RGBD Data Preparation](../../dataset/ntu-rgbd.md)\n## Train\n### Train on NTU-RGBD\nTrain CTR-GCN on NTU-RGBD scripts using single gpu\uff1a\n```bash\n# train cross subject with bone data\npython main.py --validate -c configs/recognition/agcn2s/agcn2s_ntucs_bone.yaml --seed 1\n# train cross subject with joint data\npython main.py --validate -c configs/recognition/agcn2s/agcn2s_ntucs_joint.yaml --seed 1\n# train cross view with bone data\npython main.py --validate -c configs/recognition/agcn2s/agcn2s_ntucv_bone.yaml --seed 1\n# train cross view with joint data\npython main.py --validate -c configs/recognition/agcn2s/agcn2s_ntucv_joint.yaml --seed 1"
        },
        {
            "comment": "The code provides test scripts for the 2s-AGCN model on the NTU-RGB+D dataset, both with cross-subject and cross-view splits. The accuracy results for joint and bone data are given, along with a download link to the training log.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/agcn2s.md\":40-70",
            "content": "```\nconfig file `agcn2s_ntucs_joint.yaml` corresponding to the config of 2s-AGCN on NTU-RGB+D dataset with cross-subject splits.\n## Test\n### Test on NTU-RGB+D\nTest scripts\uff1a\n```bash\n# test cross subject with bone data\npython main.py --test -c configs/recognition/2sagcn/2sagcn_ntucs_bone.yaml -w data/2SAGCN_ntucs_bone.pdparams\n# test cross subject with joint data\npython main.py --test -c configs/recognition/2sagcn/2sagcn_ntucs_joint.yaml -w data/2SAGCN_ntucs_joint.pdparams\n# test cross view with bone data\npython main.py --test -c configs/recognition/2sagcn/2sagcn_ntucv_bone.yaml -w data/2SAGCN_ntucv_bone.pdparams\n# test cross view with joint data\npython main.py --test -c configs/recognition/2sagcn/2sagcn_ntucv_joint.yaml -w data/2SAGCN_ntucv_joint.pdparams\n```\n* Specify the config file with `-c`, specify the weight path with `-w`.\nAccuracy on NTU-RGB+D dataset:\n|                |  CS   |   CV   |\n| :------------: | :---: | :----: |\n| Js-AGCN(joint) | 85.8% | 94.13% |\n| Bs-AGCN(bone)  | 86.7% | 93.9%  |\nTrain log\uff1a[download](https://github.com/ELKYang/2s-AGCN-paddle/tree/main/work_dir/ntu)"
        },
        {
            "comment": "Code snippet contains download links for different checkpoints of the AGCN-2s model trained on various datasets:\n1. ntu_cs_agcn_joint\n2. ntu_cs_agcn_bone\n3. ntu_cv_agcn_joint\n4. ntu_cv_agcn_bone",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/agcn2s.md\":72-78",
            "content": "VisualDL log\uff1a[download](https://github.com/ELKYang/2s-AGCN-paddle/tree/main/runs)\ncheckpoints\uff1a\n|                            CS-Js                             |                            CS-Bs                             |                            CV-Js                             |                            CV-Bs                             |\n| :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |\n| [ntu_cs_agcn_joint](https://github.com/ELKYang/2s-AGCN-paddle/blob/main/weights/ntu_cs_agcn_joint-48-30674.pdparams) | [ntu_cs_agcn_bone](https://github.com/ELKYang/2s-AGCN-paddle/blob/main/weights/ntu_cs_agcn_bone-44-28170.pdparams) | [ntu_cv_agcn_joint](https://github.com/ELKYang/2s-AGCN-paddle/blob/main/weights/ntu_cv_agcn_joint-38-22932.pdparams) | [ntu_cv_agcn_bone](https://github.com/ELKYang/2s-AGCN-paddle/blob/main/weights/ntu_cv_agcn_bone-49-29400.pdparams) |"
        },
        {
            "comment": "This code is exporting and inferring a model for action recognition using PaddleVideo's AGCN2s. It uses the `export_model.py` script to generate an inference model archive, which includes the model architecture file (AGCN2s_ntucs_joint.pdmodel) and parameters file (AGCN2s_ntucs_joint.pdiparams). The `predict.py` script is then used to perform inference on input data with the specified configuration and model files, using GPU if available and disabling TensorRT.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/agcn2s.md\":80-102",
            "content": "## Inference\n### export inference model\n```bash\npython3.7 tools/export_model.py -c configs/recognition/agcn2s/2sagcn_ntucs_joint.yaml \\\n                                -p data/AGCN2s_ntucs_joint.pdparams \\\n                                -o inference/AGCN2s_ntucs_joint\n```\nTo get model architecture file `AGCN2s_ntucs_joint.pdmodel` and parameters file `AGCN2s_ntucs_joint.pdiparams`.\n- Args usage please refer to [Model Inference](https://github.com/PaddlePaddle/PaddleVideo/blob/release/2.0/docs/zh-CN/start.md#2-%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86).\n### infer\n```bash\npython3.7 tools/predict.py --input_file data/example_NTU-RGB-D_sketeton.npy \\\n                           --config configs/recognition/agcn2s/2sagcn_ntucs_joint.yaml \\\n                           --model_file inference/AGCN2s_ntucs_joint/AGCN2s_ntucs_joint.pdmodel \\\n                           --params_file inference/AGCN2s_ntucs_joint/AGCN2s_ntucs_joint.pdiparams \\\n                           --use_gpu=True \\\n                           --use_tensorrt=False"
        },
        {
            "comment": "This code block shows the prediction engine result for the AGCN2S model. It displays an image of the prediction results and references the original paper on Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/agcn2s.md\":103-111",
            "content": "```\n### infer result\n![\u9884\u6d4b\u5f15\u64ce\u63a8\u7406\u7ed3\u679c\u56fe](../../../images/agcn2s_result.png)\n## Reference\n- [Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition](https://openaccess.thecvf.com/content_CVPR_2019/papers/Shi_Two-Stream_Adaptive_Graph_Convolutional_Networks_for_Skeleton-Based_Action_Recognition_CVPR_2019_paper.pdf), Lei Shi and Yifan Zhang and Jian Cheng and Hanqing Lu"
        }
    ]
}