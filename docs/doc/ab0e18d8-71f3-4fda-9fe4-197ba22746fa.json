{
    "summary": "The BMN model, using three modules and the ActivityNet dataset, is trained and inferred for temporal action proposal generation with given commands. The export_model script and predict script are utilized to perform inference, providing logs as examples.",
    "details": [
        {
            "comment": "The code describes the BMN model, which consists of three modules: Base Module, Temporal Evaluation Module, and Proposal Evaluation Module. It uses the ActivityNet dataset for training and provides instructions on how to start the training process using a command.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/localization/bmn.md\":0-34",
            "content": "[\u7b80\u4f53\u4e2d\u6587 ](../../../zh-CN/model_zoo/localization/bmn.md) | English\n# BMN\n---\n## Contents\n- [Introduction](#Introduction)\n- [Data](#Data)\n- [Train](#Train)\n- [Test](#Test)\n- [Inference](#Inference)\n- [Reference](#Reference)\n## Introduction\nBMN model contains three modules: Base Module handles the input feature sequence, and out- puts feature sequence shared by the following two modules; Temporal Evaluation Module evaluates starting and ending probabilities of each location in video to generate boundary probability sequences; Proposal Evaluation Module con- tains the BM layer to transfer feature sequence to BM fea- ture map, and contains a series of 3D and 2D convolutional layers to generate BM confidence map.\n<p align=\"center\">\n<img src=\"../../../images/BMN.png\" height=300 width=400 hspace='10'/> <br />\nBMN Overview\n</p>\n## Data\nWe use ActivityNet dataset to train this model\uff0cdata preparation please refer to [ActivityNet dataset](../../dataset/ActivityNet.md).\n## Train\nYou can start training by such command\uff1a\n```bash"
        },
        {
            "comment": "This code is launching a PaddlePaddle distributed localization model named BMN using 4 GPUs and running it on the provided configuration file. It also provides instructions for testing, specifying the required label file and model path, as well as inference commands to export the architecture and parameters files.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/localization/bmn.md\":35-67",
            "content": "export CUDA_VISIBLE_DEVICES=0,1,2,3\npython -B -m paddle.distributed.launch --gpus=\"0,1,2,3\"  --log_dir=log_bmn main.py  --validate -c configs/localization/bmn.yaml\n```\n## Test\nYou can start testing by such command\uff1a\n```bash\npython main.py --test -c configs/localization/bmn.yaml -w output/BMN/BMN_epoch_00009.pdparams -o DATASET.test_batch_size=1\n```\n- For now, we only support testing with **single card** and `batch_size=1`.\n-  Please download [activity\\_net\\_1\\_3\\_new.json](https://paddlemodels.bj.bcebos.com/video_detection/activity_net_1_3_new.json) label file and specify the path to `METRIC.ground_truth_filename` in config file.\n-  Args `-w` is used to specifiy the model path\uff0cyou can download our model in [BMN.pdparams](https://videotag.bj.bcebos.com/PaddleVideo/BMN/BMN.pdparams)\nTest accuracy in ActivityNet1.3:\n| AR@1 | AR@5 | AR@10 | AR@100 | AUC |\n| :---: | :---: | :---: | :---: | :---: |\n| 33.26 | 49.48 | 56.86 | 75.19 | 67.23% |\n## Inference\n### export inference model\n To get model architecture file `BMN.pdmodel` and parameters file `BMN.pdiparams`, use: "
        },
        {
            "comment": "The code exports the BMN model and runs inference on a set of feature files, producing output segments and scores. The export_model script requires the configuration file, the PDParams file, and outputs an inference folder. The predict script uses the configuration file, two model files, and a list of input feature files to perform inference. It prints the score and segment for each input, with example logs provided.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/localization/bmn.md\":69-95",
            "content": "```bash\npython3.7 tools/export_model.py -c configs/localization/bmn.yaml \\\n                                -p data/BMN.pdparams \\\n                                -o inference/BMN\n```\n- Args usage please refer to [Model Inference](https://github.com/PaddlePaddle/PaddleVideo/blob/release/2.0/docs/zh-CN/start.md#2-%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86).\n### infer\n```bash\npython3.7 tools/predict.py --input_file data/example_feat.list \\\n                           --config configs/localization/bmn.yaml \\\n                           --model_file inference/BMN/BMN.pdmodel \\\n                           --params_file inference/BMN/BMN.pdiparams \\\n                           --use_gpu=True \\\n                           --use_tensorrt=False\n```\nexample of logs:\n```\nBMN Inference results of data/example_feat.npy :\n{'score': 0.7968077063560486, 'segment': [0.0, 122.9877]}\n{'score': 0.49097609519958496, 'segment': [12.423000000000002, 124.23]}\n{'score': 0.21395835280418396, 'segment': [39.7536, 122.9877]}\n{'score': 0.2106524258852005, 'segment': [0.0, 109.3224]}"
        },
        {
            "comment": "The code snippet represents the inference results of BMN (Boundary-Matching Network) for temporal action proposal generation. These results, containing a score and segment information, are saved in the specified directory. The BMN paper reference is provided for further information.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/localization/bmn.md\":96-103",
            "content": "{'score': 0.06876271963119507, 'segment': [23.6037, 114.2916]}\n```\nInference results are saved in `data/bmn/BMN_INFERENCE_results`. \n## Reference\n- [BMN: Boundary-Matching Network for Temporal Action Proposal Generation](https://arxiv.org/abs/1907.09702), Tianwei Lin, Xiao Liu, Xin Li, Errui Ding, Shilei Wen."
        }
    ]
}