{
    "summary": "The code prepares the NTU RGB+D dataset for CTR-GCN through data organization and cleaning, involving obtaining, denoising, and transforming skeleton data using three scripts. The dataset consists of 60 action classes with two splits: Cross-subject and Cross-view.",
    "details": [
        {
            "comment": "NTU-RGB+D dataset contains 60 action classes and 56,880 video samples for skeleton-based action recognition. It has two splits: Cross-subject and Cross-view. ST-GCN data preparation process introduced in the following sections.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/dataset/ntu-rgbd.md\":0-22",
            "content": "[\u7b80\u4f53\u4e2d\u6587](../../zh-CN/dataset/ntu-rgbd.md) | English\n# NTU-RGB+D Preparation\n- [Introduction](#Introduction)\n- [ST-GCN Data Prepare](#ST-GCN_Data_Prepare)\n- [CTR-GTCN Data Prepare](#CTR-GCN_Data_Prepare)\n---\n## Introduction\nNTU-RGB+D contains 60 action classes and 56,880 video samples for skeleton-based action recognition. Please refer to its official website[NTU-RGB+D](https://rose1.ntu.edu.sg/dataset/actionRecognition/) for more details.\nThe dataset contains two splits when dividing the training set and test set. For Cross-subject, the dataset is divided according to character id, with 40320 samples in training set and 16560 samples in test set. For Cross-view, the dataset is divided according to camera division. The samples collected by cameras 2 and 3 are training sets, including 37930 samples, and the samples collected by camera 1 are test sets, including 18960 samples.\n## ST-GCN_Data_Prepare\nST-GCN data prepare preceduce are introducted follow.\n### Download\nWe provide the download link of the p"
        },
        {
            "comment": "This code describes a processed dataset called NTU-RGB-D, which is approximately 3.1GB in size and requires downloading and unzipping using the command \"tar -zxvf NTU-RGB-D.tar\". The resulting directory structure contains train and val data for both xsub and xview. The code also provides a script called download_dataset.sh to facilitate downloading the dataset from the official website, and shows the file tree structure after successful download.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/dataset/ntu-rgbd.md\":22-58",
            "content": "rocessed dataset [NTU-RGB-D.tar](https://videotag.bj.bcebos.com/Data/NTU-RGB-D.tar)(~3.1G). Please download and unzip with ```tar -zxvf NTU-RGB-D.tar ``` , the directory structure is as follows\uff1a\n```txt\n\u2500\u2500\u2500 NTU-RGB-D\n    \u251c\u2500\u2500 xsub\n    \u2502   \u251c\u2500\u2500 train_data.npy\n    \u2502   \u251c\u2500\u2500 train_label.pkl\n    \u2502   \u251c\u2500\u2500 val_data.npy\n    \u2502   \u2514\u2500\u2500 val_label.pkl\n    \u2514\u2500\u2500 xview\n        \u251c\u2500\u2500 train_data.npy\n        \u251c\u2500\u2500 train_label.pkl\n        \u251c\u2500\u2500 val_data.npy\n        \u2514\u2500\u2500 val_label.pkl\n```\n> This is a copies from [st-gcn](https://github.com/open-mmlab/mmskeleton/blob/master/doc/SKELETON_DATA.md).\n## CTR-GCN_Data_Prepare\nCTR-GCN data prepare preceduce are introducted follow.\n### Download\nThere is script `download_dataset.sh` to download the dataset from official website [NTU-RGB+D](https://rose1.ntu.edu.sg/dataset/actionRecognition/) in dictory `data\\ntu-rgb-d`.\n```bash\nsh data/ntu-rgb-d/download_dataset.sh\n```\nFile tree:\n```txt\n\u2500\u2500\u2500 ntu-rgb-d\n    \u251c\u2500\u2500 download_dataset.sh\n    \u251c\u2500\u2500 nturgb+d_skeletons\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 S001C001P001R001A001.skeleton\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 S001C001P001R001A002.skeleton"
        },
        {
            "comment": "The provided code describes the preparation steps for processing the NTU-RGBD dataset to be used by CTR-GCN. It involves running three separate Python scripts in order:\n1. `get_raw_skes_data.py` is responsible for obtaining the skeleton of each performer from the data folders.\n2. `get_raw_denoised_data.py` removes any bad or corrupted skeletons from the dataset.\n3. `seq_transformation.py` transforms the remaining skeletons to the center of the first frame.\nTo follow these steps, navigate to the NTU-RGBD dataset folder and run each script sequentially in your command line interface.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/dataset/ntu-rgbd.md\":59-92",
            "content": "    \u2502\u00a0\u00a0 \u251c\u2500\u2500 S001C001P001R001A003.skeleton\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 S001C001P001R001A004.skeleton\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 S001C001P001R001A005.skeleton\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 S001C001P001R001A006.skeleton\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 S001C001P001R001A007.skeleton\n    \u2502   \u251c\u2500\u2500 ....\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 S017C003P020R002A060.skeleton\n    \u251c\u2500\u2500 get_raw_denoised_data.py\n    \u251c\u2500\u2500 get_raw_skes_data.py\n    \u251c\u2500\u2500 seq_transformation.py\n    \u2514\u2500\u2500 statistics\n     \u00a0\u00a0 \u251c\u2500\u2500 camera.txt\n     \u00a0\u00a0 \u251c\u2500\u2500 label.txt\n     \u00a0\u00a0 \u251c\u2500\u2500 performer.txt\n     \u00a0\u00a0 \u251c\u2500\u2500 replication.txt\n     \u00a0\u00a0 \u251c\u2500\u2500 setup.txt\n     \u00a0\u00a0 \u2514\u2500\u2500 skes_available_name.txt\n```\n### Prepare\nrun follow script, then data will be precessed to the data format need by CTR-GCN.\n> Note\uff1aif make dataset by yourself, please prepare `data/ntu-rgb-d/statistics/skes_available_name.txt`, which is the list of skeletons files that will be precessed.\n```bash\ncd ./data/ntu-rgb-d\n# Get skeleton of each performer\npython get_raw_skes_data.py\n# Remove the bad skeleton\npython get_raw_denoised_data.py\n# Transform the skeleton to the center of the first frame\npython seq_transformation.py"
        },
        {
            "comment": "The code represents a dataset called \"ntu-rgb-d\" containing skeleton data and associated files for denoising, logging missing skeletons, and tracking frames. The dataset is organized into folders including 'nturgb+d_skeletons' containing skeleton files per actor and 'denoised_data' with various log and pickle files related to the denoising process.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/dataset/ntu-rgbd.md\":93-128",
            "content": "```\nFile tree:\n```txt\n\u2500\u2500\u2500 ntu-rgb-d\n    \u251c\u2500\u2500 download_dataset.sh\n    \u251c\u2500\u2500 nturgb+d_skeletons\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 S001C001P001R001A001.skeleton\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 S001C001P001R001A002.skeleton\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 S001C001P001R001A003.skeleton\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 S001C001P001R001A004.skeleton\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 S001C001P001R001A005.skeleton\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 S001C001P001R001A006.skeleton\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 S001C001P001R001A007.skeleton\n    \u2502   \u251c\u2500\u2500 ....\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 S017C003P020R002A060.skeleton\n    \u251c\u2500\u2500 denoised_data\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 actors_info\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 S001C001P001R001A024.txt\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 S001C001P001R001A025.txt\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 S001C001P001R001A026.txt\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 ....\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 S017C003P020R002A059.txt\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 S017C003P020R002A060.txt\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 denoised_failed_1.log\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 denoised_failed_2.log\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 frames_cnt.txt\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 missing_skes_1.log\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 missing_skes_2.log\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 missing_skes.log\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 noise_length.log\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 noise_motion.log\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 noise_spread.log\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 raw_denoised_colors.pkl\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 raw_denoised_joints.pkl"
        },
        {
            "comment": "This code appears to organize various data files related to a dataset, likely for the NTU RGB+D action recognition benchmark. The directory structure includes raw data, denoised data, and preprocessed data in separate folders (xview and xsub). There are also statistics files and Python scripts for getting raw and denoised data. The notes suggest that some of the temporal files can be deleted if the extracted xview and xsub files are available.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/dataset/ntu-rgbd.md\":129-157",
            "content": "    \u2502\u00a0\u00a0 \u2514\u2500\u2500 rgb+ske\n    \u251c\u2500\u2500 raw_data\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 frames_cnt.txt\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 frames_drop.log\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 frames_drop_skes.pkl\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 raw_skes_data.pkl\n    \u251c\u2500\u2500 get_raw_denoised_data.py\n    \u251c\u2500\u2500 get_raw_skes_data.py\n    \u251c\u2500\u2500 seq_transformation.py\n    \u251c\u2500\u2500 statistics\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 camera.txt\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 label.txt\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 performer.txt\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 replication.txt\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 setup.txt\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 skes_available_name.txt\n    \u251c\u2500\u2500 xview\n    \u2502   \u251c\u2500\u2500 train_data.npy\n    \u2502   \u251c\u2500\u2500 train_label.pkl\n    \u2502   \u251c\u2500\u2500 val_data.npy\n    \u2502   \u2514\u2500\u2500 val_label.pkl\n    \u2514\u2500\u2500 xsub\n        \u251c\u2500\u2500 train_data.npy\n        \u251c\u2500\u2500 train_label.pkl\n        \u251c\u2500\u2500 val_data.npy\n        \u2514\u2500\u2500 val_label.pkl\n```\n> Note\uff1adictory `denoised_data`\u3001`raw_data`and`nturgb+d_skeletons`, that are temporal files, can be deleted, if extracted `xview` and `xsub`."
        }
    ]
}