{
    "summary": "This code initializes layer weights in PaddlePaddle, applying truncated normal or other initializations like Gaussian and Kaiming uniform. It adjusts for different modes and supports numpy arrays and Paddle tensors.",
    "details": [
        {
            "comment": "This code defines a function that initializes the weights of a PaddlePaddle layer using specified functions. It can also set bias values and is compatible with numpy arrays and Paddle tensors.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/weight_init.py\":0-35",
            "content": "# Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport math\nimport paddle\nimport paddle.nn.initializer as init\nimport numpy as np\nfrom scipy import special\ndef weight_init_(layer,\n                 func,\n                 weight_name=None,\n                 bias_name=None,\n                 bias_value=0.0,\n                 **kwargs):\n    \"\"\"\n    In-place params init function.\n    Usage:\n    .. code-block:: python\n        import paddle\n        import numpy as np\n        data = np.ones([3, 4], dtype='float32')"
        },
        {
            "comment": "Code initializes a Linear layer, applies truncated normal initialization to its weights with specified mean and std deviation, and optionally changes the weight name. If the layer has bias, it initializes the bias with a constant value and optionally changes the bias name. The _no_grad_trunc_normal_ function is used internally by nn.init.trunc_normal_.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/weight_init.py\":36-65",
            "content": "        linear = paddle.nn.Linear(4, 4)\n        input = paddle.to_tensor(data)\n        print(linear.weight)\n        linear(input)\n        weight_init_(linear, 'Normal', 'fc_w0', 'fc_b0', std=0.01, mean=0.1)\n        print(linear.weight)\n    \"\"\"\n    if hasattr(layer, 'weight') and layer.weight is not None:\n        getattr(init, func)(**kwargs)(layer.weight)\n        if weight_name is not None:\n            # override weight name\n            layer.weight.name = weight_name\n    if hasattr(layer, 'bias') and layer.bias is not None:\n        init.Constant(bias_value)(layer.bias)\n        if bias_name is not None:\n            # override bias name\n            layer.bias.name = bias_name\ndef _no_grad_trunc_normal_(tensor, mean, std, a, b):\n    def norm_cdf(x):\n        # Computes standard normal cumulative distribution function\n        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n    if (mean < a - 2 * std) or (mean > b + 2 * std):\n        print(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n              \"The distribution of values may be incorrect.\")"
        },
        {
            "comment": "This code generates weights for a tensor following a truncated Gaussian distribution. It computes the lower and upper bounds, uniformly fills the tensor with values between these bounds, transforms them to a standard Gaussian distribution, adjusts the mean and standard deviation, clamps the values within the original bounds, and sets the tensor's value.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/weight_init.py\":67-97",
            "content": "    with paddle.no_grad():\n        # Values are generated by using a truncated uniform distribution and\n        # then using the inverse CDF for the normal distribution.\n        # Get upper and lower cdf values\n        l = norm_cdf((a - mean) / std)\n        u = norm_cdf((b - mean) / std)\n        # Uniformly fill tensor with values from [l, u], then translate to [2l-1, 2u-1].\n        tmp = np.random.uniform(2 * l - 1, 2 * u - 1,\n                                size=list(tensor.shape)).astype(np.float32)\n        # Use inverse cdf transform for normal distribution to get truncated\n        # standard normal\n        tmp = special.erfinv(tmp)\n        # Transform to proper mean, std\n        tmp *= (std * math.sqrt(2.0))\n        tmp += mean\n        # Clamp to ensure it's in the proper range\n        tmp = np.clip(tmp, a, b)\n        tensor.set_value(paddle.to_tensor(tmp))\n        return tensor\ndef _calculate_fan_in_and_fan_out(tensor):\n    dimensions = tensor.dim()\n    if dimensions < 2:\n        raise ValueError(\n            \"Fan in and fan out can not be computed for tensor with fewer than 2 dimensions\""
        },
        {
            "comment": "This code initializes weights in a convolutional layer using either truncated normal or Kaiming uniform initialization. It calculates the fan-in and fan-out based on input and output feature maps, receptive field size, and optionally adjusts for different modes. The `trunc_normal_` function generates random values within specific bounds using truncated normal distribution, while `kaiming_normal_` sets weights using Kaiming uniform initialization with an optional nonlinearity parameter.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/weight_init.py\":98-129",
            "content": "        )\n    num_input_fmaps = tensor.shape[1]\n    num_output_fmaps = tensor.shape[0]\n    receptive_field_size = 1\n    if tensor.dim() > 2:\n        receptive_field_size = tensor[0][0].numel()\n    fan_in = num_input_fmaps * receptive_field_size\n    fan_out = num_output_fmaps * receptive_field_size\n    return fan_in, fan_out\ndef trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\ndef kaiming_normal_(tensor, a=0., mode='fan_in', nonlinearity='leaky_relu'):\n    def _calculate_correct_fan(tensor, mode):\n        mode = mode.lower()\n        valid_modes = ['fan_in', 'fan_out']\n        if mode not in valid_modes:\n            raise ValueError(\n                \"Mode {} not supported, please use one of {}\".format(\n                    mode, valid_modes))\n        fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)\n        return fan_in if mode == 'fan_in' else fan_out\n    def calculate_gain(nonlinearity, param=None):\n        linear_fns = [\n            'linear', 'conv1d', 'conv2d', 'conv3d', 'conv_transpose1d',"
        },
        {
            "comment": "This function initializes the weights of a neural network layer with respect to the nonlinearity used. It returns different values depending on the nonlinearity type, calculates the fan for each layer and then applies normal initialization using Paddle's Normal initializer.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/weight_init.py\":130-155",
            "content": "            'conv_transpose2d', 'conv_transpose3d'\n        ]\n        if nonlinearity in linear_fns or nonlinearity == 'sigmoid':\n            return 1\n        elif nonlinearity == 'tanh':\n            return 5.0 / 3\n        elif nonlinearity == 'relu':\n            return math.sqrt(2.0)\n        elif nonlinearity == 'leaky_relu':\n            if param is None:\n                negative_slope = 0.01\n            elif not isinstance(param, bool) and isinstance(\n                    param, int) or isinstance(param, float):\n                negative_slope = param\n            else:\n                raise ValueError(\n                    \"negative_slope {} not a valid number\".format(param))\n            return math.sqrt(2.0 / (1 + negative_slope**2))\n        else:\n            raise ValueError(\"Unsupported nonlinearity {}\".format(nonlinearity))\n    fan = _calculate_correct_fan(tensor, mode)\n    gain = calculate_gain(nonlinearity, a)\n    std = gain / math.sqrt(fan)\n    with paddle.no_grad():\n        paddle.nn.initializer.Normal(0, std)(tensor)"
        },
        {
            "comment": "Initializes a tensor with specified values and returns it.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/weight_init.py\":156-156",
            "content": "        return tensor"
        }
    ]
}