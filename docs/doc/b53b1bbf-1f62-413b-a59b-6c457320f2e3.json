{
    "summary": "CTR-GCN is a bone-based behavior recognition model using graph convolution, trained and tested with PaddlePaddle framework on the NTU-RGB+D dataset. The code snippet represents top-1 action classification with 99.9988% accuracy.",
    "details": [
        {
            "comment": "CTR-GCN is a bone-based behavior recognition model using graph convolution on human bone data. It improves accuracy for the task by extracting spatio-temporal features with spatio-temporal graph convolution. Train CTR-GCN on NTU-RGBD data with single GPU and joint modality.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/ctrgcn.md\":0-38",
            "content": "[\u7b80\u4f53\u4e2d\u6587](../../../zh-CN/model_zoo/recognition/ctrgcn.md) | English\n# CTR-GCN\n---\n## Contents\n- [Introduction](#Introduction)\n- [Data](#Data)\n- [Train](#Train)\n- [Test](#Test)\n- [Inference](#Inference)\n- [Reference](#Reference)\n## Introduction\n[CTRGCN](https://github.com/Uason-Chen/CTR-GCN.git) is a bone based behavior recognition model proposed by iccv 2021. By applying the changes to the graph convolution of human bone data with topological structure, and using spatio-temporal graph convolution to extract spatio-temporal features for behavior recognition, the accuracy of bone based behavior recognition task is greatly improved.\n<div align=\"center\">\n<img src=\"../../../images/ctrgcn.jpg\" height=200 width=950 hspace='10'/> <br />\n</div>\n## Data\nPlease refer to NTU-RGBD data download and preparation doc [NTU-RGBD](../../dataset/ntu-rgbd.md)\n## Train\n### Train on NTU-RGBD\n- Train CTR-GCN on NTU-RGBD scripts using single gpu\uff1a\n```bash\n# joint modality\npython main.py --validate -c configs/recognition/ctrgcn/ctrgcn_ntucs_joint.yaml --seed 1"
        },
        {
            "comment": "This code snippet runs the CTR-GCN model on different modalities and datasets, performs training with multiple GPUs, and tests the trained models. It uses the PaddlePaddle framework and provides configurations for the NTU-RGB+D dataset, including joint, bone, and motion modalities. The code can be executed by providing the appropriate command line arguments to specify the model, dataset, and mode (train or test).",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/ctrgcn.md\":40-73",
            "content": "# bone modality\npython main.py --validate -c configs/recognition/ctrgcn/ctrgcn_ntucs_bone.yaml --seed 1\n# motion modality\npython main.py --validate -c configs/recognition/ctrgcn/ctrgcn_ntucs_motion.yaml --seed 1\n# bone motion modality\npython main.py --validate -c configs/recognition/ctrgcn/ctrgcn_ntucs_bone_motion.yaml --seed 1\n```\n- Train CTR-GCN on NTU-RGBD scriptsusing multi gpus:\n```bash\npython3.7 -B -m paddle.distributed.launch --gpus=\"0,1,2,3\"  --log_dir=log_ctrgcn  main.py  --validate -c configs/recognition/ctrgcn/ctrgcn_ntucs_joint.yaml\n```\n- config file `ctrgcn_ntucs_joint.yaml` corresponding to the config of CTR-GCN on NTU-RGB+D dataset with cross-subject splits.\n## Test\n### Test on NTU-RGB+D\n- Test scripts\uff1a\n```bash\n# joint modality\npython3.7 main.py --test -c configs/recognition/ctrgcn/ctrgcn_ntucs_joint.yaml -w data/CTRGCN_ntucs_joint.pdparams\n# bone modality\npython3.7 main.py --test -c configs/recognition/ctrgcn/ctrgcn_ntucs_bone.yaml -w data/CTRGCN_ntucs_bone.pdparams\n# motion modality\npython"
        },
        {
            "comment": "This code is executing Python scripts for the CTRGCN model, which utilizes config files (-c) and pre-trained weight paths (-w). The accuracy table showcases performance on NTU-RGB+D dataset across different modalities.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/ctrgcn.md\":73-89",
            "content": "3.7 main.py --test -c configs/recognition/ctrgcn/ctrgcn_ntucs_motion.yaml -w data/CTRGCN_ntucs_motion.pdparams\n# bone motion modality\npython3.7 main.py --test -c configs/recognition/ctrgcn/ctrgcn_ntucs_bone_motion.yaml -w data/CTRGCN_ntucs_bone_motion.pdparams\n```\n- Specify the config file with `-c`, specify the weight path with `-w`.\nAccuracy on NTU-RGB+D dataset:\n| split | modality | Top-1 | checkpoints |\n| :----: | :----: | :----: | :----: |\n| cross-subject | joint | 89.93 | [CTRGCN_ntucs_joint.pdparams](https://videotag.bj.bcebos.com/PaddleVideo-release2.3/CTRGCN_ntucs_joint.pdparams) |\n| cross-subject | bone | 85.24 | [CTRGCN_ntucs_bone.pdparams](https://videotag.bj.bcebos.com/PaddleVideo-release2.3/CTRGCN_ntucs_bone.pdparams) |\n| cross-subject | motion | 85.33 | [CTRGCN_ntucs_motion.pdparams](https://videotag.bj.bcebos.com/PaddleVideo-release2.3/CTRGCN_ntucs_motion.pdparams) |\n| cross-subject | bone motion | 84.53 | [CTRGCN_ntucs_bone_motion.pdparams](https://videotag.bj.bcebos.com/PaddleVideo-release2.3/CTRGCN_ntucs_bone_motion.pdparams) |"
        },
        {
            "comment": "This code exports the inference model and performs inference using PaddleVideo's CTRGCN model for action recognition. The `export_model.py` script creates the architecture file (CTRGCN.pdmodel) and parameters file (CTRGCN.pdiparams). The `predict.py` script uses these files to perform inference on a given video file, specifying the configuration file for the CTRGCN model. It runs with GPU acceleration (use_gpu=True) and without TensorRT optimization (use_tensorrt=False).",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/ctrgcn.md\":92-120",
            "content": "## Inference\n### export inference model\n```bash\npython3.7 tools/export_model.py -c configs/recognition/ctrgcn/ctrgcn_ntucs_joint.yaml \\\n                                -p data/CTRGCN_ntucs_joint.pdparams \\\n                                -o inference/CTRGCN\n```\n To get model architecture file `CTRGCN.pdmodel` and parameters file `CTRGCN.pdiparams`, use:\n- Args usage please refer to [Model Inference](https://github.com/PaddlePaddle/PaddleVideo/blob/release/2.0/docs/zh-CN/start.md#2-%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86).\n### infer\n```bash\npython3.7 tools/predict.py --input_file data/example_NTU-RGB-D_sketeton.npy \\\n                           --config configs/recognition/ctrgcn/ctrgcn_ntucs_joint.yaml \\\n                           --model_file inference/CTRGCN_joint/CTRGCN_joint.pdmodel \\\n                           --params_file inference/CTRGCN_joint/CTRGCN_joint.pdiparams \\\n                           --use_gpu=True \\\n                           --use_tensorrt=False\n```\nexample of logs:\n```\nCurrent video file: data/example_NTU-RGB-D_sketeton.npy"
        },
        {
            "comment": "The code snippet represents the top-1 class and its corresponding score in a model's prediction for skeleton-based action recognition. The top-1 class is 4, with a top-1 score of 0.999988317489624. This information can be used to identify the recognized action from multiple options.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/ctrgcn.md\":121-127",
            "content": "        top-1 class: 4\n        top-1 score: 0.999988317489624\n```\n## Reference\n- [Channel-wise Topology Refinement Graph Convolution for Skeleton-Based Action Recognition](https://arxiv.org/abs/2107.12213), Chen, Yuxin and Zhang, Ziqi and Yuan, Chunfeng and Li, Bing and Deng, Ying and Hu, Weiming"
        }
    ]
}