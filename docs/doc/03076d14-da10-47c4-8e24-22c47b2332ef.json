{
    "summary": "The code reads parameters, separates configurations, and executes inference tests on different GPUs/CPUs for batch sizes. It sets up a loop for PaddleVideo model inference, handles hardware configurations, prepares settings for exporting models, logs results, and calls the \"func_inference\" function.",
    "details": [
        {
            "comment": "The code reads a file, parses parameters for model name, Python version, GPU usage, quantization configuration file, model path, output directory, data directory, data annotation file, and batch numbers. It uses awk to extract specific lines from the file and functions defined in common_func.sh for parameter extraction. The MODE variable can have values to determine the type of task being performed.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/test_tipc/test_ptq_inference_python.sh\":0-28",
            "content": "#!/bin/bash\nsource test_tipc/common_func.sh\nFILENAME=$1\n# MODE be one of ['lite_train_lite_infer' 'lite_train_whole_infer' 'whole_train_whole_infer', 'whole_infer']\nMODE=$2\ndataline=$(awk 'NR==1, NR==32{print}'  $FILENAME)\n# parser params\nIFS=$'\\n'\nlines=(${dataline})\n# The training params\nmodel_name=$(func_parser_value \"${lines[1]}\")\npython=$(func_parser_value \"${lines[2]}\")\nuse_gpu_key=$(func_parser_key \"${lines[3]}\")\nuse_gpu_value=$(func_parser_value \"${lines[3]}\")\nquant_config_file_key=$(func_parser_key \"${lines[4]}\")\nquant_config_file_value=$(func_parser_value \"${lines[4]}\")\nmodel_path_key=$(func_parser_key \"${lines[5]}\")\nmodel_path_value=$(func_parser_value \"${lines[5]}\")\noutput_dir_key=$(func_parser_key \"${lines[6]}\")\noutput_dir_value=$(func_parser_value \"${lines[6]}\")\ndata_dir_key=$(func_parser_key \"${lines[7]}\")\ndata_dir_value=$(func_parser_value \"${lines[7]}\")\ndata_anno_key=$(func_parser_key \"${lines[8]}\")\ndata_anno_value=$(func_parser_value \"${lines[8]}\")\nbatch_num_key=$(func_parser_key \"${lines[9]}\")"
        },
        {
            "comment": "The code retrieves values and keys from a configuration file, storing them in variables for later use. It separates trainer and inference configurations, including GPU usage, inferential model parameters, video directory path, and benchmark options.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/test_tipc/test_ptq_inference_python.sh\":29-51",
            "content": "batch_num_value=$(func_parser_value \"${lines[9]}\")\nquant_batch_size_key=$(func_parser_key \"${lines[10]}\")\nquant_batch_size_value=$(func_parser_value \"${lines[10]}\")\n# parser trainer\ntrain_py=$(func_parser_value \"${lines[13]}\")\n# parser inference\ninference_py=$(func_parser_value \"${lines[16]}\")\nuse_gpu_key=$(func_parser_key \"${lines[17]}\")\nuse_gpu_list=$(func_parser_value \"${lines[17]}\")\ninfer_config_file_key=$(func_parser_key \"${lines[18]}\")\ninfer_config_file_value=$(func_parser_value \"${lines[18]}\")\ninfer_batch_size_key=$(func_parser_key \"${lines[19]}\")\ninfer_batch_size_list=$(func_parser_value \"${lines[19]}\")\ninfer_model_key=$(func_parser_key \"${lines[20]}\")\ninfer_model_value=$(func_parser_value \"${lines[20]}\")\ninfer_params_key=$(func_parser_key \"${lines[21]}\")\ninfer_params_value=$(func_parser_value \"${lines[21]}\")\ninfer_video_key=$(func_parser_key \"${lines[22]}\")\ninfer_video_dir=$(func_parser_value \"${lines[22]}\")\nbenchmark_key=$(func_parser_key \"${lines[23]}\")\nbenchmark_value=$(func_parser_value \"${lines[23]}\")"
        },
        {
            "comment": "This function executes inference on different GPUs and CPUs for various batch sizes. It sets log paths, parameters, model file path, params file path, and config file path using helper functions. The script performs inference using Python and logs the results.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/test_tipc/test_ptq_inference_python.sh\":54-73",
            "content": "function func_inference(){\n    IFS='|'\n    _python=$1\n    _script=$2\n    _model_dir=$3\n    _log_path=$4\n    _img_dir=$5\n    # inference\n    for use_gpu in ${use_gpu_list[*]}; do\n        # cpu\n        if [ ${use_gpu} = \"False\" ] || [ ${use_gpu} = \"cpu\" ]; then\n            for batch_size in ${infer_batch_size_list[*]}; do\n                _save_log_path=\"${_log_path}/python_infer_cpu_batchsize_${batch_size}.log\"\n                set_infer_data=$(func_set_params \"${infer_video_key}\" \"${_img_dir}\")\n                set_benchmark=$(func_set_params \"${benchmark_key}\" \"${benchmark_value}\")\n                set_batchsize=$(func_set_params \"${infer_batch_size_key}\" \"${batch_size}\")\n                set_model_file_path=$(func_set_params \"${infer_model_key}\" \"${infer_model_value}\")\n                set_params_file_path=$(func_set_params \"${infer_params_key}\" \"${infer_params_value}\")\n                set_config_file_path=$(func_set_params \"${infer_config_file_key}\" \"${infer_config_file_value}\")\n                command=\"${_"
        },
        {
            "comment": "This code is running a loop to execute inference tests on different GPU configurations. It sets variables for batch size, input data path, and model file path. The output logs are saved into specific files for later analysis.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/test_tipc/test_ptq_inference_python.sh\":73-87",
            "content": "python} ${_script} ${use_gpu_key}=${use_gpu} ${set_config_file_path} ${set_model_file_path} ${set_params_file_path} ${set_batchsize} ${set_infer_data} ${set_benchmark} > ${_save_log_path} 2>&1 \"\n                # echo $command\n                eval $command\n                last_status=${PIPESTATUS[0]}\n                eval \"cat ${_save_log_path}\"\n                status_check $last_status \"${command}\" \"${status_log}\" \"${model_name}\"\n            done\n        # gpu\n        elif [ ${use_gpu} = \"True\" ] || [ ${use_gpu} = \"gpu\" ]; then\n            for batch_size in ${infer_batch_size_list[*]}; do\n                _save_log_path=\"${_log_path}/python_infer_gpu_batchsize_${batch_size}.log\"\n                set_infer_data=$(func_set_params \"${infer_video_key}\" \"${_img_dir}\")\n                set_benchmark=$(func_set_params \"${benchmark_key}\" \"${benchmark_value}\")\n                set_batchsize=$(func_set_params \"${infer_batch_size_key}\" \"${batch_size}\")\n                set_model_file_path=$(func_set_params \"${infer_model_key}\" \"${infer_model_value}\")"
        },
        {
            "comment": "This code is setting up a loop to run inference on the PaddleVideo model for different hardware configurations and modes. It sets the necessary parameters, files, and batch size, then executes the command and checks the status of the execution. The output is logged in a specified directory. If the mode is \"whole_infer\", it uses IFS to separate the export settings.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/test_tipc/test_ptq_inference_python.sh\":88-111",
            "content": "                set_params_file_path=$(func_set_params \"${infer_params_key}\" \"${infer_params_value}\")\n                set_config_file_path=$(func_set_params \"${infer_config_file_key}\" \"${infer_config_file_value}\")\n                command=\"${_python} ${_script} ${use_gpu_key}=${use_gpu} ${set_config_file_path} ${set_model_file_path} ${set_params_file_path} ${set_batchsize} ${set_infer_data} ${set_benchmark} > ${_save_log_path} 2>&1 \"\n                echo $command\n                eval $command\n                last_status=${PIPESTATUS[0]}\n                eval \"cat ${_save_log_path}\"\n                status_check $last_status \"${command}\" \"${status_log}\" \"${model_name}\"\n            done\n        else\n            echo \"Does not support hardware other than CPU and GPU Currently!\"\n        fi\n    done\n}\n# log\nLOG_PATH=\"./log/${model_name}/${MODE}\"\nmkdir -p ${LOG_PATH}\nstatus_log=\"${LOG_PATH}/results_python.log\"\nif [ ${MODE} = \"whole_infer\" ]; then\n    IFS=\"|\"\n    # run export\n    set_output_dir=$(func_set_params \"${output_dir_key}\" \"${output_dir_value}\")"
        },
        {
            "comment": "This code is preparing various settings for executing a command to export a model. It sets values from input variables, exports the model with specified parameters, logs the results, and then checks the status of the export. Finally, it prepares a directory for running inference.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/test_tipc/test_ptq_inference_python.sh\":112-128",
            "content": "    set_data_dir=$(func_set_params \"${data_dir_key}\" \"${data_dir_value}\")\n    set_data_anno=$(func_set_params \"${data_anno_key}\" \"${data_anno_value}\")\n    set_batch_size=$(func_set_params \"${quant_batch_size_key}\" \"${quant_batch_size_value}\")\n    set_batch_num=$(func_set_params \"${batch_num_key}\" \"${batch_num_value}\")\n    set_model_path=$(func_set_params \"${model_path_key}\" \"${model_path_value}\")\n    set_config_file=$(func_set_params \"${quant_config_file_key}\" \"${quant_config_file_value}\")\n    set_use_gpu=$(func_set_params \"${use_gpu_key}\" \"${use_gpu_value}\")\n    export_log_path=\"${LOG_PATH}/${MODE}_export_${Count}.log\"\n    export_cmd=\"${python} ${train_py} ${set_use_gpu} ${set_config_file} ${set_model_path} ${set_batch_num} ${set_batch_size} ${set_data_dir} ${set_data_anno} ${set_output_dir} > ${export_log_path} 2>&1 \"\n    echo $export_cmd\n    eval $export_cmd\n    status_export=$?\n    status_check $status_export \"${export_cmd}\" \"${status_log}\" \"${model_name}\"\n    save_infer_dir=${output_dir_value}\n    #run inference"
        },
        {
            "comment": "The code snippet is calling a function named \"func_inference\" with arguments such as python, inference_py (likely the path of the Python script), save_infer_dir, LOG_PATH and infer_video_dir. This could be a part of an if condition block, possibly initializing or running an inference process.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/test_tipc/test_ptq_inference_python.sh\":129-131",
            "content": "    func_inference \"${python}\" \"${inference_py}\" \"${save_infer_dir}\" \"${LOG_PATH}\" \"${infer_video_dir}\"\nfi"
        }
    ]
}