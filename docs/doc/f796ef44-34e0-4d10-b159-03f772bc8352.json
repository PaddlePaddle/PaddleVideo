{
    "summary": "This code deploys PaddleVideo models with C++, supports optional settings and displays inference results, but encounters an error searching for 'libcudnn.so' due to incorrect/missing CUDNN_LIB_DIR setting.",
    "details": [
        {
            "comment": "Explanation of the code: This is an introduction to deploying PaddleVideo models using C++. It provides instructions on setting up a Linux environment and compiling OpenCV and PaddlePaddle libraries for model prediction. The code also mentions the need to install additional dependencies and provides commands for downloading, extracting, and compiling the OpenCV library. Additionally, it notes that Windows support is currently under development (TODO) and requires Visual Studio 2019 Community for compilation (TODO).",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/cpp_infer/readme.md\":0-44",
            "content": "[English](./readme_en.md) | \u7b80\u4f53\u4e2d\u6587\n# \u670d\u52a1\u5668\u7aefC++\u9884\u6d4b\n\u672c\u7ae0\u8282\u4ecb\u7ecdPaddleVideo\u6a21\u578b\u7684\u7684C++\u90e8\u7f72\u65b9\u6cd5\uff0cpython\u9884\u6d4b\u90e8\u7f72\u65b9\u6cd5\u8bf7\u53c2\u8003\u5404\u81ea\u6a21\u578b\u7684**\u6a21\u578b\u63a8\u7406**\u7ae0\u8282\u3002\nC++\u5728\u6027\u80fd\u8ba1\u7b97\u4e0a\u4f18\u4e8epython\uff0c\u56e0\u6b64\uff0c\u5728\u5927\u591a\u6570CPU\u3001GPU\u90e8\u7f72\u573a\u666f\uff0c\u591a\u91c7\u7528C++\u7684\u90e8\u7f72\u65b9\u5f0f\uff0c\u672c\u8282\u5c06\u4ecb\u7ecd\u5982\u4f55\u5728Linux\uff08CPU/GPU\uff09\u73af\u5883\u4e0b\u914d\u7f6eC++\u73af\u5883\u5e76\u5b8c\u6210\nPaddleVideo\u6a21\u578b\u90e8\u7f72\u3002\n\u5728\u5f00\u59cb\u4f7f\u7528\u4e4b\u524d\uff0c\u60a8\u9700\u8981\u6309\u7167\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5\u989d\u5916\u7684\u4f9d\u8d56\u5305\uff1a\n```bash\npython -m pip install git+https://github.com/LDOUBLEV/AutoLog\n```\n## 1. \u51c6\u5907\u73af\u5883\n- Linux\u73af\u5883\uff0c\u63a8\u8350\u4f7f\u7528docker\u3002\n- Windows\u73af\u5883\uff0c\u76ee\u524d\u652f\u6301\u57fa\u4e8e`Visual Studio 2019 Community`\u8fdb\u884c\u7f16\u8bd1\uff08TODO\uff09\n* \u8be5\u6587\u6863\u4e3b\u8981\u4ecb\u7ecd\u57fa\u4e8eLinux\u73af\u5883\u7684PaddleVideo C++\u9884\u6d4b\u6d41\u7a0b\uff0c\u5982\u679c\u9700\u8981\u5728Windows\u4e0b\u57fa\u4e8e\u9884\u6d4b\u5e93\u8fdb\u884cC++\u9884\u6d4b\uff0c\u5177\u4f53\u7f16\u8bd1\u65b9\u6cd5\u8bf7\u53c2\u8003[Windows\u4e0b\u7f16\u8bd1\u6559\u7a0b](./docs/windows_vs2019_build.md)\uff08TODO\uff09\n* **\u51c6\u5907\u73af\u5883\u7684\u76ee\u7684\u662f\u5f97\u5230\u7f16\u8bd1\u597d\u7684opencv\u5e93\u4e0epaddle\u9884\u6d4b\u5e93**\u3002\n### 1.1 \u7f16\u8bd1opencv\u5e93\n* \u9996\u5148\u9700\u8981\u4eceopencv\u5b98\u7f51\u4e0a\u4e0b\u8f7d\u5728Linux\u73af\u5883\u4e0b\u6e90\u7801\u7f16\u8bd1\u7684\u538b\u7f29\u5305\uff0c\u5e76\u89e3\u538b\u6210\u6587\u4ef6\u5939\u3002\u4ee5opencv3.4.7\u4e3a\u4f8b\uff0c\u4e0b\u8f7d\u547d\u4ee4\u5982\u4e0b\uff1a\n    ```bash\n    cd deploy/cpp_infer\n    wget https://github.com/opencv/opencv/archive/3.4.7.tar.gz\n    tar -xf 3.4.7.tar.gz\n    ```\n    \u89e3\u538b\u5b8c\u6bd5\u540e\u5728`deploy/cpp_infer`\u76ee\u5f55\u4e0b\u53ef\u4ee5\u5f97\u5230\u89e3\u538b\u51fa\u7684`opencv-3.4.7`\u7684\u6587\u4ef6\u5939\u3002\n* \u5b89\u88c5ffmpeg\n    opencv\u914d\u5408ffmpeg\u624d\u80fd\u5728linux\u4e0b\u6b63\u5e38\u8bfb\u53d6\u89c6\u9891\uff0c\u5426\u5219\u53ef\u80fd\u9047\u5230\u89c6\u9891\u5e27\u6570\u8fd4\u56de\u4e3a0\u6216\u65e0\u6cd5\u8bfb\u53d6\u4efb\u4f55\u89c6\u9891\u5e27\u7684\u60c5\u51b5\n    \u91c7\u7528\u8f83\u4e3a\u7b80\u5355\u7684apt\u5b89\u88c5\uff0c\u5b89\u88c5\u547d\u4ee4\u5982\u4e0b\uff1a\n    ```bash\n    apt-get update\n    apt install libavformat-dev\n    apt install libavcodec-dev"
        },
        {
            "comment": "Preparing to compile OpenCV, enter the `opencv-3.4.7` directory and set `root_path` and `install_path`. Remove existing `build` folder, create a new one, navigate into it, run cmake commands with specified options, make and install. Results in an `opencv3` folder with header files and libraries for C++ video inference code compilation.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/cpp_infer/readme.md\":45-90",
            "content": "    apt install libswresample-dev\n    apt install libswscale-dev\n    apt install libavutil-dev\n    apt install libsdl1.2-dev\n    apt-get install ffmpeg\n    ```\n* \u51c6\u5907\u7f16\u8bd1opencv\uff0c\u9996\u5148\u8fdb\u5165`opencv-3.4.7`\u7684\u6587\u4ef6\u5939\uff0c\u7136\u540e\u8bbe\u7f6eopencv\u6e90\u7801\u8def\u5f84`root_path`\u4ee5\u53ca\u5b89\u88c5\u8def\u5f84`install_path`\u3002\u6267\u884c\u547d\u4ee4\u5982\u4e0b\uff1a\n    ```bash\n    cd opencv-3.4.7\n    root_path=$PWD  # \u5f53\u524d\u6240\u5728\u8def\u5f84\u5373\u4e3aopencv-3.4.7\u7684\u7edd\u5bf9\u8def\u5f84\n    install_path=${root_path}/opencv3\n    rm -rf build\n    mkdir build\n    cd build\n    cmake .. \\\n        -DCMAKE_INSTALL_PREFIX=${install_path} \\\n        -DCMAKE_BUILD_TYPE=Release \\\n        -DBUILD_SHARED_LIBS=OFF \\\n        -DWITH_IPP=OFF \\\n        -DBUILD_IPP_IW=OFF \\\n        -DWITH_LAPACK=OFF \\\n        -DWITH_EIGEN=OFF \\\n        -DCMAKE_INSTALL_LIBDIR=lib64 \\\n        -DWITH_ZLIB=ON \\\n        -DBUILD_ZLIB=ON \\\n        -DWITH_JPEG=ON \\\n        -DBUILD_JPEG=ON \\\n        -DWITH_PNG=ON \\\n        -DBUILD_PNG=ON \\\n        -DWITH_TIFF=ON \\\n        -DBUILD_TIFF=ON \\\n        -DWITH_FFMPEG=ON\n    make -j\n    make install\n    ```\n    `make install`\u5b8c\u6210\u4e4b\u540e\uff0c\u4f1a\u5728\u8be5\u6587\u4ef6\u5939\u4e0b\u751f\u6210opencv\u5934\u6587\u4ef6\u548c\u5e93\u6587\u4ef6\uff0c\u7528\u4e8e\u540e\u9762\u7684Video\u63a8\u7406C++\u4ee3\u7801\u7f16\u8bd1\u3002\n    \u6700\u7ec8\u4f1a\u4ee5\u5b89\u88c5\u8def\u5f84`install_path`\u4e3a\u6307\u5b9a\u8def\u5f84\uff0c\u5f97\u5230\u4e00\u4e2a`opencv3`\u7684\u6587\u4ef6\u5939\uff0c\u5176\u6587\u4ef6\u7ed3\u6784\u5982\u4e0b\u6240\u793a\u3002"
        },
        {
            "comment": "In this code snippet, the user is provided with two methods to obtain Paddle prediction library. The first method involves directly downloading a pre-compiled version of the library from the official website based on the desired CUDA version and OS architecture. The second method involves cloning the latest source code from Paddle's GitHub repository and compiling it manually for the most recent features. The code also provides sample commands to download and extract a pre-compiled library or clone the Paddle source code using 'wget' and 'tar' commands.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/cpp_infer/readme.md\":92-124",
            "content": "    ```shell\n    opencv-3.4.7/\n    \u251c\u2500\u2500 opencv3/  # \u5b89\u88c5\u5728opencv3\u76ee\u5f55\u4e0b\n    \u2502   \u251c\u2500\u2500 bin/\n    \u2502   \u251c\u2500\u2500 include/\n    \u2502   \u251c\u2500\u2500 lib/\n    \u2502   \u251c\u2500\u2500 lib64/\n    \u2502   \u2514\u2500\u2500 share/\n    ```\n### 1.2 \u4e0b\u8f7d\u6216\u8005\u7f16\u8bd1Paddle\u9884\u6d4b\u5e93\n\u67092\u79cd\u65b9\u5f0f\u83b7\u53d6Paddle\u9884\u6d4b\u5e93\uff0c\u4e0b\u9762\u8fdb\u884c\u8be6\u7ec6\u4ecb\u7ecd\u3002\n#### 1.2.1 \u76f4\u63a5\u4e0b\u8f7d\u5b89\u88c5\n* [Paddle\u9884\u6d4b\u5e93\u5b98\u7f51](https://paddleinference.paddlepaddle.org.cn/v2.2/user_guides/download_lib.html) \u4e0a\u63d0\u4f9b\u4e86\u4e0d\u540ccuda\u7248\u672c\u7684Linux\u9884\u6d4b\u5e93\uff0c\u53ef\u4ee5\u5728\u5b98\u7f51\u67e5\u770b\u5e76**\u9009\u62e9\u5408\u9002\u7684\u9884\u6d4b\u5e93\u7248\u672c**\uff08\u5efa\u8bae\u9009\u62e9paddle\u7248\u672c>=2.0.1\u7248\u672c\u7684\u9884\u6d4b\u5e93\uff0c\u63a8\u8350\u4f7f\u75282.2.2\u7684\u9884\u6d4b\u5e93\uff09\u3002\n* \u4e0b\u8f7d\u5f97\u5230\u4e00\u4e2a`paddle_inference.tgz`\u538b\u7f29\u5305\uff0c\u7136\u540e\u5c06\u5b83\u89e3\u538b\u6210\u6587\u4ef6\u5939\uff0c\u547d\u4ee4\u5982\u4e0b(\u4ee5\u673a\u5668\u73af\u5883\u4e3agcc8.2\u4e3a\u4f8b)\uff1a\n    ```bash\n    wget https://paddle-inference-lib.bj.bcebos.com/2.2.2/cxx_c/Linux/GPU/x86-64_gcc8.2_avx_mkl_cuda10.1_cudnn7.6.5_trt6.0.1.5/paddle_inference.tgz\n    tar -xf paddle_inference.tgz\n    ```\n    \u6700\u7ec8\u4f1a\u5728\u5f53\u524d\u7684\u6587\u4ef6\u5939\u4e2d\u751f\u6210`paddle_inference/`\u7684\u5b50\u6587\u4ef6\u5939\u3002\n#### 1.2.2 \u9884\u6d4b\u5e93\u6e90\u7801\u7f16\u8bd1\n* \u5982\u679c\u5e0c\u671b\u83b7\u53d6\u6700\u65b0\u9884\u6d4b\u5e93\u7279\u6027\uff0c\u53ef\u4ee5\u4ecePaddle github\u4e0a\u514b\u9686\u6700\u65b0\u4ee3\u7801\uff0c\u6e90\u7801\u7f16\u8bd1\u9884\u6d4b\u5e93\u3002\n* \u53ef\u4ee5\u53c2\u8003[Paddle\u9884\u6d4b\u5e93\u5b89\u88c5\u7f16\u8bd1\u8bf4\u660e](https://www.paddlepaddle.org.cn/documentation/docs/zh/2.0/guides/05_inference_deployment/inference/build_and_install_lib_cn.html#congyuanmabianyi) \u7684\u8bf4\u660e\uff0c\u4ecegithub\u4e0a\u83b7\u53d6Paddle\u4ee3\u7801\uff0c\u7136\u540e\u8fdb\u884c\u7f16\u8bd1\uff0c\u751f\u6210\u6700\u65b0\u7684\u9884\u6d4b\u5e93\u3002\u4f7f\u7528git\u83b7\u53d6\u4ee3\u7801\u65b9\u6cd5\u5982\u4e0b\u3002\n    ```shell"
        },
        {
            "comment": "The provided code demonstrates how to compile the Paddle inference API library from the source code. It explains the steps for cloning and entering the Paddle repository, setting build parameters, compiling the library using make, and creating a build directory. The comments also mention where to find more information about build parameter options and what files are generated after compilation.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/cpp_infer/readme.md\":125-169",
            "content": "    git clone https://github.com/PaddlePaddle/Paddle.git\n    git checkout release/2.2\n    ```\n* \u8fdb\u5165Paddle\u76ee\u5f55\u540e\uff0c\u7f16\u8bd1\u65b9\u6cd5\u5982\u4e0b\u3002\n    ```shell\n    rm -rf build\n    mkdir build\n    cd build\n    cmake  .. \\\n        -DWITH_CONTRIB=OFF \\\n        -DWITH_MKL=ON \\\n        -DWITH_MKLDNN=ON  \\\n        -DWITH_TESTING=OFF \\\n        -DCMAKE_BUILD_TYPE=Release \\\n        -DWITH_INFERENCE_API_TEST=OFF \\\n        -DON_INFER=ON \\\n        -DWITH_PYTHON=ON\n    make -j4\n    make inference_lib_dist -j4 # 4\u4e3a\u7f16\u8bd1\u65f6\u4f7f\u7528\u6838\u6570\uff0c\u53ef\u6839\u636e\u673a\u5668\u60c5\u51b5\u81ea\u884c\u4fee\u6539\n    ```\n    \u66f4\u591a\u7f16\u8bd1\u53c2\u6570\u9009\u9879\u4ecb\u7ecd\u53ef\u4ee5\u53c2\u8003[\u6587\u6863\u8bf4\u660e](https://www.paddlepaddle.org.cn/documentation/docs/zh/2.0/guides/05_inference_deployment/inference/build_and_install_lib_cn.html#congyuanmabianyi)\u3002\n* \u7f16\u8bd1\u5b8c\u6210\u4e4b\u540e\uff0c\u53ef\u4ee5\u5728`build/paddle_inference_install_dir/`\u6587\u4ef6\u4e0b\u770b\u5230\u751f\u6210\u4e86\u4ee5\u4e0b\u6587\u4ef6\u53ca\u6587\u4ef6\u5939\u3002\n    ```bash\n    build/\n    \u2514\u2500\u2500 paddle_inference_install_dir/\n        \u251c\u2500\u2500 CMakeCache.txt\n        \u251c\u2500\u2500 paddle/\n        \u251c\u2500\u2500 third_party/\n        \u2514\u2500\u2500 version.txt\n    ```\n    \u5176\u4e2d`paddle`\u5c31\u662fC++\u9884\u6d4b\u6240\u9700\u7684Paddle\u5e93\uff0c`version.txt`\u4e2d\u5305\u542b\u5f53\u524d\u9884\u6d4b\u5e93\u7684\u7248\u672c\u4fe1\u606f\u3002\n## 2. \u7f16\u8bd1\u5e76\u8fd0\u884c\u9884\u6d4bdemo\n### 2.1 \u5c06\u6a21\u578b\u5bfc\u51fa\u4e3ainference model\n* \u8be5\u6b65\u9aa4\u4e0epython\u90e8\u7f72\u65b9\u5f0f\u4e0b\u7684\u5bfc\u51fa\u9884\u6d4b\u6a21\u578b\u76f8\u540c\uff0c\u53ef\u4ee5\u53c2\u8003\u5404\u81ea\u6a21\u578b\u7684\u6a21\u578b\u9884\u6d4b\u7ae0\u8282\u3002\u5bfc\u51fa\u7684\u51e0\u4e2a\u76f8\u5173inference model\u6587\u4ef6\u7528\u4e8e\u6a21\u578b\u9884\u6d4b\u3002**\u4ee5PP-TSM\u4e3a\u4f8b**\uff0c\u5bfc\u51fa\u9884\u6d4b\u6a21\u578b\u7684\u76ee\u5f55\u7ed3\u6784\u5982\u4e0b\u3002"
        },
        {
            "comment": "This code is providing instructions to compile the PaddleVideo C++ prediction demo for an inference model. Users need to navigate to the \"deploy/cpp_infer\" directory and execute the `bash tools/build.sh` command. They must also modify the `tools/build.sh` script with their specific openCV, Paddle Inference, CUDA library, and CUDNN library directories before running the build script.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/cpp_infer/readme.md\":171-212",
            "content": "    ```\n    inference/\n    \u2514\u2500\u2500 ppTSM/\n        \u251c\u2500\u2500 ppTSM.pdiparams\n        \u251c\u2500\u2500 ppTSM.pdiparamsinfo\n        \u2514\u2500\u2500 ppTSM.pdmodel\n    ```\n### 2.2 \u7f16\u8bd1PaddleVideo C++\u9884\u6d4bdemo\n* \u8fdb\u5165\u5230`deploy/cpp_infer`\u76ee\u5f55\u4e0b\uff0c\u6267\u884c\u4ee5\u4e0b\u7f16\u8bd1\u547d\u4ee4\n    ```shell\n    bash tools/build.sh\n    ```\n    `tools/build.sh`\u4e2d\u7684Paddle C++\u9884\u6d4b\u5e93\u3001opencv\u7b49\u5176\u4ed6\u4f9d\u8d56\u5e93\u7684\u5730\u5740\u9700\u8981\u6362\u6210\u81ea\u5df1\u673a\u5668\u4e0a\u7684\u5b9e\u9645\u5730\u5740\u3002\n* \u5177\u4f53\u5730\uff0c\u9700\u8981\u4fee\u6539`tools/build.sh`\u4e2d\u7684\u73af\u5883\u8def\u5f84\uff0c\u76f8\u5173\u5185\u5bb9\u5982\u4e0b\uff1a\n    ```shell\n    OPENCV_DIR=your_opencv_dir\n    LIB_DIR=your_paddle_inference_dir\n    CUDA_LIB_DIR=your_cuda_lib_dir\n    CUDNN_LIB_DIR=your_cudnn_lib_dir\n    ```\n    \u4e0a\u8ff0\u53c2\u6570\u5982\u4e0b(\u4ee5\u4e0b\u8def\u5f84\u7528\u6237\u53ef\u6839\u636e\u81ea\u5df1\u673a\u5668\u7684\u60c5\u51b5\u5bf9\u5e94\u4fee\u6539)\n    ```bash\n    OPENCV_DIR=/path/to/opencv3\n    LIB_DIR=/path/to/paddle_inference\n    CUDA_LIB_DIR=/usr/local/cuda/lib64\n    CUDNN_LIB_DIR=/usr/lib/x86_64-linux-gnu/\n    ```\n    `OPENCV_DIR`\u4e3aopencv\u7f16\u8bd1\u5b89\u88c5\u7684\u5730\u5740\n    `LIB_DIR`\u4e3a\u4e0b\u8f7d(`paddle_inference`\u6587\u4ef6\u5939)\u6216\u8005\u7f16\u8bd1\u751f\u6210\u7684Paddle\u9884\u6d4b\u5e93\u5730\u5740(`build/paddle_inference_install_dir`\u6587\u4ef6\u5939)\n    `CUDA_LIB_DIR`\u4e3acuda\u5e93\u6587\u4ef6\u5730\u5740\uff0c\u5728docker\u4e2d\u4e3a`/usr/local/cuda/lib64`\n    `CUDNN_LIB_DIR`\u4e3acudnn\u5e93\u6587\u4ef6\u5730\u5740\uff0c\u5728docker\u4e2d\u4e3a`/usr/lib/x86_64-linux-gnu/`\u3002\n    **\u5982\u679c\u5e0c\u671b\u9884\u6d4b\u65f6\u5f00\u542fTensorRT\u52a0\u901f\u529f\u80fd\uff0c\u90a3\u4e48\u8fd8\u9700\u8981\u4fee\u6539`tools/build.sh`3\u5904\u4ee3\u7801**"
        },
        {
            "comment": "This code sets the necessary environment variables and provides instructions for running PaddleVideo's C++ prediction demo. It supports video recognition mode with optional parameters such as model directory, inference model name, video directory, number of segments, and segment length. Users can choose from PP-TSM or PP-TSN models.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/cpp_infer/readme.md\":213-258",
            "content": "    1. \u8bbe\u7f6e`DWITH_GPU=ON`\n    2. \u8bbe\u7f6e`DWITH_TENSORRT=ON`\n    3. \u8bbe\u7f6e`TENSORRT_DIR=/path/to/TensorRT-x.x.x.x`\n    **\u4ee5\u4e0a\u8def\u5f84\u90fd\u5199\u7edd\u5bf9\u8def\u5f84\uff0c\u4e0d\u8981\u5199\u76f8\u5bf9\u8def\u5f84**\n* \u7f16\u8bd1\u5b8c\u6210\u4e4b\u540e\uff0c\u4f1a\u5728`cpp_infer/build`\u6587\u4ef6\u5939\u4e0b\u751f\u6210\u4e00\u4e2a\u540d\u4e3a`ppvideo`\u7684\u53ef\u6267\u884c\u6587\u4ef6\u3002\n### 2.3 \u8fd0\u884cPaddleVideo C++\u9884\u6d4bdemo\n\u8fd0\u884c\u65b9\u5f0f\uff1a\n```bash\n./build/ppvideo <mode> [--param1] [--param2] [...]\n```\n\u5176\u4e2d\uff0c`mode`\u4e3a\u5fc5\u9009\u53c2\u6570\uff0c\u8868\u793a\u9009\u62e9\u7684\u529f\u80fd\uff0c\u53d6\u503c\u8303\u56f4['rec']\uff0c\u8868\u793a**\u89c6\u9891\u8bc6\u522b**\uff08\u66f4\u591a\u529f\u80fd\u4f1a\u9646\u7eed\u52a0\u5165\uff09\u3002\n##### 1. \u8c03\u7528\u89c6\u9891\u8bc6\u522b\uff1a\n```bash\n# \u8c03\u7528PP-TSM\u8bc6\u522b\n./build/ppvideo rec \\\n--rec_model_dir=../../inference/ppTSM \\\n--inference_model_name=ppTSM \\\n--video_dir=./example_video_dir \\\n--num_seg=8 \\\n--seg_len=1\n# \u8c03\u7528PP-TSN\u8bc6\u522b\n./build/ppvideo rec \\\n--rec_model_dir=../../inference/ppTSN \\\n--inference_model_name=ppTSN \\\n--video_dir=./example_video_dir \\\n--num_seg=25 \\\n--seg_len=1\n```\n\u66f4\u591a\u53c2\u6570\u5982\u4e0b\uff1a\n- \u901a\u7528\u53c2\u6570\n    | \u53c2\u6570\u540d\u79f0      | \u7c7b\u578b | \u9ed8\u8ba4\u53c2\u6570        | \u610f\u4e49                                                         |\n    | ------------- | ---- | --------------- | ------------------------------------------------------------ |\n    | use_gpu       | bool | false           | \u662f\u5426\u4f7f\u7528GPU                                                  |"
        },
        {
            "comment": "This code snippet defines various parameters for video recognition model execution. It specifies GPU ID, requested GPU memory, CPU thread count for faster predictions on machines with sufficient cores, boolean values to enable mkldnn and tensorrt libraries, precision type for predictions (fp32/fp16/uint8), and a flag to start benchmarking during prediction. The video recognition model parameters include the path to the folder containing videos to be recognized.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/cpp_infer/readme.md\":259-272",
            "content": "    | gpu_id        | int  | 0               | GPU id\uff0c\u4f7f\u7528GPU\u65f6\u6709\u6548                                        |\n    | gpu_mem       | int  | 4000            | \u7533\u8bf7\u7684GPU\u5185\u5b58                                                |\n    | cpu_threads   | int  | 10              | CPU\u9884\u6d4b\u65f6\u7684\u7ebf\u7a0b\u6570\uff0c\u5728\u673a\u5668\u6838\u6570\u5145\u8db3\u7684\u60c5\u51b5\u4e0b\uff0c\u8be5\u503c\u8d8a\u5927\uff0c\u9884\u6d4b\u901f\u5ea6\u8d8a\u5feb |\n    | enable_mkldnn | bool | false           | \u662f\u5426\u4f7f\u7528mkldnn\u5e93                                             |\n    | use_tensorrt  | bool | false           | \u662f\u5426\u4f7f\u7528tensorrt\u5e93                                           |\n    | precision     | str  | \"fp32\"          | \u4f7f\u7528fp32/fp16/uint8\u7cbe\u5ea6\u6765\u9884\u6d4b                                |\n    | benchmark     | bool | true            | \u9884\u6d4b\u65f6\u662f\u5426\u5f00\u542fbenchmark\uff0c\u5f00\u542f\u540e\u4f1a\u5728\u6700\u540e\u8f93\u51fa\u914d\u7f6e\u3001\u6a21\u578b\u3001\u8017\u65f6\u7b49\u4fe1\u606f\u3002 |\n- \u89c6\u9891\u8bc6\u522b\u6a21\u578b\u76f8\u5173\n    | \u53c2\u6570\u540d\u79f0       | \u7c7b\u578b   | \u9ed8\u8ba4\u53c2\u6570                                      | \u610f\u4e49                                 |\n    | -------------- | ------ | --------------------------------------------- | ------------------------------------ |\n    | video_dir      | string | \"../example_video_dir\"                        | \u5b58\u653e\u5c06\u8981\u8bc6\u522b\u7684\u89c6\u9891\u7684\u6587\u4ef6\u5939\u8def\u5f84       |"
        },
        {
            "comment": "The code is configuring the model directory path, inference model name, number of video segments, length of each segment, batch size for prediction, and the file path containing class labels and names. An example input video is used to demonstrate how the code outputs the detected results on the screen, including video file, classification, and confidence score.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/cpp_infer/readme.md\":273-288",
            "content": "    | rec_model_dir  | string | \"\"                                            | \u5b58\u653e\u5bfc\u51fa\u7684\u9884\u6d4b\u6a21\u578b\u7684\u6587\u4ef6\u5939\u8def\u5f84       |\n    | inference_model_name | string | \"ppTSM\"                                 | \u9884\u6d4b\u6a21\u578b\u7684\u540d\u79f0 |\n    | num_seg        | int    | 8                                             | \u89c6\u9891\u5206\u6bb5\u7684\u6bb5\u6570                       |\n    | seg_len        | int    | 1                                             | \u89c6\u9891\u6bcf\u6bb5\u62bd\u53d6\u7684\u5e27\u6570                   |\n    | rec_batch_num  | int    | 1                                             | \u6a21\u578b\u9884\u6d4b\u65f6\u7684batch size               |\n    | char_list_file | str    | \"../../data/k400/Kinetics-400_label_list.txt\" | \u5b58\u653e\u6240\u6709\u7c7b\u522b\u6807\u53f7\u548c\u5bf9\u5e94\u540d\u5b57\u7684\u6587\u672c\u8def\u5f84 |\n\u200b\t\u4ee5example_video_dir\u4e0b\u7684\u6837\u4f8b\u89c6\u9891`example01.avi`\u4e3a\u8f93\u5165\u89c6\u9891\u4e3a\u4f8b\uff0c\u6700\u7ec8\u5c4f\u5e55\u4e0a\u4f1a\u8f93\u51fa\u68c0\u6d4b\u7ed3\u679c\u5982\u4e0b\u3002\n```bash\n[./inference/ppTSM]\n[./deploy/cpp_infer/example_video_dir]\ntotal videos num: 1\n./example_video_dir/example01.avi   class: 5 archery       score: 0.999556\nI1125 08:10:45.834288 13955 autolog.h:50] ----------------------- Config info -----------------------\nI1125 08:10:45.834458 13955 autolog.h:51] runtime_device: cpu"
        },
        {
            "comment": "This code configures the inference engine with options for optimizing IR, memory optimization, TensorRT and MKLDNN support. It also sets the number of CPU threads, displays data information (batch size, input shape, data count), model name and precision, and logs the total time spent for inference.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/cpp_infer/readme.md\":289-303",
            "content": "I1125 08:10:45.834467 13955 autolog.h:52] ir_optim: True\nI1125 08:10:45.834475 13955 autolog.h:53] enable_memory_optim: True\nI1125 08:10:45.834483 13955 autolog.h:54] enable_tensorrt: 0\nI1125 08:10:45.834518 13955 autolog.h:55] enable_mkldnn: False\nI1125 08:10:45.834525 13955 autolog.h:56] cpu_math_library_num_threads: 10\nI1125 08:10:45.834532 13955 autolog.h:57] ----------------------- Data info -----------------------\nI1125 08:10:45.834540 13955 autolog.h:58] batch_size: 1\nI1125 08:10:45.834547 13955 autolog.h:59] input_shape: dynamic\nI1125 08:10:45.834556 13955 autolog.h:60] data_num: 1\nI1125 08:10:45.834564 13955 autolog.h:61] ----------------------- Model info -----------------------\nI1125 08:10:45.834573 13955 autolog.h:62] model_name: rec\nI1125 08:10:45.834579 13955 autolog.h:63] precision: fp32\nI1125 08:10:45.834586 13955 autolog.h:64] ----------------------- Perf info ------------------------\nI1125 08:10:45.834594 13955 autolog.h:65] Total time spent(ms): 2739\nI1125 08:10:45.834602 13955 au"
        },
        {
            "comment": "This code snippet displays the preprocess time, inference time, and postprocess time for a certain task. It shows that the inference time is 1269.55ms and the postprocess time is 0.009118ms. The error message indicates a problem with finding the 'libcudnn.so' library due to an incorrect or missing CUDNN_LIB_DIR setting.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/cpp_infer/readme.md\":303-323",
            "content": "tolog.h:67] preprocess_time(ms): 10.6524, inference_time(ms): 1269.55, postprocess_time(ms): 0.009118\n```\n### 3 FAQ\n1. \u7f16\u8bd1demo\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u4ee5\u4e0b\u9519\u8bef\n    ```shell\n    make[2]: *** No rule to make target '/usr/lib/x86_64-linux-gn/libcudnn.so', needed by 'ppvideo'.  Stop.\n    make[2]: *** Waiting for unfinished jobs....\n    [ 16%] Building CXX object CMakeFiles/ppvideo.dir/src/main.cpp.o\n    [ 50%] Building CXX object CMakeFiles/ppvideo.dir/src/preprocess_op.cpp.o\n    [ 50%] Building CXX object CMakeFiles/ppvideo.dir/src/postprocess_op.cpp.o\n    [ 83%] Building CXX object CMakeFiles/ppvideo.dir/src/utility.cpp.o\n    [ 83%] Building CXX object CMakeFiles/ppvideo.dir/src/video_rec.cpp.o\n    CMakeFiles/Makefile2:95: recipe for target 'CMakeFiles/ppvideo.dir/all' failed\n    make[1]: *** [CMakeFiles/ppvideo.dir/all] Error 2\n    Makefile:83: recipe for target 'all' failed\n    make: *** [all] Error 2\n    ```\n    \u53ef\u80fd\u662f`CUDNN_LIB_DIR`\u8bbe\u7f6e\u7684\u4e0d\u5bf9\uff0c\u5bfc\u81f4\u627e\u4e0d\u5230\u8be5\u76ee\u5f55\u4e0b\u7684`libcudnn.so`\u3002"
        }
    ]
}