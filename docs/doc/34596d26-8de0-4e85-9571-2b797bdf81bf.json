{
    "summary": "This code calculates precision, recall, and F1-score for a TransNetV2 metric machine learning model by handling scene location transformations and errors. It iterates through different thresholds before logging the results.",
    "details": [
        {
            "comment": "This function, named \"predictions_to_scenes\", takes in a list of predictions and outputs a list of scene locations. The scenes are determined by identifying changes from 0 to 1 and vice versa. If all predictions are 1, the function adds a final scene ending at the last index. The code also includes error checking for cases where all predictions are 1 or when there is a disruption in prediction data flow.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/transnetv2_metric.py\":0-33",
            "content": "# Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nimport numpy as np\nfrom .registry import METRIC\nfrom .base import BaseMetric\nfrom paddlevideo.utils import get_logger\nlogger = get_logger(\"paddlevideo\")\ndef predictions_to_scenes(predictions):\n    scenes = []\n    t, t_prev, start = -1, 0, 0\n    for i, t in enumerate(predictions):\n        if t_prev == 1 and t == 0:\n            start = i\n        if t_prev == 0 and t == 1 and i != 0:\n            scenes.append([start, i])\n        t_prev = t\n    if t == 0:\n        scenes.append([start, i])\n    # just fix if all predictions are 1"
        },
        {
            "comment": "This function converts scene lists to transition lists. If there are no scenes, it returns a transition list with one element. The function is based on an external source and adapted for specific use cases. It can handle different tolerance margins, which affects how the pred_scenes and gt_scenes are transformed into prediction transitions (pred_trans) and ground truth transitions (gt_trans), respectively. A \"HIT\" or \"MISS\" status is determined based on these converted lists.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/transnetv2_metric.py\":34-56",
            "content": "    if len(scenes) == 0:\n        return np.array([[0, len(predictions) - 1]], dtype=np.int32)\n    return np.array(scenes, dtype=np.int32)\ndef evaluate_scenes(gt_scenes, pred_scenes, n_frames_miss_tolerance=2):\n    \"\"\"\n    Adapted from: https://github.com/gyglim/shot-detection-evaluation\n    The original based on: http://imagelab.ing.unimore.it/imagelab/researchActivity.asp?idActivity=19\n    n_frames_miss_tolerance:\n        Number of frames it is possible to miss ground truth by, and still being counted as a correct detection.\n    Examples of computation with different tolerance margin:\n    n_frames_miss_tolerance = 0\n      pred_scenes: [[0, 5], [6, 9]] -> pred_trans: [[5.5, 5.5]]\n      gt_scenes:   [[0, 5], [6, 9]] -> gt_trans:   [[5.5, 5.5]] -> HIT\n      gt_scenes:   [[0, 4], [5, 9]] -> gt_trans:   [[4.5, 4.5]] -> MISS\n    n_frames_miss_tolerance = 1\n      pred_scenes: [[0, 5], [6, 9]] -> pred_trans: [[5.0, 6.0]]\n      gt_scenes:   [[0, 5], [6, 9]] -> gt_trans:   [[5.0, 6.0]] -> HIT\n      gt_scenes:   [[0, 4], [5, 9]] -> gt_trans:   [[4.0, 5.0]] -> HIT"
        },
        {
            "comment": "This code adjusts and transforms input frame scene and transition data, and then iterates through both to calculate true positives (TP), false positives (FP), and false negatives (FN) for evaluation.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/transnetv2_metric.py\":57-79",
            "content": "      gt_scenes:   [[0, 3], [4, 9]] -> gt_trans:   [[3.0, 4.0]] -> MISS\n    n_frames_miss_tolerance = 2\n      pred_scenes: [[0, 5], [6, 9]] -> pred_trans: [[4.5, 6.5]]\n      gt_scenes:   [[0, 5], [6, 9]] -> gt_trans:   [[4.5, 6.5]] -> HIT\n      gt_scenes:   [[0, 4], [5, 9]] -> gt_trans:   [[3.5, 5.5]] -> HIT\n      gt_scenes:   [[0, 3], [4, 9]] -> gt_trans:   [[2.5, 4.5]] -> HIT\n      gt_scenes:   [[0, 2], [3, 9]] -> gt_trans:   [[1.5, 3.5]] -> MISS\n      Users should be careful about adopting these functions in any commercial matters.\n    \"\"\"\n    shift = n_frames_miss_tolerance / 2\n    gt_scenes = gt_scenes.astype(np.float32) + np.array([[-0.5 + shift, 0.5 - shift]])\n    pred_scenes = pred_scenes.astype(np.float32) + np.array([[-0.5 + shift, 0.5 - shift]])\n    gt_trans = np.stack([gt_scenes[:-1, 1], gt_scenes[1:, 0]], 1)\n    pred_trans = np.stack([pred_scenes[:-1, 1], pred_scenes[1:, 0]], 1)\n    i, j = 0, 0\n    tp, fp, fn = 0, 0, 0\n    while i < len(gt_trans) or j < len(pred_trans):\n        if j == len(pred_trans) or pred_trans[j, 0] > gt_trans[i, 1]:"
        },
        {
            "comment": "This function calculates precision, recall, and F1-score for transnetv2 metric given ground truth (gt) and predicted (pred) transcript sequences. It iterates through the sequences to count true positives (tp), false negatives (fn), and false positives (fp). Afterwards, it computes precision, recall, and F1-score based on these counts. The function also asserts that the total number of true positives matches the length of gt_trans and the total number of false positives matches the length of pred_trans. It then returns the calculated metrics and the count of tp, fp, and fn. The create_scene_based_summaries function generates precision, recall, and F1-score for different thresholds using a numpy array. It initializes these metrics as well as the counts of true positives, false negatives, false positives, and false negatives to zero, then iterates over the thresholds to calculate the metric values for each threshold.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/transnetv2_metric.py\":80-119",
            "content": "            fn += 1\n            i += 1\n        elif i == len(gt_trans) or pred_trans[j, 1] < gt_trans[i, 0]:\n            fp += 1\n            j += 1\n        else:\n            i += 1\n            j += 1\n            tp += 1\n    if tp + fp != 0:\n        p = tp / (tp + fp)\n    else:\n        p = 0\n    if tp + fn != 0:\n        r = tp / (tp + fn)\n    else:\n        r = 0\n    if p + r != 0:\n        f1 = (p * r * 2) / (p + r)\n    else:\n        f1 = 0\n    assert tp + fn == len(gt_trans)\n    assert tp + fp == len(pred_trans)\n    return p, r, f1, (tp, fp, fn)\ndef create_scene_based_summaries(one_hot_pred, one_hot_gt):\n    thresholds = np.array([\n        0.02, 0.06, 0.1, 0.15, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9\n    ])\n    precision, recall, f1, tp, fp, fn = np.zeros_like(thresholds), np.zeros_like(thresholds),\\\n                                        np.zeros_like(thresholds), np.zeros_like(thresholds),\\\n                                        np.zeros_like(thresholds), np.zeros_like(thresholds)\n    gt_scenes = predictions_to_scenes(one_hot_gt)"
        },
        {
            "comment": "This code is from the TransNetV2Metric class, which calculates metrics for a model's predictions. It iterates through different thresholds to compute precision, recall, F1 score, and true positive, false positive, and false negative counts. The update method appends predictions and computes metrics when a new file is encountered.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/transnetv2_metric.py\":120-151",
            "content": "    for i in range(len(thresholds)):\n        pred_scenes = predictions_to_scenes(\n            (one_hot_pred > thresholds[i]).astype(np.uint8)\n        )\n        precision[i], recall[i], f1[i], (tp[i], fp[i], fn[i]) = evaluate_scenes(gt_scenes, pred_scenes)\n    best_idx = np.argmax(f1)\n    return f1[best_idx]\n@METRIC.register\nclass TransNetV2Metric(BaseMetric):\n    def __init__(self, data_size, batch_size, log_interval=1):\n        \"\"\"prepare for metrics\n        \"\"\"\n        super().__init__(data_size, batch_size, log_interval)\n        self.predictions = []\n        self.total_stats = {\"tp\": 0, \"fp\": 0, \"fn\": 0}\n    def update(self, batch_id, data, one_hot):\n        \"\"\"update metrics during each iter\n        \"\"\"\n        if isinstance(one_hot, tuple):\n            one_hot = one_hot[0]\n        one_hot = paddle.nn.functional.sigmoid(one_hot)[0]\n        self.predictions.append(one_hot.numpy()[25:75])\n        gt_scenes = data[1]\n        is_new_file = data[2]\n        if is_new_file:\n            self.compute(gt_scenes)\n        # preds ensemble"
        },
        {
            "comment": "The code calculates precision, recall, and F1 score for a machine learning model. It accumulates the metrics after processing all batches and logs the results using logger. It also displays the Precision, Recall, and F1 Score at the end of computation.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/transnetv2_metric.py\":152-173",
            "content": "        if batch_id % self.log_interval == 0:\n            logger.info(\"[TEST] Processing batch {}/{} ...\".format(\n                batch_id,\n                self.data_size // (self.batch_size * self.world_size)))\n    def compute(self, gt_scenes):\n        predictions = np.concatenate(self.predictions, 0)[:len(frames)]\n        _, _, _, (tp, fp, fn), fp_mistakes, fn_mistakes = evaluate_scenes(\n            gt_scenes, predictions_to_scenes((predictions >= args.thr).astype(np.uint8)))\n        self.total_stats[\"tp\"] += tp\n        self.total_stats[\"fp\"] += fp\n        self.total_stats[\"fn\"] += fn\n    def accumulate(self):\n        \"\"\"accumulate metrics when finished all iters.\n        \"\"\"\n        p = self.total_stats[\"tp\"] / (self.total_stats[\"tp\"] + self.total_stats[\"fp\"])\n        r = self.total_stats[\"tp\"] / (self.total_stats[\"tp\"] + self.total_stats[\"fn\"])\n        f1 = (p * r * 2) / (p + r)\n        logger.info('[TEST] finished, Precision= {:5.2f}, Recall= {:5.2f} , F1 Score= {:5.2f} '.format(\n            p * 100, r * 100, f1 * 100))"
        }
    ]
}