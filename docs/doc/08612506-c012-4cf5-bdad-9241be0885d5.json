{
    "summary": "The code introduces a DropPath layer, Swin Transformer backbone with window-based multi-head attention for image processing, and implements the Swin Transformer Block 3D in PaddleVideo, which also features a 3D PatchEmbed3D and 3D backbone.",
    "details": [
        {
            "comment": "Copyright notice, import statements, and drop_path function definition for stochastic depth in residual blocks.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/swin_transformer.py\":0-32",
            "content": "# Copyright (c) 2021  PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom functools import lru_cache, reduce\nfrom operator import mul\nimport numpy as np\nimport paddle\nimport paddle.nn as nn\nimport paddle.nn.functional as F\nfrom paddle.nn.initializer import Constant\nfrom ...utils import load_ckpt\nfrom ..registry import BACKBONES\nfrom ..weight_init import trunc_normal_\nzeros_ = Constant(value=0.)\nones_ = Constant(value=1.)\ndef drop_path(x, drop_prob=0., training=False):\n    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks)."
        },
        {
            "comment": "This code snippet defines a \"DropPath\" layer that applies drop paths (Stochastic Depth) to the input, based on the provided drop probability. The drop paths are applied in the main path of residual blocks for each sample. This class also includes a forward method that drops out elements from the input with the specified probability during training but returns the original input unchanged when not training or if the drop probability is 0.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/swin_transformer.py\":33-63",
            "content": "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n    # issuecomment-532968956 ...\n    See discussion: https://github.com/tensorflow/tpu/issues/494\n    \"\"\"\n    if drop_prob == 0. or not training:\n        return x\n    keep_prob = paddle.to_tensor(1 - drop_prob)\n    shape = (paddle.shape(x)[0], ) + (1, ) * (x.ndim - 1)\n    random_tensor = keep_prob + paddle.rand(shape, dtype=x.dtype)\n    random_tensor = paddle.floor(random_tensor)  # binarize\n    output = x.divide(keep_prob) * random_tensor\n    return output\nclass DropPath(nn.Layer):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n    \"\"\"\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\nclass Mlp(nn.Layer):\n    \"\"\" Multilayer perceptron.\"\"\"\n    def __init__(self,\n                 in_features,\n                 hidden_features=None,"
        },
        {
            "comment": "The code above defines a layer for the Swin Transformer backbone. It contains two linear layers, an activation function (GELU), and a dropout layer. The `window_partition` function partitions input tensor based on specified window size.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/swin_transformer.py\":64-98",
            "content": "                 out_features=None,\n                 act_layer=nn.GELU,\n                 drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\ndef window_partition(x, window_size):\n    \"\"\"window_partition\n    Args:\n        x (Tensor): x.shape = [B, D, H, W, C]\n        window_size (tuple[int]): window_size\n    Returns:\n        Tensor: (B*num_windows, window_size*window_size, C)\n    \"\"\"\n    B, D, H, W, C = x.shape\n    x = x.reshape([\n        B, D // window_size[0], window_size[0], H // window_size[1],\n        window_size[1], W // window_size[2], window_size[2], C\n    ])\n    windows = x.transpose([0, 1, 3, 5, 2, 4, 6,"
        },
        {
            "comment": "The code defines a function `window_reverse` that takes a set of windows and rearranges them back into the original image shape. The `get_window_size` function determines the appropriate window size based on input dimensions. Both functions are used in the Swin Transformer backbone model.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/swin_transformer.py\":99-136",
            "content": "                           7]).reshape([-1, reduce(mul, window_size), C])\n    return windows\nclass Identity(nn.Layer):\n    def __init__(self):\n        super(Identity, self).__init__()\n    def forward(self, input):\n        return input\ndef window_reverse(windows, window_size, B, D, H, W):\n    \"\"\"\n    Args:\n        windows: (B*num_windows, window_size, window_size, C)\n        window_size (tuple[int]): Window size\n        H (int): Height of image\n        W (int): Width of image\n    Returns:\n        x: (B, D, H, W, C)\n    \"\"\"\n    x = windows.reshape([\n        B, D // window_size[0], H // window_size[1], W // window_size[2],\n        window_size[0], window_size[1], window_size[2], -1\n    ])\n    x = x.transpose([0, 1, 4, 2, 5, 3, 6, 7]).reshape([B, D, H, W, -1])\n    return x\ndef get_window_size(x_size, window_size, shift_size=None):\n    use_window_size = list(window_size)\n    if shift_size is not None:\n        use_shift_size = list(shift_size)\n    for i in range(len(x_size)):\n        if x_size[i] <= window_size[i]:\n            use_window_size[i] = x_size[i]"
        },
        {
            "comment": "This code defines a class called \"WindowAttention3D\" which implements a window-based multi-head self attention module with relative position bias. It supports both shifted and non-shifted windows, and takes in parameters such as the number of input channels (dim), temporal length, height and width of the window (window_size), number of attention heads (num_heads), whether to add a learnable bias to query, key, value (qkv_bias), override default qk scale of head_dim ** -0.5 if set (qk_scale), dropout ratio of attention weight (attn_drop), and dropout ratio of output (proj_drop). The function at the top part of the code determines whether to use window or shift size based on a given value.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/swin_transformer.py\":137-160",
            "content": "            if shift_size is not None:\n                use_shift_size[i] = 0\n    if shift_size is None:\n        return tuple(use_window_size)\n    else:\n        return tuple(use_window_size), tuple(use_shift_size)\nclass WindowAttention3D(nn.Layer):\n    \"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n    Args:\n        dim (int): Number of input channels.\n        window_size (tuple[int]): The temporal length, height and width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n    \"\"\"\n    def __init__(self,\n                 dim,\n                 window_size,"
        },
        {
            "comment": "This code initializes the Swin Transformer's self-attention module. It defines a window size and number of attention heads, calculates head dimensions, sets up position bias table, and adds parameters for position bias table and head dimensions. The code also creates coordinate arrays for dimension and height inside the window.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/swin_transformer.py\":161-184",
            "content": "                 num_heads,\n                 qkv_bias=False,\n                 qk_scale=None,\n                 attn_drop=0.,\n                 proj_drop=0.):\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size  # Wd, Wh, Ww\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim**-0.5\n        # define a parameter table of relative position bias\n        self.relative_position_bias_table = self.create_parameter(\n            shape=((2 * window_size[0] - 1) * (2 * window_size[1] - 1) *\n                   (2 * window_size[2] - 1), num_heads),\n            default_initializer=zeros_,\n        )  # 2*Wd-1 * 2*Wh-1 * 2*Ww-1, nH\n        self.add_parameter(\"relative_position_bias_table\",\n                           self.relative_position_bias_table)\n        # get pair-wise relative position index for each token inside the window\n        coords_d = paddle.arange(self.window_size[0])\n        coords_h = paddle.arange(self.window_size[1])"
        },
        {
            "comment": "This code performs relative position encoding for the Swin Transformer by calculating relative coordinates of patches within a sliding window. It first creates 2D and 3D coordinate grids, then subtracts them to obtain relative positions. Finally, it shifts and scales the relative coordinates to fit the range of the window size.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/swin_transformer.py\":185-203",
            "content": "        coords_w = paddle.arange(self.window_size[2])\n        coords = paddle.stack(paddle.meshgrid(coords_d, coords_h,\n                                              coords_w))  # 3, Wd, Wh, Ww\n        coords_flatten = paddle.flatten(coords, 1)  # 3, Wd*Wh*Ww\n        relative_coords = coords_flatten.unsqueeze(\n            axis=2) - coords_flatten.unsqueeze(axis=1)  # 3, Wd*Wh*Ww, Wd*Wh*Ww\n        # relative_coords = coords_flatten.unsqueeze(2) - coords_flatten.unsqueeze(1)  # 3, Wd*Wh*Ww, Wd*Wh*Ww\n        relative_coords = relative_coords.transpose([1, 2, 0\n                                                     ])  # Wd*Wh*Ww, Wd*Wh*Ww, 3\n        relative_coords[:, :,\n                        0] += self.window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += self.window_size[1] - 1\n        relative_coords[:, :, 2] += self.window_size[2] - 1\n        relative_coords[:, :, 0] *= (2 * self.window_size[1] -\n                                     1) * (2 * self.window_size[2] - 1)\n        relative_coords[:, :, 1] *= (2 * self.window_size[2] - 1)"
        },
        {
            "comment": "This code initializes a Swin Transformer backbone by registering a buffer for relative position indices and defining the linear projections, dropouts, softmax function, and forward pass. The forward function takes input features of shape (num_windows*B, N, C) and performs multi-head self-attention with learned query, key, and value matrices, scaled by the square root of the dimension. Attention is calculated using dot product between queries and keys, and then passed through a softmax function for normalization before being multiplied by values and projected back to the original feature space.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/swin_transformer.py\":204-231",
            "content": "        relative_position_index = relative_coords.sum(\n            axis=-1)  # Wd*Wh*Ww, Wd*Wh*Ww\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n        self.qkv = nn.Linear(dim, dim * 3, bias_attr=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n        trunc_normal_(self.relative_position_bias_table, std=0.02)\n        self.softmax = nn.Softmax(axis=-1)\n    def forward(self, x, mask=None):\n        \"\"\" Forward function.\n        Args:\n            x: input features with shape of (num_windows*B, N, C)\n            mask: (0/-inf) mask with shape of (num_windows, N, N) or None\n        \"\"\"\n        B_, N, C = x.shape\n        qkv = self.qkv(x).reshape(\n            [B_, N, 3, self.num_heads,\n             C // self.num_heads]).transpose([2, 0, 3, 1, 4])\n        q, k, v = qkv[0], qkv[1], qkv[2]  # B_, nH, N, C\n        q = q * self.scale\n        attn = q @ k.transpose([0, 1, 3, 2])\n        relative_position_bias = self.relative_position_bias_table["
        },
        {
            "comment": "This code defines the Swin Transformer Block 3D, which implements a self-attention mechanism for multi-dimensional data. It adds relative position biases to the attention scores, applies a mask if provided, and applies softmax normalization. Finally, it passes the result through two dropout layers before outputting the transformed feature map.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/swin_transformer.py\":232-260",
            "content": "            self.relative_position_index[:N, :N].reshape([-1])].reshape(\n                [N, N, -1])  # Wd*Wh*Ww,Wd*Wh*Ww,nH\n        relative_position_bias = relative_position_bias.transpose(\n            [2, 0, 1])  # nH, Wd*Wh*Ww, Wd*Wh*Ww\n        attn = attn + relative_position_bias.unsqueeze(0)  # B_, nH, N, N\n        if mask is not None:\n            nW = mask.shape[0]\n            attn = attn.reshape([B_ // nW, nW, self.num_heads, N, N\n                                 ]) + mask.unsqueeze(1).unsqueeze(0).astype(attn.dtype)\n            attn = attn.reshape([-1, self.num_heads, N, N])\n            attn = self.softmax(attn)\n        else:\n            attn = self.softmax(attn)\n        attn = self.attn_drop(attn)\n        x = (attn @ v).transpose([0, 2, 1, 3]).reshape([B_, N, C])\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\nclass SwinTransformerBlock3D(nn.Layer):\n    \"\"\" Swin Transformer Block.\n    Args:\n        dim (int): Number of input channels.\n        num_heads (int): Number of attention heads."
        },
        {
            "comment": "This code initializes a class for the Swin Transformer backbone, specifying the dimensions, number of heads, window size, shift size, mlp ratio, and various optional parameters like dropout rates and activation layers.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/swin_transformer.py\":261-281",
            "content": "        window_size (tuple[int]): Window size.\n        shift_size (tuple[int]): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n        act_layer (nn.Layer, optional): Activation layer. Default: nn.GELU\n        norm_layer (nn.Layer, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n    def __init__(self,\n                 dim,\n                 num_heads,\n                 window_size=(2, 7, 7),\n                 shift_size=(0, 0, 0),\n                 mlp_ratio=4.,\n                 qkv_bias=True,\n                 qk_scale=None,\n                 drop=0.,\n                 attn_drop=0.,"
        },
        {
            "comment": "The code defines a class for the Swin Transformer backbone in PaddleVideo. It takes input parameters such as dimension, number of attention heads, window size, and shift size, and initializes layers including norm_layer and attn layer. It performs assertions on shift sizes to ensure they are within the window size limits and then initializes the normalization layer.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/swin_transformer.py\":282-306",
            "content": "                 drop_path=0.,\n                 act_layer=nn.GELU,\n                 norm_layer=nn.LayerNorm,\n                 use_checkpoint=False):\n        super().__init__()\n        self.dim = dim\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.mlp_ratio = mlp_ratio\n        # self.use_checkpoint=use_checkpoint\n        assert 0 <= self.shift_size[0] < self.window_size[\n            0], \"shift_size must in 0-window_size\"\n        assert 0 <= self.shift_size[1] < self.window_size[\n            1], \"shift_size must in 0-window_size\"\n        assert 0 <= self.shift_size[2] < self.window_size[\n            2], \"shift_size must in 0-window_size\"\n        self.norm1 = norm_layer(dim)\n        self.attn = WindowAttention3D(dim,\n                                      window_size=self.window_size,\n                                      num_heads=num_heads,\n                                      qkv_bias=qkv_bias,\n                                      qk_scale=qk_scale,"
        },
        {
            "comment": "This code defines a Swin Transformer backbone class with parameters like window size, shift size, and drop path. It initializes the layers including attention and mlp blocks. The forward_part1 function pads input features to multiples of window size for processing.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/swin_transformer.py\":307-329",
            "content": "                                      attn_drop=attn_drop,\n                                      proj_drop=drop)\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim,\n                       hidden_features=mlp_hidden_dim,\n                       act_layer=act_layer,\n                       drop=drop)\n    def forward_part1(self, x, mask_matrix):\n        B = paddle.shape(x)[0]\n        _, D, H, W, C = x.shape\n        window_size, shift_size = get_window_size((D, H, W), self.window_size,\n                                                  self.shift_size)\n        x = self.norm1(x)\n        # pad feature maps to multiples of window size\n        pad_l = pad_t = pad_d0 = 0\n        pad_d1 = (window_size[0] - D % window_size[0]) % window_size[0]\n        pad_b = (window_size[1] - H % window_size[1]) % window_size[1]\n        pad_r = (window_size[2] - W % window_size[2]) % window_size[2]"
        },
        {
            "comment": "This code performs a cyclic shift on the input feature map, depending on the shift size. If any of the shift sizes are greater than 0, it applies the roll operation to the feature map along specific axes (1, 2, and 3). The shifted feature map is then partitioned into windows based on the window size specified. These windows go through a self-attention layer (self.attn) and are reshaped accordingly. Finally, a reverse cyclic shift is applied to the result before returning the output feature map. This process helps in performing window-based self-attention or spatial-wise self-attention in the Swin Transformer architecture.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/swin_transformer.py\":330-352",
            "content": "        x = F.pad(x, (pad_l, pad_r, pad_t, pad_b, pad_d0, pad_d1),\n                  data_format='NDHWC')\n        _, Dp, Hp, Wp, _ = x.shape\n        # cyclic shift\n        if any(i > 0 for i in shift_size):\n            shifted_x = paddle.roll(x,\n                                    shifts=(-shift_size[0], -shift_size[1],\n                                            -shift_size[2]),\n                                    axis=(1, 2, 3))\n            attn_mask = mask_matrix\n        else:\n            shifted_x = x\n            attn_mask = None\n        # partition windows\n        x_windows = window_partition(shifted_x,\n                                     window_size)  # B*nW, Wd*Wh*Ww, C\n        # W-MSA/SW-MSA\n        attn_windows = self.attn(x_windows, mask=attn_mask)  # B*nW, Wd*Wh*Ww, C\n        # merge windows\n        attn_windows = attn_windows.reshape([-1, *(window_size + (C, ))])\n        shifted_x = window_reverse(attn_windows, window_size, B, Dp, Hp,\n                                   Wp)  # B D' H' W' C\n        # reverse cyclic shift"
        },
        {
            "comment": "The code defines a function for the forward pass of a neural network. It consists of two parts: `forward_part1` and `forward_part2`. The function takes an input tensor, performs some operations, and returns the result. The `forward_part1` function applies a shift operation to the input based on a specified shift size, followed by a padding operation if necessary. The `forward_part2` function passes the input through a multi-layer perceptron (MLP) and applies dropout. Finally, the `forward` function combines the outputs of these two parts and returns the result after adding it to an initial shortcut connection.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/swin_transformer.py\":353-389",
            "content": "        if any(i > 0 for i in shift_size):\n            x = paddle.roll(shifted_x,\n                            shifts=(shift_size[0], shift_size[1],\n                                    shift_size[2]),\n                            axis=(1, 2, 3))\n        else:\n            x = shifted_x\n        if pad_d1 > 0 or pad_r > 0 or pad_b > 0:\n            x = x[:, :D, :H, :W, :]\n        return x\n    def forward_part2(self, x):\n        return self.drop_path(self.mlp(self.norm2(x)))\n    def forward(self, x, mask_matrix):\n        \"\"\" Forward function.\n        Args:\n            x: Input feature, tensor size (B, D, H, W, C).\n            mask_matrix: Attention mask for cyclic shift.\n        \"\"\"\n        shortcut = x\n        x = self.forward_part1(x, mask_matrix)\n        x = shortcut + self.drop_path(x).astype(shortcut.dtype)\n        x = x + self.forward_part2(x).astype(x.dtype)\n        return x\nclass PatchMerging(nn.Layer):\n    \"\"\" Patch Merging Layer\n    Args:\n        dim (int): Number of input channels.\n        norm_layer (nn.Layer, optional): Normalization layer.  Default: nn.LayerNorm"
        },
        {
            "comment": "The code defines a Swin Transformer backbone for an image model. The `__init__` method initializes the Swin Transformer with specified dimension and normalization layer. The forward function processes input feature by splitting, concatenating, normalizing, and reducing dimensions. The `compute_mask` function generates an image mask using LRU caching.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/swin_transformer.py\":390-425",
            "content": "    \"\"\"\n    def __init__(self, dim, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.dim = dim\n        self.reduction = nn.Linear(4 * dim, 2 * dim, bias_attr=False)\n        self.norm = norm_layer(4 * dim)\n    def forward(self, x):\n        \"\"\" Forward function.\n        Args:\n            x: Input feature, tensor size (B, D, H, W, C).\n        \"\"\"\n        B, D, H, W, C = x.shape\n        # padding\n        pad_input = (H % 2 == 1) or (W % 2 == 1)\n        if pad_input:\n            x = F.pad(x, (0, W % 2, 0, H % 2, 0, 0), data_format='NDHWC')\n        x0 = x[:, :, 0::2, 0::2, :]  # B D H/2 W/2 C\n        x1 = x[:, :, 1::2, 0::2, :]  # B D H/2 W/2 C\n        x2 = x[:, :, 0::2, 1::2, :]  # B D H/2 W/2 C\n        x3 = x[:, :, 1::2, 1::2, :]  # B D H/2 W/2 C\n        x = paddle.concat([x0, x1, x2, x3], -1)  # B D H/2 W/2 4*C\n        x = self.norm(x)\n        x = self.reduction(x)\n        return x\n# cache each stage results\n@lru_cache()\ndef compute_mask(D, H, W, window_size, shift_size):\n    img_mask = paddle.zeros((1, D, H, W, 1))  # 1 Dp Hp Wp 1"
        },
        {
            "comment": "This code generates an attention mask for a Swin Transformer model. It iterates through various dimensions (d, h, w) within the window size and shift size, assigning incremental values to each position in the img_mask tensor. The resulting img_mask is then partitioned into non-overlapping windows and squeezed along the last dimension to create mask_windows. Finally, attn_mask is created by subtracting the expanded version of mask_windows from itself, effectively creating a binary mask where values are either 0 or -100.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/swin_transformer.py\":426-442",
            "content": "    cnt = 0\n    for d in slice(-window_size[0]), slice(-window_size[0],\n                                           -shift_size[0]), slice(\n                                               -shift_size[0], None):\n        for h in slice(-window_size[1]), slice(-window_size[1],\n                                               -shift_size[1]), slice(\n                                                   -shift_size[1], None):\n            for w in slice(-window_size[2]), slice(-window_size[2],\n                                                   -shift_size[2]), slice(\n                                                       -shift_size[2], None):\n                img_mask[:, d, h, w, :] = cnt\n                cnt += 1\n    mask_windows = window_partition(img_mask,\n                                    window_size)  # nW, ws[0]*ws[1]*ws[2], 1\n    mask_windows = mask_windows.squeeze(-1)  # nW, ws[0]*ws[1]*ws[2]\n    attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n    # attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))"
        },
        {
            "comment": "The code defines a Swin Transformer layer for one stage in a neural network. The BasicLayer class takes various arguments such as feature channel dimensions, depth, number of heads, local window size, etc. It also includes an MLP (Multi-Layer Perceptron) with a specified ratio, and provides options to add learnable bias, scale factors, dropout rates, stochastic depth rate, and a normalization layer for each input. This basic layer can be utilized in the Swin Transformer architecture for feature extraction and classification tasks.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/swin_transformer.py\":443-463",
            "content": "    huns = -100.0 * paddle.ones_like(attn_mask)\n    attn_mask = huns * (attn_mask != 0).astype(\"float32\")\n    return attn_mask\nclass BasicLayer(nn.Layer):\n    \"\"\" A basic Swin Transformer layer for one stage.\n    Args:\n        dim (int): Number of feature channels\n        depth (int): Depths of this stage.\n        num_heads (int): Number of attention head.\n        window_size (tuple[int]): Local window size. Default: (1,7,7).\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n        norm_layer (nn.Layer, optional): Normalization layer. Default: nn.LayerNorm\n   "
        },
        {
            "comment": "This code defines a 3D Swin Transformer block with optional downsampling layer at the end. It takes parameters such as dim, depth, num_heads, window size, mlp ratio, etc., and initializes an instance of the class SwinTransformerBlock3D for each block in a LayerList. The window size is set to (1, 7, 7) by default and the shift size is determined based on whether the current index is even or odd.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/swin_transformer.py\":463-492",
            "content": "     downsample (nn.Layer | None, optional): Downsample layer at the end of the layer. Default: None\n    \"\"\"\n    def __init__(self,\n                 dim,\n                 depth,\n                 num_heads,\n                 window_size=(1, 7, 7),\n                 mlp_ratio=4.,\n                 qkv_bias=False,\n                 qk_scale=None,\n                 drop=0.,\n                 attn_drop=0.,\n                 drop_path=0.,\n                 norm_layer=nn.LayerNorm,\n                 downsample=None,\n                 use_checkpoint=False):\n        super().__init__()\n        self.window_size = window_size\n        self.shift_size = tuple(i // 2 for i in window_size)\n        self.depth = depth\n        self.use_checkpoint = use_checkpoint\n        # build blocks\n        self.blocks = nn.LayerList([\n            SwinTransformerBlock3D(\n                dim=dim,\n                num_heads=num_heads,\n                window_size=window_size,\n                shift_size=(0, 0, 0) if (i % 2 == 0) else self.shift_size,\n                mlp_ratio=mlp_ratio,"
        },
        {
            "comment": "This code defines a Swin Transformer block for the PaddleVideo library. It takes input dimensions and creates multiple linear layers for self-attention, followed by a downsampling operation if needed. The forward function calculates an attention mask based on window size and shifts before rearranging the input tensor.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/swin_transformer.py\":493-521",
            "content": "                qkv_bias=qkv_bias,\n                qk_scale=qk_scale,\n                drop=drop,\n                attn_drop=attn_drop,\n                drop_path=drop_path[i]\n                if isinstance(drop_path, list) else drop_path,\n                norm_layer=norm_layer,\n                use_checkpoint=use_checkpoint,\n            ) for i in range(depth)\n        ])\n        self.downsample = downsample\n        if self.downsample is not None:\n            self.downsample = downsample(dim=dim, norm_layer=norm_layer)\n    def forward(self, x):\n        \"\"\" Forward function.\n        Args:\n            x: Input feature, tensor size (B, C, D, H, W).\n        \"\"\"\n        # calculate attention mask for SW-MSA\n        B = paddle.shape(x)[0]\n        _, C, D, H, W = x.shape\n        window_size, shift_size = get_window_size((D, H, W), self.window_size,\n                                                  self.shift_size)\n        # x = rearrange(x, 'b c d h w -> b d h w c')\n        x = x.transpose([0, 2, 3, 4, 1])\n        Dp = int(np.ceil(D / window_size[0])) * window_size[0]"
        },
        {
            "comment": "This code implements a PatchEmbed3D class, which embeds input video frames into patches for use in the Swin Transformer model. It takes the input video frames, divides them into non-overlapping patches, and performs linear projections on the patches to obtain embeddings. The patch size, number of input channels, and embedding dimension are configurable parameters.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/swin_transformer.py\":522-550",
            "content": "        Hp = int(np.ceil(H / window_size[1])) * window_size[1]\n        Wp = int(np.ceil(W / window_size[2])) * window_size[2]\n        attn_mask = compute_mask(Dp, Hp, Wp, window_size, shift_size)\n        for blk in self.blocks:\n            x = blk(x, attn_mask)\n        x = x.reshape([B, D, H, W, C])\n        if self.downsample is not None:\n            x = self.downsample(x)\n        x = x.transpose([0, 4, 1, 2, 3])\n        return x\nclass PatchEmbed3D(nn.Layer):\n    \"\"\" Video to Patch Embedding.\n    Args:\n        patch_size (int): Patch token size. Default: (2,4,4).\n        in_chans (int): Number of input video channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Layer, optional): Normalization layer. Default: None\n    \"\"\"\n    def __init__(self,\n                 patch_size=(2, 4, 4),\n                 in_chans=3,\n                 embed_dim=96,\n                 norm_layer=None):\n        super().__init__()\n        self.patch_size = patch_size"
        },
        {
            "comment": "This code is for the Swin Transformer backbone in PaddleVideo. It initializes the module with input channels (in_chans), embed dim, and patch size. If a norm layer is provided, it also initializes the normalization layer (norm). The forward function pads the input according to the dimensions and applies a convolution operation for feature extraction. If a normalization layer was initialized, it performs normalization on the features before returning them.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/swin_transformer.py\":552-580",
            "content": "        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n        self.proj = nn.Conv3D(in_chans,\n                              embed_dim,\n                              kernel_size=patch_size,\n                              stride=patch_size)\n        if norm_layer is not None:\n            self.norm = norm_layer(embed_dim)\n        else:\n            self.norm = None\n    def forward(self, x):\n        _, _, D, H, W = x.shape\n        if W % self.patch_size[2] != 0:\n            x = F.pad(\n                x, (0, self.patch_size[2] - W % self.patch_size[2], 0, 0, 0, 0),\n                data_format='NCDHW')\n        if H % self.patch_size[1] != 0:\n            x = F.pad(\n                x, (0, 0, 0, self.patch_size[1] - H % self.patch_size[1], 0, 0),\n                data_format='NCDHW')\n        if D % self.patch_size[0] != 0:\n            x = F.pad(\n                x, (0, 0, 0, 0, 0, self.patch_size[0] - D % self.patch_size[0]),\n                data_format='NCDHW')\n        x = self.proj(x)  # B C D Wh Ww\n        if self.norm is not None:"
        },
        {
            "comment": "This code defines the Swin Transformer 3D backbone for Paddle Video. It takes an input tensor and performs normalization, transposition, and reshaping operations before returning the processed tensor. The class also registers with BACKBONES to be recognized as a valid backbone model.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/swin_transformer.py\":581-603",
            "content": "            D, Wh, Ww = x.shape[2], x.shape[3], x.shape[4]\n            x = x.flatten(2).transpose([0, 2, 1])\n            x = self.norm(x)\n            x = x.transpose([0, 2, 1]).reshape([-1, self.embed_dim, D, Wh, Ww])\n        return x\n@BACKBONES.register()\nclass SwinTransformer3D(nn.Layer):\n    \"\"\" Swin Transformer backbone.\n        A Paddle impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n          https://arxiv.org/pdf/2103.14030\n    Args:\n        patch_size (int | tuple(int)): Patch size. Default: (4,4,4).\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        depths (tuple[int]): Depths of each Swin Transformer stage.\n        num_heads (tuple[int]): Number of attention head of each stage.\n        window_size (int): Window size. Default: 7.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.\n        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: Truee"
        },
        {
            "comment": "This code defines the initialization parameters for the SWIN Transformer model in PaddleVideo. Parameters include pretrained weights, patch size, input channels, embedding dimension, depths of each stage, number of heads per stage, window size, MLP ratio, qkv_bias, qk scale, drop rate, attn drop rate, and stochastic depth rate. The normalization layer and whether to freeze any stages can also be specified during initialization.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/swin_transformer.py\":604-626",
            "content": "        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set.\n        drop_rate (float): Dropout rate.\n        attn_drop_rate (float): Attention dropout rate. Default: 0.\n        drop_path_rate (float): Stochastic depth rate. Default: 0.2.\n        norm_layer: Normalization layer. Default: nn.LayerNorm.\n        patch_norm (bool): If True, add normalization after patch embedding. Default: False.\n        frozen_stages (int): Stages to be frozen (stop grad and set eval mode).\n            -1 means not freezing any parameters.\n    \"\"\"\n    def __init__(self,\n                 pretrained=None,\n                 patch_size=(4, 4, 4),\n                 in_chans=3,\n                 embed_dim=96,\n                 depths=[2, 2, 6, 2],\n                 num_heads=[3, 6, 12, 24],\n                 window_size=(2, 7, 7),\n                 mlp_ratio=4.,\n                 qkv_bias=True,\n                 qk_scale=None,\n                 drop_rate=0.,\n                 attn_drop_rate=0.,\n                 drop_path_rate=0.2,"
        },
        {
            "comment": "The code initializes a Swin Transformer model with specified parameters, including depths, embed dimension, patch size, window size, and input channels. It creates the patch embedding layer and position dropout layer. Stochastic depth is applied using a decay rule. The layers are built using BasicLayer instances for each layer in the specified number of layers.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/swin_transformer.py\":627-658",
            "content": "                 norm_layer=nn.LayerNorm,\n                 patch_norm=False,\n                 frozen_stages=-1,\n                 use_checkpoint=False):\n        super().__init__()\n        self.pretrained = pretrained\n        self.num_layers = len(depths)\n        self.embed_dim = embed_dim\n        self.patch_norm = patch_norm\n        self.frozen_stages = frozen_stages\n        self.window_size = window_size\n        self.patch_size = patch_size\n        # split image into non-overlapping patches\n        self.patch_embed = PatchEmbed3D(\n            patch_size=patch_size,\n            in_chans=in_chans,\n            embed_dim=embed_dim,\n            norm_layer=norm_layer if self.patch_norm else None)\n        self.pos_drop = nn.Dropout(p=drop_rate)\n        # stochastic depth\n        dpr = [\n            x.item() for x in paddle.linspace(0, drop_path_rate, sum(depths))\n        ]  # stochastic depth decay rule\n        # build layers\n        self.layers = nn.LayerList()\n        for i_layer in range(self.num_layers):\n            layer = BasicLayer("
        },
        {
            "comment": "This code initializes a Swin Transformer backbone with specified parameters and adds a norm layer for each output. It also includes a function to freeze certain stages of the model if desired.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/swin_transformer.py\":659-686",
            "content": "                dim=int(embed_dim * 2**i_layer),\n                depth=depths[i_layer],\n                num_heads=num_heads[i_layer],\n                window_size=window_size,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                qk_scale=qk_scale,\n                drop=drop_rate,\n                attn_drop=attn_drop_rate,\n                drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n                norm_layer=norm_layer,\n                downsample=PatchMerging\n                if i_layer < self.num_layers - 1 else None,\n                use_checkpoint=use_checkpoint)\n            self.layers.append(layer)\n        self.num_features = int(embed_dim * 2**(self.num_layers - 1))\n        # add a norm layer for each output\n        self.norm = norm_layer(self.num_features)\n        self._freeze_stages()\n    def _freeze_stages(self):\n        if self.frozen_stages >= 0:\n            self.patch_embed.eval()\n            for param in self.patch_embed.parameters():\n                param.stop_gradient = True"
        },
        {
            "comment": "This code is part of a backbone model's initialization. It first applies an initializer function to the layers, then checks if pretrained weights are provided and loads them if available. The frozen_stages variable determines how many stages of the model should be frozen (set to eval mode) during inference.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/swin_transformer.py\":688-719",
            "content": "        if self.frozen_stages >= 1:\n            self.pos_drop.eval()\n            for i in range(0, self.frozen_stages):\n                m = self.layers[i]\n                m.eval()\n                for param in m.parameters():\n                    param.stop_gradient = True\n    def _init_fn(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=0.02)\n            if m.bias is not None:\n                zeros_(m.bias)\n        elif isinstance(m, nn.LayerNorm):\n            zeros_(m.bias)\n            ones_(m.weight)\n    def init_weights(self):\n        \"\"\"Initialize the weights in backbone.\n        Args:\n            pretrained (str, optional): Path to pre-trained weights.\n                Defaults to None.\n        \"\"\"\n        \"\"\"First init model's weight\"\"\"\n        self.apply(self._init_fn)\n        \"\"\"Second, if provide pretrained ckpt, load it\"\"\"\n        if isinstance(\n                self.pretrained, str\n        ) and self.pretrained.strip() != \"\":  # load pretrained weights\n            load_ckpt(self, self.pretrained)"
        },
        {
            "comment": "If pretrained is None or empty, do nothing. Else, raise NotImplementedError. Forward function processes input through patch embedding and positional dropout, iterates over layers, transposes dimensions, normalizes, and returns output. Train mode keeps layers unfrozen by calling the superclass method and freezing stages.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/swin_transformer.py\":720-741",
            "content": "        elif self.pretrained is None or self.pretrained.strip() == \"\":\n            pass\n        else:\n            raise NotImplementedError\n    def forward(self, x):\n        \"\"\"Forward function.\"\"\"\n        x = self.patch_embed(x)\n        x = self.pos_drop(x)\n        for layer in self.layers:\n            x = layer(x)\n        x = x.transpose([0, 2, 3, 4, 1])\n        x = self.norm(x)\n        x = x.transpose([0, 4, 1, 2, 3])\n        return x\n    def train(self, mode=True):\n        \"\"\"Convert the model into training mode while keep layers freezed.\"\"\"\n        super(SwinTransformer3D, self).train(mode)\n        self._freeze_stages()"
        }
    ]
}