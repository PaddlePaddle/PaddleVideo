{
    "summary": "Video classification tasks involve recognizing actions through RGB images and skeleton data. Concepts include temporal action localization, dense-captioning events, popular datasets, feature extraction, motion representation, and classification using deep learning methods since 2014.",
    "details": [
        {
            "comment": "Introduction to video classification (action recognition) with various applications in different fields, including online platforms and offline sectors like security, transportation, and quality inspection. Tasks include classification/recognition and detection, further subdivided by combining different scenes.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/tutorials/summarize.md\":0-17",
            "content": "[\u7b80\u4f53\u4e2d\u6587](../../zh-CN/tutorials/summarize.md) | English\n# Introduction for video classification(action recognition)\n## Wide range of application scenarios\nVideo classification has a wide range of applications in many fields, such as online video platforms such as short videos, offline such as security, transportation, quality inspection and other fields\u3002\n## Multiple subtasks\nSimilar to image tasks, video tasks can also be divided into two categories: **classification (recognition) and detection**, and these two types of tasks can be specifically subdivided by combining different scenes\uff1a\n+ Task1\uff1aTrimmed Action Recognition. Users input a trimmed video,which contains only single action,then a video tag will be output by model as depicted in fig below:\n<p align=\"center\">\n<img src=\"../../images/action_classification.png\" height=300 width=700 hspace='10'/> <br />\n Action Classification\n</p>\n  In terms of the data modality used, classification tasks can be further subdivided into classification based on si"
        },
        {
            "comment": "This code is describing different types of classification tasks in video analysis. It covers multi-modality data, RGB images, human skeleton data, and various perspectives such as first-person, third-person, and multiple perspectives. Additionally, it mentions untrimmed videos, temporal action proposals, and ROI extraction in image detection tasks.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/tutorials/summarize.md\":17-31",
            "content": "ngle modality data, classification based on multi-modality data, classification based on RGB images and classification based on human skeleton, etc, as shown in the figure below:\n  <p align=\"center\">\n  <img src=\"../../images/multimodality.png\" height=300 width=500 hspace='10'/> <br />\n multi-modality\n  </p>\nIn terms of the perspective of video, it can also be divided into first-person action recognition, \nthird-person action recognition, single perspective action recognition and multi-perspective fusion action recognition. \nUsers who are interested in these fields can refer to relevant literatures.\n+ Task2\uff1aUntrimmed Video Classification. \nUnlike trimmed videos, untrimmed videos often contain multiple actions and have a long time span. \nThere are a lot of movements that we may need not paying attention to. Through the global analysis of the input long video, and then make a soft classify to mutiple categories.\n+ Task3\uff1aTemporal Action Proposal. It is similar to the ROI extraction in the image detection task. "
        },
        {
            "comment": "Task 4: Temporal Action Localization - find video segments with possible actions, classify them.\nTask 5: Dense-Captioning Events - describe untrimmed videos' actions in temporal dimension.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/tutorials/summarize.md\":32-48",
            "content": "The task is to find the video clips that may contain action in a long video with a lot of actions.\n+ Task4\uff1aTemporal Action Localization. Compared with the temporal action proposal task as mentioned above, \ntemporal action localization task is more consistent with detection task in the field of imgae, \nit requires not only to find the video segments with possible actions from the video but also to classify them,\nas shown in the figure below\n <p align=\"center\">\n<img src=\"../../images/action_detection.png\" height=200 width=1000 hspace='10'/> <br />\n Action Detection\n</p>\n+ Task5\uff1aDense-Captioning Events. The reason why it is called dense captioning events is mainly \nbecause that this task requires video action description on the basis of temporal action localization \n(detection). That is to say, the task needs to locate the actions in a **untrimmed** video,in **temporal \ndimension** and describe the behavior of the **whole video** after obtaining many video segments which contain actions.\n## Introduction of datasets"
        },
        {
            "comment": "The code provides a brief overview of popular video action recognition datasets, such as KTH and UCF101. It mentions that the datasets are essential for training and validating models, but overfitting may occur with larger 3D networks on smaller datasets like KTH.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/tutorials/summarize.md\":50-71",
            "content": "### Classification datasets\nThe training and validation of the model cannot be done without comprehensive, \nlarge and well annotated datasets. With the deepening of research on video action recognition, \nmore and more datasets are applied to the research in this field. \nTypical datasets are as follows:\n+ KTH[<sup>1</sup>](#1)\nKTH dataset is an early small action recognition dataset, \nincluding 599 videos of 6 types of actions (walking, jumping, running, punching, waving and clapping). \nThe background is relatively still, except for the zoom in and out of the camera, \nthe camera movement is relatively slight. Since this data set is relatively small, \nit is easy to overfit when training heavy 3D networks, \nso most current researches are not based on this it.\n+ UCF10[<sup>2</sup>](#2)\nUCF101 is a medium-size dataset in which most videos are from YouTube. \nIt contains 13,320 videos with 101 types of actions. \nEach type of action is performed by 25 people, each of whom performs 4-7 sets of actions. \nThe UCF101 and HMDB51 datasets used to be the benchmarks to evaluate the effectiveness of action "
        },
        {
            "comment": "HMDB51 is a dataset proposed by Brown University in 2011, consisting of movie and online video sources. It contains 6849 samples across 51 classes with at least 101 samples each. Kinetics is the largest action recognition dataset, created by Google's DeepMind team in 2017. It uses YouTube videos, now expanded to 600k videos in 700 categories. The categories are divided into human, human and animal, and human and human interaction. Kinetics can train deep networks like 3D-RESNET up to 152 layers without overfitting, solving the issue of small training datasets.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/tutorials/summarize.md\":72-86",
            "content": "recognition model for a long time before the Kinetics dataset was released.\n+ HMDB51[<sup>3</sup>](#3)\nBrown University's proposed dataset named HMDB51 was released in 2011. \nMost of the videos come from movies, \nbut some come from public databases and online video libraries such as YouTube. \nThe datasets contains 6849 samples divided into 51 classes, \neach of which contains at least 101 samples.\n+ Kinetics[<sup>4</sup>](#4)\nKinetics is the most important large-scale action recognition dataset, which was proposed by Google's DeepMind team in 2017. The video data also comes from YouTube, with 400 categories (now expanded to 700 categories) and more than 300,000 videos (now expanded to 600,000 videos), each lasting about 10 seconds. \nThe action categories are mainly divided into three categories: \"human\", \"human and animal\", \"human and human interaction\". Kinetics can train 3D-RESNET up to 152 layers without over-fitting, \nwhich solves the problem that the previous training dataset is too small to train deep 3D network. "
        },
        {
            "comment": "Kinetics is the benchmark for action recognition, replacing UCF101 and HMDB51. Most studies use this dataset for evaluation and pre-training. SomethingV1 has 108,499 annotated videos with 174 kinds of actions, requiring strong temporal modeling ability. Other datasets include Charades (complex action recognition), Breakfast Action, Sports 1M, THUMOS 2014 (action detection).",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/tutorials/summarize.md\":87-103",
            "content": "Kinetics has replaced UCF101 and HMDB51 as the benchmark in the field of action recognition. \nAt present, most studies use this dataset for evaluation and pre-training.\n+ Something-Something[<sup>5</sup>](#5)\nSomethingV1 contains 108,499 annotated videos (V2 has expanded to 220,847), each of which last two to six seconds. These videos contain 174 kinds of actions. Different from the previous dataset, \nthe identification of this data set requires stronger time information, \nso this dataset has a very important reference value in testing the temporal modeling ability of the model.\nIn addition to the above datasets, there are Charades[<sup>6</sup>](#6) dataset for complex Action recognition, Breakfast Action[<sup>7</sup>](#7), and Sports 1M[<sup>8</sup>](#8).\n### Detection datasets\n+ THUMOS 2014\nThis dataset is from THUMOS Challenge 2014, Its training set is UCF101, validation set and test set include 1010 and 1574 undivided video clips respectively. In the action detection task, only 20 kinds of unsegmented videos of actions were labeled with sequential action fragments, "
        },
        {
            "comment": "The code describes two datasets, Mexaction2 and ActivityNet. Mexaction2 has horse riding and bullfighting actions, split into training, validation, and test sets. It includes YouTube clips, UCF101 horseback riding videos, and an unsegmented 77-hour INA video with low marked action proportions. ActivityNet is the largest database, including classification and detection tasks, but only provides YouTube links without direct downloads.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/tutorials/summarize.md\":104-120",
            "content": "including 200 validation sets (3007 action fragments) and 213 test sets (3358 action fragments).\n+ MEXaction2\nThe Mexaction2 dataset contains two types of action: horse riding and bullfighting. \nThe dataset consists of three parts: YouTube videos, horseback riding videos in UCF101, and INA videos. \nYouTube clips and horseback riding videos in UCF101 are short segmented video clips that are used as training sets. \nThe INA video is a long unsegmented video with a total length of 77 hours, \nand it is divided into three parts: training, validation and test. \nThere are 1336 action segments in the training set, 310 in the validation set and 329 in the test set. \nMoreover, the Mexaction2 dataset is characterized by very long unsegmented video lengths, \nand marked action segments only account for a very low proportion of the total video length.\n+ ActivityNet\nAt present the largest database, also contains two tasks of classification and detection. \nThis dataset only provides a YouTube link to the video, not a direct download of the video, "
        },
        {
            "comment": "The code discusses the process of feature extraction, motion representation and classification in action recognition. It highlights two stages - manual feature-based method and deep learning-based method. It mentions DTP and IDT as typical motion descriptors used before deep-learning was applied. The code also shows a framework diagram for action recognition.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/tutorials/summarize.md\":121-137",
            "content": "so you also need to use the YouTube download tool in Python to automatically download the videos. \nThe dataset contains 200 action categories, 20,000 (training + verification + test set) videos, \nand a total of about 700 hours of video.\n## Introduction of classic models\nAs shown in the figure, \nthe action recognition framework mainly includes three steps: \nfeature extraction, motion representation and classification. \nHow to extract spatiotemporal features of video is the core problem of action recognition and video classification.\n <p align=\"center\">\n<img src=\"../../images/action_framework.png\" height=300 width=700 hspace='10'/> <br />\nFramework of action recognition\n</p>\nAccording to different methods, action recognition (video classification) methods can be generally summarized into two stages: \nmanual feature-based method and deep learning-based method. \nTypical motion descriptors in the manual feature-based method stage include DTP and IDT, \nwhich are also the most excellent motion descriptors accepted by most researchers before deep-learning is applied in this field. "
        },
        {
            "comment": "The code discusses the application of deep learning methods in video classification since 2014, highlighting their effectiveness beyond manual motion design. It mentions various classic network structures proposed by researchers for representing motion characteristics and includes images to illustrate these models. The code also introduces PaddleVideo's inclusion of such models like TSN, TSM, slowfast, etc., and anticipates future analysis of these classical models and papers in the field. Additionally, it references an ActivityNet competition for further context.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/tutorials/summarize.md\":138-153",
            "content": "Interinterested readers may refer to the relevant references at the end of this paper. \nSince 2014, deep learning methods have been gradually applied to the field of video classification. \nAt present, deep learning-based methods have become a hotspot of research in both academic and the practice, and the  effect is far beyond the motion features of manual design. \nSince 2014, many classic network structures have been put forward by the researchers regarding the problem of how to represent motion characteristics, \nas shown in the figure below:\n <p align=\"center\">\n<img src=\"../../images/classic_model.png\" height=300 width=700 hspace='10'/> <br />\nClassic Models\n</p>\nAt present,Paddlevideo has contained several classic models such as:TSN[<sup>9</sup>](#9),TSM[<sup>10</sup>](#10),slowfast[<sup>11</sup>](#11),et al.In the future,\nwe will analyze the classic models and papers in these fields. Please look forward to it\n## Introduction of competetion\n+ [ActivityNet](http://activity-net.org/challenges/2020/challenge.html)"
        },
        {
            "comment": "This code snippet provides information about the ActivityNet competition, which is a large-scale action recognition event held annually since 2016. It focuses on identifying everyday activities from user-generated YouTube videos and has become the most influential in the field of action recognition. The code also includes references to relevant research papers for further reading.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/tutorials/summarize.md\":155-172",
            "content": "ActivityNet is a large-scale action recognition competition. Since 2016, \nit has been held simultaneously with CVPR every year. Up to this year, \nit has been held for 4 consecutive sessions. It focuses on identifying everyday, high-level, goal-oriented activities from \nuser-generated videos taken from the Internet video portal YouTube. \nAt present, ActivityNet competition has become the most influential competition in the field of action recognition.\n## Reference\n<div id='1'>\n[1] Schuldt C, Laptev I, Caputo B.Recognizing Human Actions: A Local SVM Approach Proceedings of International Conference on Pattern Recognition. Piscataway, NJ: IEEE, 2004:23-26\n</div>\n<br/>\n<div id='2'>\n[2] Soomro K, Zamir A R, Shah M. UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild. arXiv:1212.0402,2012.\n</div>\n<br/>\n<div id='3'>\n[3] Kuehne H, Jhuang H, Garrote E, et al. HMDB: a large video database for human motion recognition Proceedings of IEEE International Conference on Computer Vision. Piscataway, NJ: IEEE, 2011:2556-2563."
        },
        {
            "comment": "This code provides references to various research papers related to action recognition and video classification, such as the \"Quo Vadis, Action Recognition?\" paper by Carreira and Zisserman, and the \"Hollywood in Homes\" paper by Sigurdsson et al. These references are from well-known conferences like IEEE Conference on Computer Vision and Pattern Recognition (CVPR) and arXiv preprints.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/tutorials/summarize.md\":173-192",
            "content": "</div>\n<br/>\n<div id='4'>\n[4] Carreira J , Zisserman A . Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset Proceedings of IEEE Conference on Computer Vision and Pattern Recognition. Piscataway, NJ: IEEE, 2017:6299-6308.\n</div>\n<br/>\n<div id='5'>\n[5] Goyal R, Kahou S E, Michalski V. The \u201csomething something\u201d video database for learning and evaluating visual common sense. arXiv:1706.04261,2017.\n</div>\n<br/>\n<div id='6'>\n[6] Sigurdsson G A , Varol G\u00fcl, Wang Xiaolong, et al. Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding. arXiv: 604.01753,2016\n</div>\n<br/>\n<div id='7'>\n[7] Kuehne H, Arslan A, Serre T. The Language of Actions Recovering the Syntax and Semantics of Goal-Directed Human Activities  Proceedings of IEEE Conference on Computer Vision and Pattern Recognition. Piscataway, NJ: IEEE, 2014.\n</div>\n<br/>\n<div id='8'>\n[8] Karpathy A , Toderici G , Shetty S , et al. Large-Scale Video Classification with Convolutional Neural Networks Proceedings of IEEE Conference on Computer Vision and Pattern Recognition. Piscataway, NJ: IEEE, 2014:1725-1732."
        },
        {
            "comment": "The code represents a list of references for papers related to video recognition. Each reference has an identifier (id) and the corresponding paper details like authors, title, and publication information.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/tutorials/summarize.md\":193-205",
            "content": "</div>\n<br/>\n<div id='9'>\n[9] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoo Tang,and Luc Van Gool. Temporal segment networks for action recognition in videos? In Proceedings of the European Conference on Computer Vision,pages 20\u201336. Springer, 2016.\n</div>\n<br/>\n<div id='10'>\n[10] Lin Ji , Gan Chuang , Han Song . TSM: Temporal Shift Module for Efficient Video Understanding. arXiv:1811.08383,2018.\n</div>\n<br/>\n<div id='11'>\n[11] Feichtenhofer C , Fan Haoqi , Malik J , et al. SlowFast Networks for Video Recognition. arXiv:1812.03982,2018.\n</div>"
        }
    ]
}