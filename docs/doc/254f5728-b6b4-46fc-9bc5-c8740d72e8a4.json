{
    "summary": "This code creates PaddleVideo dataset loaders and sets up signal handlers for graceful termination of a process group upon receiving SIGINT or SIGTERM signals.",
    "details": [
        {
            "comment": "This code snippet is a part of the PaddleVideo library and contains a function named build_pipeline. It imports various modules, defines a logger for logging purposes, and uses a function called build from utils. This function seems to be building some kind of pipeline based on the provided configuration (cfg). The purpose of this pipeline might be to process data or prepare it for model training in the context of PaddleVideo.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/EIVideo/EIVideo/paddlevideo/loader/builder.py\":0-30",
            "content": "# Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport signal\nimport os\nimport paddle\nfrom paddle.io import BatchSampler, DataLoader, DistributedBatchSampler\nfrom .pipelines.compose import Compose\nfrom .registry import DATASETS, PIPELINES, DATALOADERS, BATCH_SAMPLERS, SAMPLERS\nfrom ..utils import get_logger\nfrom ..utils.build_utils import build\nimport numpy as np\nlogger = get_logger(\"paddlevideo\")\ndef build_pipeline(cfg):\n    \"\"\"Build pipeline.\n    Args:\n        cfg (dict): root config dict."
        },
        {
            "comment": "This code defines several functions to build different components for a dataset loader. The main function is `build_dataset` which takes a configuration dictionary and returns a dataset object after building the pipeline, dataset, sampler, and dataloader as per the given configuration. It uses other helper functions like `build_pipeline`, `build_sampler`, `build_batch_pipeline`, and `build_custom_dataloader` to build these components.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/EIVideo/EIVideo/paddlevideo/loader/builder.py\":31-79",
            "content": "    \"\"\"\n    if cfg == None:\n        return\n    return Compose(cfg)\ndef build_dataset(cfg):\n    \"\"\"Build dataset.\n    Args:\n        cfg (dict): root config dict.\n    Returns:\n        dataset: dataset.\n    \"\"\"\n    # XXX: ugly code here!\n    cfg_dataset, cfg_pipeline = cfg\n    cfg_dataset.pipeline = build_pipeline(cfg_pipeline)\n    dataset = build(cfg_dataset, DATASETS, key=\"format\")\n    return dataset\ndef build_sampler(cfg):\n    \"\"\"Build batch_sampler.\n    Args:\n        cfg (dict): root config dict.\n    Returns:\n        batch_sampler: batch_sampler.\n    \"\"\"\n    sampler = build(cfg, SAMPLERS)\n    return sampler\ndef build_batch_pipeline(cfg):\n    batch_pipeline = build(cfg, PIPELINES)\n    return batch_pipeline\ndef build_custom_dataloader(cfg):\n    custom_dataloader = build(cfg, DATALOADERS, key='dataloader')\n    return custom_dataloader\ndef build_dataloader(dataset,\n                     batch_size,\n                     num_workers,\n                     places=None,\n                     shuffle=True,\n                     drop_last=True,"
        },
        {
            "comment": "The code builds a Paddle Dataloader with optional custom sampler, shuffles data if necessary, and handles distributed batch sampling. It takes dataset, batch size, number of workers, and shuffle settings as input arguments.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/EIVideo/EIVideo/paddlevideo/loader/builder.py\":80-105",
            "content": "                     multigrid=False,\n                     collate_fn_cfg=None,\n                     **kwargs):\n    \"\"\"Build Paddle Dataloader.\n    XXX explain how the batch_sampler work!\n    Args:\n        dataset (paddle.dataset): A PaddlePaddle dataset object.\n        batch_size (int): batch size on single card.\n        num_worker (int): num_worker\n        shuffle(bool): whether to shuffle the data at every epoch.\n    \"\"\"\n    if not kwargs.get('sampler'):\n        batch_sampler = DistributedBatchSampler(dataset,\n                                                batch_size=batch_size,\n                                                shuffle=shuffle,\n                                                drop_last=drop_last)\n    else:\n        sampler = build_sampler(kwargs['sampler'])\n        batch_sampler = BatchSampler(dataset,\n                                     sampler=sampler,\n                                     batch_size=batch_size,\n                                     shuffle=shuffle,\n                                     drop_last=drop_last)"
        },
        {
            "comment": "This code defines a mix_collate_fn for handling batches of data in a specific way. It first builds a batch pipeline and applies it to the input batch. Then, it collates the batch so that each item is stacked horizontally (axis=0) into a new batch. This function is used as the collate_fn if the collate_fn_cfg is not None.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/EIVideo/EIVideo/paddlevideo/loader/builder.py\":106-133",
            "content": "    kwargs.update({'batch_sampler': batch_sampler})\n    # NOTE(shipping): when switch the mix operator on, such as: mixup, cutmix.\n    # batch like: [[img, label, attibute, ...], [imgs, label, attribute, ...], ...] will recollate to:\n    # [[img, img, ...], [label, label, ...], [attribute, attribute, ...], ...] as using numpy.transpose.\n    def mix_collate_fn(batch):\n        pipeline = build_batch_pipeline(collate_fn_cfg)\n        batch = pipeline(batch)\n        slots = []\n        for items in batch:\n            for i, item in enumerate(items):\n                if len(slots) < len(items):\n                    slots.append([item])\n                else:\n                    slots[i].append(item)\n        return [np.stack(slot, axis=0) for slot in slots]\n    # if collate_fn_cfg is not None:\n    # ugly code here. collate_fn is mix op config\n    #    collate_fn = mix_collate_fn(collate_fn_cfg)\n    data_loader = DataLoader(\n        dataset,\n        places=places,\n        num_workers=num_workers,\n        collate_fn=mix_collate_fn if collate_fn_cfg is not None else None,"
        },
        {
            "comment": "This code is setting up signal handlers for SIGINT and SIGTERM signals. It retrieves the process ID (pid) and process group ID (pgid), logs a message, then sends a SIGKILL signal to all processes in the group upon receiving either of those signals.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/EIVideo/EIVideo/paddlevideo/loader/builder.py\":134-150",
            "content": "        **kwargs)\n    return data_loader\ndef term_mp(sig_num, frame):\n    \"\"\" kill all child processes\n    \"\"\"\n    pid = os.getpid()\n    pgid = os.getpgid(os.getpid())\n    logger.info(\"main proc {} exit, kill process group \" \"{}\".format(pid, pgid))\n    os.killpg(pgid, signal.SIGKILL)\n    return\nsignal.signal(signal.SIGINT, term_mp)\nsignal.signal(signal.SIGTERM, term_mp)"
        }
    ]
}