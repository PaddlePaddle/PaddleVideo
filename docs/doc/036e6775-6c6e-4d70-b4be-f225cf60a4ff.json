{
    "summary": "This Python module provides video processing layers and functions, including Depthwise Separable Convolution layers initialization, PPTSMV2 model with convolutional layers, and batch normalization. It defines a PPTSM_v2 backbone model for video analysis with customizable options like pretrained models, scaling, depths, dropout probability, and additional arguments.",
    "details": [
        {
            "comment": "This code is a Python module for the PaddlePaddle framework. It contains definitions of various layers and functions used in neural network backbones, including convolutional layers, pooling layers, batch normalization, linear layers, and more. The code also includes comments about copyright and licensing information, as well as imports necessary modules for these operations. Additionally, it references utility functions for weight initialization and model loading from checkpoints.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/pptsm_v2.py\":0-26",
            "content": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom __future__ import absolute_import, division, print_function\nimport paddle\nimport paddle.nn as nn\nfrom paddle import ParamAttr\nfrom paddle.nn import AdaptiveAvgPool2D, BatchNorm, Conv2D, Dropout, Linear, BatchNorm2D\nfrom paddle.regularizer import L2Decay\nfrom paddle.nn.initializer import KaimingNormal\nimport paddle.nn.functional as F\nfrom ..registry import BACKBONES\nfrom ..weight_init import weight_init_\nfrom ...utils import load_ckpt"
        },
        {
            "comment": "This code defines the PPLCNetV2 backbone model for video processing tasks. It includes the URL to download a pretrained model, stages of the network (PPLCNet), and network configurations. The make_divisible function is used to round up numbers for better performance. The GlobalAttention class is a lightweight temporal attention module used in the model.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/pptsm_v2.py\":28-63",
            "content": "# MODEL_URLS = {\n#     \"PPLCNetV2\":\n#     \"https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/legendary_models/PPLCNetV2_base_ssld_pretrained.pdparams\",\n# }\nMODEL_STAGES_PATTERN = {\n    \"PPLCNet\": [\"blocks2\", \"blocks3\", \"blocks4\", \"blocks5\", \"blocks6\"]\n}\nNET_CONFIG = {\n    # in_channels, kernel_size, split_pw, use_rep, use_se, use_shortcut\n    \"stage1\": [64, 3, False, False, False, False],\n    \"stage2\": [128, 3, False, False, False, False],\n    \"stage3\": [256, 5, True, True, True, False],\n    \"stage4\": [512, 5, False, True, False, True],\n}\ndef make_divisible(v, divisor=8, min_value=None):\n    if min_value is None:\n        min_value = divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    if new_v < 0.9 * v:\n        new_v += divisor\n    return new_v\nclass GlobalAttention(nn.Layer):\n    \"\"\"\n    Lightweight temporal attention module.\n    \"\"\"\n    def __init__(self, num_seg=8):\n        super().__init__()\n        self.fc = nn.Linear(in_features=num_seg,\n                            out_features=num_seg,"
        },
        {
            "comment": "The code defines a ConvBNLayer class and a PPTSMV2 class. The ConvBNLayer class is a convolution layer followed by batch normalization, and the PPTSMV2 class is an encoder model that takes input of shape (-1, 3, H, W) where H and W are height and width respectively, and returns output of the same shape after processing. It first resizes the input, applies convolution with specified parameters, calculates attention maps, and performs element-wise multiplication between original input and attention maps to extract relevant features for each segmented region.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/pptsm_v2.py\":64-95",
            "content": "                            weight_attr=ParamAttr(learning_rate=5.0,\n                                                  regularizer=L2Decay(1e-4)),\n                            bias_attr=ParamAttr(learning_rate=10.0,\n                                                regularizer=L2Decay(0.0)))\n        self.num_seg = num_seg\n    def forward(self, x):\n        _, C, H, W = x.shape\n        x0 = x\n        x = x.reshape([-1, self.num_seg, C * H * W])\n        x = paddle.mean(x, axis=2)  # efficient way of avg_pool\n        x = x.squeeze(axis=-1)\n        x = self.fc(x)\n        attention = F.sigmoid(x)\n        attention = attention.reshape(\n            (-1, self.num_seg, 1, 1, 1))  #for broadcast\n        x0 = x0.reshape([-1, self.num_seg, C, H, W])\n        y = paddle.multiply(x0, attention)\n        y = y.reshape_([-1, C, H, W])\n        return y\nclass ConvBNLayer(nn.Layer):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride,\n                 groups=1,"
        },
        {
            "comment": "The code defines a Conv2D layer followed by a BatchNorm2D layer and an optional ReLU activation function. The SEModule class inherits from nn.Layer and contains an AdaptiveAvgPool2D layer for average pooling operations.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/pptsm_v2.py\":96-126",
            "content": "                 use_act=True):\n        super().__init__()\n        self.use_act = use_act\n        self.conv = Conv2D(in_channels=in_channels,\n                           out_channels=out_channels,\n                           kernel_size=kernel_size,\n                           stride=stride,\n                           padding=(kernel_size - 1) // 2,\n                           groups=groups,\n                           weight_attr=ParamAttr(initializer=KaimingNormal()),\n                           bias_attr=False)\n        self.bn = BatchNorm2D(out_channels,\n                              weight_attr=ParamAttr(regularizer=L2Decay(0.0)),\n                              bias_attr=ParamAttr(regularizer=L2Decay(0.0)))\n        if self.use_act:\n            self.act = nn.ReLU()\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        if self.use_act:\n            x = self.act(x)\n        return x\nclass SEModule(nn.Layer):\n    def __init__(self, channel, reduction=4):\n        super().__init__()\n        self.avg_pool = AdaptiveAvgPool2D(1)"
        },
        {
            "comment": "This code initializes a depthwise separable convolution layer with optional parameters like in_channels, out_channels, stride, dw_size, split_pw, use_rep, and use_se. It contains Conv2D layers for convolution operations, ReLU activation, Sigmoid activation, and element-wise multiplication for identity shortcut connection.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/pptsm_v2.py\":127-160",
            "content": "        self.conv1 = Conv2D(in_channels=channel,\n                            out_channels=channel // reduction,\n                            kernel_size=1,\n                            stride=1,\n                            padding=0)\n        self.relu = nn.ReLU()\n        self.conv2 = Conv2D(in_channels=channel // reduction,\n                            out_channels=channel,\n                            kernel_size=1,\n                            stride=1,\n                            padding=0)\n        self.hardsigmoid = nn.Sigmoid()\n    def forward(self, x):\n        identity = x\n        x = self.avg_pool(x)\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.hardsigmoid(x)\n        x = paddle.multiply(x=identity, y=x)\n        return x\nclass RepDepthwiseSeparable(nn.Layer):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 stride,\n                 dw_size=3,\n                 split_pw=False,\n                 use_rep=False,\n                 use_se=False,"
        },
        {
            "comment": "This code initializes a PPTSM backbone model. It creates a ConvBNLayer for each kernel size in the dw_size range, skipping 1x1 if stride is not 1. The layers are stored in the dw_conv_list. An additional Conv2D layer with the same number of input and output channels is also created. This model can be used for image classification tasks.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/pptsm_v2.py\":161-184",
            "content": "                 use_shortcut=False):\n        super().__init__()\n        self.is_repped = False\n        self.dw_size = dw_size\n        self.split_pw = split_pw\n        self.use_rep = use_rep\n        self.use_se = use_se\n        self.use_shortcut = True if use_shortcut and stride == 1 and in_channels == out_channels else False\n        if self.use_rep:\n            self.dw_conv_list = nn.LayerList()\n            for kernel_size in range(self.dw_size, 0, -2):\n                if kernel_size == 1 and stride != 1:\n                    continue\n                dw_conv = ConvBNLayer(in_channels=in_channels,\n                                      out_channels=in_channels,\n                                      kernel_size=kernel_size,\n                                      stride=stride,\n                                      groups=in_channels,\n                                      use_act=False)\n                self.dw_conv_list.append(dw_conv)\n            self.dw_conv = nn.Conv2D(in_channels=in_channels,\n                                     out_channels=in_channels,"
        },
        {
            "comment": "Code creates a ConvBNLayer object for downsample convolution with optional SE module and split point-wise convolution. It handles different configurations based on dw_size, stride, and use_se parameters.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/pptsm_v2.py\":185-208",
            "content": "                                     kernel_size=dw_size,\n                                     stride=stride,\n                                     padding=(dw_size - 1) // 2,\n                                     groups=in_channels)\n        else:\n            self.dw_conv = ConvBNLayer(in_channels=in_channels,\n                                       out_channels=in_channels,\n                                       kernel_size=dw_size,\n                                       stride=stride,\n                                       groups=in_channels)\n        self.act = nn.ReLU()\n        if use_se:\n            self.se = SEModule(in_channels)\n        if self.split_pw:\n            pw_ratio = 0.5\n            self.pw_conv_1 = ConvBNLayer(in_channels=in_channels,\n                                         kernel_size=1,\n                                         out_channels=int(out_channels *\n                                                          pw_ratio),\n                                         stride=1)\n            self.pw_conv_2 = ConvBNLayer(in_channels=int(out_channels *"
        },
        {
            "comment": "This code defines a backbone for a deep learning model, specifically the PPTSM_v2 architecture. It uses convolutional layers and Batch Normalization to process input data. The use of Point-wise Convolution (pw_conv) or Depth-wise Separable Convolutions (dw_conv) depends on certain conditions. If \"use_rep\" is True, it applies repeated depth-wise convolutions if the current layer has been repped. It also includes optional Squeeze and Excitation blocks for feature enhancement.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/pptsm_v2.py\":209-237",
            "content": "                                                         pw_ratio),\n                                         kernel_size=1,\n                                         out_channels=out_channels,\n                                         stride=1)\n        else:\n            self.pw_conv = ConvBNLayer(in_channels=in_channels,\n                                       kernel_size=1,\n                                       out_channels=out_channels,\n                                       stride=1)\n    def forward(self, x):\n        if self.use_rep:\n            input_x = x\n            if self.is_repped:\n                x = self.act(self.dw_conv(x))\n            else:\n                y = self.dw_conv_list[0](x)\n                for dw_conv in self.dw_conv_list[1:]:\n                    y += dw_conv(x)\n                x = self.act(y)\n        else:\n            x = self.dw_conv(x)\n        if self.use_se:\n            x = self.se(x)\n        if self.split_pw:\n            x = self.pw_conv_1(x)\n            x = self.pw_conv_2(x)\n        else:"
        },
        {
            "comment": "This code implements a backbone for PPTSM_V2 model. It performs pointwise convolution, adds shortcut connection if enabled, and includes functions for representation fusion and fusing batch normalization tensor.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/pptsm_v2.py\":238-268",
            "content": "            x = self.pw_conv(x)\n        if self.use_shortcut:\n            x = x + input_x\n        return x\n    def rep(self):\n        if self.use_rep:\n            self.is_repped = True\n            kernel, bias = self._get_equivalent_kernel_bias()\n            self.dw_conv.weight.set_value(kernel)\n            self.dw_conv.bias.set_value(bias)\n    def _get_equivalent_kernel_bias(self):\n        kernel_sum = 0\n        bias_sum = 0\n        for dw_conv in self.dw_conv_list:\n            kernel, bias = self._fuse_bn_tensor(dw_conv)\n            kernel = self._pad_tensor(kernel, to_size=self.dw_size)\n            kernel_sum += kernel\n            bias_sum += bias\n        return kernel_sum, bias_sum\n    def _fuse_bn_tensor(self, branch):\n        kernel = branch.conv.weight\n        running_mean = branch.bn._mean\n        running_var = branch.bn._variance\n        gamma = branch.bn.weight\n        beta = branch.bn.bias\n        eps = branch.bn._epsilon\n        std = (running_var + eps).sqrt()\n        t = (gamma / std).reshape((-1, 1, 1, 1))"
        },
        {
            "comment": "The code defines a PPTSM_v2_LCNet class, which is a type of backbone neural network. It includes initialization parameters such as scale, depths, and class_num. The class also has methods for kernel multiplication, tensor padding, and other operations related to image processing and neural network layers.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/pptsm_v2.py\":269-302",
            "content": "        return kernel * t, beta - running_mean * gamma / std\n    def _pad_tensor(self, tensor, to_size):\n        from_size = tensor.shape[-1]\n        if from_size == to_size:\n            return tensor\n        pad = (to_size - from_size) // 2\n        return F.pad(tensor, [pad, pad, pad, pad])\nclass PPTSM_v2_LCNet(nn.Layer):\n    def __init__(self,\n                 scale,\n                 depths,\n                 class_num=400,\n                 dropout_prob=0,\n                 num_seg=8,\n                 use_temporal_att=False,\n                 pretrained=None,\n                 use_last_conv=True,\n                 class_expand=1280):\n        super().__init__()\n        self.scale = scale\n        self.use_last_conv = use_last_conv\n        self.class_expand = class_expand\n        self.num_seg = num_seg\n        self.use_temporal_att = use_temporal_att\n        self.pretrained = pretrained\n        self.stem = nn.Sequential(*[\n            ConvBNLayer(in_channels=3,\n                        kernel_size=3,\n                        out_channels=make_divisible(32 * scale),"
        },
        {
            "comment": "This code defines a PPTSM-v2 backbone model, using DepthwiseSeparable blocks with varying configurations for different stages. It utilizes `make_divisible()` function to adjust the number of channels and kernel sizes, and a LayerList to create a sequence of layers for each stage. The NET_CONFIG determines the specifics of each stage's parameters like in_channels, kernel_size, split_pw, use_rep, use_se, and use_shortcut.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/pptsm_v2.py\":303-323",
            "content": "                        stride=2),\n            RepDepthwiseSeparable(in_channels=make_divisible(32 * scale),\n                                  out_channels=make_divisible(64 * scale),\n                                  stride=1,\n                                  dw_size=3)\n        ])\n        # stages\n        self.stages = nn.LayerList()\n        for depth_idx, k in enumerate(NET_CONFIG):\n            in_channels, kernel_size, split_pw, use_rep, use_se, use_shortcut = NET_CONFIG[\n                k]\n            self.stages.append(\n                nn.Sequential(*[\n                    RepDepthwiseSeparable(in_channels=make_divisible(\n                        (in_channels if i == 0 else in_channels * 2) * scale),\n                                          out_channels=make_divisible(\n                                              in_channels * 2 * scale),\n                                          stride=2 if i == 0 else 1,\n                                          dw_size=kernel_size,\n                                          split_pw=split_pw,"
        },
        {
            "comment": "This code defines a PPTSM_V2 backbone for the PaddleVideo model. It includes multiple Conv2D layers, BatchNorm layers, AdaptiveAvgPool2D, and optional final convolutional layer, flatten layer, linear layer. The use of these components depends on certain conditions such as `use_rep`, `use_se`, `use_shortcut`, `use_last_conv`, and other parameters.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/pptsm_v2.py\":324-346",
            "content": "                                          use_rep=use_rep,\n                                          use_se=use_se,\n                                          use_shortcut=use_shortcut)\n                    for i in range(depths[depth_idx])\n                ]))\n        self.avg_pool = AdaptiveAvgPool2D(1)\n        if self.use_last_conv:\n            self.last_conv = Conv2D(in_channels=make_divisible(\n                NET_CONFIG[\"stage4\"][0] * 2 * scale),\n                                    out_channels=self.class_expand,\n                                    kernel_size=1,\n                                    stride=1,\n                                    padding=0,\n                                    bias_attr=False)\n            self.act = nn.ReLU()\n            self.dropout = Dropout(p=dropout_prob, mode=\"downscale_in_infer\")\n        self.flatten = nn.Flatten(start_axis=1, stop_axis=-1)\n        in_features = self.class_expand if self.use_last_conv else NET_CONFIG[\n            \"stage4\"][0] * 2 * scale\n        self.fc = Linear(in_features, class_num)"
        },
        {
            "comment": "Code initializes weights for a PPTSM_v2 backbone model. It first checks if the pretrained weights are provided and then initializes the layers with specified methods. Stage 3 adds temporal attention and Temporal Shift Module (TSM) operations for efficiency.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/pptsm_v2.py\":347-371",
            "content": "        if self.use_temporal_att:\n            self.global_attention = GlobalAttention(num_seg=self.num_seg)\n    def init_weights(self):\n        \"\"\"Initiate the parameters.\n        \"\"\"\n        if isinstance(self.pretrained, str) and self.pretrained.strip() != \"\":\n            load_ckpt(self, self.pretrained)\n        elif self.pretrained is None or self.pretrained.strip() == \"\":\n            for layer in self.sublayers():\n                if isinstance(layer, nn.Conv2D):\n                    weight_init_(layer, 'KaimingNormal')\n                elif isinstance(layer, nn.BatchNorm2D):\n                    weight_init_(layer, 'Constant', value=1)\n    def forward(self, x):\n        x = self.stem(x)\n        count = 0\n        for stage in self.stages:\n            # only add temporal attention and tsm in stage3 for efficiency\n            if count == 2:\n                # add temporal attention\n                if self.use_temporal_att:\n                    x = self.global_attention(x)\n                x = F.temporal_shift(x, self.num_seg, 1.0 / self.num_seg)"
        },
        {
            "comment": "Code snippet defines a PPTSM_v2 backbone model for video analysis. It consists of stages, an average pooling layer, and a convolution layer. The function also includes feature aggregation and reshaping operations before feeding the data to a fully connected layer. The pretrained model can be loaded from a given path, and it supports custom scaling, depths, dropout probability, and additional keyword arguments.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/pptsm_v2.py\":372-404",
            "content": "            count += 1\n            x = stage(x)\n        x = self.avg_pool(x)\n        if self.use_last_conv:\n            x = self.last_conv(x)\n            x = self.act(x)\n            x = self.dropout(x)\n        # Feature aggregation\n        x = paddle.reshape(x, [-1, self.num_seg, x.shape[1]])\n        x = paddle.mean(x, axis=1)\n        x = paddle.reshape(x, shape=[-1, self.class_expand])\n        x = self.fc(x)\n        return x\n@BACKBONES.register()\ndef PPTSM_v2(pretrained=None, use_ssld=False, **kwargs):\n    \"\"\"\n    PP-TSM_v2 model.\n    Args:\n        pretrained: str, means the path of the pretrained model.\n    Returns:\n        model: nn.Layer.\n    \"\"\"\n    model = PPTSM_v2_LCNet(pretrained=pretrained,\n                           scale=1.0,\n                           depths=[2, 2, 6, 2],\n                           dropout_prob=0.2,\n                           **kwargs)\n    return model"
        }
    ]
}