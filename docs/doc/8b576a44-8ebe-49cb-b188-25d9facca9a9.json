{
    "summary": "The TimeSformer model is a top-performing video classifier that uses time series modeling and space-time attention, trained on Kinetics-400 using 8 GPUs with mixed-precision training.",
    "details": [
        {
            "comment": "This code implements the TimeSformer, a video classification model based on vision transformer with global receptive field and strong time series modeling ability. It achieves SOTA accuracy on Kinetics-400 dataset and has shorter training time compared to other models. The code showcases the time-space separated attention cascade network architecture, and requires data preparation from Kinetics-400 dataset.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/timesformer.md\":0-25",
            "content": "[\u7b80\u4f53\u4e2d\u6587](../../../zh-CN/model_zoo/recognition/timesformer.md) | English\n# TimeSformer\n## Content\n- [Introduction](#Introduction)\n- [Data](#DATA)\n- [Train](#Train)\n- [Test](#Test)\n- [Inference](#Inference)\n- [Reference](#Reference)\n## Introduction\nTimeSformer is a video classification model based on vision transformer, which has the characteristics of no convolution, global receptive field, and strong time series modeling ability. At present, it has achieved SOTA accuracy on the Kinetics-400 data set, surpassing the classic CNN-based video classification models TSN, TSM and Slowfast, and has a shorter training time (the Kinetics-400 data set training time is 39 hourss). **This code implements the time-space separated attention cascade network in the paper**.\n<div align=\"center\">\n<img src=\"../../../images/timesformer_attention_arch.png\" alt=\"image-20210628210446041\"/><img src=\"../../../images/timesformer_attention_visualize.png\" alt=\"image-20210628210446041\"  />\n</div>\n## Data\nK400 data download and preparation please refer to [Kinetics-400 data preparation](../../dataset/k400.md)"
        },
        {
            "comment": "Download and prepare UCF101 data, then download the ViT_base_patch16_224 pre-trained model. Update the config file with the model's path and train the Kinetics-400 dataset using 8 GPUs.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/timesformer.md\":27-56",
            "content": "UCF101 data download and preparation please refer to [UCF-101 data preparation](../../dataset/ucf101.md)\n## Train\n### Kinetics-400 data set training\n#### Download and add pre-trained models\n1. Download the image pre-training model [ViT_base_patch16_224](https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/ViT_base_patch16_224_pretrained.pdparams) as Backbone initialization parameters, or download through the wget command\n   ```bash\n   wget https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/ViT_base_patch16_224_pretrained.pdparams\n   ```\n2. Open `PaddleVideo/configs/recognition/timesformer/timesformer_k400_videos.yaml`, and fill in the downloaded weight storage path below `pretrained:`\n    ```yaml\n    MODEL:\n        framework: \"RecognizerTransformer\"\n        backbone:\n            name: \"VisionTransformer\"\n            pretrained: fill in the path here\n    ```\n#### Start training\n- The Kinetics400 data set uses 8 cards for training, and the start command of the training method is as follows:\n```bash"
        },
        {
            "comment": "This code executes the training of a Paddle Video model called \"timesformer\" on 8 GPUs for video data. The command is to be run in a Linux terminal, and it uses mixed-precision training with AMP (Automatic Mixed Precision) to speed up the process. The command also sets some environment variables to configure CUDA behavior. The configuration file name includes the model and dataset names as well as data format and sampling method. For more details on configuring parameters, refer to the provided link.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/timesformer.md\":57-71",
            "content": "# videos data format\npython3.7 -B -m paddle.distributed.launch --gpus=\"0,1,2,3,4,5,6,7\" --log_dir=log_timesformer main.py --validate -c configs/recognition/ timesformer/timesformer_k400_videos.yaml\n```\n- Turn on amp mixed-precision training to speed up the training process. The training start command is as follows:\n```bash\nexport FLAGS_conv_workspace_size_limit=800 # MB\nexport FLAGS_cudnn_exhaustive_search=1\nexport FLAGS_cudnn_batchnorm_spatial_persistent=1\n# videos data format\npython3.7 -B -m paddle.distributed.launch --gpus=\"0,1,2,3,4,5,6,7\" --log_dir=log_timesformer main.py --amp --validate -c configs/recognition/ timesformer/timesformer_k400_videos.yaml\n```\n- In addition, you can customize and modify the parameter configuration to achieve the purpose of training/testing on different data sets. It is recommended that the naming method of the configuration file is `model_dataset name_file format_data format_sampling method.yaml` , Please refer to [config](../../tutorials/config.md) for parameter usage."
        },
        {
            "comment": "The code provides instructions for testing the TimeSformer model, using a different sampling method in test mode for higher accuracy. The best model is identified by the log's \"best\" keyword, and final test scores are obtained after training by using the provided command.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/timesformer.md\":74-89",
            "content": "## Test\n- The TimeSformer model is verified synchronously during training. You can find the keyword `best` in the training log to obtain the model test accuracy. The log example is as follows:\n  ```\n  Already save the best model (top1 acc)0.7258\n  ```\n- Since the sampling method of the TimeSformer model test mode is **UniformCrop** with a slower speed but higher accuracy, which is different from the **RandomCrop** used in the verification mode during the training process, so the verification index recorded in the training log is `topk Acc `Does not represent the final test score, so after the training is completed, you can use the test mode to test the best model to obtain the final index, the command is as follows:\n  ```bash\n  python3.7 -B -m paddle.distributed.launch --gpus=\"0,1,2,3,4,5,6,7\" --log_dir=log_timesformer main.py --test -c configs/recognition/ timesformer/timesformer_k400_videos.yaml -w \"output/TimeSformer/TimeSformer_best.pdparams\"\n  ```\n  When the test configuration uses the following parameters, the test indicators on the validation data set of Kinetics-400 are as follows:"
        },
        {
            "comment": "This code snippet is for exporting the TimeSformer inference model. It uses the PaddlePaddle framework and requires a configuration file, a pre-trained model file, and an output directory. The TimeSformer is a video recognition model that utilizes the Vision Transformer architecture and Linspace sampling strategy for its inference process.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/timesformer.md\":92-106",
            "content": "  | backbone | Sampling method | num_seg | target_size | Top-1 | checkpoints |\n  | :----------------: | :-----: | :-----: | :---------: | :----: | :----------------------------------------------------------: |\n  | Vision Transformer | UniformCrop | 8 | 224 | 77.29 | [TimeSformer_k400.pdparams](https://videotag.bj.bcebos.com/PaddleVideo-release2.2/TimeSformer_k400.pdparams) |\n- During the test, the TimeSformer video sampling strategy is to use Linspace sampling: in time sequence, num_seg sparse sampling points are uniformly generated from the video sequence to be sampled; in space, select the two ends of the long side and the middle position (left middle right or top middle bottom) 3 regions are sampled. A total of 1 clip is sampled for 1 video.\n## Inference\n### Export inference model\n```bash\npython3.7 tools/export_model.py -c configs/recognition/timesformer/timesformer_k400_videos.yaml \\\n                                -p data/TimeSformer_k400.pdparams \\\n                                -o inference/TimeSformer"
        },
        {
            "comment": "This code snippet demonstrates the process of using the TimeSformer model to predict the video file 'data/example.avi'. The model is trained on Kinetics-400 and the prediction command uses python3.7 to run 'tools/predict.py' with relevant parameters such as input_file, config, model_file, params_file, use_gpu, and use_tensorrt. The output shows the top-1 class and its corresponding score for the video file.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/timesformer.md\":107-132",
            "content": "```\nThe above command will generate the model structure file `TimeSformer.pdmodel` and the model weight file `TimeSformer.pdiparams` required for prediction.\n- For the meaning of each parameter, please refer to [Model Reasoning Method](../../start.md#2-infer)\n### Use prediction engine inference\n```bash\npython3.7 tools/predict.py --input_file data/example.avi \\\n                           --config configs/recognition/timesformer/timesformer_k400_videos.yaml \\\n                           --model_file inference/TimeSformer/TimeSformer.pdmodel \\\n                           --params_file inference/TimeSformer/TimeSformer.pdiparams \\\n                           --use_gpu=True \\\n                           --use_tensorrt=False\n```\nThe output example is as follows:\n```\nCurrent video file: data/example.avi\n    top-1 class: 5\n    top-1 score: 0.9999722242355347\n```\nIt can be seen that using the TimeSformer model trained on Kinetics-400 to predict `data/example.avi`, the output top1 category id is `5`, and the confiden"
        },
        {
            "comment": "Code comments: The code calculates the category id and name from a table, which is used to determine the predicted category name. It references a paper on space-time attention for video understanding.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/recognition/timesformer.md\":132-136",
            "content": "ce is 0.99. By consulting the category id and name correspondence table `data/k400/Kinetics-400_label_list.txt`, it can be seen that the predicted category name is `archery`.\n## Reference\n- [Is Space-Time Attention All You Need for Video Understanding?](https://arxiv.org/pdf/2102.05095.pdf), Gedas Bertasius, Heng Wang, Lorenzo Torresani"
        }
    ]
}