{
    "summary": "The `BaseDataset` class serves as a base for creating video feature datasets, handling missing values and encoding text while supporting efficient dataset partitioning.",
    "details": [
        {
            "comment": "Copyright and license information, importing necessary libraries, and type guarding.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/base/base_dataset.py\":0-35",
            "content": "# Copyright (c) 2021  PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nimport time\nimport json\nimport random\nimport paddle\nimport inspect\nimport logging\nimport functools\nimport data_loader\nimport numpy as np\nimport pickle as pkl\nfrom pathlib import Path\nfrom abc import abstractmethod\nfrom typing import Dict, Union\nfrom numpy.random import randint\nfrom typeguard import typechecked\nfrom collections import OrderedDict\nfrom zsvision.zs_utils import memcache\ntry:\n    from paddlenlp.transformers import BertTokenizer\nexcept ImportError as e:\n    print(\n        f\"{e}, [paddlenlp] package and it's dependencies is required for T2VLAD.\""
        },
        {
            "comment": "This code defines a base class `BaseDataset` for creating and loading video features dataset. It contains methods for generating required paths, performing sanity checks on loaded data, and loading features from disk. The class is abstract and requires subclass implementation of these methods. It also includes utility functions and settings like `dataset_paths`, `sanity_checks`, and `load_features`.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/base/base_dataset.py\":36-75",
            "content": "    )\nfrom utils import ensure_tensor, expert_tensor_storage\n# For SLURM usage, buffering makes it difficult to see events as they happen, so we set\n# the global print statement to enforce flushing\nprint = functools.partial(print, flush=True)\nclass BaseDataset(paddle.io.Dataset):\n    @staticmethod\n    @abstractmethod\n    @typechecked\n    def dataset_paths() -> Dict[str, Union[Path, str]]:\n        \"\"\"Generates a datastructure containing all the paths required to load features\n        \"\"\"\n        raise NotImplementedError\n    @abstractmethod\n    def sanity_checks(self):\n        \"\"\"Run sanity checks on loaded data\n        \"\"\"\n        raise NotImplementedError\n    @abstractmethod\n    def load_features(self):\n        \"\"\"Load features from disk\n        \"\"\"\n        raise NotImplementedError\n    @typechecked\n    def __init__(\n        self,\n        data_dir: Path,\n        eval_only: bool,\n        use_zeros_for_missing: bool,\n        text_agg: str,\n        text_feat: str,\n        split_name: str,\n        cls_partition: str,\n        root_feat_folder: str,"
        },
        {
            "comment": "The code above defines a class for a dataset, with various parameters such as text_dim, num_test_captions, and max_tokens. It sets the necessary attributes including logger, text_feat, data_dir, and experts. The class also initializes the tokenizer and sets the restrict_test_captions and text_features attributes before calling load_features() method.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/base/base_dataset.py\":76-100",
            "content": "        text_dim: int,\n        num_test_captions: int,\n        restrict_train_captions: int,\n        max_tokens: Dict[str, int],\n        logger: logging.Logger,\n        raw_input_dims: Dict[str, int],\n        feat_aggregation: Dict[str, Dict],\n    ):\n        self.eval_only = eval_only\n        self.logger = logger\n        self.text_feat = text_feat\n        self.data_dir = data_dir\n        self.text_dim = text_dim\n        self.restrict_train_captions = restrict_train_captions\n        self.max_tokens = max_tokens\n        self.cls_partition = cls_partition\n        self.num_test_captions = num_test_captions\n        self.feat_aggregation = feat_aggregation\n        self.root_feat = data_dir / root_feat_folder\n        self.experts = set(raw_input_dims.keys())\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        # This attributes can be overloaded by different datasets, so it must be set\n        # before the `load_features() method call`\n        self.restrict_test_captions = None\n        self.text_features = None"
        },
        {
            "comment": "This code initializes class variables for a dataset object. It sets the label features, video labels, raw captions, and features to None. It loads the word2int mapping from a JSON file. The code allows for one caption per video in training minibatches. It creates an ordered list of experts based on input dimensions. The training and test lists are set by dataset-specific subclasses. The code is for retrieval tasks and uses a single dataloader, handling retrieval data separately. It sets the sample list to the training partition and calculates the total number of samples.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/base/base_dataset.py\":101-124",
            "content": "        self.label_features = None\n        self.video_labels = None\n        self.raw_captions = None\n        self.features = None\n        self.word2int = json.load(open('word2int.json'))\n        # Use a single caption per video when forming training minibatches (different\n        # captions from the same video may still be used across different minibatches)\n        self.captions_per_video = 1\n        self.ordered_experts = list(raw_input_dims.keys())\n        # Training and test lists are set by dataset-specific subclasses\n        self.partition_lists = {}\n        self.configure_train_test_splits(split_name=split_name)\n        # All retrieval-based tasks use a single dataloader (and handle the retrieval\n        # data separately), whereas for classification we use one dataloader for\n        # training and one for validation.\n        self.logger.info(\"The current task is retrieval\")\n        self.sample_list = self.partition_lists[\"train\"]\n        self.num_samples = len(self.sample_list)\n        num_val = len(self.partition_lists[\"val\"])"
        },
        {
            "comment": "The code sets default paths for video retrieval, defines missing value strategy based on use_zeros_for_missing argument, loads dataset-specific features into memory and averages text features when text_agg is set to \"avg\".",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/base/base_dataset.py\":126-151",
            "content": "        self.raw_input_dims = raw_input_dims\n        # we store default paths to enable visualisations (this can be overloaded by\n        # dataset-specific classes)\n        self.video_path_retrieval = [\n            f\"videos/{x}.mp4\" for x in self.partition_lists[\"val\"]\n        ]\n        # NOTE: We use nans rather than zeros to indicate missing faces, unless we wish\n        # to test single modality strength, which requires passing zeroed features for\n        # missing videos\n        if use_zeros_for_missing:\n            self.MISSING_VAL = 0\n        else:\n            self.MISSING_VAL = np.nan\n        # load the dataset-specific features into memory\n        self.load_features()\n        if text_agg == \"avg\":\n            self.logger.info(\"averaging the text features...\")\n            for key, val in self.text_features.items():\n                self.text_features[key] = [\n                    np.mean(x, 0, keepdims=1) for x in val\n                ]\n            self.logger.info(\"finished averaging the text features\")"
        },
        {
            "comment": "This code initializes training and raw configuration dictionaries, creates a tensor storage object, iterates through static experts, adds their relevant configurations to the dictionaries, and then builds a retrieval dictionary for both fixed and variable experts.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/base/base_dataset.py\":153-174",
            "content": "        self.trn_config = {}\n        self.raw_config = {}\n        self.tensor_storage = expert_tensor_storage(self.experts,\n                                                    self.feat_aggregation)\n        for static_expert in self.tensor_storage[\"fixed\"]:\n            if static_expert in self.feat_aggregation:\n                if \"trn_seg\" in self.feat_aggregation[static_expert].keys():\n                    self.trn_config[static_expert] = \\\n                        self.feat_aggregation[static_expert][\"trn_seg\"]\n                if \"raw\" in self.feat_aggregation[static_expert][\"temporal\"]:\n                    self.raw_config[static_expert] = 1\n        retrieval = {\n            expert: np.zeros(\n                (num_val, self.max_tokens[expert], raw_input_dims[expert]))\n            for expert in self.tensor_storage[\"variable\"]\n        }\n        retrieval.update({\n            expert: np.zeros((num_val, raw_input_dims[expert]))\n            for expert in self.tensor_storage[\"fixed\"]\n        })\n        self.retrieval = retrieval"
        },
        {
            "comment": "The code is initializing various arrays and tensors for evaluating the model on validation data. It sets up masks, retrieval tensors for text, captions, and attention, and prepares an empty list for saving the validation captions. This code is part of a larger function that appears to be setting up a dataset for video captioning or related task.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/base/base_dataset.py\":175-196",
            "content": "        self.test_ind = {\n            expert: paddle.ones([num_val])\n            for expert in self.experts\n        }\n        self.raw_captions_retrieval = [None] * num_val\n        # avoid evaluation on missing queries\n        self.query_masks = np.zeros((num_val, num_test_captions))\n        self.text_token_mask = np.zeros((num_val, num_test_captions))\n        self.text_retrieval = np.zeros((num_val, self.num_test_captions,\n                                        self.max_tokens[\"text\"], self.text_dim))\n        self.cap_retrieval = paddle.zeros(\n            [num_val, self.num_test_captions, self.max_tokens[\"text\"]],\n            dtype='int64'\n        )  #self.cap_retrieval = th.zeros((num_val, self.num_test_captions, self.max_tokens[\"text\"]))\n        self.att_retrieval = paddle.zeros(\n            [num_val, self.num_test_captions, self.max_tokens[\"text\"]],\n            dtype='int64'\n        )  #self.att_retrieval = th.zeros((num_val, self.num_test_captions, self.max_tokens[\"text\"]))\n        save_cap = []\n        for ii, video_name in enumerate(self.partition_lists[\"val\"]):"
        },
        {
            "comment": "This code initializes the retrieval and test indices for each expert in both fixed and variable tensor storage. It handles missing values by replacing them with 'MISSING_VAL' and binarizing non-missing features using marker values if requested.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/base/base_dataset.py\":198-216",
            "content": "            self.raw_captions_retrieval[ii] = self.raw_captions[video_name]\n            for expert in self.tensor_storage[\"fixed\"].intersection(\n                    self.experts):\n                feats = self.features[expert][video_name]\n                drop = self.has_missing_values(feats)\n                self.test_ind[expert][ii] = not drop\n                self.retrieval[expert][ii] = feats\n                if drop:\n                    self.retrieval[expert][ii][:] = self.MISSING_VAL\n                if self.feat_aggregation[expert].get(\"binarise\", False):\n                    keep = np.logical_not(\n                        np.isnan(self.retrieval[expert][:, 0, 0]))\n                    marker = np.ones_like(self.retrieval[expert][keep])\n                    self.retrieval[expert][keep] = marker\n            for expert in self.tensor_storage[\"variable\"].intersection(\n                    self.experts):\n                feats = self.features[expert][video_name]\n                drop = self.has_missing_values(feats)"
        },
        {
            "comment": "The code is handling the process of selecting video features and test captions for a specific expert. It drops certain entries, sets missing values where needed, applies binarization if required, and limits the number of tokens based on maximum token limit. It also restricts test captions if specified by the user. Finally, it sets query masks to prepare for further processing.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/base/base_dataset.py\":217-236",
            "content": "                self.test_ind[expert][ii] = not drop\n                if drop:\n                    self.retrieval[expert][ii][:] = self.MISSING_VAL\n                if self.feat_aggregation[expert].get(\"binarise\", False):\n                    keep = np.logical_not(\n                        np.isnan(self.retrieval[expert][:, 0, 0]))\n                    marker = np.ones_like(self.retrieval[expert][keep])\n                    self.retrieval[expert][keep] = marker\n                if self.test_ind[expert][ii]:\n                    keep = min(self.max_tokens[expert], len(feats))\n                    self.retrieval[expert][ii, :keep, :] = feats[:keep]\n            candidates_sentences = self.text_features[video_name]\n            if self.restrict_test_captions is not None:\n                keep_sent_idx = self.restrict_test_captions[video_name]\n                candidates_sentences = [candidates_sentences[keep_sent_idx]]\n            self.query_masks[ii, :len(candidates_sentences)] = 1\n            for test_caption_idx in range(self.num_test_captions):"
        },
        {
            "comment": "This code is iterating over a list of candidate sentences, breaking when the index exceeds the list length. For each sentence, it sets the number of tokens to keep based on the maximum allowed and masks the corresponding tokens. It then encodes the sentence into tokenized input IDs and attention mask for PaddlePaddle's model, appending the original sentence to a save list, storing the tokenized inputs in 'cap_retrieval', and the attention masks in 'att_retrieval'.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/base/base_dataset.py\":237-256",
            "content": "                if len(candidates_sentences) <= test_caption_idx:\n                    break\n                keep = min(len(candidates_sentences[test_caption_idx]),\n                           self.max_tokens[\"text\"])\n                self.text_token_mask[ii, test_caption_idx] = keep\n                sent = self.raw_captions_retrieval[ii][test_caption_idx]\n                sent = \" \".join(sent)\n                sent = sent.strip()\n                encoded_dict = self.tokenizer.__call__(\n                    sent,\n                    max_seq_len=self.max_tokens[\"text\"],\n                    pad_to_max_seq_len=True,\n                    return_attention_mask=True,\n                    truncation_strategy='longest_first')\n                cap_ids = paddle.to_tensor(encoded_dict['input_ids'])\n                attention_mask = paddle.to_tensor(\n                    encoded_dict['attention_mask'])\n                save_cap.append(sent)\n                self.cap_retrieval[ii, test_caption_idx, :] = cap_ids\n                self.att_retrieval[ii, test_caption_idx, :] = attention_mask"
        },
        {
            "comment": "The code is checking the progress of a dataset evaluation, creating text features for each sentence in the list, storing them in an array and then dumping the saved captions into a file called 'run_cap.pkl'. It also includes a function to configure train/test splits of the dataset.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/base/base_dataset.py\":257-279",
            "content": "                if ii % 500 == 0 and test_caption_idx == 0:\n                    msg = (\n                        f\"{ii}/{len(self.partition_lists['val'])} will evaluate \"\n                        f\"sentence {test_caption_idx} out of \"\n                        f\"{len(candidates_sentences)} (has {keep} words) \"\n                        f\"{video_name}\")\n                    self.logger.info(msg)\n                text_feats = candidates_sentences[test_caption_idx][:keep]\n                if text_feats.shape[0] == 0:\n                    text_feats = 0\n                    raise ValueError(\"empty text features!\")\n                self.text_retrieval[ii, test_caption_idx, :keep, :] = text_feats\n        with open('run_cap.pkl', 'wb') as f:\n            pkl.dump(save_cap, f)\n        self.sanity_checks()\n    def configure_train_test_splits(self, split_name):\n        \"\"\"Partition the datset into train/val/test splits.\n        Args:\n            split_name (str): the name of the split\n        \"\"\"\n        self.paths = type(self).dataset_paths()"
        },
        {
            "comment": "The code loads training/validation splits, reads and stores them in partition lists for later use, and initializes tensor storage for the PaddleVideo application.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/base/base_dataset.py\":280-303",
            "content": "        print(\"loading training/val splits....\")\n        tic = time.time()\n        for subset, path in self.paths[\"subset_list_paths\"][split_name].items():\n            root_feat = Path(self.root_feat)\n            subset_list_path = root_feat / path\n            if subset == \"train\" and self.eval_only:\n                rows = []\n            else:\n                with open(subset_list_path) as f:\n                    rows = f.read().splitlines()\n            self.partition_lists[subset] = rows\n        print(\"done in {:.3f}s\".format(time.time() - tic))\n        self.split_name = split_name\n    def collate_data(self, data):\n        batch_size = len(data)\n        tensors = {}\n        for expert in self.tensor_storage[\"fixed\"]:\n            if expert in self.trn_config.keys():\n                tensors[expert] = paddle.to_tensor(\n                    np.zeros((batch_size, self.trn_config[expert],\n                              self.raw_input_dims[expert])))\n            else:\n                tensors[expert] = paddle.to_tensor("
        },
        {
            "comment": "This code initializes tensors for a batch of data in a dataset. It creates zero-initialized tensors for each expert (modality), and separate tensors for text data including token masks, cap IDs, and attention mask. These will be filled with actual data as the batch is processed.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/base/base_dataset.py\":304-326",
            "content": "                    np.zeros((batch_size, self.raw_input_dims[expert])))\n        # Track which indices of each modality are available in the present batch\n        ind = {\n            expert: paddle.to_tensor(np.zeros(batch_size))\n            for expert in self.experts\n        }\n        tensors.update({\n            expert: paddle.to_tensor(\n                np.zeros((batch_size, self.max_tokens[expert],\n                          self.raw_input_dims[expert])))\n            for expert in self.tensor_storage[\"variable\"]\n        })\n        text_tensor = paddle.to_tensor(\n            np.zeros((batch_size, self.captions_per_video,\n                      self.max_tokens[\"text\"], self.text_dim)))\n        text_token_mask = paddle.to_tensor(\n            np.zeros((batch_size, self.captions_per_video)))\n        text_cap_id = paddle.zeros([batch_size, self.max_tokens[\"text\"]],\n                                   dtype='int64')\n        text_att_mask = paddle.zeros([batch_size, self.max_tokens[\"text\"]],\n                                     dtype='int64')"
        },
        {
            "comment": "This code iterates through a dataset, extracting data for various experts and creating tensors from it. It handles missing values and stores text and mask information in separate tensors.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/base/base_dataset.py\":328-349",
            "content": "        for ii, _ in enumerate(data):\n            datum = data[ii]\n            for expert in self.experts:\n                ind[expert][ii] = datum[f\"{expert}_ind\"]\n            for expert in self.tensor_storage[\"fixed\"]:\n                tensors[expert][ii] = datum[expert]\n            for expert in self.tensor_storage[\"variable\"]:\n                if ind[expert][ii]:\n                    keep = min(len(datum[expert]), self.max_tokens[expert])\n                    if keep:\n                        tensors[expert][ii, :keep, :] = datum[expert][:keep]\n                else:\n                    tensors[expert][ii, :, :] = self.MISSING_VAL\n            text = datum[\"text\"]\n            cap_id = datum[\"cap_id\"]\n            att_mask = datum[\"att_mask\"]\n            text_cap_id[ii, :] = paddle.to_tensor(cap_id)\n            text_att_mask[ii, :] = paddle.to_tensor(att_mask)\n            for jj in range(self.captions_per_video):\n                keep = min(len(text[jj]), self.max_tokens[\"text\"])\n                text_tensor[ii, jj, :keep, :] = text[jj][:keep]"
        },
        {
            "comment": "This code creates a minibatch for video features and text data. It applies binarization to some features, converts tensors, and prepares inputs for machine learning models. The process_sent function sets default values for EOS and UNK consistent with the word2int.json file.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/base/base_dataset.py\":350-371",
            "content": "                text_token_mask[ii, jj] = keep\n        ind = {key: ensure_tensor(val) for key, val in ind.items()}\n        experts = OrderedDict(\n            (expert, paddle.to_tensor(tensors[expert], dtype='float32'))\n            for expert in self.ordered_experts)\n        for expert in self.experts:\n            if self.feat_aggregation[expert].get(\"binarise\", False):\n                replace = np.logical_not(paddle.isnan(experts[expert][:, 0, 0]))\n                experts[expert][replace] = paddle.ones_like(\n                    experts[expert][replace])\n        minibatch = {\"experts\": experts, \"ind\": ind}\n        minibatch[\"text\"] = paddle.to_tensor(text_tensor, dtype='float32')\n        minibatch[\"cap_id\"] = paddle.to_tensor(text_cap_id, dtype='int64')\n        minibatch[\"att_mask\"] = paddle.to_tensor(text_att_mask, dtype='int64')\n        minibatch[\"text_token_mask\"] = paddle.to_tensor(text_token_mask)\n        return minibatch\n    def process_sent(self, sent, max_words, EOS: int = 1, UNK: int = 2):\n        # set EOS=1, UNK=2 by default, consistent with file 'word2int.json'."
        },
        {
            "comment": "This code defines a dataset class that loads and processes video features for text-to-video retrieval. It takes a list of videos, extracts expert features, and pads them to a fixed length. The class also supports indexing and has methods for getting the number of samples in the dataset.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/base/base_dataset.py\":372-396",
            "content": "        tokens = [self.word2int.get(w, UNK) for w in sent]\n        tokens = tokens[:max_words]\n        tokens_len = len(tokens)\n        tokens = np.array(tokens + [EOS] * (max_words - tokens_len))\n        return tokens, tokens_len\n    def __len__(self):\n        return self.num_samples\n    def __getitem__(self, idx):\n        if idx < self.num_samples:\n            vid = self.sample_list[idx]\n            features = {}\n            for expert in self.experts:\n                if expert not in self.trn_config.keys():\n                    if expert in self.raw_config.keys():\n                        features[expert] = np.mean(self.features[expert][vid],\n                                                   axis=0)\n                    else:\n                        features[expert] = self.features[expert][vid]\n                else:\n                    raw_frame_feats = self.features[expert][vid]\n                    new_length = 1\n                    num_frames = raw_frame_feats.shape[0]\n                    avg_duration = ((num_frames - new_length + 1) //"
        },
        {
            "comment": "The code segments video frame features into smaller segments with a specified average duration, accounts for the last segment if the duration is not divisible by the specified interval, and ensures the number of new feature segments matches the expected number.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/base/base_dataset.py\":397-412",
            "content": "                                    self.trn_config[expert])\n                    assert avg_duration > 0, \"average duration must be positive\"\n                    if avg_duration > 0:\n                        # maybe we could change to use average for each tiny segment\n                        # seems like use everything per iter\n                        offsets = np.multiply(\n                            list(range(self.trn_config[expert])), avg_duration)\n                        offsets += randint(avg_duration,\n                                           size=self.trn_config[expert])\n                        new_frame_feats = np.zeros(\n                            (self.trn_config[expert], raw_frame_feats.shape[1]))\n                        for idx, xx in enumerate(offsets):\n                            new_frame_feats[idx, :] = raw_frame_feats[xx, :]\n                        msg = \"returning a wrong feature != segment num\"\n                        assert new_frame_feats.shape[0] == self.trn_config[\n                            expert], msg"
        },
        {
            "comment": "This code is responsible for handling inconsistencies in text features storage. It randomly selects a caption from a list of captions for a given video, applies tokenization, and ensures that the sequence length does not exceed a maximum threshold. The result is stored in the encoded_dict variable.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/base/base_dataset.py\":413-436",
            "content": "                        features[expert] = new_frame_feats\n            ind = {}\n            for expert in self.ordered_experts:\n                if expert in self.tensor_storage[\"flaky\"]:\n                    ind[expert] = not self.has_missing_values(features[expert])\n                else:\n                    ind[expert] = 1\n            # Handle some inconsistencies between how the text features are stored\n            text = self.text_features[vid]\n            if isinstance(text, list):\n                pick = np.random.choice(len(text), size=self.captions_per_video)\n                sent = self.raw_captions[vid][pick[0]]\n                sent = \" \".join(sent)\n                sent = sent.strip()\n                text = np.array(text)[pick]\n                encoded_dict = self.tokenizer.__call__(\n                    sent,\n                    max_seq_len=self.max_tokens[\"text\"],\n                    pad_to_max_seq_len=True,\n                    return_attention_mask=True,\n                    truncation_strategy='longest_first')"
        },
        {
            "comment": "This code is initializing a sample for video dataset, using either given or randomly chosen text. It creates a dictionary with cap_id, attention mask, and other tensors as key-value pairs, and returns the sample. The get_retrieval_data function converts retrieval data to tensors and adds them to a dictionary containing text and experts keys before returning it.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/base/base_dataset.py\":437-462",
            "content": "                cap_id = encoded_dict['input_ids']\n                token_type_ids = encoded_dict['token_type_ids']\n                attention_mask = encoded_dict['attention_mask']\n            else:\n                pick = None\n                text = np.random.choice(text, size=self.captions_per_video)\n        # Return both the missing indices as well as the tensors\n        sample = {\"text\": text}\n        sample.update({\"cap_id\": cap_id})\n        sample.update({\"att_mask\": attention_mask})\n        sample.update({f\"{key}_ind\": val for key, val in ind.items()})\n        sample.update(features)\n        return sample\n    def get_retrieval_data(self):\n        experts = OrderedDict(\n            (expert, paddle.to_tensor(self.retrieval[expert], dtype='float32'))\n            for expert in self.ordered_experts)\n        retrieval_data = {\n            \"text\":\n            paddle.to_tensor(ensure_tensor(self.text_retrieval),\n                             dtype='float32'),\n            \"experts\":\n            experts,\n            \"cap_id\":"
        },
        {
            "comment": "The function defines a dictionary 'retrieval_data' containing cap_retrieval, att_mask, test_ind, and text_token_mask. It also defines the 'meta' dictionary containing query_masks, raw_captions, and paths. The function returns both 'retrieval_data' and 'meta'. The code provides a path lookup for visual features and skips loading if the feature is not requested.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/base/base_dataset.py\":463-491",
            "content": "            paddle.to_tensor(self.cap_retrieval, dtype='int64'),\n            \"att_mask\":\n            paddle.to_tensor(self.att_retrieval, dtype='int64'),\n            \"ind\":\n            self.test_ind,\n            \"text_token_mask\":\n            paddle.to_tensor(self.text_token_mask)\n        }\n        meta = {\n            \"query_masks\": self.query_masks,\n            \"raw_captions\": self.raw_captions_retrieval,\n            \"paths\": self.video_path_retrieval,\n        }\n        return retrieval_data, meta\n    def has_missing_values(self, x):\n        return isinstance(x, float) and np.isnan(x)\n    def visual_feat_paths(self, model_spec, tag=None):\n        \"\"\"Canonical path lookup for visual features\n        \"\"\"\n        if model_spec not in self.ordered_experts:\n            self.logger.info(\n                f\"Skipping load for {model_spec} (feature not requested)\")\n            return f\"SKIPPED-{model_spec}\"\n        feat_type, model_name, _ = model_spec.split(\".\")\n        aggs = self.feat_aggregation[model_spec]\n        base = f\"aggregated_{feat_type.replace('-', '_')}\""
        },
        {
            "comment": "The code defines a function that generates feature paths based on the provided arguments. It assembles a base string with parameters like fps, pixel_dim, and stride. If the feature type is \"facecrops\" or \"faceboxes\", it includes those parameters in the base string. For other types except for \"ocr\", \"speech\", and \"audio\", it also includes those parameters in the base string. It then adds optional parameters like offset and inner_stride if present. Finally, it generates a feature path list with file names and appends the tag if provided. The function also defines a logging assertion function that writes assertions to logs using a recipe from an external link.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/base/base_dataset.py\":492-515",
            "content": "        required = (\"fps\", \"pixel_dim\", \"stride\")\n        fps, pixel_dim, stride = [aggs.get(x, None) for x in required]\n        if feat_type in {\"facecrops\", \"faceboxes\"}:\n            base = f\"{base}_{fps}fps_{pixel_dim}px_stride{stride}\"\n        elif feat_type not in {\"ocr\", \"speech\", \"audio\"}:\n            base = f\"{base}_{fps}fps_{pixel_dim}px_stride{stride}\"\n        for option in \"offset\", \"inner_stride\":\n            if aggs.get(option, None) is not None:\n                base += f\"_{option}{aggs[option]}\"\n        feat_paths = []\n        for agg in aggs[\"temporal\"].split(\"-\"):\n            fname = f\"{model_name}-{agg}\"\n            if aggs[\"type\"] == \"logits\":\n                fname = f\"{fname}-logits\"\n            if tag is not None:\n                fname += f\"-{tag}\"\n            feat_paths.append(Path(base) / f\"{fname}.pickle\")\n        return feat_paths\n    def log_assert(self, bool_, msg=\"\", verbose=True):\n        \"\"\"Use assertions that will be written to the logs. This is a recipe from:\n        http://code.activestate.com/recipes/577074-logging-asserts/"
        },
        {
            "comment": "The code snippet is a function that checks an assertion. If the assertion fails, it constructs an exception message containing the traceback from the calling frame and raises an AssertionError with this message. Another function called \"summary_stats\" reports basic statistics about feature availability and variable lengths across different data subsets.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/base/base_dataset.py\":516-538",
            "content": "        \"\"\"\n        try:\n            assert bool_, msg\n        except AssertionError:\n            # construct an exception message from the code of the calling frame\n            last_stackframe = inspect.stack()[-2]\n            source_file, line_no, func = last_stackframe[1:4]\n            source = f\"Traceback (most recent call last):\\n\" + \\\n                     f\" File {source_file}, line {line_no}, in {func}\\n\"\n            if verbose:\n                # include more lines than that where the statement was made\n                source_code = open(source_file).readlines()\n                source += \"\".join(source_code[line_no - 3:line_no + 1])\n            else:\n                source += last_stackframe[-2][0].strip()\n            self.logger.debug(f\"{msg}\\n{source}\")\n            raise AssertionError(f\"{msg}\\n{source}\")\n    def summary_stats(self):\n        \"\"\"Report basic statistics about feature availability and variable lengths\n        across the different subsets of the data.\n        \"\"\"\n        self.logger.info(\"Computing feature stats...\")"
        },
        {
            "comment": "This code partitions datasets based on predefined subsets and checks the sizes of the features. It prints a summary for each subset, counting missing values and displaying the minimum, maximum, and mean sizes of features. This ensures that the dataset is properly partitioned and allows for efficient analysis.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/base/base_dataset.py\":539-561",
            "content": "        queries = self.ordered_experts + [\"text\"]\n        for subset, keep in self.partition_lists.items():\n            keep = set(keep)\n            print(f\"Summary for {subset}\")\n            for expert in queries:\n                if expert in self.features:\n                    feats = self.features[expert]\n                else:\n                    feats = self.text_features\n                vals = [feats[key] for key in keep]\n                missing = 0\n                sizes = []\n                for val in vals:\n                    if self.has_missing_values(val):\n                        missing += 1\n                    else:\n                        sizes.append(len(val))\n                if sizes:\n                    stat_str = (f\"min: {np.min(sizes):4}, \"\n                                f\"max: {np.max(sizes):4}, \"\n                                f\"mean: {np.mean(sizes):.1f}\")\n                    print(\n                        f\"{subset}: missing: {missing:4}, {stat_str} {expert}\")"
        }
    ]
}