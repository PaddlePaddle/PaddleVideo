{
    "summary": "The code creates a function for label change detection in video segmentation, computes precision, recall, and F1 score, uses Levenstein distance, and evaluates ground truth and predicted actions.",
    "details": [
        {
            "comment": "This code is a part of PaddleVideo library and defines a function get_labels_scores_start_end_time that takes input, frame-wise labels, actions dictionary, and optional background class. It returns labels, starts, ends, and scores based on the input and labels. The function also keeps track of the boundary score pointer and the last label.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/segmentation_metric.py\":0-34",
            "content": "# Copyright (c) 2021  PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nimport numpy as np\nimport argparse\nimport pandas as pd\nfrom .registry import METRIC\nfrom .base import BaseMetric\nfrom paddlevideo.utils import get_logger\nlogger = get_logger(\"paddlevideo\")\ndef get_labels_scores_start_end_time(input_np,\n                                     frame_wise_labels,\n                                     actions_dict,\n                                     bg_class=[\"background\", \"None\"]):\n    labels = []\n    starts = []\n    ends = []\n    scores = []\n    boundary_score_ptr = 0\n    last_label = frame_wise_labels[0]"
        },
        {
            "comment": "This code segment is a part of a larger video analysis algorithm. It identifies changes in frame-wise labels and calculates scores for those changes based on input_np data associated with the actions_dict[labels]. The scores are then appended to the 'scores' list, while starts and ends lists keep track of the start and end indices for each identified change. Finally, if the last label is not in bg_class, it adds an ending index to the 'ends' list and calculates a score using input_np data associated with starts and ends indices. The code then updates boundary_score_ptr and proceeds to the next iteration.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/segmentation_metric.py\":35-56",
            "content": "    if frame_wise_labels[0] not in bg_class:\n        labels.append(frame_wise_labels[0])\n        starts.append(0)\n    for i in range(len(frame_wise_labels)):\n        if frame_wise_labels[i] != last_label:\n            if frame_wise_labels[i] not in bg_class:\n                labels.append(frame_wise_labels[i])\n                starts.append(i)\n            if last_label not in bg_class:\n                ends.append(i)\n                score = np.mean(\n                        input_np[actions_dict[labels[boundary_score_ptr]], \\\n                            starts[boundary_score_ptr]:(ends[boundary_score_ptr] + 1)]\n                        )\n                scores.append(score)\n                boundary_score_ptr = boundary_score_ptr + 1\n            last_label = frame_wise_labels[i]\n    if last_label not in bg_class:\n        ends.append(i + 1)\n        score = np.mean(\n                    input_np[actions_dict[labels[boundary_score_ptr]], \\\n                        starts[boundary_score_ptr]:(ends[boundary_score_ptr] + 1)]"
        },
        {
            "comment": "The first code chunk is a function that takes a list of frame-wise labels, iterates over the frames, and returns lists for the label names, starting indices, and ending indices. It appends new label names to the labels list and new starting indices to the starts list when a new label appears, and adds corresponding ending indices to the ends list if the previous label was not \"background\" or \"None\". The last label's ending index is added after the loop.\n\nThe second code chunk defines a function that calculates the Levenshtein distance between two strings (p and y) using dynamic programming, which measures the minimum number of operations required to transform one string into another (insertion, deletion, or substitution). The function creates a 2D array D of size (m_row + 1) x (n_col + 1), where m_row is the length of p and n_col is the length of y. It then fills the array using dynamic programming, considering different operations at each step to calculate the minimum distance. This function likely uses the D array for further calculations or returns it as a result.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/segmentation_metric.py\":57-90",
            "content": "                    )\n        scores.append(score)\n        boundary_score_ptr = boundary_score_ptr + 1\n    return labels, starts, ends, scores\ndef get_labels_start_end_time(frame_wise_labels,\n                              bg_class=[\"background\", \"None\"]):\n    labels = []\n    starts = []\n    ends = []\n    last_label = frame_wise_labels[0]\n    if frame_wise_labels[0] not in bg_class:\n        labels.append(frame_wise_labels[0])\n        starts.append(0)\n    for i in range(len(frame_wise_labels)):\n        if frame_wise_labels[i] != last_label:\n            if frame_wise_labels[i] not in bg_class:\n                labels.append(frame_wise_labels[i])\n                starts.append(i)\n            if last_label not in bg_class:\n                ends.append(i)\n            last_label = frame_wise_labels[i]\n    if last_label not in bg_class:\n        ends.append(i + 1)\n    return labels, starts, ends\ndef levenstein(p, y, norm=False):\n    m_row = len(p)\n    n_col = len(y)\n    D = np.zeros([m_row + 1, n_col + 1], np.float)\n    for i in range(m_row + 1):"
        },
        {
            "comment": "The code contains a function called \"levenstein\" that calculates the Levenstein distance between two sequences. The Levenstein distance is a metric used to measure the difference between two strings of characters, such as words or labels. In this case, it's used to compare the recognized and ground truth labels for video segmentation. The function takes in a pair of lists P and Y representing recognized and ground truth labels respectively, and an optional norm parameter which normalizes the score if True. The output is a single numeric value representing the distance between the two lists of labels. This score can be used to evaluate the accuracy of the recognition algorithm in comparison with the ground truth data.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/segmentation_metric.py\":91-125",
            "content": "        D[i, 0] = i\n    for i in range(n_col + 1):\n        D[0, i] = i\n    for j in range(1, n_col + 1):\n        for i in range(1, m_row + 1):\n            if y[j - 1] == p[i - 1]:\n                D[i, j] = D[i - 1, j - 1]\n            else:\n                D[i, j] = min(D[i - 1, j] + 1, D[i, j - 1] + 1,\n                              D[i - 1, j - 1] + 1)\n    if norm:\n        score = (1 - D[-1, -1] / max(m_row, n_col)) * 100\n    else:\n        score = D[-1, -1]\n    return score\ndef edit_score(recognized,\n               ground_truth,\n               norm=True,\n               bg_class=[\"background\", \"None\"]):\n    P, _, _ = get_labels_start_end_time(recognized, bg_class)\n    Y, _, _ = get_labels_start_end_time(ground_truth, bg_class)\n    return levenstein(P, Y, norm)\ndef f_score(recognized, ground_truth, overlap, bg_class=[\"background\", \"None\"]):\n    p_label, p_start, p_end = get_labels_start_end_time(recognized, bg_class)\n    y_label, y_start, y_end = get_labels_start_end_time(ground_truth, bg_class)\n    tp = 0\n    fp = 0"
        },
        {
            "comment": "The code calculates the precision, recall, and F1 score for image segmentation by iterating through predicted and ground truth labels. It assigns hits when there is a match between predictions and ground truth, and counts true positives (TP), false positives (FP), and false negatives (FN). The boundary_AR function takes in predicted and ground truth boundaries, sorts them based on scores, and calculates various metrics for image segmentation.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/segmentation_metric.py\":127-160",
            "content": "    hits = np.zeros(len(y_label))\n    for j in range(len(p_label)):\n        intersection = np.minimum(p_end[j], y_end) - np.maximum(\n            p_start[j], y_start)\n        union = np.maximum(p_end[j], y_end) - np.minimum(p_start[j], y_start)\n        IoU = (1.0 * intersection / union) * (\n            [p_label[j] == y_label[x] for x in range(len(y_label))])\n        # Get the best scoring segment\n        idx = np.array(IoU).argmax()\n        if IoU[idx] >= overlap and not hits[idx]:\n            tp += 1\n            hits[idx] = 1\n        else:\n            fp += 1\n    fn = len(y_label) - sum(hits)\n    return float(tp), float(fp), float(fn)\ndef boundary_AR(pred_boundary, gt_boundary, overlap_list, max_proposal):\n    p_label, p_start, p_end, p_scores = pred_boundary\n    y_label, y_start, y_end, _ = gt_boundary\n    # sort proposal\n    pred_dict = {\n        \"label\": p_label,\n        \"start\": p_start,\n        \"end\": p_end,\n        \"scores\": p_scores\n    }\n    pdf = pd.DataFrame(pred_dict)\n    pdf = pdf.sort_values(by=\"scores\", ascending=False)"
        },
        {
            "comment": "This code segment handles the refinement of proposals in an object detection model. If the number of proposals is less than the maximum allowed, it repeats the last proposal to meet the requirement. If there are more proposals than the maximum allowed, it discards extra proposals. The code then calculates the average recall (AR) by iterating over the overlap list and counting true positives (tp) and false positives (fp). It also initializes hits for each proposal in the ground truth labels.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/segmentation_metric.py\":161-190",
            "content": "    p_label = list(pdf[\"label\"])\n    p_start = list(pdf[\"start\"])\n    p_end = list(pdf[\"end\"])\n    p_scores = list(pdf[\"scores\"])\n    # refine AN\n    if len(p_label) < max_proposal and len(p_label) > 0:\n        p_label = p_label + [p_label[-1]] * (max_proposal - len(p_label))\n        p_start = p_start + [p_start[-1]] * (max_proposal - len(p_start))\n        p_start = p_start + p_start[len(p_start) -\n                                    (max_proposal - len(p_start)):]\n        p_end = p_end + [p_end[-1]] * (max_proposal - len(p_end))\n        p_scores = p_scores + [p_scores[-1]] * (max_proposal - len(p_scores))\n    elif len(p_label) > max_proposal:\n        p_label[max_proposal:] = []\n        p_start[max_proposal:] = []\n        p_end[max_proposal:] = []\n        p_scores[max_proposal:] = []\n    t_AR = np.zeros(len(overlap_list))\n    for i in range(len(overlap_list)):\n        overlap = overlap_list[i]\n        tp = 0\n        fp = 0\n        hits = np.zeros(len(y_label))\n        for j in range(len(p_label)):\n            intersection = np.minimum(p_end[j], y_end) - np.maximum("
        },
        {
            "comment": "This code calculates the Average Recall (AR) for video segmentation models. It iterates through ground truth and predicted labels, calculates intersection over union (IoU), counts true positives (tp), false positives (fp), and false negatives (fn). Then it computes recall and averages them to obtain AR. The SegmentationMetric class initializes with various parameters for the metric calculation.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/segmentation_metric.py\":191-229",
            "content": "                p_start[j], y_start)\n            union = np.maximum(p_end[j], y_end) - np.minimum(\n                p_start[j], y_start)\n            IoU = (1.0 * intersection / union)\n            # Get the best scoring segment\n            idx = np.array(IoU).argmax()\n            if IoU[idx] >= overlap and not hits[idx]:\n                tp += 1\n                hits[idx] = 1\n            else:\n                fp += 1\n        fn = len(y_label) - sum(hits)\n        recall = float(tp) / (float(tp) + float(fn))\n        t_AR[i] = recall\n    AR = np.mean(t_AR)\n    return AR\n@METRIC.register\nclass SegmentationMetric(BaseMetric):\n    \"\"\"\n    Test for Video Segmentation based model.\n    \"\"\"\n    def __init__(self,\n                 data_size,\n                 batch_size,\n                 overlap,\n                 actions_map_file_path,\n                 log_interval=1,\n                 tolerance=5,\n                 boundary_threshold=0.7,\n                 max_proposal=100):\n        \"\"\"prepare for metrics\n        \"\"\"\n        super().__init__(data_size, batch_size, log_interval)"
        },
        {
            "comment": "This code initializes a SegmentationMetric object, reads an actions map file, and prepares to update metrics during each iteration. It calculates true positives (cls_tp), false positives (cls_fp), and false negatives (cls_fn) for each frame's overlap. The AR_at_AN is also initialized with empty lists for each max_proposal value.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/segmentation_metric.py\":230-263",
            "content": "        # actions dict generate\n        file_ptr = open(actions_map_file_path, 'r')\n        actions = file_ptr.read().split('\\n')[:-1]\n        file_ptr.close()\n        self.actions_dict = dict()\n        for a in actions:\n            self.actions_dict[a.split()[1]] = int(a.split()[0])\n        # cls score\n        self.overlap = overlap\n        self.overlap_len = len(overlap)\n        self.cls_tp = np.zeros(self.overlap_len)\n        self.cls_fp = np.zeros(self.overlap_len)\n        self.cls_fn = np.zeros(self.overlap_len)\n        self.total_correct = 0\n        self.total_edit = 0\n        self.total_frame = 0\n        self.total_video = 0\n        # boundary score\n        self.max_proposal = max_proposal\n        self.AR_at_AN = [[] for _ in range(max_proposal)]\n    def update(self, batch_id, data, outputs):\n        \"\"\"update metrics during each iter\n        \"\"\"\n        groundTruth = data[1]\n        predicted = outputs['predict']\n        output_np = outputs['output_np']\n        outputs_np = predicted.numpy()\n        outputs_arr = output_np.numpy()[0, :]"
        },
        {
            "comment": "This code segment compares ground truth and predicted actions for a video. It converts the numpy arrays to lists, generates predicted and ground truth boundaries using the `get_labels_scores_start_end_time` function, and then initializes variables for accuracy calculation.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/segmentation_metric.py\":264-294",
            "content": "        gt_np = groundTruth.numpy()[0, :]\n        recognition = []\n        for i in range(outputs_np.shape[0]):\n            recognition = np.concatenate((recognition, [\n                list(self.actions_dict.keys())[list(\n                    self.actions_dict.values()).index(outputs_np[i])]\n            ]))\n        recog_content = list(recognition)\n        gt_content = []\n        for i in range(gt_np.shape[0]):\n            gt_content = np.concatenate((gt_content, [\n                list(self.actions_dict.keys())[list(\n                    self.actions_dict.values()).index(gt_np[i])]\n            ]))\n        gt_content = list(gt_content)\n        pred_boundary = get_labels_scores_start_end_time(\n            outputs_arr, recog_content, self.actions_dict)\n        gt_boundary = get_labels_scores_start_end_time(\n            np.ones(outputs_arr.shape), gt_content, self.actions_dict)\n        # cls score\n        correct = 0\n        total = 0\n        edit = 0\n        for i in range(len(gt_content)):\n            total += 1\n            #accumulate"
        },
        {
            "comment": "This code calculates segmentation metrics, including accuracy, false positives, and false negatives for video frames. It also keeps track of total correct predictions, edit distances, proposal scores, and accumulates these metrics per video. The `accumulate` function is used to calculate classification accuracy when all iterations are finished.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/segmentation_metric.py\":295-329",
            "content": "            self.total_frame += 1\n            if gt_content[i] == recog_content[i]:\n                correct += 1\n                #accumulate\n                self.total_correct += 1\n        edit_num = edit_score(recog_content, gt_content)\n        edit += edit_num\n        self.total_edit += edit_num\n        for s in range(self.overlap_len):\n            tp1, fp1, fn1 = f_score(recog_content, gt_content, self.overlap[s])\n            # accumulate\n            self.cls_tp[s] += tp1\n            self.cls_fp[s] += fp1\n            self.cls_fn[s] += fn1\n        # accumulate\n        self.total_video += 1\n        # proposal score\n        for AN in range(self.max_proposal):\n            AR = boundary_AR(pred_boundary,\n                             gt_boundary,\n                             self.overlap,\n                             max_proposal=(AN + 1))\n            self.AR_at_AN[AN].append(AR)\n    def accumulate(self):\n        \"\"\"accumulate metrics when finished all iters.\n        \"\"\"\n        # cls metric\n        Acc = 100 * float(self.total_correct) / self.total_frame"
        },
        {
            "comment": "This code calculates segmentation metrics, including Edit distance, F1 score at different overlap levels, and proposal area under the curve (AUC). It then stores these values in a dictionary and computes average AUCs for different overlap thresholds. The code also calculates an ensemble metric based on accuracy (Acc) and Edit distance. Finally, it logs this information as a string.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/segmentation_metric.py\":330-355",
            "content": "        Edit = (1.0 * self.total_edit) / self.total_video\n        Fscore = dict()\n        for s in range(self.overlap_len):\n            precision = self.cls_tp[s] / float(self.cls_tp[s] + self.cls_fp[s])\n            recall = self.cls_tp[s] / float(self.cls_tp[s] + self.cls_fn[s])\n            f1 = 2.0 * (precision * recall) / (precision + recall)\n            f1 = np.nan_to_num(f1) * 100\n            Fscore[self.overlap[s]] = f1\n        # proposal metric\n        proposal_AUC = np.array(self.AR_at_AN) * 100\n        AUC = np.mean(proposal_AUC)\n        AR_at_AN1 = np.mean(proposal_AUC[0, :])\n        AR_at_AN5 = np.mean(proposal_AUC[4, :])\n        AR_at_AN15 = np.mean(proposal_AUC[14, :])\n        # log metric\n        log_mertic_info = \"dataset model performence: \"\n        # preds ensemble\n        log_mertic_info += \"Acc: {:.4f}, \".format(Acc)\n        log_mertic_info += 'Edit: {:.4f}, '.format(Edit)\n        for s in range(len(self.overlap)):\n            log_mertic_info += 'F1@{:0.2f}: {:.4f}, '.format(\n                self.overlap[s], Fscore[self.overlap[s]])"
        },
        {
            "comment": "This code calculates and logs various segmentation metrics, including AUC, AR@AN1, AR@AN5, and AR@AN15. It also updates the metric dictionary with F1 scores for different overlap thresholds and clears the classifier statistics for the next epoch.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/segmentation_metric.py\":357-384",
            "content": "        # boundary metric\n        log_mertic_info += \"Auc: {:.4f}, \".format(AUC)\n        log_mertic_info += \"AR@AN1: {:.4f}, \".format(AR_at_AN1)\n        log_mertic_info += \"AR@AN5: {:.4f}, \".format(AR_at_AN5)\n        log_mertic_info += \"AR@AN15: {:.4f}, \".format(AR_at_AN15)\n        logger.info(log_mertic_info)\n        # log metric\n        metric_dict = dict()\n        metric_dict['Acc'] = Acc\n        metric_dict['Edit'] = Edit\n        for s in range(len(self.overlap)):\n            metric_dict['F1@{:0.2f}'.format(\n                self.overlap[s])] = Fscore[self.overlap[s]]\n        metric_dict['Auc'] = AUC\n        metric_dict['AR@AN1'] = AR_at_AN1\n        metric_dict['AR@AN5'] = AR_at_AN5\n        metric_dict['AR@AN15'] = AR_at_AN15\n        # clear for next epoch\n        # cls\n        self.cls_tp = np.zeros(self.overlap_len)\n        self.cls_fp = np.zeros(self.overlap_len)\n        self.cls_fn = np.zeros(self.overlap_len)\n        self.total_correct = 0\n        self.total_edit = 0\n        self.total_frame = 0\n        self.total_video = 0"
        },
        {
            "comment": "This code initializes the attribute \"AR_at_AN\" as a list of empty lists, with the length equal to the maximum number of proposals. This is done within the context of a class method and the list will likely be used for storing evaluation metrics related to these proposals. The method then returns a dictionary (metric_dict) containing other potentially calculated metrics.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/metrics/segmentation_metric.py\":385-388",
            "content": "        # proposal\n        self.AR_at_AN = [[] for _ in range(self.max_proposal)]\n        return metric_dict"
        }
    ]
}