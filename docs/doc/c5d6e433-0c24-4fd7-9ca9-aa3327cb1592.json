{
    "summary": "The code combines T2VLAD and BERT in a CENet model for video analysis, initializes MOE with Transformer layers, extracts visual features, and uses VLAD for cross-view localization. The function calculates video-text similarity scores, includes batch normalization, global pooling, and availability masking, reshapes weights, normalizes embeddings, computes text-video similarity with weighting, checks for NaN values, and raises ValueError if found.",
    "details": [
        {
            "comment": "This code snippet is importing necessary libraries and models for the T2VLAD model. It includes copyright and license information, as well as imports from base, net_vlad, paddlenlp, and various other modules. The code aims to create a T2VLAD model using PaddlePaddle framework with potential dependencies on BertModel and paddlenlp packages.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/model/model.py\":0-33",
            "content": "# Copyright 2021 Antoine Miech All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport copy\nimport time\nimport itertools\nimport paddle\nimport numpy as np\nimport paddle.nn as nn\nimport paddle.nn.functional as F\nfrom paddle import Tensor\nfrom typing import Optional\nfrom collections import OrderedDict\nfrom base import BaseModel\nfrom model.net_vlad import NetVLAD\ntry:\n    from paddlenlp.transformers import BertModel\nexcept ImportError as e:\n    print(\n        f\"{e}, [paddlenlp] package and it's dependencies is required for T2VLAD.\""
        },
        {
            "comment": "The code defines three functions: 'Mish', 'kronecker_prod', and 'drop_nans'. The 'Mish' function implements the mish activation function, which applies the mish formula element-wise. The 'kronecker_prod' function performs a Kronecker product of two tensors along the last dimension. Finally, the 'drop_nans' function removes NaN values from input features, considering any missing indices as containing NaN.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/model/model.py\":34-65",
            "content": "    )\nclass Mish(nn.Layer):\n    '''\n    Applies the mish function element-wise:\n    mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + exp(x)))\n    SRC: https://github.com/digantamisra98/Mish/blob/master/Mish/Torch/mish.py\n    '''\n    def forward(self, input):\n        '''\n        Forward pass of the function.\n        '''\n        return input * paddle.tanh(F.softplus(input))\ndef kronecker_prod(t1, t2):\n    # kronecker is performed along the last dim\n    kron = paddle.bmm(t1.reshape([-1, t1.size(-1)], 1),\n                      t2.reshape([-1, 1, t2.size(-1)]))\n    return kron.reshape[(t1.shape[0], t1.shape[1], -1)]\ndef drop_nans(x, ind, validate_missing):\n    \"\"\"Remove nans, which we expect to find at missing indices.\n    Args:\n        x (paddle.Tensor): features\n        ind (paddle.Tensor): binary values denoting whether or not a given feature is present\n        validate_missing (bool): whether to validate that the missing location contains a nan.\n    Returns:\n        (paddle.tensor): the features, with the missing values masked to zero."
        },
        {
            "comment": "The code defines a CENet model and checks for any NaN values in the input tensor 'x'. It sets missing locations to 0 and raises a ValueError if there are still NaN values after removing them. The model consists of expert_dims, vlad_clusters, feat_aggregation, ce_shared_dim, use_mish, and mimic_ce_dims. The text_pooling layer is implemented as NetVLAD for feature extraction if the vlad_clusters[\"text\"] is non-zero.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/model/model.py\":66-97",
            "content": "    \"\"\"\n    missing = paddle.nonzero(ind == 0).flatten()\n    if missing.numel():\n        if validate_missing:\n            vals = x[missing[0]]\n            assert paddle.isnan(vals.reshape(\n                [-1])[0]), \"expected nans at missing locations\"\n        #Prevent overwrite of the original tensor\n        x_ = x\n        x_[missing] = 0\n        x = x_\n    if paddle.isnan(x).sum() > 0:\n        raise ValueError(\"Still find nans after removing it!\")\n    return x\nclass CENet(BaseModel):\n    def __init__(self, text_dim, expert_dims, vlad_clusters, ghost_clusters,\n                 feat_aggregation, ce_shared_dim, use_mish, mimic_ce_dims):\n        super().__init__()\n        self.expert_dims = expert_dims\n        self.feat_aggregation = feat_aggregation\n        vlad_feat_sizes = {key: val for key, val in vlad_clusters.items()}\n        if vlad_clusters[\"text\"] == 0:\n            self.text_pooling = nn.Sequential()\n        else:\n            self.text_pooling = NetVLAD(\n                feature_size=text_dim,\n                cluster_size=vlad_clusters[\"text\"],"
        },
        {
            "comment": "The code initializes a model with specified expert dimensions, and handles nan-checks for the experts. It also creates a time estimation start point.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/model/model.py\":98-129",
            "content": "                ghost_clusters=ghost_clusters[\"text\"],\n            )\n            self.text_bert = BertModel.from_pretrained('bert-base-uncased')\n            text_dim = self.text_pooling.out_dim\n        self.ce = CEModule(\n            text_dim=text_dim,\n            expert_dims=expert_dims,\n            vlad_feat_sizes=vlad_feat_sizes,\n            mimic_ce_dims=mimic_ce_dims,\n            use_mish=use_mish,\n            same_dim=ce_shared_dim,\n        )\n    def forward(self,\n                experts,\n                ind,\n                cap_id=None,\n                att_mask=None,\n                text=None,\n                raw_captions=None,\n                text_token_mask=None):\n        aggregated_experts = OrderedDict()\n        # Handle all nan-checks\n        for mod in self.expert_dims:\n            experts[mod] = drop_nans(x=experts[mod],\n                                     ind=ind[mod],\n                                     validate_missing=True)\n            aggregated_experts[mod] = experts[mod]\n        start = time.time()"
        },
        {
            "comment": "This code is reshaping the input text tensor to account for multiple captions per video, applying a pooling operation specific to the chosen text_pooling method (NetVLAD in this case), and then passing the text through a BERT model before performing pooling again. The resulting output is shaped according to the required format for further processing.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/model/model.py\":130-147",
            "content": "        # When pooling multiple captions for a single video, we treat them as separate\n        # members of the minibatch, so the total pooling op does the following:\n        # pooling: B x captions_per_video x max_sentence_length x text_feat_dim\n        # -> B x captions_per_video (cluster_dim * text_feat_dim)\n        B, captions_per_video, max_words, text_feat_dim = text.shape\n        text = text.reshape([B * captions_per_video, max_words, text_feat_dim])\n        if isinstance(self.text_pooling, NetVLAD):\n            kwargs = {\"mask\": text_token_mask}\n        else:\n            kwargs = {}\n        cap_id = cap_id.reshape([B * captions_per_video, -1])\n        att_mask = att_mask.reshape([B * captions_per_video, -1])\n        att_mask = att_mask.unsqueeze(axis=[1, 2])\n        bert_out = self.text_bert(cap_id,\n                                  token_type_ids=None,\n                                  attention_mask=att_mask)\n        text = bert_out[0]\n        text, _, save_ass = self.text_pooling(text, **kwargs)"
        },
        {
            "comment": "The given code contains a function that performs multi-head attention, feedforward model implementation, and LayerNorm normalization in Transformer layers. The `nn.MultiHeadAttention` applies the self-attention mechanism, while `nn.Linear` layers are used for linear transformations. Dropout and ReLU activations are also applied to prevent overfitting and introduce nonlinearity respectively.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/model/model.py\":148-178",
            "content": "        text = text.reshape([B, captions_per_video, -1])\n        return self.ce(text, aggregated_experts, ind, raw_captions,\n                       self.text_pooling, start)\ndef _get_clones(module, N):\n    return nn.LayerList([copy.deepcopy(module) for i in range(N)])\nclass TransformerLayer(nn.Layer):\n    def __init__(self,\n                 d_model,\n                 nhead,\n                 dim_feedforward=2048,\n                 dropout=0.1,\n                 activation=\"relu\",\n                 normalize_before=True):\n        super().__init__()\n        self.self_attn = nn.MultiHeadAttention(d_model, nhead, dropout=dropout)\n        # Implementation of Feedforward model\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n        self.activation = F.relu"
        },
        {
            "comment": "This code defines a class with three forward functions: `forward_post`, `forward_pre`, and an undefined `forward`. The `forward_post` function applies self-attention to the input source tensor, while the `forward_pre` function normalizes the input source tensor before applying self-attention. Both functions take an optional mask and position embedding for the input tensor. The code also includes a class attribute `normalize_before` that determines whether to normalize the input tensor or not.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/model/model.py\":179-206",
            "content": "        self.normalize_before = normalize_before\n    def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n        return tensor if pos is None else tensor + pos\n    def forward_post(self,\n                     src,\n                     src_mask: Optional[Tensor] = None,\n                     pos: Optional[Tensor] = None):\n        q = k = self.with_pos_embed(src, pos)\n        q = q.transpose([1, 0, 2])\n        k = k.transpose([1, 0, 2])\n        src = src.transpose([1, 0, 2])\n        src2 = self.self_attn(q, k, value=src, attn_mask=src_mask)\n        src2 = src2.transpose([1, 0, 2])\n        src = src + self.dropout1(src2)\n        src = self.norm1(src)\n        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n        src = src + self.dropout2(src2)\n        src = self.norm2(src)\n        return src\n    def forward_pre(self,\n                    src,\n                    src_mask: Optional[Tensor] = None,\n                    pos: Optional[Tensor] = None):\n        src2 = self.norm1(src)\n        q = k = self.with_pos_embed(src2, pos)"
        },
        {
            "comment": "The code defines a Transformer class that performs multi-head self-attention and feedforward operations. The class takes an encoder_layer as input and num_layers as parameters, allowing for multiple layers of transformation. The Transformer class has a forward function that can perform the transformations before or after normalization depending on the value of normalize_before flag. The _reset_parameters function is used to reset the parameters of the class.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/model/model.py\":207-236",
            "content": "        q = q.transpose([1, 0, 2])\n        k = k.transpose([1, 0, 2])\n        src2 = src2.transpose([1, 0, 2])\n        src2 = self.self_attn(q, key=k, value=src2, attn_mask=src_mask)\n        src2 = src2.transpose([1, 0, 2])\n        src = src + self.dropout1(src2)\n        src2 = self.norm2(src)\n        src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))\n        src = src + self.dropout2(src2)\n        return src\n    def forward(self,\n                src,\n                src_mask: Optional[Tensor] = None,\n                pos: Optional[Tensor] = None):\n        if self.normalize_before:\n            return self.forward_pre(src, src_mask, pos)\n        return self.forward_post(src, src_mask, pos)\nclass Transformer(nn.Layer):\n    def __init__(self, encoder_layer, num_layers, norm=None):\n        super().__init__()\n        self.layers = _get_clones(encoder_layer, num_layers)\n        self.num_layers = num_layers\n        self.norm = norm\n        self._reset_parameters()\n    def _reset_parameters(self):\n        for p in self.parameters():  # may have a problem"
        },
        {
            "comment": "This code defines a CEModule class with expert_dims, modalities, mimic_ce_dims, vlad_feat_sizes, and same_dim parameters. It uses the Mish function for non-linear activation if use_mish is True, otherwise using ReLU. It also includes a ContextGating object and a VisTransformer boolean.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/model/model.py\":237-274",
            "content": "            if p.dim() > 1:\n                nn.initializer.XavierUniform(p)\n    def forward(self,\n                src,\n                mask: Optional[Tensor] = None,\n                pos: Optional[Tensor] = None):\n        output = src\n        for layer in self.layers:\n            output = layer(output)\n        if self.norm is not None:\n            output = self.norm(output)\n        return output\nclass CEModule(nn.Layer):\n    def __init__(self, expert_dims, text_dim, use_mish, mimic_ce_dims,\n                 vlad_feat_sizes, same_dim):\n        super().__init__()\n        modalities = list(expert_dims.keys())\n        self.expert_dims = expert_dims\n        self.modalities = modalities\n        self.mimic_ce_dims = mimic_ce_dims\n        self.same_dim = same_dim\n        self.use_mish = use_mish\n        self.vlad_feat_sizes = vlad_feat_sizes\n        self.reduce_dim = 64\n        self.moe_cg = ContextGating\n        self.vis_transformer = True\n        if self.use_mish:\n            self.non_lin = Mish()\n        else:\n            self.non_lin = nn.ReLU()"
        },
        {
            "comment": "This code is initializing a MOE (Multi-Output Expert) model with specified modalities and dimensions, setting up the linear layer, weights, and batch normalization flags. It also defines the temporal repeat for each modality and calculates the input dimensions based on expert dimensions and temporal repetitions.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/model/model.py\":276-296",
            "content": "        num_mods = len(expert_dims)\n        self.moe_fc = nn.Linear(text_dim, len(expert_dims))\n        self.moe_weights = paddle.ones([1, num_mods]) / num_mods\n        # The batch size of the face input can vary (due to missing inputs), so we\n        # probably shouldn't use BN on this branch. It's probably fine to leave it\n        # n for the corresponding text inputs, (but we should switch to GN)\n        use_bns = [True for modality in self.modalities]\n        # NOTE: When use_ce is not used, the text features are projected to\n        # subspaces of different dimensions.  When use_ce is used, they must all\n        # be projected to `same_dim` (to allow fusion). The only excpetion is for an\n        # ablation in which we mimic the `same_dim` reduction to measure whether this\n        # projection influences overall performance.\n        self.repeat_temporal = {}\n        for mod in modalities:\n            self.repeat_temporal[mod] = 1\n        in_dims = [\n            expert_dims[mod][0] * self.repeat_temporal[mod]"
        },
        {
            "comment": "The code initializes and prepares model components for modalities, including dimensions for expert features and feature sizes. It also creates a transformer if visual transformation is enabled, and sets up feature reducers if cross-entropy loss dims are mimicked.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/model/model.py\":297-322",
            "content": "            for mod in modalities\n        ]\n        agg_dims = [\n            expert_dims[mod][1] * self.repeat_temporal[mod]\n            for mod in modalities\n        ]\n        feat_dims = [\n            expert_dims[mod][0] // self.vlad_feat_sizes[mod]\n            for mod in modalities\n        ]\n        if self.vis_transformer:\n            num_encoder_layers = 1\n            d_model = 768\n            nhead = 4\n            dim_feedforward = 768\n            dropout = 0  #dropout=0.1\n            normalize_before = True\n            encoder_layer = TransformerLayer(d_model, nhead, dim_feedforward,\n                                             dropout)\n            encoder_norm = nn.LayerNorm(d_model) if normalize_before else None\n            self.transformers = Transformer(encoder_layer, num_encoder_layers,\n                                            encoder_norm)\n        if self.mimic_ce_dims:\n            dim_reducers = [ReduceDim(in_dim, same_dim) for in_dim in feat_dims]\n            self.video_dim_reduce = nn.LayerList(dim_reducers)"
        },
        {
            "comment": "The code creates GatedEmbeddingUnit instances for both video and text features of different dimensions, initializes LayerLists to store them as video_GU and text_GU. The compute_moe_weights function calculates softmax weights for multiple captions (K) assigned to the same video, with an assertion for 1-10 modalities. Text is reshaped before applying MOE fully connected layer, then reshaped back to BxKxM shape.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/model/model.py\":324-349",
            "content": "        gated_vid_embds = [\n            GatedEmbeddingUnit(in_dim, same_dim, use_bn=True)\n            for in_dim in feat_dims\n        ]\n        text_out_dims = [same_dim for _ in agg_dims]\n        self.video_GU = nn.LayerList(gated_vid_embds)\n        gated_text_embds = [\n            GatedEmbeddingUnit(text_dim, dim, use_bn=True)\n            for dim in text_out_dims\n        ]\n        self.text_GU = nn.LayerList(gated_text_embds)\n    def compute_moe_weights(self, text, ind):\n        # compute weights for all captions (including when assigned K captions to\n        # the same video)\n        B, K, D = text.shape\n        M = len(self.modalities)\n        msg = f\"expected between 1 and 10 modalities, found {M} ({self.modalities})\"\n        assert 1 <= M <= 10, msg\n        # Treat each caption independently in the softmax (which runs over modalities)\n        text = text.reshape([B * K, D])\n        moe_weights = self.moe_fc(text)  # BK x D -> BK x M\n        moe_weights = F.softmax(moe_weights, axis=1)\n        moe_weights = moe_weights.reshape([B, K, M])"
        },
        {
            "comment": "This code is implementing a method for passing text embeddings through gated units. It first reshapes the input text, then iterates over the modalities and gated units to compute the text embeddings, which are stored in a dictionary. Finally, it reshapes the result back to its original shape.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/model/model.py\":350-373",
            "content": "        return moe_weights\n    def forward(self, text, experts, ind, raw_captions, vis_vlad, stime):\n        \"\"\"Compute joint embeddings and, if requested, a confusion matrix between\n        video and text representations in the minibatch.\n        Notation: B = batch size, M = number of modalities\n        \"\"\"\n        # Pass text embeddings through gated units\n        text_embd = {}\n        # Unroll repeated captions into present minibatch\n        B, captions_per_video, feat_dim = text.shape\n        text = text.reshape([B * captions_per_video, feat_dim])\n        for modality, layer in zip(self.modalities, self.text_GU):\n            # NOTE: Due to the batch norm, the gated units are sensitive to passing\n            # in a lot of zeroes, so we do the masking step after the forwards pass\n            text_ = layer(text)\n            # We always assume that text is available for retrieval\n            text_ = text_.reshape([B, captions_per_video, -1])\n            text_embd[modality] = text_\n        text = text.reshape([B, captions_per_video, -1])"
        },
        {
            "comment": "This code section is performing MOE weights computation and feature extraction for a Multi-Modal Video Analysis task. It excludes specific features to handle NAN values, then computes the MOE weights using text data and reshapes it accordingly. The visual features are extracted for each modality, then all the visual features are concatenated along the dimension. Finally, if vis_transformer is present, it is applied on the extracted visual features.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/model/model.py\":375-396",
            "content": "        # vladded nans are handled earlier (during pooling)\n        # We also avoid zeroing random features, since this will leak information\n        # exclude = list(self.vlad_feat_sizes.keys()) + list(self.random_feats)\n        # experts = self.mask_missing_embeddings(experts, ind, exclude=exclude)\n        # MOE weights computation + normalization - note that we use the first caption\n        # sample to predict the weights\n        moe_weights = self.compute_moe_weights(text, ind=ind)\n        text_local = text.reshape([B * captions_per_video, -1])\n        vis_local = {}\n        for modality in self.modalities:\n            vis_local[modality] = experts[modality]\n        all_vis_feat = []\n        if hasattr(self, \"video_dim_reduce\"):\n            # Embed all features to a common dimension\n            for modality, layer in zip(self.modalities, self.video_dim_reduce):\n                all_vis_feat.append(layer(vis_local[modality]))\n        all_vis_feat = paddle.concat(all_vis_feat, axis=1)\n        if self.vis_transformer:"
        },
        {
            "comment": "This code performs cross-view video localization by calculating the cross-view confidence matrix using VLAD and MOE weights. It also applies transformers, max pooling, and sharded inner products for each modality. The result is a dictionary containing the modalities used in the computation.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/model/model.py\":397-421",
            "content": "            experts_tensor = all_vis_feat\n            experts_tensor = experts_tensor.transpose([1, 0, 2])\n            att_out = self.transformers(experts_tensor, mask=None, pos=None)\n            all_vis_feat = att_out.transpose([1, 0, 2])\n        vis_local, _, save_ass = vis_vlad(all_vis_feat, freeze=True)\n        cross_view_conf_matrix_tv = paddle.matmul(text_local, vis_local.t())\n        for modality in self.modalities:\n            experts[modality] = experts[modality].max(axis=1)\n        for modality, layer in zip(self.modalities, self.video_GU):\n            experts[modality] = layer(experts[modality])\n        cross_view_conf_matrix = sharded_cross_view_inner_product(\n            ind=ind,\n            vid_embds=experts,\n            text_embds=text_embd,\n            text_weights=moe_weights,\n            subspaces=self.modalities,\n            raw_captions=raw_captions,\n        )\n        cross_view_conf_matrix = 0.5 * cross_view_conf_matrix + 0.5 * cross_view_conf_matrix_tv\n        return {\n            \"modalities\": self.modalities,"
        },
        {
            "comment": "This code defines several neural network layers: \"GatedEmbeddingUnit\", \"ReduceDim\", and \"ContextGating\". These layers are used for feature extraction, normalization, and context gating in the T2VLAD model. The GatedEmbeddingUnit layer combines a linear transformation and context gating to produce normalized output. The ReduceDim layer reduces the dimension of input features through a linear transformation followed by normalization. The ContextGating layer performs a linear transformation and optional batch normalization for context gating.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/model/model.py\":422-455",
            "content": "            \"cross_view_conf_matrix\": cross_view_conf_matrix,\n        }\nclass GatedEmbeddingUnit(nn.Layer):\n    def __init__(self, input_dimension, output_dimension, use_bn):\n        super(GatedEmbeddingUnit, self).__init__()\n        self.fc = nn.Linear(input_dimension, output_dimension)\n        self.cg = ContextGating(output_dimension, add_batch_norm=use_bn)\n    def forward(self, x):\n        x = self.fc(x)\n        x = self.cg(x)\n        x = F.normalize(x)\n        return x\nclass ReduceDim(nn.Layer):\n    def __init__(self, input_dimension, output_dimension):\n        super(ReduceDim, self).__init__()\n        self.fc = nn.Linear(input_dimension, output_dimension)\n    def forward(self, x):\n        x = self.fc(x)\n        x = F.normalize(x, axis=-1)\n        return x\nclass ContextGating(nn.Layer):\n    def __init__(self, dimension, add_batch_norm=True):\n        super(ContextGating, self).__init__()\n        self.fc = nn.Linear(dimension, dimension)\n        self.add_batch_norm = add_batch_norm\n        self.batch_norm = nn.BatchNorm1D(dimension)"
        },
        {
            "comment": "This code defines a function for computing the similarity matrix between two sets of embeddings, which are divided into smaller shards. The function takes these sharded embeddings and weights for each set, and returns a similarity matrix of size BK x BK. The code includes batch normalization and global pooling operations in its forward pass.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/model/model.py\":457-484",
            "content": "    def forward(self, x):\n        x1 = self.fc(x)\n        if self.add_batch_norm:\n            x1 = self.batch_norm(x1)\n        x = paddle.concat([x, x1], axis=1)\n        return F.glu(x, axis=1)\ndef sharded_cross_view_inner_product(vid_embds,\n                                     text_embds,\n                                     text_weights,\n                                     subspaces,\n                                     ind,\n                                     tol=1E-5,\n                                     raw_captions=None):\n    \"\"\"Compute a similarity matrix from sharded vectors.\n    Args:\n        embds1 (dict[str:paddle.Tensor]): the set of sub-embeddings that, when\n            concatenated, form the whole. The ith shard has shape `B x K x F_i`\n            (i.e. they can differ in the last dimension).\n        embds2 (dict[str:paddle.Tensor]): same format.\n        weights2 (paddle.Tensor): weights for the shards in `embds2`.\n    Returns:\n        (paddle.tensor): similarity matrix of size `BK x BK`.\n    NOTE: If multiple captions are provided, we can aggregate their similarities to"
        },
        {
            "comment": "This code calculates video-text similarity scores and handles the modalities of available experts. It initializes variables for storing similarity scores (sims) and text weights (text_weights). The code also calculates mean and standard deviation for text_weights, and stores these values as mus and stds respectively. Then it creates an availability mask for each modality, marking them either 0 or 1, with the assertion that the mask should only contain 0s or 1s.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/model/model.py\":485-506",
            "content": "    provide a single video-text similarity score.\n    \"\"\"\n    B = vid_embds[subspaces[0]].shape[0]\n    T, num_caps, _ = text_embds[subspaces[0]].shape\n    # unroll separate captions onto first dimension and treat them separately\n    sims = paddle.zeros([T * num_caps, B])\n    text_weights = text_weights.reshape([T * num_caps, -1])\n    if True:\n        mus = [round(x, 3) for x in text_weights.mean(0).numpy().tolist()]\n        stds = [round(x, 3) for x in text_weights.std(0).numpy().tolist()]\n        summary = \">>>\"\n        for mod, mu, std in zip(subspaces, mus, stds):\n            summary += f\"{mod}: {mu} +/- {std} \"\n    # mark expert availabilities along the second axis\n    available = paddle.ones([1, B, len(subspaces)], dtype=text_weights.dtype)\n    for ii, modality in enumerate(subspaces):\n        ind[modality] = paddle.to_tensor(ind[modality], dtype='float32')\n        available[:, :, ii] = ind[modality]\n    msg = \"expected `available` modality mask to only contain 0s or 1s\"\n    assert set(paddle.unique(available).cpu().numpy()).issubset(set([0,"
        },
        {
            "comment": "This code reshapes the text_weights and combines them with availabilities to produce a tensor of size T x B x num_experts. It then normalizes these weights by accounting for missing experts. Lastly, it calculates the L2-masses for both video and text embeddings and applies the weights to the corresponding embeddings. The code also includes an assertion message to ensure correct shape matching between text_embd_ and text_weights.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/model/model.py\":507-525",
            "content": "                                                                     1])), msg\n    # set the text weights along the first axis and combine with availabilities to\n    # produce a <T x B x num_experts> tensor\n    text_weight_tensor = text_weights.reshape([T * num_caps, 1,\n                                               len(subspaces)]) * available\n    # normalise to account for missing experts\n    normalising_weights = text_weight_tensor.sum(2).reshape(\n        [T * num_caps, B, 1])\n    text_weight_tensor = paddle.divide(text_weight_tensor, normalising_weights)\n    l2_mass_text, l2_mass_vid = 1, 1\n    for idx, modality in enumerate(subspaces):\n        vid_embd_ = vid_embds[modality].reshape([B, -1]) / l2_mass_vid\n        text_embd_ = text_embds[modality].reshape([T * num_caps, -1])\n        msg = \"expected weights to be applied to text embeddings\"\n        assert text_embd_.shape[0] == text_weights.shape[0], msg\n        text_embd_ = text_embd_ / l2_mass_text\n        weighting = text_weight_tensor[:, :, idx]"
        },
        {
            "comment": "This code calculates the similarity between text and video embeddings, multiplying them together with a weighting factor. It then checks for NaN values in the resulting similarity matrix and raises a ValueError if any are found.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/T2VLAD/model/model.py\":526-532",
            "content": "        sims += weighting * paddle.matmul(text_embd_,\n                                          vid_embd_.t())  # (T x num_caps) x (B)\n    if paddle.isnan(sims).sum().item():\n        raise ValueError(\"Found nans in similarity matrix!\")\n    return sims"
        }
    ]
}