{
    "summary": "This code defines Distillation Entropy Loss and KL divergence loss classes, implementing CrossEntropy loss for single/triple labels and KL divergence respectively, with optional weighted average and activation functions.",
    "details": [
        {
            "comment": "Defines a Distillation Entropy Loss class, which inherits from BaseWeightedLoss and takes score and labels as input for its forward function.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/losses/distillation_loss.py\":0-29",
            "content": "# Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport paddle\nimport paddle.nn.functional as F\nimport paddle.nn as nn\nfrom ..registry import LOSSES\nfrom .base import BaseWeightedLoss\n@LOSSES.register()\nclass DistillationCELoss(BaseWeightedLoss):\n    \"\"\"Distillation Entropy Loss.\"\"\"\n    def _forward(self, score, labels, **kwargs):\n        \"\"\"Forward function.\n        Args:\n            score (paddle.Tensor): The class score.\n            labels (paddle.Tensor): The ground truth labels."
        },
        {
            "comment": "The code defines a loss function that calculates CrossEntropy loss and supports both single and triple labels. For single label, it directly calculates the CrossEntropy loss. For triple labels, it first calculates two separate CrossEntropy losses, then combines them with a weighted average based on a given lambda value (lam). The DistillationDMLLoss class implements this behavior and also handles the act parameter for specifying different activation functions.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/losses/distillation_loss.py\":30-59",
            "content": "            kwargs: Any keyword argument to be used to calculate\n                CrossEntropy loss.\n        Returns:\n            loss (paddle.Tensor): The returned CrossEntropy loss.\n        \"\"\"\n        if len(labels) == 1:\n            label = labels[0]\n            loss = F.cross_entropy(score, label, **kwargs)\n        # Deal with VideoMix\n        elif len(labels) == 3:\n            label_a, label_b, lam = labels\n            loss_a = F.cross_entropy(score, label_a, **kwargs)\n            loss_b = F.cross_entropy(score, label_b, **kwargs)\n            loss = lam * loss_a + (1 - lam) * loss_b\n            loss = paddle.mean(loss)  #lam shape is bs\n        return loss\n@LOSSES.register()\nclass DistillationDMLLoss(BaseWeightedLoss):\n    \"\"\"\n    DistillationDMLLoss\n    \"\"\"\n    def __init__(self, act=\"softmax\", eps=1e-12, **kargs):\n        super().__init__(**kargs)\n        if act is not None:\n            assert act in [\"softmax\", \"sigmoid\"]\n        if act == \"softmax\":\n            self.act = nn.Softmax(axis=-1)\n        elif act == \"sigmoid\":"
        },
        {
            "comment": "This code defines a class for implementing the Kullback-Leibler (KL) divergence loss. The constructor takes an optional activation function and epsilon for numerical stability. The _kldiv method calculates the KL divergence between two vectors, while the _forward method applies the activation function if provided and computes the final loss by averaging the KL divergences in both directions.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/losses/distillation_loss.py\":60-78",
            "content": "            self.act = nn.Sigmoid()\n        else:\n            self.act = None\n        self.eps = eps\n    def _kldiv(self, x, target):\n        class_num = x.shape[-1]\n        cost = target * paddle.log(\n            (target + self.eps) / (x + self.eps)) * class_num\n        return cost\n    def _forward(self, x, target):\n        if self.act is not None:\n            x = self.act(x)\n            target = self.act(target)\n        loss = self._kldiv(x, target) + self._kldiv(target, x)\n        loss = loss / 2\n        loss = paddle.mean(loss)\n        return loss"
        }
    ]
}