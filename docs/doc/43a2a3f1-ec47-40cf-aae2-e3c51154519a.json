{
    "summary": "This code optimizes PaddleVideo model performance by configuring environment variables for efficient training or export tasks, evaluates models, saves trained models, logs, and runs evaluation scripts.",
    "details": [
        {
            "comment": "The code reads a file containing training parameters and parses the values using different functions. These parameters include the model name, Python version, GPU list, auto-cast settings, epoch number, batch size, pre-trained model, and training model name. The parsed values will be used for further processing in the script.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/test_tipc/test_train_inference_python.sh\":0-29",
            "content": "#!/bin/bash\nsource test_tipc/common_func.sh\nFILENAME=$1\n# MODE be one of ['lite_train_lite_infer' 'lite_train_whole_infer' 'whole_train_whole_infer', 'whole_infer']\nMODE=$2\ndataline=$(cat ${FILENAME})\n# parser params\nIFS=$'\\n'\nlines=(${dataline})\n# The training params\nmodel_name=$(func_parser_value \"${lines[1]}\")\npython=$(func_parser_value \"${lines[2]}\")\ngpu_list=$(func_parser_value \"${lines[3]}\")\ntrain_use_gpu_key=$(func_parser_key \"${lines[4]}\")\ntrain_use_gpu_value=$(func_parser_value \"${lines[4]}\")\nautocast_list=$(func_parser_value \"${lines[5]}\")\nautocast_key=$(func_parser_key \"${lines[5]}\")\nepoch_key=$(func_parser_key \"${lines[6]}\")\nepoch_num=$(func_parser_value \"${lines[6]}\")\nsave_model_key=$(func_parser_key \"${lines[7]}\")\ntrain_batch_key=$(func_parser_key \"${lines[8]}\")\ntrain_batch_value=$(func_parser_value \"${lines[8]}\")\npretrain_model_key=$(func_parser_key \"${lines[9]}\")\npretrain_model_value=$(func_parser_value \"${lines[9]}\")\ntrain_model_name=$(func_parser_value \"${lines[10]}\")\ntrain_param_key1=$(func_parser_key \"${lines[12]}\")"
        },
        {
            "comment": "This code is parsing key-value pairs from different lines and assigning them to specific variables. The variables are used for trainer, pact, fpgm, distill, amp, evaluator, save_infer, and export weight configurations. This information will likely be utilized in subsequent parts of the script or program.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/test_tipc/test_train_inference_python.sh\":30-55",
            "content": "train_param_value1=$(func_parser_value \"${lines[12]}\")\ntrain_param_key2=$(func_parser_key \"${lines[11]}\")\ntrain_param_value2=$(func_parser_value \"${lines[11]}\")\ntrainer_list=$(func_parser_value \"${lines[14]}\")\ntrainer_norm=$(func_parser_key \"${lines[15]}\")\nnorm_trainer=$(func_parser_value \"${lines[15]}\")\npact_key=$(func_parser_key \"${lines[16]}\")\npact_trainer=$(func_parser_value \"${lines[16]}\")\nfpgm_key=$(func_parser_key \"${lines[17]}\")\nfpgm_trainer=$(func_parser_value \"${lines[17]}\")\ndistill_key=$(func_parser_key \"${lines[18]}\")\ndistill_trainer=$(func_parser_value \"${lines[18]}\")\namp_key=$(func_parser_key \"${lines[19]}\")\namp_trainer=$(func_parser_value \"${lines[19]}\")\ntrainer_key2=$(func_parser_key \"${lines[20]}\")\ntrainer_value2=$(func_parser_value \"${lines[20]}\")\neval_py=$(func_parser_value \"${lines[23]}\")\neval_key1=$(func_parser_key \"${lines[24]}\")\neval_value1=$(func_parser_value \"${lines[24]}\")\nsave_infer_key=$(func_parser_key \"${lines[27]}\")\nsave_infer_value=$(func_parser_value \"${lines[27]}\")\nexport_weight=$(func_parser_key \"${lines[28]}\")"
        },
        {
            "comment": "The code parses various configuration values and keys from the lines of a script. It extracts normalization, quantization, and distillation settings; inference directory path; model directories for inference; whether to use GPU, MKLDNN, specify CPU threads, and batch size. The variables are assigned with these parsed values.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/test_tipc/test_train_inference_python.sh\":56-78",
            "content": "norm_export=$(func_parser_value \"${lines[29]}\")\npact_export=$(func_parser_value \"${lines[30]}\")\nfpgm_export=$(func_parser_value \"${lines[31]}\")\ndistill_export=$(func_parser_value \"${lines[32]}\")\nexport_key1=$(func_parser_key \"${lines[33]}\")\nexport_value1=$(func_parser_value \"${lines[33]}\")\nexport_key2=$(func_parser_key \"${lines[34]}\")\nexport_value2=$(func_parser_value \"${lines[34]}\")\ninference_dir=$(func_parser_value \"${lines[35]}\")\n# parser inference model\ninfer_model_dir_list=$(func_parser_value \"${lines[36]}\")\ninfer_export_list=$(func_parser_value \"${lines[37]}\")\ninfer_is_quant=$(func_parser_value \"${lines[38]}\")\n# parser inference\ninference_py=$(func_parser_value \"${lines[39]}\")\nuse_gpu_key=$(func_parser_key \"${lines[40]}\")\nuse_gpu_list=$(func_parser_value \"${lines[40]}\")\nuse_mkldnn_key=$(func_parser_key \"${lines[41]}\")\nuse_mkldnn_list=$(func_parser_value \"${lines[41]}\")\ncpu_threads_key=$(func_parser_key \"${lines[42]}\")\ncpu_threads_list=$(func_parser_value \"${lines[42]}\")\nbatch_size_key=$(func_parser_key \"${lines[43]}\")"
        },
        {
            "comment": "This code is parsing the function parameters from a configuration file. The batch size, use_trt, precision, infer model value, video directory path, log saving flag, and benchmark are being assigned to respective variables. A specific line number is obtained using grep command for a keyword \"to_static_train_benchmark_params\". Then it checks if the mode is set to \"klquant_whole_infer\" and processes the first and 17th lines of the configuration file.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/test_tipc/test_train_inference_python.sh\":79-103",
            "content": "batch_size_list=$(func_parser_value \"${lines[43]}\")\nuse_trt_key=$(func_parser_key \"${lines[44]}\")\nuse_trt_list=$(func_parser_value \"${lines[44]}\")\nprecision_key=$(func_parser_key \"${lines[45]}\")\nprecision_list=$(func_parser_value \"${lines[45]}\")\ninfer_model_key=$(func_parser_key \"${lines[46]}\")\ninfer_model_value=$(func_parser_value \"${lines[46]}\")\nvideo_dir_key=$(func_parser_key \"${lines[47]}\")\ninfer_video_dir=$(func_parser_value \"${lines[47]}\")\nsave_log_key=$(func_parser_key \"${lines[48]}\")\nbenchmark_key=$(func_parser_key \"${lines[49]}\")\nbenchmark_value=$(func_parser_value \"${lines[49]}\")\ninfer_key1=$(func_parser_key \"${lines[50]}\")\ninfer_value1=$(func_parser_value \"${lines[50]}\")\nline_num=`grep -n -w \"to_static_train_benchmark_params\" $FILENAME  | cut -d \":\" -f 1`\nto_static_key=$(func_parser_key \"${lines[line_num]}\")\nto_static_trainer=$(func_parser_value \"${lines[line_num]}\")\n# parser klquant_infer\nif [ ${MODE} = \"klquant_whole_infer\" ]; then\n    dataline=$(awk 'NR==1 NR==17{print}'  $FILENAME)\n    lines=(${dataline})"
        },
        {
            "comment": "The code is parsing the configuration file to extract specific values for different variables like model name, python version, inference model directory list, and more. These values are used later in the script to execute specific commands related to the test and train operations.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/test_tipc/test_train_inference_python.sh\":104-124",
            "content": "    model_name=$(func_parser_value \"${lines[1]}\")\n    python=$(func_parser_value \"${lines[2]}\")\n    # parser inference model\n    infer_model_dir_list=$(func_parser_value \"${lines[3]}\")\n    infer_export_list=$(func_parser_value \"${lines[4]}\")\n    infer_is_quant=$(func_parser_value \"${lines[5]}\")\n    # parser inference\n    inference_py=$(func_parser_value \"${lines[6]}\")\n    use_gpu_key=$(func_parser_key \"${lines[7]}\")\n    use_gpu_list=$(func_parser_value \"${lines[7]}\")\n    use_mkldnn_key=$(func_parser_key \"${lines[8]}\")\n    use_mkldnn_list=$(func_parser_value \"${lines[8]}\")\n    cpu_threads_key=$(func_parser_key \"${lines[9]}\")\n    cpu_threads_list=$(func_parser_value \"${lines[9]}\")\n    batch_size_key=$(func_parser_key \"${lines[10]}\")\n    batch_size_list=$(func_parser_value \"${lines[10]}\")\n    use_trt_key=$(func_parser_key \"${lines[11]}\")\n    use_trt_list=$(func_parser_value \"${lines[11]}\")\n    precision_key=$(func_parser_key \"${lines[12]}\")\n    precision_list=$(func_parser_value \"${lines[12]}\")\n    infer_model_key=$(func_parser_key \"${lines[13]}\")"
        },
        {
            "comment": "This code sets variables for video directory, log path, inference functions, and other parameters. It then loops through various conditions to perform inferences with different combinations of GPU and MKLDNN usage, while keeping track of the results in the specified log file.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/test_tipc/test_train_inference_python.sh\":125-156",
            "content": "    video_dir_key=$(func_parser_key \"${lines[14]}\")\n    infer_video_dir=$(func_parser_value \"${lines[14]}\")\n    save_log_key=$(func_parser_key \"${lines[15]}\")\n    benchmark_key=$(func_parser_key \"${lines[16]}\")\n    benchmark_value=$(func_parser_value \"${lines[16]}\")\n    infer_key1=$(func_parser_key \"${lines[17]}\")\n    infer_value1=$(func_parser_value \"${lines[17]}\")\nfi\nLOG_PATH=\"./test_tipc/output/${model_name}/${MODE}\"\nmkdir -p ${LOG_PATH}\nstatus_log=\"${LOG_PATH}/results_python.log\"\nfunction func_inference(){\n    IFS='|'\n    _python=$1\n    _script=$2\n    _model_dir=$3\n    _log_path=$4\n    _video_dir=$5\n    _flag_quant=$6\n    _gpu=$7\n    # inference\n    for use_gpu in ${use_gpu_list[*]}; do\n        if [ ${use_gpu} = \"False\" ] || [ ${use_gpu} = \"cpu\" ]; then\n            for use_mkldnn in ${use_mkldnn_list[*]}; do\n                if [[ ${use_mkldnn} = \"False\" ]] && [[ ${_flag_quant} = \"True\" ]]; then\n                    continue\n                fi\n                for threads in ${cpu_threads_list[*]}; do\n                    for batch_size in ${batch_size_list[*]}; do"
        },
        {
            "comment": "This code is iterating over a list of precision values, checking conditions to decide whether to continue or skip the current iteration. It sets the appropriate log path, creates directories if needed, and calls functions to set parameters for inference data and benchmarking.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/test_tipc/test_train_inference_python.sh\":157-169",
            "content": "                        for precision in ${precision_list[*]}; do\n                            if [[ ${use_mkldnn} = \"False\" ]] && [[ ${precision} = \"fp16\" ]]; then\n                                continue\n                            fi # skip when enable fp16 but disable mkldnn\n                            if [[ ${_flag_quant} = \"True\" ]] && [[ ${precision} != \"int8\" ]]; then\n                                continue\n                            fi # skip when quant model inference but precision is not int8\n                            set_precision=$(func_set_params \"${precision_key}\" \"${precision}\")\n                            _save_log_path=\"${_log_path}/python_infer_cpu_gpus_${_gpu}_usemkldnn_${use_mkldnn}_threads_${threads}_precision_${precision}_batchsize_${batch_size}.log\"\n                            mkdir -p ${_log_path}\n                            set_infer_data=$(func_set_params \"${video_dir_key}\" \"${infer_video_dir}\")\n                            set_benchmark=$(func_set_params \"${benchmark_key}\" \"${benchmark_value}\")"
        },
        {
            "comment": "The code is setting variables, constructing a command using environment variables, and executing it. It then checks the status of the execution and logs the output for later inspection. This appears to be part of a loop that's running multiple tests or experiments with varying parameters.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/test_tipc/test_train_inference_python.sh\":170-180",
            "content": "                            set_batchsize=$(func_set_params \"${batch_size_key}\" \"${batch_size}\")\n                            set_cpu_threads=$(func_set_params \"${cpu_threads_key}\" \"${threads}\")\n                            set_model_dir=$(func_set_params \"${infer_model_key}\" \"${_model_dir}/${infer_model_value}\")\n                            set_infer_params1=$(func_set_params \"${infer_key1}\" \"${_model_dir}/${infer_value1}\")\n                            command=\"${_python} ${_script} ${use_gpu_key}=${use_gpu} ${use_mkldnn_key}=${use_mkldnn} ${set_cpu_threads} ${set_model_dir} ${set_batchsize} ${set_infer_data} ${set_benchmark} ${set_precision} ${set_infer_params1} > ${_save_log_path} 2>&1 \"\n                            eval $command\n                            last_status=${PIPESTATUS[0]}\n                            eval \"cat ${_save_log_path}\"\n                            status_check $last_status \"${command}\" \"${status_log}\" \"${model_name}\" \"${_save_log_path}\"\n                        done\n                    done"
        },
        {
            "comment": "This code snippet is checking various conditions for model inference using different parameters like GPU usage, precision, and batch size. It iterates through a list of options to set up the necessary configurations for logging and execution. The purpose seems to be running inference tests with varying settings to optimize performance.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/test_tipc/test_train_inference_python.sh\":181-197",
            "content": "                done\n            done\n        elif [ ${use_gpu} = \"True\" ] || [ ${use_gpu} = \"gpu\" ]; then\n            for use_trt in ${use_trt_list[*]}; do\n                for precision in ${precision_list[*]}; do\n                    if [[ ${_flag_quant} = \"False\" ]] && [[ ${precision} =~ \"int8\" ]]; then\n                        continue\n                    fi\n                    if [[ ${precision} =~ \"fp16\" || ${precision} =~ \"int8\" ]] && [[ ${use_trt} = \"False\" ]]; then\n                        continue\n                    fi\n                    if [[ ${use_trt} = \"False\" || ${precision} =~ \"int8\" ]] && [[ ${_flag_quant} = \"True\" ]]; then\n                        continue\n                    fi\n                    for batch_size in ${batch_size_list[*]}; do\n                        _save_log_path=\"${_log_path}/python_infer_gpu_gpus_${_gpu}_usetrt_${use_trt}_precision_${precision}_batchsize_${batch_size}.log\"\n                        set_infer_data=$(func_set_params \"${video_dir_key}\" \"${infer_video_dir}\")"
        },
        {
            "comment": "This code sets parameters for benchmark, batch size, tensorrt usage, precision, model directory, and infer params1. It then executes a command with these parameters to run inference, saves the log, and checks the status of the execution.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/test_tipc/test_train_inference_python.sh\":199-211",
            "content": "                        set_benchmark=$(func_set_params \"${benchmark_key}\" \"${benchmark_value}\")\n                        set_batchsize=$(func_set_params \"${batch_size_key}\" \"${batch_size}\")\n                        set_tensorrt=$(func_set_params \"${use_trt_key}\" \"${use_trt}\")\n                        set_precision=$(func_set_params \"${precision_key}\" \"${precision}\")\n                        set_model_dir=$(func_set_params \"${infer_model_key}\" \"${_model_dir}/${infer_model_value}\")\n                        set_infer_params1=$(func_set_params \"${infer_key1}\" \"${_model_dir}/${infer_value1}\")\n                        command=\"${_python} ${_script} ${use_gpu_key}=${use_gpu} ${set_tensorrt} ${set_precision} ${set_model_dir} ${set_batchsize} ${set_infer_data} ${set_benchmark} ${set_infer_params1} > ${_save_log_path} 2>&1 \"\n                        eval $command\n                        last_status=${PIPESTATUS[0]}\n                        eval \"cat ${_save_log_path}\"\n                        status_check $last_status \"${command}\" \"${status_log}\" \"${model_name}\" \"${_save_log_path}\""
        },
        {
            "comment": "This code is part of a script that tests and runs inference models using PaddleVideo. It checks the hardware being used (CPU or GPU) and sets appropriate environment variables accordingly. It then iterates through each inference model, running them with specific exported weights and saving the output logs for further analysis.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/test_tipc/test_train_inference_python.sh\":213-242",
            "content": "                    done\n                done\n            done\n        else\n            echo \"Does not support hardware other than CPU and GPU Currently!\"\n        fi\n    done\n}\nif [ ${MODE} = \"whole_infer\" ] || [ ${MODE} = \"klquant_whole_infer\" ]; then\n    GPUID=$3\n    if [ ${#GPUID} -le 0 ];then\n        env=\" \"\n    else\n        env=\"export CUDA_VISIBLE_DEVICES=${GPUID}\"\n    fi\n    set CUDA_VISIBLE_DEVICES\n    eval $env\n    export Count=0\n    IFS=\"|\"\n    infer_run_exports=(${infer_export_list})\n    infer_quant_flag=(${infer_is_quant})\n    for infer_model in ${infer_model_dir_list[*]}; do\n        # run export\n        if [ ${infer_run_exports[Count]} != \"null\" ];then\n            save_infer_dir=$(dirname $infer_model)\n            set_export_weight=$(func_set_params \"${export_weight}\" \"${infer_model}\")\n            set_save_infer_key=$(func_set_params \"${save_infer_key}\" \"${save_infer_dir}\")\n            export_log_path=\"${LOG_PATH}_export_${Count}.log\"\n            export_cmd=\"${python} ${infer_run_exports[Count]} ${set_export_weight} ${set_save_infer_key} > ${export_log_path} 2>&1 \""
        },
        {
            "comment": "This code is iterating through a list of GPUs, setting the visible CUDA devices accordingly and running inference for each GPU. It also checks if exporting is needed, saves the infer directory, runs quantized inference if it's a klquant_infer mode, and keeps track of the count to ensure all GPUs are considered. If a GPU list item is -1, it uses no GPU, otherwise, it sets the environment for that specific GPU. The code block also checks whether the GPU count is less than or equal to 15 to avoid potential issues with larger lists.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/test_tipc/test_train_inference_python.sh\":243-273",
            "content": "            echo ${infer_run_exports[Count]}\n            eval $export_cmd\n            echo $export_cmd\n            status_export=$?\n            status_check $status_export \"${export_cmd}\" \"${status_log}\" \"${model_name}\" \"${export_log_path}\"\n        else\n            save_infer_dir=${infer_model}\n        fi\n        #run inference\n        is_quant=${infer_quant_flag[Count]}\n        if [ ${MODE} = \"klquant_infer\" ]; then\n            is_quant=\"True\"\n        fi\n        func_inference \"${python}\" \"${inference_py}\" \"${save_infer_dir}\" \"${LOG_PATH}\" \"${infer_video_dir}\" ${is_quant} \"${gpu}\"\n        Count=$(($Count + 1))\n    done\nelse\n    IFS=\"|\"\n    export Count=0\n    USE_GPU_KEY=(${train_use_gpu_value})\n    for gpu in ${gpu_list[*]}; do\n        train_use_gpu=${USE_GPU_KEY[Count]}\n        Count=$(($Count + 1))\n        ips=\"\"\n        if [ ${gpu} = \"-1\" ];then\n            env=\"\"\n        elif [ ${#gpu} -le 1 ];then\n            env=\"export CUDA_VISIBLE_DEVICES=${gpu}\"\n            eval ${env}\n        elif [ ${#gpu} -le 15 ];then"
        },
        {
            "comment": "This code is setting up environment variables for parallel GPU usage, iterating through different autocast and trainer configurations to execute training or export tasks based on the provided key values. The flag_quant variable tracks whether quantization is required for a particular configuration.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/test_tipc/test_train_inference_python.sh\":274-302",
            "content": "            IFS=\",\"\n            array=(${gpu})\n            env=\"export CUDA_VISIBLE_DEVICES=${array[0]}\"\n            IFS=\"|\"\n        else\n            IFS=\";\"\n            array=(${gpu})\n            ips=${array[0]}\n            gpu=${array[1]}\n            IFS=\"|\"\n            env=\" \"\n        fi\n        for autocast in ${autocast_list[*]}; do\n            if [ ${autocast} = \"fp16\" ]; then\n                set_amp_config=\"--amp --amp_level 'O2'\"\n            else\n                set_amp_config=\" \"\n            fi\n            for trainer in ${trainer_list[*]}; do\n                flag_quant=False\n                if [ ${trainer} = ${pact_key} ]; then\n                    run_train=${pact_trainer}\n                    run_export=${pact_export}\n                    flag_quant=True\n                elif [ ${trainer} = \"${fpgm_key}\" ]; then\n                    run_train=${fpgm_trainer}\n                    run_export=${fpgm_export}\n                elif [ ${trainer} = \"${distill_key}\" ]; then\n                    run_train=${distill_trainer}"
        },
        {
            "comment": "This code uses conditional statements to assign values to `run_train` and `run_export` variables based on the value of `trainer`. It handles multiple scenarios, including cases with specific keys, and triggers \"to_static\" logic in 'train.py' when needed. If `run_train` is assigned as null, it continues without executing further.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/test_tipc/test_train_inference_python.sh\":303-324",
            "content": "                    run_export=${distill_export}\n                elif [ ${trainer} = ${amp_key} ]; then\n                    run_train=${amp_trainer}\n                    run_export=${norm_export}\n                elif [[ ${trainer} = ${trainer_key2} ]]; then\n                    run_train=${trainer_value2}\n                    run_export=${export_value2}\n                # In case of @to_static, we re-used norm_traier,\n                # but append \"-o to_static=True\" for config\n                # to trigger \"to_static\" logic in 'train.py'\n                elif [ ${trainer} = \"${to_static_key}\" ]; then\n                    run_train=\"${norm_trainer}  ${to_static_trainer}\"\n                    run_export=${norm_export}\n                else\n                    run_train=${norm_trainer}\n                    run_export=${norm_export}\n                fi\n                if [ ${run_train} = \"null\" ]; then\n                    continue\n                fi\n                if [[ ${MODE} != \"benchmark_train\" ]] && [[ ! ${MODE} =~ \"whole_train\" ]]; then"
        },
        {
            "comment": "This code is setting up parameters for model training and inference. It appends --max_iters=30 and --log_interval=1 to the run_train string for better data output, sets autocast, epoch, pretrain values, and checks if MODE includes \"whole_train\" to set certain variables to empty strings or nulls. The code also uses func_set_params to set batch size and train parameters.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/test_tipc/test_train_inference_python.sh\":325-346",
            "content": "                    # \u8bad\u7ec3\u53c2\u6570\u672b\u5c3e\u52a0\u4e0a--max_iters=30\u548c--log_interval=1\uff0c\u4ee5\u4fbf\u8fd0\u884c\u5e76\u8f93\u51fa\u8db3\u91cf\u6570\u636e\n                    run_train=${run_train}\" --max_iters=30\"\n                fi\n                set_autocast=$(func_set_params \"${autocast_key}\" \"${autocast}\")\n                set_epoch=$(func_set_params \"${epoch_key}\" \"${epoch_num}\")\n                if [[ $MODE =~ \"whole_train\" ]]; then\n                    set_epoch=\"\"\n                fi\n                set_pretrain=$(func_set_params \"${pretrain_model_key}\" \"${pretrain_model_value}\")\n                if [[ $MODE =~ \"whole_train\" ]]; then\n                    train_batch_key=\"\"\n                    train_batch_value=\"\"\n                fi\n                set_batchsize=$(func_set_params \"${train_batch_key}\" \"${train_batch_value}\")\n                if [[ $MODE =~ \"whole_train\" ]]; then\n                    train_param_key1=\"\"\n                    train_param_value1=\"\"\n                fi\n                set_train_params1=$(func_set_params \"${train_param_key1}\" \"${train_param_value1}\")\n                if [[ $MODE =~ \"whole_train\" ]]; then"
        },
        {
            "comment": "This code sets up parameters for training and inference of a PaddleVideo model. It determines whether the training is on a single machine or multiple machines based on the number of IPs provided. Depending on this, it sets the number of nodes, logs information accordingly, and loads pre-training from normal training if the current trainer is PACT or FPGM.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/test_tipc/test_train_inference_python.sh\":347-366",
            "content": "                    train_param_key2=\"\"\n                    train_param_value2=\"\"\n                fi\n                set_train_params2=$(func_set_params \"${train_param_key2}\" \"${train_param_value2}\")\n                set_use_gpu=$(func_set_params \"${train_use_gpu_key}\" \"${train_use_gpu}\")\n                if [ ${#ips} -le 15 ];then\n                    # len(ips)<=15, single machine\n                    nodes=1\n                    save_log=\"${LOG_PATH}/${trainer}_gpus_${gpu}_autocast_${autocast}_nodes_${nodes}\"\n                else\n                    # if length of ips > 15, then it is seen as multi-machine\n                    # 15 is the min length of ips info for multi-machine: 0.0.0.0,0.0.0.0\n                    IFS=\",\"\n                    ips_array=(${ips})\n                    IFS=\"|\"\n                    nodes=${#ips_array[@]}\n                    save_log=\"${LOG_PATH}/${trainer}_gpus_${gpu}_autocast_${autocast}_nodes_${nodes}\"\n                fi\n                # load pretrain from norm training if current trainer is pact or fpgm trainer"
        },
        {
            "comment": "This code checks if the trainer is either 'pact_key' or 'fpgm_key', and if the number of nodes is less than or equal to 1. If true, it sets the 'set_pretrain' variable to 'load_norm_train_model'. The code then determines the appropriate command based on whether the number of GPUs is 2 or less (train with CPU or single GPU), up to 15 (train with multi-GPU), or more than 15 (train with multiple machines). The command uses PaddlePaddle's distributed training capabilities.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/test_tipc/test_train_inference_python.sh\":367-377",
            "content": "                if ([ ${trainer} = ${pact_key} ] || [ ${trainer} = ${fpgm_key} ]) && [ ${nodes} -le 1 ]; then\n                    set_pretrain=\"${load_norm_train_model}\"\n                fi\n                set_save_model=$(func_set_params \"${save_model_key}\" \"${save_log}\")\n                if [ ${#gpu} -le 2 ];then  # train with cpu or single gpu\n                    cmd=\"${python} ${run_train} ${set_amp_config} ${set_use_gpu}  ${set_save_model} ${set_epoch} ${set_pretrain} ${set_batchsize} ${set_train_params1} ${set_train_params2} > ${LOG_PATH}/train.log 2>&1\"\n                elif [ ${#ips} -le 15 ];then  # train with multi-gpu\n                    cmd=\"${python} -B -m paddle.distributed.launch --devices=\\\"${gpu}\\\" ${run_train} ${set_amp_config} ${set_use_gpu} ${set_save_model} ${set_epoch} ${set_pretrain} ${set_batchsize} ${set_train_params1} ${set_train_params2} > ${LOG_PATH}/train.log 2>&1\"\n                else     # train with multi-machine\n                    cmd=\"${python} -B -m paddle.distr"
        },
        {
            "comment": "This code snippet is running a training script for a PaddleVideo model. It sets parameters, evaluates pre-trained models, and saves the trained models. The script also displays logs for benchmarking and checks the status of the operation. If there's a single node and trainer, it loads a norm-train model for further usage. Finally, it runs an evaluation script if specified.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/test_tipc/test_train_inference_python.sh\":377-394",
            "content": "ibuted.launch --ips=${ips} --devices=\\\"${gpu}\\\" ${run_train} ${set_amp_config} ${set_use_gpu} ${set_save_model} ${set_pretrain} ${set_epoch} ${set_batchsize} ${set_train_params1} ${set_train_params2} > ${LOG_PATH}/train.log 2>&1\"\n                fi\n                # run train\n                eval $cmd\n                # display log for benchmark train\n                eval \"cat ${LOG_PATH}/train.log\"\n                eval \"cat ${LOG_PATH}/train.log >> ${save_log}.log\"\n                status_check $? \"${cmd}\" \"${status_log}\" \"${model_name}\" \"${save_log}.log\"\n                # set_eval_pretrain=$(func_set_params \"${pretrain_model_key}\" \"${save_log}/${train_model_name}\")\n                # save norm trained models to set pretrain for pact training and fpgm training\n                if [ [${trainer} = ${trainer_norm}] ] && [ [${nodes} -le 1] ]; then\n                    load_norm_train_model=${set_eval_pretrain}\n                fi\n                # run eval\n                if [ ${eval_py} != \"null\" ]; then\n                    real_model_name=${model_name/PP-/pp}"
        },
        {
            "comment": "The code sets the evaluation parameters and prepares a command to evaluate a model using specified inputs. If the MODE includes \"lite_infer\" and train_param_key1 is not null, it appends additional parameters to the command. Finally, it runs the command and checks the status of the evaluation.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/test_tipc/test_train_inference_python.sh\":395-409",
            "content": "                    set_eval_params1=$(func_set_params \"${eval_key1}\" \"${save_log}/${real_model_name}_epoch_00001.pdparams\")\n                    eval_log_path=\"${LOG_PATH}/${trainer}_gpus_${gpu}_autocast_${autocast}_nodes_${nodes}_eval.log\"\n                    if [[ $MODE =~ \"lite_infer\" ]] && [[ ${train_param_key1} != \"null\" ]]; then\n                        eval_cmd=\"${python} ${eval_py} ${set_use_gpu} ${set_eval_params1} ${train_param_key1}=${train_param_value1} > ${eval_log_path} 2>&1 \"\n                    else\n                        eval_cmd=\"${python} ${eval_py} ${set_use_gpu} ${set_eval_params1} > ${eval_log_path} 2>&1 \"\n                    fi\n                    eval $eval_cmd\n                    status_check $? \"${eval_cmd}\" \"${status_log}\" \"${model_name}\" \"${eval_log_path}\"\n                fi\n                # run export model\n                if [ ${run_export} != \"null\" ]; then\n                    save_infer_path=\"${save_log}\"\n                    real_model_name=${model_name/PP-/pp}\n     "
        },
        {
            "comment": "This code is setting up variables for exporting weights, saving inference key, and defining the export command. It then executes the export command and checks its status before running inference. If an inference directory is provided, it sets the inference model directory accordingly. Finally, it calls a function for inference processing.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/test_tipc/test_train_inference_python.sh\":409-425",
            "content": "               set_export_weight=$(func_set_params \"${export_weight}\" \"${save_log}/${real_model_name}_epoch_00001.pdparams\")\n                    set_save_infer_key=$(func_set_params \"${save_infer_key}\" \"${save_log}\")\n                    export_log_path=\"${LOG_PATH}/${trainer}_gpus_${gpu}_autocast_${autocast}_nodes_${nodes}_export.log\"\n                    export_cmd=\"${python} ${run_export} ${set_export_weight} ${set_save_infer_key} > ${export_log_path} 2>&1 \"\n                    eval $export_cmd\n                    status_check $? \"${export_cmd}\" \"${status_log}\" \"${model_name}\" \"${export_log_path}\"\n                    #run inference\n                    eval $env\n                    save_infer_path=\"${save_log}\"\n                    if [ ${inference_dir} != \"null\" ] && [ ${inference_dir} != '##' ]; then\n                        infer_model_dir=${save_infer_path}\n                    else\n                        infer_model_dir=${save_infer_path}\n                    fi\n                    func_inference "
        },
        {
            "comment": "This code snippet is a bash script that iterates through trainers, autocast options, and GPUs. It sets CUDA_VISIBLE_DEVICES to empty if the current mode is inference. The purpose is likely to set up an environment for training or inference based on different configurations.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/test_tipc/test_train_inference_python.sh\":425-432",
            "content": "\"${python}\" \"${inference_py}\" \"${infer_model_dir}\" \"${LOG_PATH}\" \"${infer_video_dir}\" \"${flag_quant}\" \"${gpu}\"\n                    eval \"unset CUDA_VISIBLE_DEVICES\"\n                fi\n            done  # done with:    for trainer in ${trainer_list[*]}; do\n        done      # done with:    for autocast in ${autocast_list[*]}; do\n    done          # done with:    for gpu in ${gpu_list[*]}; do\nfi  # end if [ ${MODE} = \"infer\" ]; then"
        }
    ]
}