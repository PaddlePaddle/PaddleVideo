{
    "summary": "This code defines an LSTM Attention Model class with parameters and a forward method for computation, applying LSTM layers in both directions and performing dynamic LSTM on input tensor. It uses dropout, FC layer, sequence_softmax, scaling, and sum pooling to obtain the final output.",
    "details": [
        {
            "comment": "This code is for a class called LSTMAttentionModel, which represents an LSTM Attention Model. It has three parameters: bias_attr, embedding_size (default 512), lstm_size (default 1024), and drop_rate (default 0.5). The class has an __init__ method to initialize these parameters and a forward method for performing the model's computation.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/VideoTag/models/attention_lstm/lstm_attention.py\":0-30",
            "content": "#  Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n#Licensed under the Apache License, Version 2.0 (the \"License\");\n#you may not use this file except in compliance with the License.\n#You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n#Unless required by applicable law or agreed to in writing, software\n#distributed under the License is distributed on an \"AS IS\" BASIS,\n#WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#See the License for the specific language governing permissions and\n#limitations under the License.\nimport numpy as np\nimport paddle\nimport paddle.static as static\nclass LSTMAttentionModel(object):\n    \"\"\"LSTM Attention Model\"\"\"\n    def __init__(self,\n                 bias_attr,\n                 embedding_size=512,\n                 lstm_size=1024,\n                 drop_rate=0.5):\n        self.lstm_size = lstm_size\n        self.embedding_size = embedding_size\n        self.drop_rate = drop_rate\n    def forward(self, input, is_training):"
        },
        {
            "comment": "This code initializes an LSTM layer for video tagging. It applies two fully connected layers (fc) to the input, one for forward and one for backward direction. The forward LSTM layer is created using dynamic_lstm function with size 4 times the lstm_size attribute and no reverse operation. The backward LSTM layer is also created similarly.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/VideoTag/models/attention_lstm/lstm_attention.py\":31-57",
            "content": "        input_fc = static.nn.fc(\n            x=input,\n            size=self.embedding_size,\n            activation='tanh',\n            bias_attr=paddle.ParamAttr(\n                regularizer=paddle.regularizer.L2Decay(coeff=0.0),\n                initializer=paddle.nn.initializer.Normal(std=0.0)),\n            name='rgb_fc')\n        lstm_forward_fc = static.nn.fc(\n            x=input_fc,\n            size=self.lstm_size * 4,\n            activation=None,\n            bias_attr=False,  # video_tag\n            name='rgb_fc_forward')\n        lstm_forward, _ = paddle.fluid.layers.dynamic_lstm(input=lstm_forward_fc,\n                                                    size=self.lstm_size * 4,\n                                                    is_reverse=False,\n                                                    name='rgb_lstm_forward')\n        lsmt_backward_fc = static.nn.fc(\n            x=input_fc,\n            size=self.lstm_size * 4,\n            activation=None,\n            bias_attr=False,  #video_tag\n            name='rgb_fc_backward')"
        },
        {
            "comment": "This code performs dynamic LSTM on input tensor with forward and backward directions, concatenates the results, applies dropout, then feeds the result into an FC layer for weight assignment using sequence_softmax. The final output is obtained by scaling the previous result with the weights and applying a sum pooling.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/VideoTag/models/attention_lstm/lstm_attention.py\":59-82",
            "content": "        lstm_backward, _ = paddle.fluid.layers.dynamic_lstm(input=lsmt_backward_fc,\n                                                     size=self.lstm_size * 4,\n                                                     is_reverse=True,\n                                                     name='rgb_lstm_backward')\n        lstm_concat = paddle.concat(x=[lstm_forward, lstm_backward],\n                                          axis=1)\n        lstm_dropout = paddle.nn.functional.dropout2d(x=lstm_concat,\n                                            p=self.drop_rate,\n                                            training=is_training)\n        lstm_weight = static.nn.fc(\n            x=lstm_dropout,\n            size=1,\n            activation='sequence_softmax',\n            bias_attr=False,  #video_tag\n            name='rgb_weight')\n        scaled = paddle.multiply(x=lstm_dropout,\n                                              y=lstm_weight)\n        lstm_pool = paddle.static.nn.sequence_pool(input=scaled, pool_type='sum')\n        return lstm_pool"
        }
    ]
}