{
    "summary": "YOWO is a single-stage feature extraction network with channel fusion and attention. Pre-trained on UCF101-24, it provides model structure and weight files for prediction with high confidence.",
    "details": [
        {
            "comment": "YOWO is a single-stage network with 2 branches for spatial and spatio-temporal feature extraction. It uses channel fusion and attention mechanism to aggregate features, then performs frame-level detection. UCF101-24 data preparation instructions provided. Pre-trained models like resnext-101-kinetics are needed for training.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/localization/yowo.md\":0-35",
            "content": "[\u7b80\u4f53\u4e2d\u6587](../../../zh-CN/model_zoo/localization/yowo.md) | English\n# YOWO\n## Content\n- [Introduction](#Introduction)\n- [Data](#DATA)\n- [Train](#Train)\n- [Test](#Test)\n- [Inference](#Inference)\n- [Reference](#Reference)\n## Introduction\nYOWO is a single-stage network with two branches. One branch extracts spatial features of key frames (i.e., the current frame) via 2D-CNN, while the other branch acquires spatio-temporal features of clips consisting of previous frames via 3D-CNN. To accurately aggregate these features, YOWO uses a channel fusion and attention mechanism that maximizes the inter-channel dependencies. Finally, the fused features are subjected to frame-level detection.\n<div align=\"center\">\n<img src=\"../../../images/yowo.jpg\">\n</div>\n## Data\nUCF101-24 data download and preparation please refer to [UCF101-24 data preparation](../../dataset/ucf24.md)\n## Train\n### UCF101-24 data set training\n#### Download and add pre-trained models\n1. Download the pre-training model [resnext-101-kinetics](https://vide"
        },
        {
            "comment": "This code provides instructions for downloading and configuring pre-trained models (`darknet.pdparam` and `resnext101_kinetics.pdparams`) for the YOWOLocalizer model in PaddleVideo. The models need to be added under `pretrained_2d:` and `pretrained_3d:` respectively in the `yowo.yaml` file. After that, use the command `python3 main.py -c configs/localization/yowo.yaml --validate --seed=1` to start training on the UCF101-24 dataset using 1 card.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/localization/yowo.md\":35-59",
            "content": "otag.bj.bcebos.com/PaddleVideo-release2.3/resnext101_kinetics.pdparams) \u548c [darknet](https://videotag.bj.bcebos.com/PaddleVideo-release2.3/darknet.pdparam) as Backbone initialization parameters, or download through the wget command\n   ```bash\n    wget -nc https://videotag.bj.bcebos.com/PaddleVideo-release2.3/darknet.pdparam\n    wget -nc https://videotag.bj.bcebos.com/PaddleVideo-release2.3/resnext101_kinetics.pdparams\n   ```\n2. Open `PaddleVideo/configs/localization/yowo.yaml`, and fill in the downloaded weight storage path below `pretrained_2d:` and `pretrained_3d:` respectively\n    ```yaml\n    MODEL:\n        framework: \"YOWOLocalizer\"\n        backbone:\n            name: \"YOWO\"\n            num_class: 24\n            pretrained_2d: fill in the path of 2D pre-training model here\n            pretrained_3d: fill in the path of 3D pre-training model here\n    ```\n#### Start training\n- The UCF101-24 data set uses 1 card for training, and the start command of the training method is as follows:\n    ```bash\n    python3 main.py -c configs/localization/yowo.yaml --validate --seed=1"
        },
        {
            "comment": "Enables AMP mixed-precision for faster training, using the command 'python3 main.py --amp -c configs/localization/yowo.yaml --validate --seed=1'. Customize parameters to train or test on different datasets, following the naming format 'model_dataset name_file format_data format_sampling method.yaml'. During training, find 'best' in logs to obtain model test accuracy using Frame-mAP (@ IoU 0.5), which differs from verification fscore used during training.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/localization/yowo.md\":60-79",
            "content": "    ```\n- Turn on amp mixed-precision training to speed up the training process. The training start command is as follows:\n    ```bash\n    python3 main.py --amp -c configs/localization/yowo.yaml --validate --seed=1\n    ```\n- In addition, you can customize and modify the parameter configuration to achieve the purpose of training/testing on different data sets. It is recommended that the naming method of the configuration file is `model_dataset name_file format_data format_sampling method.yaml` , Please refer to [config](../../tutorials/config.md) for parameter usage.\n## Test\n- The YOWO model is verified synchronously during training. You can find the keyword `best` in the training log to obtain the model test accuracy. The log example is as follows:\n  ```\n  Already save the best model (fsocre)0.8779\n  ```\n- Since the verification index of the YOWO model test mode is **Frame-mAP (@ IoU 0.5)**, which is different from the **fscore** used in the verification mode during the training process, so the v"
        },
        {
            "comment": "The code snippet shows how to evaluate the YOWO model's performance using a test mode and provides information about the input size, frame-mAP with IoU 0.5, and the checkpoint used for testing on UCF101-24 dataset. Additionally, it demonstrates how to export the inference model for future use.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/localization/yowo.md\":79-100",
            "content": "erification index recorded in the training log, called `fscore `, does not represent the final test score, so after the training is completed, you can use the test mode to test the best model to obtain the final index, the command is as follows:\n  ```bash\n  python3 main.py -c configs/localization/yowo.yaml --test --seed=1 -w 'output/YOWO/YOWO_epoch_00005.pdparams'\n  ```\n  When the test configuration uses the following parameters, the test indicators on the validation data set of UCF101-24 are as follows:\n  | Model    | 3D-CNN backbone | 2D-CNN backbone | Dataset  |Input    | Frame-mAP <br>(@ IoU 0.5)    |   checkpoints  |\n  | :-----------: | :-----------: | :-----------: | :-----------: | :-----------: | :-----------: | :-----------: |\n  | YOWO | 3D-ResNext-101 | Darknet-19 | UCF101-24 | 16-frames, d=1 | 80.94 | [YOWO.pdparams](https://videotag.bj.bcebos.com/PaddleVideo-release2.3/YOWO_epoch_00005.pdparams) |\n## Inference\n### Export inference model\n```bash\npython3 tools/export_model.py -c configs/localization/yowo.yaml -p 'output/YOWO/YOWO_epoch_00005.pdparams'"
        },
        {
            "comment": "This code explains how to generate the YOWO model structure file and weight file for prediction. It also provides instructions on how to use the prediction engine for inference using a test video, downloading it if necessary, and saving the results as an image sequence that can be converted into a gif for visualization.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/localization/yowo.md\":101-121",
            "content": "```\nThe above command will generate the model structure file `YOWO.pdmodel` and the model weight file `YOWO.pdiparams` required for prediction.\n- For the meaning of each parameter, please refer to [Model Reasoning Method](../../usage.md#2-infer)\n### Use prediction engine inference\n- Download the test video [HorseRiding.avi](https://videotag.bj.bcebos.com/Data/HorseRiding.avi) for a quick experience, or via the wget command. The downloaded video should be placed in the `data/ucf24` directory:\n```bash\nwget -nc https://videotag.bj.bcebos.com/Data/HorseRiding.avi\n```\n- Run the following command for inference:\n```bash\npython3 tools/predict.py -c configs/localization/yowo.yaml -i 'data/ucf24/HorseRiding.avi' --model_file ./inference/YOWO.pdmodel --params_file ./inference/YOWO.pdiparams\n```\n- When inference is over, the prediction results in image form will be saved in the `inference/YOWO_infer` directory. The image sequence can be converted to a gif by running the following command to complete the final visualisation."
        },
        {
            "comment": "This code is running a visualization script for the YOWO model trained on UCF101-24. It predicts the category of frames in \"data/ucf24/HorseRiding.avi\" as HorseRiding with high confidence (about 0.80).",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/localization/yowo.md\":123-137",
            "content": "```\npython3 data/ucf24/visualization.py --frames_dir ./inference/YOWO_infer/HorseRiding --duration 0.04\n```\nThe resulting visualization is as follows:\n<div align=\"center\">\n  <img  src=\"../../../images/horse_riding.gif\" alt=\"Horse Riding\">\n</div>\nIt can be seen that using the YOWO model trained on UCF101-24 to predict `data/ucf24/HorseRiding.avi`, the category of each frame output is HorseRiding with a confidence level of about 0.80.\n## Reference\n- [You Only Watch Once: A Unified CNN Architecture for Real-Time Spatiotemporal Action Localization](https://arxiv.org/pdf/1911.06644.pdf), K\u00f6p\u00fckl\u00fc O, Wei X, Rigoll G."
        }
    ]
}