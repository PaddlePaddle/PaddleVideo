{
    "summary": "The code introduces CustomWarmupCosineDecay and CustomWarmupPiecewiseDecay schedulers for PaddleVideo, combining warm-up, cosine decay, and piecewise decay. The CustomWarmupAdjustDecay scheduler combines warmup and cosine decay and adjusts based on epoch number.",
    "details": [
        {
            "comment": "This code defines a custom learning rate scheduler, CustomWarmupCosineDecay, which combines warmup and stepwise-cosine decay for use in PaddleVideo. It is part of the PaddlePaddle framework and can be used to adjust learning rates during training.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/solver/custom_lr.py\":0-30",
            "content": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport math\nfrom paddle.optimizer.lr import *\nimport numpy as np\n\"\"\"\nPaddleVideo Learning Rate Schedule:\nYou can use paddle.optimizer.lr\nor define your custom_lr in this file.\n\"\"\"\nclass CustomWarmupCosineDecay(LRScheduler):\n    r\"\"\"\n    We combine warmup and stepwise-cosine which is used in slowfast model.\n    Args:\n        warmup_start_lr (float): start learning rate used in warmup stage.\n        warmup_epochs (int): the number epochs of warmup."
        },
        {
            "comment": "This code defines a class `CosineAnnealingDecay` that schedules the learning rate for training. It takes parameters like warmup start lr, warmup epochs, cosine base lr, max epoch, num_iters, last_epoch (optional), and verbose (optional). The class initializes these parameters and provides a `step()` method to update the learning rate based on cosine annealing schedule. If verbose is set to True, it will print messages for each update.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/solver/custom_lr.py\":31-53",
            "content": "        cosine_base_lr (float|int, optional): base learning rate in cosine schedule.\n        max_epoch (int): total training epochs.\n        num_iters(int): number iterations of each epoch.\n        last_epoch (int, optional):  The index of last epoch. Can be set to restart training. Default: -1, means initial learning rate.\n        verbose (bool, optional): If ``True``, prints a message to stdout for each update. Default: ``False`` .\n    Returns:\n        ``CosineAnnealingDecay`` instance to schedule learning rate.\n    \"\"\"\n    def __init__(self,\n                 warmup_start_lr,\n                 warmup_epochs,\n                 cosine_base_lr,\n                 max_epoch,\n                 num_iters,\n                 last_epoch=-1,\n                 verbose=False):\n        self.warmup_start_lr = warmup_start_lr\n        self.warmup_epochs = warmup_epochs\n        self.cosine_base_lr = cosine_base_lr\n        self.max_epoch = max_epoch\n        self.num_iters = num_iters\n        #call step() in base class, last_lr/last_epoch/base_lr will be update"
        },
        {
            "comment": "This code defines a custom learning rate scheduler for PaddleVideo, implementing the CustomWarmupCosineDecay class. The step() method updates the learning rate based on current epoch and calls get_lr() to set the new learning rate. The _lr_func_cosine() function calculates the learning rate using a cosine annealing schedule.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/solver/custom_lr.py\":54-79",
            "content": "        super(CustomWarmupCosineDecay, self).__init__(last_epoch=last_epoch,\n                                                      verbose=verbose)\n    def step(self, epoch=None):\n        \"\"\"\n        ``step`` should be called after ``optimizer.step`` . It will update the learning rate in optimizer according to current ``epoch`` .\n        The new learning rate will take effect on next ``optimizer.step`` .\n        Args:\n            epoch (int, None): specify current epoch. Default: None. Auto-increment from last_epoch=-1.\n        Returns:\n            None\n        \"\"\"\n        if epoch is None:\n            if self.last_epoch == -1:\n                self.last_epoch += 1\n            else:\n                self.last_epoch += 1 / self.num_iters  # update step with iters\n        else:\n            self.last_epoch = epoch\n        self.last_lr = self.get_lr()\n        if self.verbose:\n            print('Epoch {}: {} set learning rate to {}.'.format(\n                self.last_epoch, self.__class__.__name__, self.last_lr))\n    def _lr_func_cosine(self, cur_epoch, cosine_base_lr, max_epoch):"
        },
        {
            "comment": "The code defines a custom learning rate (LR) scheduler that includes warmup and stepwise-cosine decay. It first performs a warmup stage with a linear increase in LR from the warmup_start_lr to lr_end over warmup_epochs, then applies cosine annealing for the rest of the epochs, resulting in a learning rate that decreases from the base value according to the cosine function.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/solver/custom_lr.py\":80-105",
            "content": "        return cosine_base_lr * (math.cos(math.pi * cur_epoch / max_epoch) +\n                                 1.0) * 0.5\n    def get_lr(self):\n        \"\"\"Define lr policy\"\"\"\n        lr = self._lr_func_cosine(self.last_epoch, self.cosine_base_lr,\n                                  self.max_epoch)\n        lr_end = self._lr_func_cosine(self.warmup_epochs, self.cosine_base_lr,\n                                      self.max_epoch)\n        # Perform warm up.\n        if self.last_epoch < self.warmup_epochs:\n            lr_start = self.warmup_start_lr\n            alpha = (lr_end - lr_start) / self.warmup_epochs\n            lr = self.last_epoch * alpha + lr_start\n        return lr\nclass CustomWarmupPiecewiseDecay(LRScheduler):\n    r\"\"\"\n    This op combine warmup and stepwise-cosine which is used in slowfast model.\n    Args:\n        warmup_start_lr (float): start learning rate used in warmup stage.\n        warmup_epochs (int): the number epochs of warmup.\n        step_base_lr (float|int, optional): base learning rate in step schedule."
        },
        {
            "comment": "This code defines a class `CustomWarmupPiecewiseDecay` which schedules learning rate for training. The class takes parameters like warmup_start_lr, warmup_epochs, step_base_lr, lrs, gamma, steps, max_epoch, num_iters, last_epoch, and verbose. It initializes these parameters in the constructor (__init__). The learning rate is scheduled to decay over time following a piecewise function with warm-up and custom decays.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/solver/custom_lr.py\":106-132",
            "content": "        max_epoch (int): total training epochs.\n        num_iters(int): number iterations of each epoch.\n        last_epoch (int, optional):  The index of last epoch. Can be set to restart training. Default: -1, means initial learning rate.\n        verbose (bool, optional): If ``True``, prints a message to stdout for each update. Default: ``False`` .\n    Returns:\n        ``CustomWarmupPiecewiseDecay`` instance to schedule learning rate.\n    \"\"\"\n    def __init__(self,\n                 warmup_start_lr,\n                 warmup_epochs,\n                 step_base_lr,\n                 lrs,\n                 gamma,\n                 steps,\n                 max_epoch,\n                 num_iters,\n                 last_epoch=0,\n                 verbose=False):\n        self.warmup_start_lr = warmup_start_lr\n        self.warmup_epochs = warmup_epochs\n        self.step_base_lr = step_base_lr\n        self.lrs = lrs\n        self.gamma = gamma\n        self.steps = steps\n        self.max_epoch = max_epoch\n        self.num_iters = num_iters"
        },
        {
            "comment": "This code defines a custom learning rate scheduler for the PaddleVideo library. The `step` method updates the learning rate based on the current epoch and returns None. It should be called after `optimizer.step`. If no epoch is specified, it increments the last epoch by the number of iterations divided by the total number of iterations. The last learning rate is stored in `self.last_lr`, and if verbose is set to True, it prints the current epoch, scheduler name, and updated learning rate.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/solver/custom_lr.py\":133-157",
            "content": "        self.last_epoch = last_epoch\n        self.last_lr = self.warmup_start_lr  # used in first iter\n        self.verbose = verbose\n        self._var_name = None\n    def step(self, epoch=None, rebuild=False):\n        \"\"\"\n        ``step`` should be called after ``optimizer.step`` . It will update the learning rate in optimizer according to current ``epoch`` .\n        The new learning rate will take effect on next ``optimizer.step`` .\n        Args:\n            epoch (int, None): specify current epoch. Default: None. Auto-increment from last_epoch=-1.\n        Returns:\n            None\n        \"\"\"\n        if epoch is None:\n            if not rebuild:\n                self.last_epoch += 1 / self.num_iters  # update step with iters\n        else:\n            self.last_epoch = epoch\n        self.last_lr = self.get_lr()\n        if self.verbose:\n            print(\n                'step Epoch {}: {} set learning rate to {}.self.num_iters={}, 1/self.num_iters={}'\n                .format(self.last_epoch, self.__class__.__name__, self.last_lr,"
        },
        {
            "comment": "This function defines a learning rate (lr) policy that varies based on the current epoch, predefined learning rates, base lr, steps, and maximum epoch. It calculates the learning rate for each step using a relative learning rate function and returns it. The function also includes a warmup phase where the learning rate gradually increases from 0 to its final value over the specified number of epochs.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/solver/custom_lr.py\":158-187",
            "content": "                        self.num_iters, 1 / self.num_iters))\n    def _lr_func_steps_with_relative_lrs(self, cur_epoch, lrs, base_lr, steps,\n                                         max_epoch):\n        # get step index\n        steps = steps + [max_epoch]\n        for ind, step in enumerate(steps):\n            if cur_epoch < step:\n                break\n        if self.verbose:\n            print(\n                '_lr_func_steps_with_relative_lrs, cur_epoch {}: {}, steps {}, ind {}, step{}, max_epoch{}'\n                .format(cur_epoch, self.__class__.__name__, steps, ind, step,\n                        max_epoch))\n        return lrs[ind - 1] * base_lr\n    def get_lr(self):\n        \"\"\"Define lr policy\"\"\"\n        lr = self._lr_func_steps_with_relative_lrs(\n            self.last_epoch,\n            self.lrs,\n            self.step_base_lr,\n            self.steps,\n            self.max_epoch,\n        )\n        lr_end = self._lr_func_steps_with_relative_lrs(\n            self.warmup_epochs,\n            self.lrs,\n            self.step_base_lr,"
        },
        {
            "comment": "This code implements a CustomWarmupCosineStepDecay learning rate scheduler, which performs warm up and then applies piecewise decay. The learning rate is determined based on the current epoch, warmup epochs, warmup start and end rates, and the number of steps. A CustomPiecewiseDecay class is also defined, which inherits from PiecewiseDecay and overrides the num_iters parameter.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/solver/custom_lr.py\":188-221",
            "content": "            self.steps,\n            self.max_epoch,\n        )\n        # Perform warm up.\n        if self.last_epoch < self.warmup_epochs:\n            lr_start = self.warmup_start_lr\n            alpha = (lr_end - lr_start) / self.warmup_epochs\n            lr = self.last_epoch * alpha + lr_start\n        if self.verbose:\n            print(\n                'get_lr, Epoch {}: {}, lr {}, lr_end {}, self.lrs{}, self.step_base_lr{}, self.steps{}, self.max_epoch{}'\n                .format(self.last_epoch, self.__class__.__name__, lr, lr_end,\n                        self.lrs, self.step_base_lr, self.steps,\n                        self.max_epoch))\n        return lr\nclass CustomPiecewiseDecay(PiecewiseDecay):\n    def __init__(self, **kargs):\n        kargs.pop('num_iters')\n        super().__init__(**kargs)\nclass CustomWarmupCosineStepDecay(LRScheduler):\n    def __init__(self,\n                 warmup_iters,\n                 warmup_ratio=0.1,\n                 min_lr=0,\n                 base_lr=3e-5,\n                 max_epoch=30,"
        },
        {
            "comment": "This function initializes the custom learning rate scheduler. It sets warmup ratio, minimum learning rate, and warmup iterations. The total number of iterations, maximum epochs, base learning rate for cosine annealing, and a regular learning rate are calculated. The function also defines a helper method 'annealing_cos' for cosine annealing.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/solver/custom_lr.py\":222-248",
            "content": "                 last_epoch=-1,\n                 num_iters=None,\n                 verbose=False):\n        self.warmup_ratio = warmup_ratio\n        self.min_lr = min_lr\n        self.warmup_epochs = warmup_iters\n        self.warmup_iters = warmup_iters * num_iters\n        self.cnt_iters = 0\n        self.cnt_epoch = 0\n        self.num_iters = num_iters\n        self.tot_iters = max_epoch * num_iters\n        self.max_epoch = max_epoch\n        self.cosine_base_lr = base_lr  # initial lr for all param groups\n        self.regular_lr = self.get_regular_lr()\n        super().__init__(last_epoch=last_epoch, verbose=verbose)\n    def annealing_cos(self, start, end, factor, weight=1):\n        cos_out = math.cos(math.pi * factor) + 1\n        return end + 0.5 * weight * (start - end) * cos_out\n    def get_regular_lr(self):\n        progress = self.cnt_epoch\n        max_progress = self.max_epoch\n        target_lr = self.min_lr\n        return self.annealing_cos(self.cosine_base_lr, target_lr, progress /\n                                  max_progress)  # self.cosine_base_lr"
        },
        {
            "comment": "This code defines a custom learning rate scheduler that combines warmup and stepwise-cosine decay. The get_warmup_lr function calculates the warmup learning rate, while the get_lr function determines whether the current iteration is in the warmup stage or not, returning either the regular learning rate or the warmed-up learning rate. The step function updates the learning rate and counts the number of iterations.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/solver/custom_lr.py\":250-281",
            "content": "    def get_warmup_lr(self, cur_iters):\n        k = (1 - cur_iters / self.warmup_iters) * (1 - self.warmup_ratio)\n        warmup_lr = self.regular_lr * (1 - k)  # 3e-5 * (1-k)\n        return warmup_lr\n    def step(self, epoch=None):\n        self.regular_lr = self.get_regular_lr()\n        self.last_lr = self.get_lr()\n        self.cnt_epoch = (self.cnt_iters +\n                          1) // self.num_iters  # update step with iters\n        self.cnt_iters += 1\n        if self.verbose:\n            print('Epoch {}: {} set learning rate to {}.'.format(\n                self.last_epoch, self.__class__.__name__, self.last_lr))\n    def get_lr(self):\n        \"\"\"Define lr policy\"\"\"\n        cur_iter = self.cnt_iters\n        if cur_iter >= self.warmup_iters:\n            return self.regular_lr\n        else:\n            warmup_lr = self.get_warmup_lr(cur_iter)\n            return warmup_lr\nclass CustomWarmupAdjustDecay(LRScheduler):\n    r\"\"\"\n    We combine warmup and stepwise-cosine which is used in slowfast model.\n    Args:\n        step_base_lr (float): start learning rate used in warmup stage."
        },
        {
            "comment": "Custom learning rate scheduler with warmup, decay, and boundary steps. Initializes the LR scheduler with step base LR, warmup epochs, decay rate, boundaries, number of iterations (optional), last epoch (optional), and verbosity level (optional).",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/solver/custom_lr.py\":282-304",
            "content": "        warmup_epochs (int): the number epochs of warmup.\n        lr_decay_rate (float|int, optional): base learning rate decay rate.\n        step (int): step in change learning rate.\n        last_epoch (int, optional):  The index of last epoch. Can be set to restart training. Default: -1, means initial learning rate.\n        verbose (bool, optional): If ``True``, prints a message to stdout for each update. Default: ``False`` .\n    Returns:\n        ``CosineAnnealingDecay`` instance to schedule learning rate.\n    \"\"\"\n    def __init__(self,\n                 step_base_lr,\n                 warmup_epochs,\n                 lr_decay_rate,\n                 boundaries,\n                 num_iters=None,\n                 last_epoch=-1,\n                 verbose=False):\n        self.step_base_lr = step_base_lr\n        self.warmup_epochs = warmup_epochs\n        self.lr_decay_rate = lr_decay_rate\n        self.boundaries = boundaries\n        self.num_iters = num_iters\n        #call step() in base class, last_lr/last_epoch/base_lr will be update"
        },
        {
            "comment": "The code defines a custom learning rate scheduler, CustomWarmupAdjustDecay, which adjusts the learning rate based on epoch number. It initializes the scheduler and provides a step method for updating the learning rate after optimizer.step is called. The get_lr method returns the current learning rate. The last_epoch variable keeps track of the current epoch. If no epoch is specified, it auto-increments from the last_epoch value. If an epoch is provided, the last_epoch is set to that value. Finally, if verbose is True, it prints the current epoch and the learning rate set.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/solver/custom_lr.py\":305-331",
            "content": "        super(CustomWarmupAdjustDecay, self).__init__(last_epoch=last_epoch,\n                                                      verbose=verbose)\n    def step(self, epoch=None):\n        \"\"\"\n        ``step`` should be called after ``optimizer.step`` . It will update the learning rate in optimizer according to current ``epoch`` .\n        The new learning rate will take effect on next ``optimizer.step`` .\n        Args:\n            epoch (int, None): specify current epoch. Default: None. Auto-increment from last_epoch=-1.\n        Returns:\n            None\n        \"\"\"\n        if epoch is None:\n            if self.last_epoch == -1:\n                self.last_epoch += 1\n            else:\n                self.last_epoch += 1 / self.num_iters  # update step with iters\n        else:\n            self.last_epoch = epoch\n        self.last_lr = self.get_lr()\n        if self.verbose:\n            print('Epoch {}: {} set learning rate to {}.'.format(\n                self.last_epoch, self.__class__.__name__, self.last_lr))\n    def get_lr(self):"
        },
        {
            "comment": "This code calculates the learning rate based on whether the current epoch is within the warmup phase or not. If in warmup, it linearly increases the base learning rate. Otherwise, it applies a decay rate to determine the learning rate.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/solver/custom_lr.py\":332-337",
            "content": "        if self.last_epoch < self.warmup_epochs:\n            lr = self.step_base_lr * (self.last_epoch + 1) / self.warmup_epochs\n        else:\n            lr = self.step_base_lr * (self.lr_decay_rate**np.sum(\n                self.last_epoch >= np.array(self.boundaries)))\n        return lr"
        }
    ]
}