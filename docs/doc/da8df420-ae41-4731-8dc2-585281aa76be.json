{
    "summary": "The code deploys Paddle Serving, a model serving framework in PaddleVideo, through Docker on Linux with GPU and CPU options. It uses paddle-video-deploy for model conversion and includes client scripts, environment setup, and troubleshooting for missing libraries.",
    "details": [
        {
            "comment": "This code snippet provides instructions for deploying Paddle Serving, a model serving framework, as part of the PaddleVideo codebase. It explains that this is done through Docker and covers GPU-accelerated and CPU-based installations, including the necessary package installation commands. The code assumes Linux platform, and does not support Windows at present.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/cpp_serving/readme.md\":0-31",
            "content": "\u7b80\u4f53\u4e2d\u6587 | [English](./readme_en.md)\n# \u6a21\u578b\u670d\u52a1\u5316\u90e8\u7f72\n## \u7b80\u4ecb\n[Paddle Serving](https://github.com/PaddlePaddle/Serving) \u65e8\u5728\u5e2e\u52a9\u6df1\u5ea6\u5b66\u4e60\u5f00\u53d1\u8005\u8f7b\u677e\u90e8\u7f72\u5728\u7ebf\u9884\u6d4b\u670d\u52a1\uff0c\u652f\u6301\u4e00\u952e\u90e8\u7f72\u5de5\u4e1a\u7ea7\u7684\u670d\u52a1\u80fd\u529b\u3001\u5ba2\u6237\u7aef\u548c\u670d\u52a1\u7aef\u4e4b\u95f4\u9ad8\u5e76\u53d1\u548c\u9ad8\u6548\u901a\u4fe1\u3001\u5e76\u652f\u6301\u591a\u79cd\u7f16\u7a0b\u8bed\u8a00\u5f00\u53d1\u5ba2\u6237\u7aef\u3002\n\u8be5\u90e8\u5206\u4ee5 HTTP \u9884\u6d4b\u670d\u52a1\u90e8\u7f72\u4e3a\u4f8b\uff0c\u4ecb\u7ecd\u600e\u6837\u5728 PaddleVideo \u4e2d\u4f7f\u7528 PaddleServing \u90e8\u7f72\u6a21\u578b\u670d\u52a1\u3002\u76ee\u524d\u53ea\u652f\u6301 Linux \u5e73\u53f0\u90e8\u7f72\uff0c\u6682\u4e0d\u652f\u6301 Windows \u5e73\u53f0\u3002\n## Serving \u5b89\u88c5\nServing \u5b98\u7f51\u63a8\u8350\u4f7f\u7528 docker \u5b89\u88c5\u5e76\u90e8\u7f72 Serving \u73af\u5883\u3002\u9996\u5148\u9700\u8981\u62c9\u53d6 docker \u73af\u5883\u5e76\u521b\u5efa\u57fa\u4e8e Serving \u7684 docker\u3002\n```bash\n# \u542f\u52a8GPU docker\ndocker pull paddlepaddle/serving:0.7.0-cuda10.2-cudnn7-devel\nnvidia-docker run -p 9292:9292 --name test -dit paddlepaddle/serving:0.7.0-cuda10.2-cudnn7-devel bash\nnvidia-docker exec -it test bash\n# \u542f\u52a8CPU docker\ndocker pull paddlepaddle/serving:0.7.0-devel\ndocker run -p 9292:9292 --name test -dit paddlepaddle/serving:0.7.0-devel bash\ndocker exec -it test bash\n```\n\u8fdb\u5165 docker \u540e\uff0c\u9700\u8981\u5b89\u88c5 Serving \u76f8\u5173\u7684 python \u5305\u3002\n```bash\npython3.7 -m pip install paddle-serving-client==0.7.0\npython3.7 -m pip install paddle-serving-app==0.7.0\n#\u82e5\u4e3aCPU\u90e8\u7f72\u73af\u5883:\npython3.7 -m pip install paddle-serving-server==0.7.0  # CPU\npython3.7 -m pip install paddlepaddle==2.2.0           # CPU"
        },
        {
            "comment": "This code provides instructions for installing different versions of PaddleServing Server with various GPU environments and specifies the required pip commands. It also mentions an alternative method to speed up the installation process by changing the source. Furthermore, it highlights how to convert a PP-TSM inference model into Serving format for deploying behavior recognition service.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/cpp_serving/readme.md\":33-63",
            "content": "#\u82e5\u4e3aGPU\u90e8\u7f72\u73af\u5883\npython3.7 -m pip install paddle-serving-server-gpu==0.7.0.post102  # GPU with CUDA10.2 + TensorRT6\npython3.7 -m pip install paddlepaddle-gpu==2.2.0                   # GPU with CUDA10.2\n#\u5176\u4ed6GPU\u73af\u5883\u9700\u8981\u786e\u8ba4\u73af\u5883\u518d\u9009\u62e9\u6267\u884c\u54ea\u4e00\u6761\npython3.7 -m pip install paddle-serving-server-gpu==0.7.0.post101  # GPU with CUDA10.1 + TensorRT6\npython3.7 -m pip install paddle-serving-server-gpu==0.7.0.post112  # GPU with CUDA11.2 + TensorRT8\n```\n* \u5982\u679c\u5b89\u88c5\u901f\u5ea6\u592a\u6162\uff0c\u53ef\u4ee5\u901a\u8fc7 `-i https://pypi.tuna.tsinghua.edu.cn/simple` \u66f4\u6362\u6e90\uff0c\u52a0\u901f\u5b89\u88c5\u8fc7\u7a0b\u3002\n* \u66f4\u591a\u73af\u5883\u548c\u5bf9\u5e94\u7684\u5b89\u88c5\u5305\u8be6\u89c1\uff1ahttps://github.com/PaddlePaddle/Serving/blob/v0.9.0/doc/Install_Linux_Env_CN.md\n## \u884c\u4e3a\u8bc6\u522b\u670d\u52a1\u90e8\u7f72\n### \u6a21\u578b\u8f6c\u6362\n\u4f7f\u7528 PaddleServing \u505a\u670d\u52a1\u5316\u90e8\u7f72\u65f6\uff0c\u9700\u8981\u5c06\u4fdd\u5b58\u7684 inference \u6a21\u578b\u8f6c\u6362\u4e3a Serving \u6a21\u578b\u3002\u4e0b\u9762\u4ee5 PP-TSM \u6a21\u578b\u4e3a\u4f8b\uff0c\u4ecb\u7ecd\u5982\u4f55\u90e8\u7f72\u884c\u4e3a\u8bc6\u522b\u670d\u52a1\u3002\n- \u4e0b\u8f7d PP-TSM \u63a8\u7406\u6a21\u578b\u5e76\u8f6c\u6362\u4e3a Serving \u6a21\u578b\uff1a\n  ```bash\n  # \u8fdb\u5165PaddleVideo\u76ee\u5f55\n  cd PaddleVideo\n  # \u4e0b\u8f7d\u63a8\u7406\u6a21\u578b\u5e76\u89e3\u538b\u5230./inference\u4e0b\n  mkdir ./inference\n  pushd ./inference\n  wget  https://videotag.bj.bcebos.com/PaddleVideo-release2.3/ppTSM.zip\n  unzip ppTSM.zip\n  popd\n  # \u8f6c\u6362\u6210 Serving \u6a21\u578b\n  pushd deploy/cpp_serving\n  python3.7 -m paddle_serving_client.convert \\"
        },
        {
            "comment": "This code is using PaddleVideo's paddle-video-deploy to convert a model and store it in the \"inference/ppTSM\" directory. It specifies the model filename as \"ppTSM.pdmodel\", the params filename as \"ppTSM.pdiparams\", and generates serving_server and serving_client files in the \"ppTSM\" folder. The converted model will be saved in the \"deploy/cpp_serving\" directory, with the client configuration stored in the \"ppTSM_serving_client\" and server configurations stored in the \"ppTSM_serving_server\".",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/cpp_serving/readme.md\":64-80",
            "content": "  --dirname ../../inference/ppTSM \\\n  --model_filename ppTSM.pdmodel \\\n  --params_filename ppTSM.pdiparams \\\n  --serving_server ./ppTSM_serving_server \\\n  --serving_client ./ppTSM_serving_client\n  popd\n  ```\n  | \u53c2\u6570              | \u7c7b\u578b | \u9ed8\u8ba4\u503c             | \u63cf\u8ff0                                                         |\n  | ----------------- | ---- | ------------------ | ------------------------------------------------------------ |\n  | `dirname`         | str  | -                  | \u9700\u8981\u8f6c\u6362\u7684\u6a21\u578b\u6587\u4ef6\u5b58\u50a8\u8def\u5f84\uff0cProgram\u7ed3\u6784\u6587\u4ef6\u548c\u53c2\u6570\u6587\u4ef6\u5747\u4fdd\u5b58\u5728\u6b64\u76ee\u5f55\u3002 |\n  | `model_filename`  | str  | None               | \u5b58\u50a8\u9700\u8981\u8f6c\u6362\u7684\u6a21\u578bInference Program\u7ed3\u6784\u7684\u6587\u4ef6\u540d\u79f0\u3002\u5982\u679c\u8bbe\u7f6e\u4e3aNone\uff0c\u5219\u4f7f\u7528 `__model__` \u4f5c\u4e3a\u9ed8\u8ba4\u7684\u6587\u4ef6\u540d |\n  | `params_filename` | str  | None               | \u5b58\u50a8\u9700\u8981\u8f6c\u6362\u7684\u6a21\u578b\u6240\u6709\u53c2\u6570\u7684\u6587\u4ef6\u540d\u79f0\u3002\u5f53\u4e14\u4ec5\u5f53\u6240\u6709\u6a21\u578b\u53c2\u6570\u88ab\u4fdd>\u5b58\u5728\u4e00\u4e2a\u5355\u72ec\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u4e2d\uff0c\u5b83\u624d\u9700\u8981\u88ab\u6307\u5b9a\u3002\u5982\u679c\u6a21\u578b\u53c2\u6570\u662f\u5b58\u50a8\u5728\u5404\u81ea\u5206\u79bb\u7684\u6587\u4ef6\u4e2d\uff0c\u8bbe\u7f6e\u5b83\u7684\u503c\u4e3aNone |\n  | `serving_server`  | str  | `\"serving_server\"` | \u8f6c\u6362\u540e\u7684\u6a21\u578b\u6587\u4ef6\u548c\u914d\u7f6e\u6587\u4ef6\u7684\u5b58\u50a8\u8def\u5f84\u3002\u9ed8\u8ba4\u503c\u4e3aserving_server |\n  | `serving_client`  | str  | `\"serving_client\"` | \u8f6c\u6362\u540e\u7684\u5ba2\u6237\u7aef\u914d\u7f6e\u6587\u4ef6\u5b58\u50a8\u8def\u5f84\u3002\u9ed8\u8ba4\u503c\u4e3aserving_client       |\n- \u63a8\u7406\u6a21\u578b\u8f6c\u6362\u5b8c\u6210\u540e\uff0c\u4f1a\u5728`deploy/cpp_serving`\u6587\u4ef6\u5939\u4e0b\u751f\u6210 `ppTSM_serving_client` \u548c `ppTSM_serving_server` \u4e24\u4e2a\u6587\u4ef6\u5939\uff0c\u5177\u5907\u5982\u4e0b\u683c\u5f0f\uff1a"
        },
        {
            "comment": "This code snippet shows the necessary changes to be made in `serving_client_conf.prototxt` and `serving_server_conf.prototxt` files after getting the model file. The purpose is to rename `alias_name` as 'outputs' for `fetch_var`. This allows Serving to support different models without modifying code during deployment.\n\nThe modified `serving_server_conf.prototxt` file shows a feed variable and a fetch variable with their respective names, alias names, shapes, and types.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/cpp_serving/readme.md\":81-117",
            "content": "  ```bash\n  PaddleVideo/deploy/cpp_serving\n  \u251c\u2500\u2500 ppTSM_serving_client\n  \u2502   \u251c\u2500\u2500 serving_client_conf.prototxt\n  \u2502   \u2514\u2500\u2500 serving_client_conf.stream.prototxt\n  \u2514\u2500\u2500 ppTSM_serving_server\n      \u251c\u2500\u2500 ppTSM.pdiparams\n      \u251c\u2500\u2500 ppTSM.pdmodel\n      \u251c\u2500\u2500 serving_server_conf.prototxt\n      \u2514\u2500\u2500 serving_server_conf.stream.prototxt\n  ```\n  \u5f97\u5230\u6a21\u578b\u6587\u4ef6\u4e4b\u540e\uff0c\u9700\u8981\u5206\u522b\u4fee\u6539 `ppTSM_serving_client` \u4e0b\u7684 `serving_client_conf.prototxt` \u548c `ppTSM_serving_server` \u4e0b\u7684 `serving_server_conf.prototxt`\uff0c\u5c06\u4e24\u4efd\u6587\u4ef6\u4e2d`fetch_var` \u4e0b\u7684 `alias_name` \u5747\u6539\u4e3a `outputs`\n  **\u5907\u6ce8**:  Serving \u4e3a\u4e86\u517c\u5bb9\u4e0d\u540c\u6a21\u578b\u7684\u90e8\u7f72\uff0c\u63d0\u4f9b\u4e86\u8f93\u5165\u8f93\u51fa\u91cd\u547d\u540d\u7684\u529f\u80fd\u3002\u8fd9\u6837\uff0c\u4e0d\u540c\u7684\u6a21\u578b\u5728\u63a8\u7406\u90e8\u7f72\u65f6\uff0c\u53ea\u9700\u8981\u4fee\u6539\u914d\u7f6e\u6587\u4ef6\u7684`alias_name`\u5373\u53ef\uff0c\u65e0\u9700\u4fee\u6539\u4ee3\u7801\u5373\u53ef\u5b8c\u6210\u63a8\u7406\u90e8\u7f72\u3002\n  \u4fee\u6539\u540e\u7684`serving_server_conf.prototxt`\u5982\u4e0b\u6240\u793a:\n  ```yaml\n  feed_var {\n    name: \"data_batch_0\"\n    alias_name: \"data_batch_0\"\n    is_lod_tensor: false\n    feed_type: 1\n    shape: 8\n    shape: 3\n    shape: 224\n    shape: 224\n  }\n  fetch_var {\n    name: \"linear_2.tmp_1\"\n    alias_name: \"outputs\"\n    is_lod_tensor: false\n    fetch_type: 1\n    shape: 400\n  }\n  ```\n### \u670d\u52a1\u90e8\u7f72\u548c\u8bf7\u6c42\n`cpp_serving` \u76ee\u5f55\u5305\u542b\u4e86\u542f\u52a8 pipeline \u670d\u52a1\u3001C++ serving\u670d\u52a1\u548c\u53d1\u9001\u9884\u6d4b\u8bf7\u6c42\u7684\u4ee3\u7801\uff0c\u5177\u4f53\u5305\u62ec\uff1a"
        },
        {
            "comment": "This code provides the instructions to set up and run a C++ serving server, send requests from a client script, and interpret the results. It also includes a script to install the required environment and troubleshoot common issues such as missing libraries.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/cpp_serving/readme.md\":118-157",
            "content": "  ```bash\n  run_cpp_serving.sh          # \u542f\u52a8C++ serving server\u7aef\u7684\u811a\u672c\n  pipeline_http_client.py     # client\u7aef\u53d1\u9001\u6570\u636e\u5e76\u83b7\u53d6\u9884\u6d4b\u7ed3\u679c\u7684\u811a\u672c\n  paddle_env_install.sh       # \u5b89\u88c5C++ serving\u73af\u5883\u811a\u672c\n  preprocess_ops.py           # \u5b58\u653e\u9884\u5904\u7406\u51fd\u6570\u7684\u6587\u4ef6\n  ```\n#### C++ Serving\n- \u8fdb\u5165\u5de5\u4f5c\u76ee\u5f55\uff1a\n  ```bash\n  cd deploy/cpp_serving\n  ```\n- \u542f\u52a8\u670d\u52a1\uff1a\n  ```bash\n  # \u5728\u540e\u53f0\u542f\u52a8\uff0c\u8fc7\u7a0b\u4e2d\u6253\u5370\u8f93\u51fa\u7684\u65e5\u5fd7\u4f1a\u91cd\u5b9a\u5411\u4fdd\u5b58\u5230nohup.txt\u4e2d\uff0c\u53ef\u4ee5\u4f7f\u7528tailf nohup.txt\u67e5\u770b\u8f93\u51fa\n  bash run_cpp_serving.sh\n  ```\n- \u53d1\u9001\u8bf7\u6c42\u5e76\u83b7\u53d6\u7ed3\u679c\uff1a\n  ```bash\n  python3.7 serving_client.py \\\n  -n PPTSM \\\n  -c ./ppTSM_serving_client/serving_client_conf.prototxt \\\n  --input_file=../../data/example.avi\n  ```\n\u6210\u529f\u8fd0\u884c\u540e\uff0c\u6a21\u578b\u9884\u6d4b\u7684\u7ed3\u679c\u4f1a\u6253\u5370\u5728 cmd \u7a97\u53e3\u4e2d\uff0c\u7ed3\u679c\u5982\u4e0b\uff1a\n  ```bash\n  I0510 04:33:00.110025 37097 naming_service_thread.cpp:202] brpc::policy::ListNamingService(\"127.0.0.1:9993\"): added 1\n  I0510 04:33:01.904764 37097 general_model.cpp:490] [client]logid=0,client_cost=1640.96ms,server_cost=1623.21ms.\n  {'class_id': '[5]', 'prob': '[0.9907387495040894]'}\n  ```\n**\u5982\u679c\u8fc7\u7a0b\u4e2d\u62a5\u9519\u663e\u793a\u627e\u4e0d\u5230libnvinfer.so.6\uff0c\u53ef\u4ee5\u6267\u884c\u811a\u672c`paddle_env_install.sh`\u5b89\u88c5\u76f8\u5173\u73af\u5883**\n  ```bash\n  bash paddle_env_install.sh\n  ```\n## FAQ\n**Q1**\uff1a \u53d1\u9001\u8bf7\u6c42\u540e\u6ca1\u6709\u7ed3\u679c\u8fd4\u56de\u6216\u8005\u63d0\u793a\u8f93\u51fa\u89e3\u7801\u62a5\u9519"
        },
        {
            "comment": "Avoid setting proxies when starting the service and sending requests. Disable proxies by using \"unset https_proxy\" and \"unset http_proxy\" commands beforehand.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/cpp_serving/readme.md\":159-163",
            "content": "**A1**\uff1a \u542f\u52a8\u670d\u52a1\u548c\u53d1\u9001\u8bf7\u6c42\u65f6\u4e0d\u8981\u8bbe\u7f6e\u4ee3\u7406\uff0c\u53ef\u4ee5\u5728\u542f\u52a8\u670d\u52a1\u524d\u548c\u53d1\u9001\u8bf7\u6c42\u524d\u5173\u95ed\u4ee3\u7406\uff0c\u5173\u95ed\u4ee3\u7406\u7684\u547d\u4ee4\u662f\uff1a\n```\nunset https_proxy\nunset http_proxy\n```"
        }
    ]
}