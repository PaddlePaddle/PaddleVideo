{
    "summary": "This code trains and evaluates the MS-TCN video action segmentation model using provided datasets, compares performance with PaddleVideo's MSTCN, exports inference models, uses metrics like accuracy and F1 score, and runs with GPU usage enabled.",
    "details": [
        {
            "comment": "Introduction: MS-TCN model for video motion segmentation was published in 2019 and optimized for higher precision results in PaddleVideo.\nData: Choose from 50salads, breakfast, gtea datasets for training. Refer to Video Action Segmentation dataset download and preparation doc.\nTrain: After preparing the dataset, run scripts with provided command example.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/segmentation/mstcn.md\":0-34",
            "content": "[\u7b80\u4f53\u4e2d\u6587](../../../zh-CN/model_zoo/segmentation/mstcn.md) | English\n# MS-TCN : Video Action Segmentation Model\n---\n## Contents\n- [Introduction](#Introduction)\n- [Data](#Data)\n- [Train](#Train)\n- [Test](#Test)\n- [Inference](#Inference)\n- [Reference](#Reference)\n## Introduction\nMs-tcn model is a classic model of video motion segmentation model, which was published on CVPR in 2019. We optimized the officially implemented pytorch code and obtained higher precision results in paddlevideo.\n<p align=\"center\">\n<img src=\"../../../images/mstcn.PNG\" height=300 width=400 hspace='10'/> <br />\nMS-TCN Overview\n</p>\n## Data\nMS-TCN can choose 50salads, breakfast, gtea as trianing set. Please refer to Video Action Segmentation dataset download and preparation doc [Video Action Segmentation dataset](../../dataset/SegmentationDataset.md)\n## Train\nAfter prepare dataset, we can run sprits.\n```bash\n# gtea dataset\nexport CUDA_VISIBLE_DEVICES=3\npython3.7 main.py  --validate -c configs/segmentation/ms_tcn/ms_tcn_gtea.yaml --seed 1538574472"
        },
        {
            "comment": "The code snippet provides instructions for training and testing the video action segmentation model, specifically MSTCN. It mentions that single sample training is supported, and demonstrates how to test MSTCN on a dataset using the provided command line or script program. Additionally, it explains the evaluation method used for datasets and refers to the author's provided evaluation script.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/segmentation/mstcn.md\":35-51",
            "content": "```\n- Start the training by using the above command line or script program. There is no need to use the pre training model. The video action segmentation model is usually a full convolution network. Due to the different lengths of videos, the `DATASET.batch_size` of the video action segmentation model is usually set to `1`, that is, batch training is not required. At present, only **single sample** training is supported.\n## Test\nTest MS-TCN on dataset scripts:\n```bash\npython main.py  --test -c configs/segmentation/ms_tcn/ms_tcn_gtea.yaml --weights=./output/MSTCN/MSTCN_split_1.pdparams\n```\n- The specific implementation of the index is to calculate ACC, edit and F1 scores by referring to the test script[evel.py](https://github.com/yabufarha/ms-tcn/blob/master/eval.py) provided by the author of ms-tcn.\n- The evaluation method of data set adopts the folding verification method in ms-tcn paper, and the division method of folding is the same as that in ms-tcn paper.\nAccuracy on Breakfast dataset(4 folding verification):"
        },
        {
            "comment": "This table compares the performance of a paper model and PaddleVideo's MSTCN model on different datasets. The metrics include accuracy (Acc), edit distance, and F1 score (F1@0.1, F1@0.25, F1@0.5). The models are validated with 5-fold cross-validation on the 50salads dataset and 4-fold on the gtea dataset. The provided checkpoints are for gtea dataset splits.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/segmentation/mstcn.md\":53-77",
            "content": "| Model | Acc | Edit | F1@0.1 | F1@0.25 | F1@0.5 |\n| :---: | :---: | :---: | :---: | :---: | :---: |\n| paper | 66.3% | 61.7% | 48.1% | 48.1% | 37.9% |\n| paddle | 65.2% | 61.5% | 53.7% | 49.2% | 38.8% |\nAccuracy on 50salads dataset(5 folding verification):\n| Model | Acc | Edit | F1@0.1 | F1@0.25 | F1@0.5 |\n| :---: | :---: | :---: | :---: | :---: | :---: |\n| paper | 80.7% | 67.9% | 76.3% | 74.0% | 64.5% |\n| paddle | 81.1% | 71.5% | 77.9% | 75.5% | 66.5% |\nAccuracy on gtea dataset(4 folding verification):\n| Model | Acc | Edit | F1@0.1 | F1@0.25 | F1@0.5 |\n| :---: | :---: | :---: | :---: | :---: | :---: |\n| paper | 79.2% | 81.4% | 87.5% | 85.4% | 74.6% |\n| paddle | 76.9% | 81.8% | 86.4% | 84.7% | 74.8% |\nModel weight for gtea\nTest_Data| F1@0.5 | checkpoints |\n| :----: | :----: | :---- |\n| gtea_split1 | 70.2509 | [MSTCN_gtea_split_1.pdparams](https://videotag.bj.bcebos.com/PaddleVideo-release2.2/MSTCN_gtea_split_1.pdparams) |\n| gtea_split2 | 70.7224 | [MSTCN_gtea_split_2.pdparams](https://videotag.bj.bcebos.com/PaddleVideo-release2.2/MSTCN_gtea_split_2.pdparams) |"
        },
        {
            "comment": "This code provides instructions for exporting and using an inference model. The `export_model.py` script is used to create the architecture file (`MSTCN.pdmodel`) and parameters file (`MSTCN.pdiparams`). These files can be obtained by running the script with the given configuration file, pre-trained parameters file path, and output directory. The inference process involves providing a list of input files in the format `S1_<item>_C1.npy`. To execute the inference, run the `predict.py` script with the input file list as an argument.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/segmentation/mstcn.md\":78-107",
            "content": "| gtea_split3 | 80.0 | [MSTCN_gtea_split_3.pdparams](https://videotag.bj.bcebos.com/PaddleVideo-release2.2/MSTCN_gtea_split_3.pdparams) |\n| gtea_split4 | 78.1609 | [MSTCN_gtea_split_4.pdparams](https://videotag.bj.bcebos.com/PaddleVideo-release2.2/MSTCN_gtea_split_4.pdparams) |\n## Infer\n### export inference model\n```bash\npython3.7 tools/export_model.py -c configs/segmentation/ms_tcn/ms_tcn_gtea.yaml \\\n                                -p data/MSTCN_gtea_split_1.pdparams \\\n                                -o inference/MSTCN\n```\nTo get model architecture file `MSTCN.pdmodel` and parameters file `MSTCN.pdiparams`, use:\n- Args usage please refer to [Model Inference](https://github.com/PaddlePaddle/PaddleVideo/blob/release/2.0/docs/zh-CN/start.md#2-%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86).\n### infer\nInput file are the file list for infering, for example:\n```\nS1_Cheese_C1.npy\nS1_CofHoney_C1.npy\nS1_Coffee_C1.npy\nS1_Hotdog_C1.npy\n...\n```\n```bash\npython3.7 tools/predict.py --input_file data/gtea/splits/test.split1.bundle \\"
        },
        {
            "comment": "The code is specifying the configuration file, model file, and parameter file for running the MSTCN (Multi-Stage Temporal Convolutional Network) segmentation model. It also sets the GPU usage to True and TensorRT to False.\nExample logs show the results being written into respective text files in the inference/infer_results folder.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/segmentation/mstcn.md\":108-129",
            "content": "                           --config configs/segmentation/ms_tcn/ms_tcn_gtea.yaml \\\n                           --model_file inference/MSTCN/MSTCN.pdmodel \\\n                           --params_file inference/MSTCN/MSTCN.pdiparams \\\n                           --use_gpu=True \\\n                           --use_tensorrt=False\n```\nexample of logs:\n```bash\nresult write in : ./inference/infer_results/S1_Cheese_C1.txt\nresult write in : ./inference/infer_results/S1_CofHoney_C1.txt\nresult write in : ./inference/infer_results/S1_Coffee_C1.txt\nresult write in : ./inference/infer_results/S1_Hotdog_C1.txt\nresult write in : ./inference/infer_results/S1_Pealate_C1.txt\nresult write in : ./inference/infer_results/S1_Peanut_C1.txt\nresult write in : ./inference/infer_results/S1_Tea_C1.txt\n```\n## Reference\n- [MS-TCN: Multi-Stage Temporal Convolutional Network for Action Segmentation](https://arxiv.org/pdf/1903.01945.pdf), Y. Abu Farha and J. Gall."
        }
    ]
}