{
    "summary": "The code imports modules, sets up environment, creates an ONNX predictor for video object detection, and performs inference on batches of input files while supporting benchmarking if enabled.",
    "details": [
        {
            "comment": "This code imports necessary modules and defines a function for parsing command-line arguments. It sets up the environment to execute PaddleVideo Inference model scripts. The code also includes license information, ensuring compliance with the Apache License, Version 2.0.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/paddle2onnx/predict_onnx.py\":0-30",
            "content": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport argparse\nimport os\nimport sys\nfrom os import path as osp\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(os.path.abspath(os.path.join(__dir__, '../../tools')))\nfrom utils import build_inference_helper, get_config\ndef parse_args():\n    def str2bool(v):\n        return v.lower() in (\"true\", \"t\", \"1\")\n    # general params\n    parser = argparse.ArgumentParser(\"PaddleVideo Inference model script\")"
        },
        {
            "comment": "This code snippet is parsing command line arguments for config file, input file path, and ONNX model file path. It also includes parameters for ONNX prediction like batch size, use of GPU, precision, IR optimization, enable benchmark, and CPU threads.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/paddle2onnx/predict_onnx.py\":31-53",
            "content": "    parser.add_argument('-c',\n                        '--config',\n                        type=str,\n                        default='configs/example.yaml',\n                        help='config file path')\n    parser.add_argument(\"-i\", \"--input_file\", type=str, help=\"input file path\")\n    parser.add_argument(\"--onnx_file\", type=str, help=\"onnx model file path\")\n    # params for onnx predict\n    parser.add_argument(\"-b\", \"--batch_size\", type=int, default=1)\n    parser.add_argument(\"--use_gpu\",\n                        type=str2bool,\n                        default=False,\n                        help=\"set to False when using onnx\")\n    parser.add_argument(\"--precision\", type=str, default=\"fp32\")\n    parser.add_argument(\"--ir_optim\", type=str2bool, default=True)\n    parser.add_argument(\"--enable_benchmark\",\n                        type=str2bool,\n                        default=False,\n                        help=\"set to False when using onnx\")\n    parser.add_argument(\"--cpu_threads\", type=int, default=4)\n    return parser.parse_args()"
        },
        {
            "comment": "The code defines a function to create an ONNX predictor by loading an ONNX file and setting configuration options. It also includes functions for parsing file paths and handling command-line arguments. This code is used for onnx model inference, specifically for video object detection tasks. The main function calls other utility functions to parse the input file path and load configuration settings before executing the actual prediction using the created ONNX predictor.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/paddle2onnx/predict_onnx.py\":56-91",
            "content": "def create_onnx_predictor(args, cfg=None):\n    import onnxruntime as ort\n    onnx_file = args.onnx_file\n    config = ort.SessionOptions()\n    if args.use_gpu:\n        raise ValueError(\n            \"onnx inference now only supports cpu! please set `use_gpu` to False.\"\n        )\n    else:\n        config.intra_op_num_threads = args.cpu_threads\n        if args.ir_optim:\n            config.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n    predictor = ort.InferenceSession(onnx_file, sess_options=config)\n    return config, predictor\ndef parse_file_paths(input_path: str) -> list:\n    if osp.isfile(input_path):\n        files = [\n            input_path,\n        ]\n    else:\n        files = os.listdir(input_path)\n        files = [\n            file for file in files\n            if (file.endswith(\".avi\") or file.endswith(\".mp4\"))\n        ]\n        files = [osp.join(input_path, file) for file in files]\n    return files\ndef main():\n    \"\"\"predict using onnx model\n    \"\"\"\n    args = parse_args()\n    cfg = get_config(args.config, show=False)"
        },
        {
            "comment": "This code builds an inference helper, creates an ONNX predictor, gets input and output names, processes file paths, performs benchmarking, and initializes an auto log for the given model.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/paddle2onnx/predict_onnx.py\":93-121",
            "content": "    model_name = cfg.model_name\n    print(f\"Inference model({model_name})...\")\n    InferenceHelper = build_inference_helper(cfg.INFERENCE)\n    inference_config, predictor = create_onnx_predictor(args)\n    # get input_tensor and output_tensor\n    input_names = predictor.get_inputs()[0].name\n    output_names = predictor.get_outputs()[0].name\n    # get the absolute file path(s) to be processed\n    files = parse_file_paths(args.input_file)\n    if args.enable_benchmark:\n        test_video_num = 12\n        num_warmup = 3\n        # instantiate auto log\n        try:\n            import auto_log\n        except ImportError as e:\n            print(f\"{e}, [git+https://github.com/LDOUBLEV/AutoLog] \"\n                  f\"package and it's dependencies is required for \"\n                  f\"python-inference when enable_benchmark=True.\")\n        pid = os.getpid()\n        autolog = auto_log.AutoLogger(\n            model_name=cfg.model_name,\n            model_precision=args.precision,\n            batch_size=args.batch_size,\n            data_shape=\"dynamic\","
        },
        {
            "comment": "Code snippet performs video inference on batches of input files using a predictor. It preprocesses the batch inputs, runs inference for each batch, and records pre-processing and inference time costs if benchmarking is enabled.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/paddle2onnx/predict_onnx.py\":122-152",
            "content": "            save_path=\"./output/auto_log.lpg\",\n            inference_config=inference_config,\n            pids=pid,\n            process_name=None,\n            gpu_ids=None,\n            time_keys=['preprocess_time', 'inference_time', 'postprocess_time'],\n            warmup=num_warmup)\n        files = [args.input_file for _ in range(test_video_num + num_warmup)]\n    # Inferencing process\n    batch_num = args.batch_size\n    for st_idx in range(0, len(files), batch_num):\n        ed_idx = min(st_idx + batch_num, len(files))\n        # auto log start\n        if args.enable_benchmark:\n            autolog.times.start()\n        # Pre process batched input\n        batched_inputs = InferenceHelper.preprocess_batch(files[st_idx:ed_idx])\n        # get pre process time cost\n        if args.enable_benchmark:\n            autolog.times.stamp()\n        # run inference\n        batched_outputs = predictor.run(\n            output_names=[output_names],\n            input_feed={input_names: batched_inputs[0]})\n        # get inference process time cost"
        },
        {
            "comment": "The code segment is controlling the benchmark execution. If `args.enable_benchmark` is True, it stamps the current time using autolog, then calls postprocess function on batched outputs with `not args.enable_benchmark`. After that, it ends the timer using autolog and reports the benchmark log if `args.enable_benchmark` is still True.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/paddle2onnx/predict_onnx.py\":153-170",
            "content": "        if args.enable_benchmark:\n            autolog.times.stamp()\n        InferenceHelper.postprocess(batched_outputs, not args.enable_benchmark)\n        # get post process time cost\n        if args.enable_benchmark:\n            autolog.times.end(stamp=True)\n        # time.sleep(0.01)  # sleep for T4 GPU\n    # report benchmark log if enabled\n    if args.enable_benchmark:\n        autolog.report()\nif __name__ == \"__main__\":\n    main()"
        }
    ]
}