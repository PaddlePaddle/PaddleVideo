{
    "summary": "ADDS-DepthNet code estimates depth using day and night images, requiring scikit-image and matplotlib, utilizes Oxford RobotCar dataset, offers Resnet18_Imagenet pre-trained model addition, and provides training, testing instructions, and download URL. It demonstrates PaddlePaddle's predict.py tool for inference and saves results as pseudo-colored depth maps with two input images (RGB and depth estimation).",
    "details": [
        {
            "comment": "This code is for the ADDS-DepthNet model, which is based on a self-supervised monocular depth estimation paper by Baidu Robotics and Autonomous Driving Laboratory. The code utilizes day and night images to reproduce the model and achieve advanced depth estimation results on the Oxford RobotCar dataset, mitigating the impact of lighting changes between day and night images. Additional dependencies like scikit-image and matplotlib are required before using the model.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/estimation/adds.md\":0-22",
            "content": "[Simplified Chinese](../../../zh-CN/model_zoo/estimation/adds.md) | English\n# ADDS-DepthNet model\n## content\n- [Introduction](#Introduction)\n- [Data](#Data)\n- [Train](#Train)\n- [Test](#Test)\n- [Inference](#Inference)\n- [Reference](#Reference)\nBefore getting started, you need to install additional dependencies as follows:\n```bash\npython -m pip install scikit-image\npython -m pip install matplotlib\n```\n## Introduction\nThis model is based on the ICCV 2021 paper **[Self-supervised Monocular Depth Estimation for All Day Images using Domain Separation](https://arxiv.org/abs/2108.07628)** of Baidu Robotics and Autonomous Driving Laboratory,\nThe self-supervised monocular depth estimation model based on day and night images is reproduced, which utilizes the complementary nature of day and night image data, and slows down the large domain shift of day and night images and the accuracy of depth estimation caused by lighting changes. Impact, the most advanced depth estimation results of all-sky images have been achieved on the challenging Oxford RobotCar data set."
        },
        {
            "comment": "This code provides instructions for downloading and adding a pre-trained model to the Oxford RobotCar dataset. It mentions the necessary steps to download the pre-training model, Resnet18_Imagenet.pdparams, using the wget command and specifying its path in the adds.yaml file. The code also highlights the importance of filling in the correct fields in the configuration file for proper association with the relevant model types and frameworks.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/estimation/adds.md\":25-48",
            "content": "## Data\nFor data download and preparation of Oxford RobotCar dataset, please refer to [Oxford RobotCar dataset data preparation](../../dataset/Oxford_RobotCar.md)\n## Train\n### Oxford RobotCar dataset training\n#### Download and add pre-trained models\n1. Download the image pre-training model [resnet18.pdparams](https://videotag.bj.bcebos.com/PaddleVideo-release2.2/Resnet18_Imagenet.pdparams) as Backbone initialization parameters, or download through the wget command\n   ```bash\n   wget -P ./data https://videotag.bj.bcebos.com/PaddleVideo-release2.2/Resnet18_Imagenet.pdparams\n   ```\n2. Open `PaddleVideo/configs/estimation/adds/adds.yaml`, and fill in the downloaded weight storage path below `pretrained:`\n    ```yaml\n    MODEL: #MODEL field\n        framework: \"DepthEstimator\" #Mandatory, indicate the type of network, associate to the'paddlevideo/modeling/framework/'.\n        backbone: #Mandatory, indicate the type of backbone, associate to the'paddlevideo/modeling/backbones/'.\n            name: 'ADDS_DepthNet'"
        },
        {
            "comment": "This code snippet provides instructions for training and testing the ADDS-DepthNet model using the Oxford RobotCar dataset. The provided commands initiate the training process with specific configuration file (`configs/estimation/adds/adds.yaml`) and seed value (20). Testing involves running separate commands to test day and night data sets, then recording their respective indicators. A download URL for a pre-trained model is also provided.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/estimation/adds.md\":49-71",
            "content": "            pretrained: fill in the path here\n    ```\n#### Start training\n- The Oxford RobotCar dataset uses a single card for training, and the starting command for the training method is as follows:\n    ```bash\n    python3.7 main.py --validate -c configs/estimation/adds/adds.yaml --seed 20\n    ```\n## Test\n- The ADDS-DepthNet model is verified synchronously during training (only the day or night data is verified). You can find the keyword `best` in the training log to obtain the model test accuracy. The log example is as follows:\n  ```bash\n  Already save the best model (rmse)8.5531\n  ```\n- Because the model can only test one day or night data set at a given path in the yaml file at a time, to get the complete test score at the beginning of this document, you need to run 4 test commands and record their indicators ( 40m during the day, 60m during the day, 40m at night, 60m at night)\n- Download URL of the trained model: [ADDS_car.pdparams](https://videotag.bj.bcebos.com/PaddleVideo-release2.2/ADDS_car.pdparams)"
        },
        {
            "comment": "The code provides test commands for running the ADDS model on the Oxford RobotCar dataset with varying maximum ground truth depth limits and different light conditions (night and daytime). It uses Python 3.7 to execute the main.py file from the PaddleVideo library, configs/estimation/adds/adds.yaml configuration, and specific dataset files for testing.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/estimation/adds.md\":73-89",
            "content": "- The test commands are as follows:\n  ```bash\n  # Night 40m\n  python3.7 main.py --test -c configs/estimation/adds/adds.yaml -w \"output/ADDS/ADDS_best.pdparams\" -o DATASET.test.file_path=\"data/oxford/splits/oxford_day/val_night_files.txt\" -o MODEL.head.max_gt_depth=40\n  # Night 60m\n  python3.7 main.py --test -c configs/estimation/adds/adds.yaml -w \"output/ADDS/ADDS_best.pdparams\" -o DATASET.test.file_path=\"data/oxford/splits/oxford_day/val_night_files.txt\" -o MODEL.head.max_gt_depth=60\n  # Daytime 40m\n  python3.7 main.py --test -c configs/estimation/adds/adds.yaml -w \"output/ADDS/ADDS_best.pdparams\" -o DATASET.test.file_path=\"data/oxford/splits/oxford_day/val_day_files.txt\" -o MODEL.head.max_gt_depth=40\n  # Daytime 60m\n  python3.7 main.py --test -c configs/estimation/adds/adds.yaml -w \"output/ADDS/ADDS_best.pdparams\" -o DATASET.test.file_path=\"data/oxford/splits/oxford_day/val_day_files.txt\" -o MODEL.head.max_gt_depth=60\n  ```\n    The test indicators on the validation dataset of Oxford RobotCar dataset are as follows:"
        },
        {
            "comment": "The code presents a table comparing performance metrics of different models under various conditions. It shows the version, maximum depth, and several error measures for each model. The table also includes whether or not the delta value is less than 1.25 raised to different powers. The text describes how to run a command to export an inference model using Python script with specific configuration file, pre-trained parameters, and output directory.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/estimation/adds.md\":91-106",
            "content": "  | version | Max Depth | Abs Rel | Sq Rel | RMSE | RMSE log | <img src=\"https://latex.codecogs.com/svg.image?\\delta&space;<&space;1.25&space;\" title=\"\\delta < 1.25 \" /> | <img src=\"https://latex.codecogs.com/svg.image?\\delta&space;<&space;1.25^2\" title=\"\\delta < 1.25^2\" /> | <img src=\"https://latex.codecogs.com/svg.image?\\delta&space;<&space;1.25^3\" title=\"\\delta < 1.25^3\" /> |\n  | ----------- | --------- | ------- | ------ | ----- | ------- | ----------------- |------------------- | ------------------- |\n  | ours(night) | 40 | 0.209 | 1.741 | 6.031 | 0.243 | 0.708 | 0.923 | 0.975 |\n  | ours(night) | 60 | 0.207 | 2.052 | 7.888 | 0.258 | 0.686 | 0.909 | 0.970 |\n  | ours(day) | 40 | 0.114 | 0.574 | 3.411 | 0.157 | 0.860 | 0.977 | 0.993 |\n  | ours(day) | 60 | 0.119 | 0.793 | 4.842 | 0.173 | 0.838 | 0.967 | 0.991 |\n## Inference\n### Export inference model\n```bash\npython3.7 tools/export_model.py -c configs/estimation/adds/adds.yaml -p data/ADDS_car.pdparams -o inference/ADDS\n```\nThe above command will"
        },
        {
            "comment": "This code snippet demonstrates the usage of PaddlePaddle's predict.py tool for model inference. It uses a pre-trained model, ADDS, to estimate depth maps from input images. The model files are specified using the --model_file and --params_files parameters, while the input image file is provided with --input_file. The command also includes options for GPU usage (--use_gpu) and TensorRT acceleration (--use_tensorrt). The inference results will be saved as pseudo-colored depth maps by default.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/estimation/adds.md\":106-123",
            "content": " generate the model structure file `ADDS.pdmodel` and model weight files `ADDS.pdiparams` and `ADDS.pdiparams.info` files needed for prediction, all of which are stored in the `inference/ADDS/` directory\nFor the meaning of each parameter in the above bash command, please refer to [Model Inference Method](https://github.com/PaddlePaddle/PaddleVideo/blob/release/2.0/docs/en/start.md#2-%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86)\n### Use predictive engine inference\n```bash\npython3.7 tools/predict.py --input_file data/example.png \\\n                           --config configs/estimation/adds/adds.yaml \\\n                           --model_file inference/ADDS/ADDS.pdmodel \\\n                           --params_file inference/ADDS/ADDS.pdiparams \\\n                           --use_gpu=True \\\n                           --use_tensorrt=False\n```\nAt the end of the inference, the depth map estimated by the model will be saved in pseudo-color by default.\nThe following is a sample picture and the corresponding predicted depth map\uff1a"
        },
        {
            "comment": "The code includes two images, one for regular RGB image and the other for depth estimation from the paper \"Self-supervised Monocular Depth Estimation for All Day Images using Domain Separation\" by Liu et al.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/english_documents/model_zoo/estimation/adds.md\":125-132",
            "content": "<img src=\"../../../images/oxford_image.png\" width = \"512\" height = \"256\" alt=\"image\" align=center />\n<img src=\"../../../images/oxford_image_depth.png\" width = \"512\" height = \"256\" alt=\"depth\" align=center />\n## Reference\n- [Self-supervised Monocular Depth Estimation for All Day Images using Domain Separation](https://arxiv.org/abs/2108.07628), Liu, Lina and Song, Xibin and Wang, Mengmeng and Liu, Yong and Zhang, Liangjun"
        }
    ]
}