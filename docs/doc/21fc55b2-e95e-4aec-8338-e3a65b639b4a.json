{
    "summary": "The PaddleVideo code offers video processing functions, including a VisionTransformer class. It initializes and applies the model using parameters, transformations, and blocks while setting up components for future use.",
    "details": [
        {
            "comment": "This code snippet is from the PaddleVideo library and contains a copyright notice, license information, and several helper functions. The VisionTransformer class will be defined later in the file, which serves as a backbone model for video processing tasks. The code defines constants for zero and one values, a function to convert a single value into a tuple of length 2 (to_2tuple), and a drop path function that applies dropout to inputs with a specified probability during training.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/vit.py\":0-36",
            "content": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom collections.abc import Callable\nimport numpy as np\nimport paddle\nimport paddle.nn as nn\nimport paddle.nn.functional as F\nfrom paddle.nn.initializer import Constant\nfrom ...utils import load_ckpt\nfrom ..registry import BACKBONES\nfrom ..weight_init import trunc_normal_\n__all__ = ['VisionTransformer']\nzeros_ = Constant(value=0.)\nones_ = Constant(value=1.)\ndef to_2tuple(x):\n    return tuple([x] * 2)\ndef drop_path(x, drop_prob=0., training=False):"
        },
        {
            "comment": "This code defines three classes: \"DropPath\", \"Identity\". The DropPath class implements dropout paths (Stochastic Depth) for each sample in the main path of residual blocks. It takes a single parameter, 'drop_prob', to control the probability of dropping out features. If 'drop_prob' is 0 or not training, it returns the input unchanged. The Identity class simply returns its input without any transformation.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/vit.py\":37-64",
            "content": "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n    # issuecomment-532968956 ...\n    See discussion: https://github.com/tensorflow/tpu/issues/494\n    \"\"\"\n    if drop_prob == 0. or not training:\n        return x\n    keep_prob = paddle.to_tensor(1 - drop_prob, dtype=x.dtype)\n    shape = (paddle.shape(x)[0], ) + (1, ) * (x.ndim - 1)\n    random_tensor = keep_prob + paddle.rand(shape).astype(x.dtype)\n    random_tensor = paddle.floor(random_tensor)  # binarize\n    output = x.divide(keep_prob) * random_tensor\n    return output\nclass DropPath(nn.Layer):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n    \"\"\"\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\nclass Identity(nn.Layer):"
        },
        {
            "comment": "The code defines three classes: Identity, Mlp, and Attention. Identity is a simple class that returns its input unchanged. Mlp stands for Multilayer Perceptron, and it's a feed-forward neural network layer. Attention is a class for implementing attention mechanisms in the model. Both Mlp and Attention classes take inputs and return outputs after applying their respective operations.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/vit.py\":65-103",
            "content": "    def __init__(self):\n        super(Identity, self).__init__()\n    def forward(self, input):\n        return input\nclass Mlp(nn.Layer):\n    def __init__(self,\n                 in_features,\n                 hidden_features=None,\n                 out_features=None,\n                 act_layer=nn.GELU,\n                 drop=0.0):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\nclass Attention(nn.Layer):\n    def __init__(self,\n                 dim,\n                 num_heads=8,\n                 qkv_bias=False,\n                 qk_scale=None,\n                 attn_drop=0.0,\n                 proj_drop=0.0):"
        },
        {
            "comment": "This code initializes a multi-head attention layer, and defines the forward pass. It reshapes input into query (Q), key (K), and value (V) matrices, calculates attention scores, applies dropout, and reconstructs output using residual connections and layer normalization. The `Block` class is also defined for building a Vision Transformer model.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/vit.py\":104-137",
            "content": "        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim**-0.5\n        self.qkv = nn.Linear(dim, dim * 3, bias_attr=qkv_bias)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n        self.attn_drop = nn.Dropout(attn_drop)\n    def forward(self, x):\n        N, C = x.shape[1:]\n        qkv = self.qkv(x).reshape(\n            (-1, N, 3, self.num_heads, C // self.num_heads)).transpose(\n                (2, 0, 3, 1, 4))\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        attn = (q.matmul(k.transpose((0, 1, 3, 2)))) * self.scale\n        attn = nn.functional.softmax(attn, axis=-1)\n        attn = self.attn_drop(attn)\n        x = (attn.matmul(v)).transpose((0, 2, 1, 3)).reshape((-1, N, C))\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\nclass Block(nn.Layer):\n    def __init__(self,\n                 dim,\n                 num_heads,\n                 mlp_ratio=4.0,\n                 qkv_bias=False,"
        },
        {
            "comment": "This function is initializing a backbone model with specified parameters. It takes in arguments like attention_type, norm_layer, and others to define the model's layers, including its attention layer. If norm_layer is a string, it uses the given string as the normalization layer; if it's a Callable, it uses that function as the normalization layer. The code also checks if the attention type is 'divided_space_time'.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/vit.py\":138-165",
            "content": "                 qk_scale=None,\n                 drop=0.0,\n                 attn_drop=0.0,\n                 drop_path=0.1,\n                 act_layer=nn.GELU,\n                 norm_layer='nn.LayerNorm',\n                 epsilon=1e-5,\n                 attention_type='divided_space_time'):\n        super().__init__()\n        self.attention_type = attention_type\n        if isinstance(norm_layer, str):\n            self.norm1 = eval(norm_layer)(dim, epsilon=epsilon)\n        elif isinstance(norm_layer, Callable):\n            self.norm1 = norm_layer(dim, epsilon=epsilon)\n        else:\n            raise TypeError(\n                \"The norm_layer must be str or paddle.nn.layer.Layer class\")\n        self.attn = Attention(dim,\n                              num_heads=num_heads,\n                              qkv_bias=qkv_bias,\n                              qk_scale=qk_scale,\n                              attn_drop=attn_drop,\n                              proj_drop=drop)\n        # Temporal Attention Parameters\n        if self.attention_type == 'divided_space_time':"
        },
        {
            "comment": "This code initializes the temporal normalization layer and attention mechanism for a Vision Transformer backbone. It also creates a linear layer and drop path, based on provided configurations. The norm_layer parameter can be a string representing the desired normalization layer or a Callable object. If not a valid type, it raises a TypeError.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/vit.py\":166-184",
            "content": "            if isinstance(norm_layer, str):\n                self.temporal_norm1 = eval(norm_layer)(dim, epsilon=epsilon)\n            elif isinstance(norm_layer, Callable):\n                self.temporal_norm1 = norm_layer(dim, epsilon=epsilon)\n            else:\n                raise TypeError(\n                    \"The norm_layer must be str or paddle.nn.layer.Layer class\")\n            self.temporal_attn = Attention(dim,\n                                           num_heads=num_heads,\n                                           qkv_bias=qkv_bias,\n                                           qk_scale=qk_scale,\n                                           attn_drop=attn_drop,\n                                           proj_drop=drop)\n            self.temporal_fc = nn.Linear(dim, dim)\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else Identity()\n        if isinstance(norm_layer, str):\n            self.norm2 = eval(norm_layer)(dim, epsilon=epsilon)"
        },
        {
            "comment": "The code defines a class and its forward method. It sets the normalization layer, calculates the number of spatial tokens, checks the attention type, applies normalization and MLP layers to the input, and performs divided space-time attention.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/vit.py\":185-209",
            "content": "        elif isinstance(norm_layer, Callable):\n            self.norm2 = norm_layer(dim, epsilon=epsilon)\n        else:\n            raise TypeError(\n                \"The norm_layer must be str or paddle.nn.layer.Layer class\")\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim,\n                       hidden_features=mlp_hidden_dim,\n                       act_layer=act_layer,\n                       drop=drop)\n    def forward(self, x, B, T, W):\n        num_spatial_tokens = (x.shape[1] - 1) // T\n        H = num_spatial_tokens // W\n        if self.attention_type in ['space_only', 'joint_space_time']:\n            x = x + self.drop_path(self.attn(self.norm1(x)))\n            x = x + self.drop_path(self.mlp(self.norm2(x)))\n            return x\n        elif self.attention_type == 'divided_space_time':\n            ########## Temporal ##########\n            xt = x[:, 1:, :]\n            _, _, _, _t, _m = B, H, W, T, xt.shape[-1]\n            xt = xt.reshape([-1, _t, _m])\n            res_temporal = self.drop_path("
        },
        {
            "comment": "This code performs spatial attention in the Vision Transformer model. It creates a cls_token, reshapes the input, concatenates it with the cls_token, and then passes it through a drop path and an attention layer. Finally, it extracts the cls_token for further use.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/vit.py\":210-234",
            "content": "                self.temporal_attn(self.temporal_norm1(xt)))\n            _, _h, _w, _t, _m = B, H, W, T, res_temporal.shape[-1]\n            res_temporal = res_temporal.reshape([-1, _h * _w * _t, _m])\n            res_temporal = self.temporal_fc(res_temporal)\n            xt = x[:, 1:, :] + res_temporal\n            ########## Spatial ##########\n            init_cls_token = x[:, 0, :].unsqueeze(1)\n            cls_token = init_cls_token.tile((1, T, 1))\n            _b, _t, _m = cls_token.shape\n            cls_token = cls_token.reshape([-1, _m]).unsqueeze(1)\n            xs = xt\n            _, _h, _w, _t, _m = B, H, W, T, xs.shape[-1]\n            xs = xs.reshape([-1, _h, _w, _t, _m]).transpose(\n                (0, 3, 1, 2, 4)).reshape([-1, _h * _w, _m])\n            xs = paddle.concat((cls_token, xs), axis=1)\n            res_spatial = self.drop_path(self.attn(self.norm1(xs)))\n            # Taking care of CLS token\n            cls_token = res_spatial[:, 0, :]\n            _, _t, _m = B, T, cls_token.shape[-1]\n            cls_token = cls_token.reshape([-1, _t, _m])"
        },
        {
            "comment": "This code performs averaging across frames, reshapes the spatial features, concatenates initial class token and input sequence, adds a drop path and MLP layer, and returns the output. It also defines PatchEmbed for image to patch embedding.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/vit.py\":235-266",
            "content": "            # averaging for every frame\n            cls_token = paddle.mean(cls_token, axis=1, keepdim=True)\n            res_spatial = res_spatial[:, 1:, :]\n            _, _t, _h, _w, _m = B, T, H, W, res_spatial.shape[-1]\n            res_spatial = res_spatial.reshape([-1, _t, _h, _w, _m]).transpose(\n                (0, 2, 3, 1, 4)).reshape([-1, _h * _w * _t, _m])\n            res = res_spatial\n            x = xt\n            x = paddle.concat((init_cls_token, x), axis=1) + paddle.concat(\n                (cls_token, res), axis=1)\n            # Mlp\n            x = x + self.drop_path(self.mlp(self.norm2(x)))\n            return x\n        else:\n            raise NotImplementedError\nclass PatchEmbed(nn.Layer):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n    def __init__(self,\n                 img_size=224,\n                 patch_size=16,\n                 in_channels=3,\n                 embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] //"
        },
        {
            "comment": "The code defines a VisionTransformer class that takes input patches of an image. It initializes the model parameters such as img_size, patch_size and num_patches. The forward function performs the transformation by projecting the input into embedding space using a convolutional layer. If the input image size does not match the expected model size, it raises an assertion error. This class is registered with BACKBONES for future use.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/vit.py\":267-297",
            "content": "                                                        patch_size[0])\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n        self.proj = nn.Conv2D(in_channels,\n                              embed_dim,\n                              kernel_size=patch_size,\n                              stride=patch_size)\n    def forward(self, x):\n        B, C, T, H, W = x.shape\n        assert H == self.img_size[0] and W == self.img_size[1], \\\n            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = x.transpose((0, 2, 1, 3, 4))\n        x = x.reshape([-1, C, H, W])\n        x = self.proj(x)\n        W = x.shape[-1]\n        x = x.flatten(2).transpose((0, 2, 1))\n        return x, T, W\n@BACKBONES.register()\nclass VisionTransformer(nn.Layer):\n    \"\"\" Vision Transformer with support for patch input\n    \"\"\"\n    def __init__(self,\n                 pretrained=None,\n                 img_size=224,\n                 patch_size=16,"
        },
        {
            "comment": "This code initializes a Vision Transformer (ViT) backbone model with specified parameters such as input dimensions, embedding dimension, depth, number of heads, mlp ratio, and attention type. The code sets up the patch embedding layer, creates a class token, and defines the number of patches based on the input size provided.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/vit.py\":298-326",
            "content": "                 in_channels=3,\n                 embed_dim=768,\n                 depth=12,\n                 num_heads=12,\n                 mlp_ratio=4,\n                 qkv_bias=False,\n                 qk_scale=None,\n                 drop_rate=0.,\n                 attn_drop_rate=0.,\n                 drop_path_rate=0.1,\n                 norm_layer='nn.LayerNorm',\n                 epsilon=1e-5,\n                 num_seg=8,\n                 attention_type='divided_space_time',\n                 **args):\n        super().__init__()\n        self.pretrained = pretrained\n        self.num_seg = num_seg\n        self.attention_type = attention_type\n        self.num_features = self.embed_dim = embed_dim\n        self.patch_embed = PatchEmbed(img_size=img_size,\n                                      patch_size=patch_size,\n                                      in_channels=in_channels,\n                                      embed_dim=embed_dim)\n        num_patches = self.patch_embed.num_patches\n        # Positional Embeddings\n        self.cls_token = self.create_parameter(shape=(1, 1, embed_dim),"
        },
        {
            "comment": "This code initializes various components of a vision transformer model, including positional embeddings (pos_embed), classification token (cls_token), and dropout layers (pos_drop, time_drop). It also creates a LayerList of blocks with specified dimensions and parameters.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/vit.py\":327-349",
            "content": "                                               default_initializer=zeros_)\n        self.pos_embed = self.create_parameter(shape=(1, num_patches + 1,\n                                                      embed_dim),\n                                               default_initializer=zeros_)\n        self.pos_drop = nn.Dropout(p=drop_rate)\n        if self.attention_type != 'space_only':\n            self.time_embed = self.create_parameter(shape=(1, num_seg,\n                                                           embed_dim),\n                                                    default_initializer=zeros_)\n            self.time_drop = nn.Dropout(p=drop_rate)\n        self.add_parameter(\"pos_embed\", self.pos_embed)\n        self.add_parameter(\"cls_token\", self.cls_token)\n        dpr = np.linspace(0, drop_path_rate, depth)\n        self.blocks = nn.LayerList([\n            Block(dim=embed_dim,\n                  num_heads=num_heads,\n                  mlp_ratio=mlp_ratio,\n                  qkv_bias=qkv_bias,\n                  qk_scale=qk_scale,"
        },
        {
            "comment": "This code initializes a Vision Transformer (ViT) model. It creates a series of blocks with specified dimensions, applies normalization layers, and initializes the weight values using truncated normal distribution. Additionally, if pre-trained weights are provided, it loads them into the model.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/vit.py\":350-378",
            "content": "                  drop=drop_rate,\n                  attn_drop=attn_drop_rate,\n                  drop_path=dpr[i],\n                  norm_layer=norm_layer,\n                  epsilon=epsilon,\n                  attention_type=self.attention_type) for i in range(depth)\n        ])\n        self.norm = eval(norm_layer)(embed_dim, epsilon=epsilon)\n    def init_weights(self):\n        \"\"\"First init model's weight\"\"\"\n        trunc_normal_(self.pos_embed, std=0.02)\n        trunc_normal_(self.cls_token, std=0.02)\n        self.apply(self._init_fn)\n        if self.attention_type == 'divided_space_time':\n            i = 0\n            for m in self.blocks.sublayers(include_self=True):\n                m_str = str(m)\n                if 'Block' in m_str:\n                    if i > 0:\n                        zeros_(m.temporal_fc.weight)\n                        zeros_(m.temporal_fc.bias)\n                    i += 1\n        \"\"\"Second, if provide pretrained ckpt, load it\"\"\"\n        if isinstance(\n                self.pretrained, str\n        ) and self.pretrained.strip() != \"\":  # load pretrained weights"
        },
        {
            "comment": "This code initializes the forward function of a Vision Transformer (ViT) model. It extracts features from input images, adds positional embeddings, and handles batch size changes. The trunc_normal_ and zeros_ functions are used to initialize weights and biases for layers like Linear and LayerNorm, respectively.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/vit.py\":379-404",
            "content": "            load_ckpt(self,\n                      self.pretrained,\n                      num_patches=self.patch_embed.num_patches,\n                      num_seg=self.num_seg,\n                      attention_type=self.attention_type)\n    def _init_fn(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight)\n            if m.bias is not None:\n                zeros_(m.bias)\n        elif isinstance(m, nn.LayerNorm):\n            ones_(m.weight)\n            zeros_(m.bias)\n    def forward_features(self, x):\n        # B = x.shape[0]\n        B = paddle.shape(x)[0]\n        x, T, W = self.patch_embed(x)  # [BT,nH*nW,F]\n        cls_tokens = self.cls_token.expand((B * T, -1, -1))  # [1,1,F]->[BT,1,F]\n        x = paddle.concat((cls_tokens, x), axis=1)\n        pos_interp = (x.shape[1] != self.pos_embed.shape[1])\n        if pos_interp:\n            pos_embed = self.pos_embed\n            cls_pos_embed = pos_embed[0, 0, :].unsqueeze(0).unsqueeze(1)\n            other_pos_embed = pos_embed[0, 1:, :].unsqueeze(0).transpose("
        },
        {
            "comment": "The code is applying relative position embeddings to the input features (x) for a vision transformer model. It first checks if a specific flag is set, then interpolates other position embeddings based on the size of the input and adds them to class position embeddings. If the flag is not set, it simply adds the position embeddings from the model. Afterward, the code applies time embeddings if the attention type is not \"space_only\".",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/vit.py\":405-429",
            "content": "                (0, 2, 1))\n            P = int(other_pos_embed.shape[2]**0.5)\n            H = x.shape[1] // W\n            other_pos_embed = other_pos_embed.reshape([1, x.shape[2], P, P])\n            new_pos_embed = F.interpolate(other_pos_embed,\n                                          size=(H, W),\n                                          mode='nearest')\n            new_pos_embed = new_pos_embed.flatten(2)\n            new_pos_embed = new_pos_embed.transpose((0, 2, 1))\n            new_pos_embed = paddle.concat((cls_pos_embed, new_pos_embed),\n                                          axis=1)\n            x = x + new_pos_embed\n        else:\n            x = x + self.pos_embed\n        x = self.pos_drop(x)\n        # Time Embeddings\n        if self.attention_type != 'space_only':\n            cls_tokens = x[:B, 0, :].unsqueeze(1) if B > 0 else x.split(\n                T)[0].index_select(paddle.to_tensor([0]), axis=1)\n            x = x[:, 1:]\n            _, _n, _m = x.shape\n            _t = T\n            x = x.reshape([-1, _t, _n, _m]).transpose("
        },
        {
            "comment": "This code performs time embeddings resizing and adds them to the input feature maps. It then flattens the tensor, concatenates class tokens, processes through attention blocks, and finally, for space-only attention type, it makes predictions.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/vit.py\":430-454",
            "content": "                (0, 2, 1, 3)).reshape([-1, _t, _m])\n            # Resizing time embeddings in case they don't match\n            time_interp = (T != self.time_embed.shape[1])\n            if time_interp:  # T' != T\n                time_embed = self.time_embed.transpose((0, 2, 1)).unsqueeze(0)\n                new_time_embed = F.interpolate(time_embed,\n                                               size=(T, x.shape[-1]),\n                                               mode='nearest').squeeze(0)\n                new_time_embed = new_time_embed.transpose((0, 2, 1))\n                x = x + new_time_embed\n            else:\n                x = x + self.time_embed\n            x = self.time_drop(x)\n            _, _t, _m = x.shape\n            x = x.reshape([-1, W * W * T, _m])\n            x = paddle.concat((cls_tokens, x), axis=1)\n        # Attention blocks\n        for blk in self.blocks:\n            x = blk(x, B, T, W)\n        # Predictions for space-only baseline\n        if self.attention_type == 'space_only':\n            _, _n, _m = x.shape"
        },
        {
            "comment": "This code snippet is part of a Vision Transformer (ViT) model implementation. The function averages predictions for every frame and applies normalization before returning the embeddings for each image in the input sequence.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/paddlevideo/modeling/backbones/vit.py\":455-464",
            "content": "            _t = T\n            x = x.reshape([-1, _t, _n, _m])\n            x = paddle.mean(x, 1)  # averaging predictions for every frame\n        x = self.norm(x)\n        return x[:, 0]  # [B,  embed_dim]\n    def forward(self, x):\n        x = self.forward_features(x)\n        return x"
        }
    ]
}