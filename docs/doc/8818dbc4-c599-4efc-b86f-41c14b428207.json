{
    "summary": "The code evaluates precision, recall, and F1 scores for a model's predictions using IoU thresholds and label ranges. It iterates through score thresholds, selects the best F1 score, and saves the results.",
    "details": [
        {
            "comment": "This code imports necessary libraries, defines global variables fps and mode, loads a JSON file containing indexed labels for 8 categories, and initializes a gts_data dictionary with the frame rate (fps) and an empty dictionary to store ground truth data. It also iterates over eval_datasets, label_files, and individual gt data to update fps, populate gts_data, and assign mode for each ground truth item.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/FootballAction/predict/eval.py\":0-35",
            "content": "\"\"\"\nget instance for lstm\n\u6839\u636egts\u8ba1\u7b97\u6bcf\u4e2aproposal_bmn\u7684iou\u3001ioa\u3001label\u7b49\u4fe1\u606f\n\"\"\"\nimport os\nimport sys\nimport json\nimport random\nimport pickle\nimport numpy as np\nimport io\nsys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding = 'utf-8')\ndataset = \"/home/work/datasets\"\nlabel_index_file = './configs/index_label_football_8.json'\neval_datasets = ['EuroCup2016']\nlabel_files = {'train': 'label_cls8_train.json',\n               'validation': 'label_cls8_val.json'}\nglobal fps, mode\nlabel_index = json.load(open(label_index_file, 'rb'))\ndef load_gts():\n    global fps\n    gts_data = {'fps': 0, 'gts': {}}\n    for eval_data in eval_datasets:\n        for item, value in label_files.items():\n            label_file = '{}/{}/{}'.format(dataset, eval_data, value)\n            gts = json.load(open(label_file, 'rb'))\n            gts_data['fps'] = gts['fps']\n            fps = gts['fps']\n            for gt in gts['gts']:\n                gt['mode'] = item\n                basename = '{}/{}/mp4/{}'.format(dataset, eval_data, os.path.basename(gt['url']))"
        },
        {
            "comment": "The code defines functions for evaluating ground truth (GT) labels and computing Intersection over Union (IoU). It also includes a function to convert proposals with score threshold filtering. The IoU function calculates the area of intersection, the area of union, and returns the IoU value. The 'computeIoU' function can be used for both regular and proposal modes. The 'convert_proposal' function sorts boxes based on scores and selects those above a given threshold to generate new proposals. It assigns each proposal an ID and calculates their respective start and end times.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/FootballAction/predict/eval.py\":36-66",
            "content": "                gts_data['gts'][basename] = gt\n    return gts_data['gts']\ndef computeIoU(e1, e2):\n    \"\"\"\n    clc iou and ioa\n    \"\"\"\n    if not (e1['label'] == e2['label'] and e1['basename'] == e2['basename']):\n        return 0.\n    area1 = e1[\"end\"] - e1[\"start\"]\n    area2 = e2[\"end\"] - e2[\"start\"]\n    x1 = np.maximum(e1[\"start\"], e2[\"start\"])\n    x2 = np.minimum(e1[\"end\"], e2[\"end\"])\n    inter = np.maximum(0.0, x2 - x1)\n    iou = 0.0 if (area1 + area2 - inter) == 0 else inter * 1.0 / (area1 + area2 - inter)\n    if not mode == 'proposal':\n        iou = 0.0 if area2 == 0 else inter * 1.0 / area2\n    return iou\ndef convert_proposal(boxes, basename, score_threshold=0.01):\n    boxes = sorted(boxes, key=lambda x:float(x['score']), reverse=True)\n    res = []\n    for box in boxes:\n        if not float(box['score']) >= score_threshold:\n            continue\n        res.append({'basename': basename,\n                    'start': int(float(box['start']) / fps),\n                    'end': int(float(box['end']) / fps),\n                    'label': 0})"
        },
        {
            "comment": "This code is defining a function called convert_classify that takes in boxes, basename, iou_threshold and score_threshold as parameters. The function sorts the boxes based on their classify_score and iou_score in descending order. If both iou_score and classify_score meet the threshold values, it appends the box details to a list named res. It returns this list of results. \n\nThe code also defines another function called convert_groundtruth that takes in boxes, basename and phase as parameters. This function iterates through each box and its corresponding label IDs. If the phase is 'proposal', it assigns a value of 0 to the label variable; otherwise, it assigns the item from box['label_ids']. It appends the result to a list named res.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/FootballAction/predict/eval.py\":67-92",
            "content": "    return res\ndef convert_classify(boxes, basename, iou_threshold, score_threshold):\n    boxes = sorted(boxes, key=lambda x:(float(x['classify_score']), float(x['iou_score'])), reverse=True)\n    def convert_time_to_frame(time_type):\n        return int(time_type)\n        h, m, s = time_type.split(':')\n        return int(h) * 3600 + int(m) * 60 + int(s)\n    res = []\n    for box in boxes:\n        if not (box['iou_score'] >= iou_threshold and\n                box['classify_score'] >= score_threshold):\n            continue\n        res.append({'basename': basename,\n                    'start': convert_time_to_frame(box['start_time']),\n                    'end': convert_time_to_frame(box['end_time']),\n                    'label': box['label_id']})\n    return res\ndef convert_groundtruth(boxes, basename, phase=None):\n    res = []\n    for box in boxes:\n        for item in box['label_ids']:\n            label = 0 if phase == 'proposal' else item\n            res.append({'basename': basename,\n                        'start': box['start_id'],"
        },
        {
            "comment": "This code defines four functions: \"evaluation\", \"print_result\", \"print_head\", and \"computeIoU\". The \"evaluation\" function computes the intersection over union (IoU) between predicted boxes and ground truth boxes. It then passes these results to \"print_head\" and \"print_result\" for displaying progress and final evaluation results, respectively. The other two functions are used internally by the main \"evaluation\" function.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/FootballAction/predict/eval.py\":93-119",
            "content": "                        'end': box['end_id'],\n                        'label': label})\n    return res\ndef print_head(iou):\n    print(\"\\nioa = {:.1f}\".format(iou))\n    res_str = ''\n    for item in ['label_name']:\n        res_str += '{:<12s}'.format(item)\n    for item in ['label_id', 'precision', 'recall', 'hit_prop', 'num_prop', 'hit_gts', 'num_gts']:\n        res_str += '{:<10s}'.format(item)\n    print(res_str)\ndef print_result(res_dict, label='avg'):\n    if label == 'avg':\n        res_str = '{:<22s}'.format(str(label))\n    else:\n        res_str = '{0:{2}<6s}{1:<10s}'.format(label_index[str(label)], str(label), chr(12288))\n    for item in ['prec', 'recall']:\n        res_str += '{:<10.4f}'.format(res_dict[item])\n    for item in ['hit_prop', 'num_prop', 'hit_gts', 'num_gts']:\n        res_str += '{:<10d}'.format(res_dict[item])\n    print(res_str)\ndef evaluation(res_boxes, gts_boxes, label_range, iou_range, show_sub = False):\n    iou_map = [computeIoU(resId, gtsId) for resId in res_boxes \\\n                                        for gtsId in gts_boxes]"
        },
        {
            "comment": "This code calculates the precision, recall, and F1 score for a set of predicted boxes and ground truth boxes. It iterates over a range of intersection over union (IOU) thresholds and label ranges to produce average results. The results are stored in dictionaries for each IOU threshold and label range combination.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/FootballAction/predict/eval.py\":120-143",
            "content": "    iou_map = np.array(iou_map).reshape((len(res_boxes), len(gts_boxes)))\n    hit_map_prop_total = np.max(iou_map, axis=1)\n    hit_map_index_total = np.argmax(iou_map, axis=1)\n    res_dict = ['hit_prop', 'num_prop', 'hit_gts', 'num_gts']\n    for iou_threshold in iou_range:\n        if show_sub:\n            print_head(iou_threshold)\n        iou_prop = np.array([k >= iou_threshold for k in hit_map_prop_total])\n        average_results = {}\n        for label_id in label_range:\n            sub_results = {}\n            label_prop = np.array([k['label'] == label_id for k in res_boxes])\n            label_gts = np.array([k['label'] == label_id for k in gts_boxes])\n            sub_results['num_prop'] = sum(label_prop)\n            sub_results['num_gts'] = sum(label_gts)\n            if sub_results['num_prop'] == 0:\n                hit_prop_index = []\n            else:\n                hit_prop_index = label_prop & iou_prop\n            sub_results['hit_prop'] = sum(hit_prop_index)\n            sub_results['hit_gts'] = len(set(hit_map_index_total[hit_prop_index]))"
        },
        {
            "comment": "This code calculates precision and recall values for various subtasks, averages them if applicable, and stores the results in a dictionary. It also prints the result for each subtask if show_sub is set to True.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/FootballAction/predict/eval.py\":145-160",
            "content": "            sub_results['prec'] = 0.0 if sub_results['num_prop'] == 0 \\\n                                      else sub_results['hit_prop'] * 1.0 / sub_results['num_prop']\n            sub_results['recall'] = 0.0 if sub_results['num_gts'] == 0 \\\n                                        else sub_results['hit_gts'] * 1.0 / sub_results['num_gts']\n            if show_sub:\n                print_result(sub_results, label=label_id)\n            for item in res_dict:\n                if not item in average_results:\n                    average_results[item] = 0\n                average_results[item] += sub_results[item]\n        if len(label_range) == 1:   # proposal \u4e0d\u9700\u8981\u8f93\u51faaverage\u503c\n            continue\n        average_results['prec'] = 0.0 if average_results['num_prop'] == 0 \\\n                                      else average_results['hit_prop'] * 1.0 / average_results['num_prop']\n        average_results['recall'] = 0.0 if average_results['num_gts'] == 0 \\\n                                        else average_results['hit_gts'] * 1.0 / average_results['num_gts']"
        },
        {
            "comment": "This code calculates evaluation results for FootballAction model predictions. It checks if the prediction is for a specific evaluation dataset and then processes proposal phase results, extracting bounding box coordinates from predicted proposals, and appending them to res_boxes list. F1 score is calculated based on precision and recall values. The function returns the average evaluation results (F1, precision, recall, IoU) for each video in the predicts dataset.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/FootballAction/predict/eval.py\":161-188",
            "content": "        if show_sub:\n            print_result(average_results)\n        average_results['F1'] = 0.0 if (average_results['prec'] + average_results['recall'] == 0) \\\n                                    else 2 * average_results['prec'] * average_results['recall'] / \\\n                                            (average_results['prec'] + average_results['recall'])\n        return average_results\ndef get_eval_results(predicts, gts_data, phase, iou_threshold = 0.3, score_threshold = 0.3, show_sub = False):\n    global mode\n    mode = phase\n    res_boxes = []\n    gts_boxes = []\n    for ped_data in predicts:\n        basename = ped_data['video_name']\n        # eval sub data\n        such_eval = False\n        for eval_name in eval_datasets:\n            if eval_name in basename:\n                such_eval = True\n                break\n        if not such_eval:\n            continue\n        gts = gts_data[basename]['actions']\n        if phase == 'proposal':\n            res_boxes.extend(convert_proposal(ped_data['bmn_results'], basename, score_threshold))"
        },
        {
            "comment": "The code handles the evaluation of football action predictions. If ground truth is given, it extends proposal boxes and sets label range and iou range accordingly; otherwise, it extends classify results, ground truth boxes, and sets label range and iou range. It then calculates evaluation results using the specified functions. The code also allows for testing different iou_threshold and score_threshold combinations to find the best ones.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/FootballAction/predict/eval.py\":189-217",
            "content": "            gts_boxes.extend(convert_groundtruth(gts, basename, phase='proposal'))\n            label_range = [0]\n            iou_range = np.arange(0.1, 1, 0.1)\n        else:\n            res_boxes.extend(convert_classify(ped_data['action_results'], basename, iou_threshold, score_threshold))\n            gts_boxes.extend(convert_groundtruth(gts, basename))\n            label_range = range(1, len(label_index))\n            iou_range = np.arange(0.5, 0.6, 0.1)\n    eval_results = evaluation(res_boxes, gts_boxes, label_range, iou_range, show_sub = show_sub)\n    return eval_results\nif __name__ == \"__main__\":\n    result_file = sys.argv[1]\n    predicts = json.load(open(result_file, 'r', encoding='utf-8'))\n    gts_data = load_gts()\n    get_eval_results(predicts, gts_data, 'proposal', \n                     score_threshold = 0.03,\n                     show_sub = True)\n    #get_eval_results(predicts, gts_data, 'actions')\n    best_F1 = -0.1\n    best_res = {}\n    best_iou_threshold = 0.\n    best_score_threshold = 0.\n    for iou_threshold in np.arange(0.1, 0.9, 0.1):"
        },
        {
            "comment": "The code iterates through score thresholds and calculates the average results for each threshold. It selects the best F1 score, stores corresponding iou_threshold and score_threshold. Finally, it prints these values along with a headline and a detailed result, then saves the best results by running the get_eval_results function again.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/applications/FootballAction/predict/eval.py\":218-236",
            "content": "        for score_threshold in np.arange(0.1, 1, 0.1):\n            avg_res = get_eval_results(predicts, gts_data, 'actions', \n                                       iou_threshold = iou_threshold,\n                                       score_threshold = score_threshold,\n                                       show_sub = False)\n            if best_F1 < avg_res['F1']:\n                best_F1 = avg_res['F1']\n                best_res = avg_res\n                best_iou_threshold = iou_threshold\n                best_score_threshold = score_threshold\n    print(\"best iou threshold = {:.1f}\".format(best_iou_threshold))\n    print(\"best score threshold = {:.1f}\".format(best_score_threshold))\n    print('best F1 score = {:.4f}'.format(best_F1))\n    print_head(0.5)\n    print_result(best_res)\n    get_eval_results(predicts, gts_data, 'actions', iou_threshold = best_iou_threshold,\n                                                    score_threshold = best_score_threshold,\n                                                    show_sub = True)"
        }
    ]
}