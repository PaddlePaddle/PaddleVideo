{
    "summary": "This code accelerates PaddleServing installation with Docker, supports Linux and GPU, simplifies action recognition service deployment, and provides a C++ serving environment setup guide.",
    "details": [
        {
            "comment": "This code introduces the installation process for PaddleServing. It uses Docker to pull a GPU-based docker environment and creates a Serving-based Docker named \"test\". The port 9292 is mapped to access the serving environment, and this setup supports Linux platforms, with Windows currently unsupported.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/cpp_serving/readme_en.md\":0-16",
            "content": "English | [\u7b80\u4f53\u4e2d\u6587](./readme.md)\n# Model service deployment\n## Introduction\n[Paddle Serving](https://github.com/PaddlePaddle/Serving) aims to help deep learning developers easily deploy online prediction services, support one-click deployment of industrial-grade service capabilities, high concurrency between client and server Efficient communication and support for developing clients in multiple programming languages.\nThis section takes the HTTP prediction service deployment as an example to introduce how to use PaddleServing to deploy the model service in PaddleVideo. Currently, only Linux platform deployment is supported, and Windows platform is not currently supported.\n## Serving installation\nThe Serving official website recommends using docker to install and deploy the Serving environment. First, you need to pull the docker environment and create a Serving-based docker.\n```bash\n# start GPU docker\ndocker pull paddlepaddle/serving:0.7.0-cuda10.2-cudnn7-devel\nnvidia-docker run -p 9292:9292 --name test -dit paddlepaddle/serving:0.7.0-cuda10.2-cudnn7-devel bash"
        },
        {
            "comment": "Code installs necessary packages for PaddlePaddle serving client, app, server (CPU/GPU) and PaddlePaddle (CPU/GPU) in a Docker container using pip. The GPU versions are specified with different CUDA and TensorRT versions.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/cpp_serving/readme_en.md\":17-40",
            "content": "nvidia-docker exec -it test bash\n# start CPU docker\ndocker pull paddlepaddle/serving:0.7.0-devel\ndocker run -p 9292:9292 --name test -dit paddlepaddle/serving:0.7.0-devel bash\ndocker exec -it test bash\n```\nAfter entering docker, you need to install Serving-related python packages.\n```bash\npython3.7 -m pip install paddle-serving-client==0.7.0\npython3.7 -m pip install paddle-serving-app==0.7.0\n#If it is a CPU deployment environment:\npython3.7 -m pip install paddle-serving-server==0.7.0 #CPU\npython3.7 -m pip install paddlepaddle==2.2.0 # CPU\n#If it is a GPU deployment environment\npython3.7 -m pip install paddle-serving-server-gpu==0.7.0.post102 # GPU with CUDA10.2 + TensorRT6\npython3.7 -m pip install paddlepaddle-gpu==2.2.0 # GPU with CUDA10.2\n#Other GPU environments need to confirm the environment and then choose which one to execute\npython3.7 -m pip install paddle-serving-server-gpu==0.7.0.post101 # GPU with CUDA10.1 + TensorRT6\npython3.7 -m pip install paddle-serving-server-gpu==0.7.0.post112 # GPU with CUDA11.2 + TensorRT8"
        },
        {
            "comment": "This code snippet provides instructions for speeding up the installation process and deploying an action recognition service using PaddleServing. It explains how to convert a saved inference model into a Serving model, using PP-TSM as an example.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/cpp_serving/readme_en.md\":41-64",
            "content": "```\n* If the installation speed is too slow, you can change the source through `-i https://pypi.tuna.tsinghua.edu.cn/simple` to speed up the installation process.\n* For more environment and corresponding installation packages, see: https://github.com/PaddlePaddle/Serving/blob/v0.9.0/doc/Install_Linux_Env_CN.md\n## Action recognition service deployment\n### Model conversion\nWhen using PaddleServing for service deployment, you need to convert the saved inference model into a Serving model. The following uses the PP-TSM model as an example to introduce how to deploy the action recognition service.\n- Download PP-TSM inference model and convert to Serving model:\n  ```bash\n  # Enter PaddleVideo directory\n  cd PaddleVideo\n  # Download the inference model and extract it to ./inference\n  mkdir ./inference\n  pushd ./inference\n  wget https://videotag.bj.bcebos.com/PaddleVideo-release2.3/ppTSM.zip\n  unzip ppTSM.zip\n  popd\n  # Convert to Serving model\n  pushd deploy/cpp_serving\n  python3.7 -m paddle_serving_client.convert \\"
        },
        {
            "comment": "The code is specifying the directory, model filename, and parameters filename for a PaddleVideo inference program conversion. It also sets the serving server and client executables to be used after the conversion. The `dirname` parameter holds the storage path of the converted model files. If no specific filenames are provided (model_filename or params_filename), the code defaults to \"None\" which will use default filenames (\"__model__\" and None respectively). The serving server and client executables are specified in the code to be used after the conversion process, allowing the model to be served for inference.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/cpp_serving/readme_en.md\":65-78",
            "content": "  --dirname ../../inference/ppTSM \\\n  --model_filename ppTSM.pdmodel \\\n  --params_filename ppTSM.pdiparams \\\n  --serving_server ./ppTSM_serving_server \\\n  --serving_client ./ppTSM_serving_client\n  popd\n  ```\n  | parameter | type | default value | description |\n  | ----------------- | ---- | ------------------ | ------- -------------------------------------------------- --- |\n  | `dirname` | str | - | The storage path of the model file to be converted. The program structure file and parameter file are saved in this directory. |\n  | `model_filename` | str | None | The name of the file storing the model Inference Program structure that needs to be converted. If set to None, use `__model__` as the default filename |\n  | `params_filename` | str | None | File name where all parameters of the model to be converted are stored. It needs to be specified if and only if all model parameters are stored in a single binary file. If the model parameters are stored in separate files, set it to None |\n  | `serving_"
        },
        {
            "comment": "The code specifies two paths, \"serving_server\" and \"serving_client\", representing the storage locations for model files and configuration files. After model conversion, it generates two folders with associated file formats in the specified folder. Upon obtaining the model files, modify two specific text files to change `alias_name` under `fetch_var`.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/cpp_serving/readme_en.md\":78-93",
            "content": "server` | str | `\"serving_server\"` | The storage path of the converted model files and configuration files. Default is serving_server |\n  | `serving_client` | str | `\"serving_client\"` | The converted client configuration file storage path. Default is serving_client |\n- After the inference model conversion is completed, two folders, `ppTSM_serving_client` and `ppTSM_serving_server` will be generated under the `deploy/cpp_serving` folder, with the following formats:\n  ```bash\n  PaddleVideo/deploy/cpp_serving\n  \u251c\u2500\u2500 ppTSM_serving_client\n  \u2502   \u251c\u2500\u2500 serving_client_conf.prototxt\n  \u2502   \u2514\u2500\u2500 serving_client_conf.stream.prototxt\n  \u2514\u2500\u2500 ppTSM_serving_server\n      \u251c\u2500\u2500 ppTSM.pdiparams\n      \u251c\u2500\u2500 ppTSM.pdmodel\n      \u251c\u2500\u2500 serving_server_conf.prototxt\n      \u2514\u2500\u2500 serving_server_conf.stream.prototxt\n  ```\n  After getting the model file, you need to modify `serving_client_conf.prototxt` under `ppTSM_serving_client` and `serving_server_conf.prototxt` under `ppTSM_serving_server` respectively, and change `alias_name` under `fetch_var` in both files to `outputs`"
        },
        {
            "comment": "This code demonstrates a rename function for compatibility in model deployment. The modified `serving_server_conf.prototxt` shows how to alias the input and output names in the configuration file. This allows different models to be inferred and deployed without modifying the code, only by altering the `alias_name`. The `cpp_serving` directory contains scripts for starting the pipeline service, C++ serving service, and sending prediction requests.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/cpp_serving/readme_en.md\":95-121",
            "content": "  **Remarks**: In order to be compatible with the deployment of different models, Serving provides the function of input and output renaming. In this way, when different models are inferred and deployed, they only need to modify the `alias_name` of the configuration file, and the inference deployment can be completed without modifying the code.\n  The modified `serving_server_conf.prototxt` looks like this:\n  ```yaml\n  feed_var {\n    name: \"data_batch_0\"\n    alias_name: \"data_batch_0\"\n    is_lod_tensor: false\n    feed_type: 1\n    shape: 8\n    shape: 3\n    shape: 224\n    shape: 224\n  }\n  fetch_var {\n    name: \"linear_2.tmp_1\"\n    alias_name: \"outputs\"\n    is_lod_tensor: false\n    fetch_type: 1\n    shape: 400\n  }\n  ```\n### Service deployment and requests\nThe `cpp_serving` directory contains the code for starting the pipeline service, the C++ serving service and sending the prediction request, including:\n  ```bash\n  run_cpp_serving.sh # Start the script on the C++ serving server side\n  pipeline_http_client.py # The script on the client side to send data and get the prediction results"
        },
        {
            "comment": "This code provides instructions for setting up and running a C++ serving environment for PaddleVideo. It explains how to navigate to the deployment directory, start the service, send requests using serving_client.py, and obtain the model prediction results. If an error occurs during execution, it will display the corresponding log information.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/cpp_serving/readme_en.md\":122-151",
            "content": "  paddle_env_install.sh # Install C++ serving environment script\n  preprocess_ops.py # file to store preprocessing functions\n  ```\n#### C++ Serving\n- Go to the working directory:\n  ```bash\n  cd deploy/cpp_serving\n  ```\n- Start the service:\n  ```bash\n  # Start in the background, the logs printed during the process will be redirected and saved to nohup.txt\n  bash run_cpp_serving.sh\n  ```\n- Send the request and get the result:\n```bash\npython3.7 serving_client.py \\\n-n PPTSM \\\n-c ./ppTSM_serving_client/serving_client_conf.prototxt \\\n--input_file=../../data/example.avi\n```\nAfter a successful run, the results of the model prediction will be printed in the cmd window, and the results are as follows:\n  ```bash\n  I0510 04:33:00.110025 37097 naming_service_thread.cpp:202] brpc::policy::ListNamingService(\"127.0.0.1:9993\"): added 1\n  I0510 04:33:01.904764 37097 general_model.cpp:490] [client]logid=0,client_cost=1640.96ms,server_cost=1623.21ms.\n   {'class_id': '[5]', 'prob': '[0.9907387495040894]'}\n   ```\n**If an error is re"
        },
        {
            "comment": "This code provides instructions for installing the necessary environment and resolving an issue where no result is returned or a decoding error occurs due to proxy settings. Users are advised not to set proxies when starting the service and sending requests, and should use the provided commands to close proxies beforehand. The script `paddle_env_install.sh` can be executed to install relevant environment requirements.",
            "location": "\"/media/root/Prima/works/PaddleVideo/docs/src/deploy/cpp_serving/readme_en.md\":151-164",
            "content": "ported during the process and it shows that libnvinfer.so.6 cannot be found, you can execute the script `paddle_env_install.sh` to install the relevant environment**\n   ```bash\n   bash paddle_env_install.sh\n   ```\n## FAQ\n**Q1**: No result is returned after the request is sent or an output decoding error is prompted\n**A1**: Do not set the proxy when starting the service and sending the request. You can close the proxy before starting the service and sending the request. The command to close the proxy is:\n```\nunset https_proxy\nunset http_proxy\n```"
        }
    ]
}