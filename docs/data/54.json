{
    "5400": {
        "file_id": 448,
        "content": "                now_sample = samples[-1]\n                new_sample = {}\n                for elem in now_sample.keys():\n                    if 'meta' in elem:\n                        new_sample[elem] = now_sample[elem].copy()\n                        new_sample[elem]['flip'] = True\n                        continue\n                    tmp = now_sample[elem]\n                    tmp = tmp[:, ::-1].copy()\n                    new_sample[elem] = tmp\n                samples.append(new_sample)\n        return samples\n@PIPELINES.register()\nclass MultiNorm(object):\n    def __call__(self, samples):\n        for idx in range(len(samples)):\n            sample = samples[idx]\n            for elem in sample.keys():\n                if 'meta' in elem:\n                    continue\n                tmp = sample[elem]\n                if tmp is None:\n                    continue\n                if tmp.ndim == 2:\n                    tmp = tmp[:, :, np.newaxis]\n                else:\n                    tmp = tmp / 255.\n                    tmp -= (0.485, 0.456, 0.406)",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/segmentation.py:93-124"
    },
    "5401": {
        "file_id": 448,
        "content": "This code segment is from the PaddleVideo library, specifically in the loader/pipelines/segmentation.py file. It appears to be a function that adds flipped image data to a list of samples, after normalizing each image by dividing it by 255 and subtracting (0.485, 0.456, 0.406). This function is part of the MultiNorm pipeline registered in the PIPELINES module.",
        "type": "comment"
    },
    "5402": {
        "file_id": 448,
        "content": "                    tmp /= (0.229, 0.224, 0.225)\n                tmp = tmp.transpose((2, 0, 1))\n                samples[idx][elem] = tmp\n        return samples",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/segmentation.py:125-130"
    },
    "5403": {
        "file_id": 448,
        "content": "This code segment performs image normalization and transposition before storing the result in a dictionary-like samples data structure. It divides each RGB channel value by the average RGB values, then transposes the image channels. Finally, it adds the transformed image to the samples dictionary for the given index and element.",
        "type": "comment"
    },
    "5404": {
        "file_id": 449,
        "content": "/paddlevideo/loader/pipelines/segmentation_pipline.py",
        "type": "filepath"
    },
    "5405": {
        "file_id": 449,
        "content": "This Python code defines a SegmentationSampler class in PaddleVideo, which samples data at a specified rate and registers it for the Action Segmentation Dataset. It is part of a video processing library's pipeline, likely for segmentation purposes.",
        "type": "summary"
    },
    "5406": {
        "file_id": 449,
        "content": "#  Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport copy\nimport os\nimport numpy as np\nimport random\nimport paddle\nfrom ..registry import PIPELINES\n\"\"\"\npipeline ops for Action Segmentation Dataset.\n\"\"\"\n@PIPELINES.register()\nclass SegmentationSampler(object):\n    def __init__(self, sample_rate):\n        self.sample_rate = sample_rate\n    def __call__(self, results):\n        for key, data in results.items():\n            if len(data.shape) == 1:\n                data = data[::self.sample_rate]",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/segmentation_pipline.py:1-35"
    },
    "5407": {
        "file_id": 449,
        "content": "This Python code is from the PaddleVideo library and defines a SegmentationSampler class. It samples data at a specified rate, only keeps every nth element in a 1D array, and registers this pipeline operation for the Action Segmentation Dataset.",
        "type": "comment"
    },
    "5408": {
        "file_id": 449,
        "content": "                results[key] = copy.deepcopy(data)\n            else:\n                data = data[:, ::self.sample_rate]\n                results[key] = copy.deepcopy(data)\n        return results",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/segmentation_pipline.py:36-40"
    },
    "5409": {
        "file_id": 449,
        "content": "This code segment appears to be part of a pipeline in a video processing library, possibly for segmentation. It selects specific data based on the sample rate and stores it in a results dictionary with deep copy.",
        "type": "comment"
    },
    "5410": {
        "file_id": 450,
        "content": "/paddlevideo/loader/pipelines/skeleton_pipeline.py",
        "type": "filepath"
    },
    "5411": {
        "file_id": 450,
        "content": "The code develops efficient data processing classes for PaddleVideo, including interpolation, cropping, and pipeline optimization. It performs image flipping operations, augments 'Flow' modality images, transforms data formats, collapses dimensions, includes ML data pipeline support, generates heatmaps for keypoints and limbs in image sequences using input parameters, applies data augmentation, and uses Gaussian filtering with specified sigma value.",
        "type": "summary"
    },
    "5412": {
        "file_id": 450,
        "content": "#  Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\nimport collections\nfrom itertools import repeat\nimport copy as cp\nfrom collections import abc\nimport numpy as np\nimport paddle.nn.functional as F\nimport random\nimport paddle\nfrom ..registry import PIPELINES\nfrom .augmentations_ava import iminvert, imflip_\n\"\"\"pipeline ops for Activity Net.\n\"\"\"\ndef _ntuple(n):\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return tuple(x)\n        return tuple(repeat(x, n))",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:1-34"
    },
    "5413": {
        "file_id": 450,
        "content": "This code is importing necessary libraries and defining a function to create ActivityNet-style pipeline operations. It also registers these pipelines for future use.",
        "type": "comment"
    },
    "5414": {
        "file_id": 450,
        "content": "    return parse\n_single = _ntuple(1)\n_pair = _ntuple(2)\n_triple = _ntuple(3)\n_quadruple = _ntuple(4)\ndef _init_lazy_if_proper(results, lazy):\n    \"\"\"Initialize lazy operation properly.\n    Make sure that a lazy operation is properly initialized,\n    and avoid a non-lazy operation accidentally getting mixed in.\n    Required keys in results are \"imgs\" if \"img_shape\" not in results,\n    otherwise, Required keys in results are \"img_shape\", add or modified keys\n    are \"img_shape\", \"lazy\".\n    Add or modified keys in \"lazy\" are \"original_shape\", \"crop_bbox\", \"flip\",\n    \"flip_direction\", \"interpolation\".\n    Args:\n        results (dict): A dict stores data pipeline result.\n        lazy (bool): Determine whether to apply lazy operation. Default: False.\n    \"\"\"\n    if 'img_shape' not in results:\n        results['img_shape'] = results['imgs'][0].shape[:2]\n    if lazy:\n        if 'lazy' not in results:\n            img_h, img_w = results['img_shape']\n            lazyop = dict()\n            lazyop['original_shape'] = results['img_shape']",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:36-68"
    },
    "5415": {
        "file_id": 450,
        "content": "This function initializes the lazy operation properly by checking if \"img_shape\" is in results, and adds or modifies keys \"lazy\", \"original_shape\", \"crop_bbox\", \"flip\", \"flip_direction\", and \"interpolation\" based on whether \"lazy\" is True.",
        "type": "comment"
    },
    "5416": {
        "file_id": 450,
        "content": "            lazyop['crop_bbox'] = np.array([0, 0, img_w, img_h],\n                                           dtype=np.float32)\n            lazyop['flip'] = False\n            lazyop['flip_direction'] = None\n            lazyop['interpolation'] = None\n            results['lazy'] = lazyop\n    else:\n        assert 'lazy' not in results, 'Use Fuse after lazy operations'\n@PIPELINES.register()\nclass AutoPadding(object):\n    \"\"\"\n    Sample or Padding frame skeleton feature.\n    Args:\n        window_size: int, temporal size of skeleton feature.\n        random_pad: bool, whether do random padding when frame length < window size. Default: False.\n    \"\"\"\n    def __init__(self, window_size, random_pad=False):\n        self.window_size = window_size\n        self.random_pad = random_pad\n    def get_frame_num(self, data):\n        C, T, V, M = data.shape\n        for i in range(T - 1, -1, -1):\n            tmp = np.sum(data[:, i, :, :])\n            if tmp > 0:\n                T = i + 1\n                break\n        return T\n    def __call__(self, results):",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:69-101"
    },
    "5417": {
        "file_id": 450,
        "content": "The code is defining a Pipeline class, specifically for auto-padding and skeleton feature extraction from image data. It first checks if the 'lazy' operation has been performed, then initializes necessary parameters for padding or sampling frames based on window size and random_pad setting. The get_frame_num function calculates the number of frames containing valid data, and the __call__ method applies the pipeline to the results.",
        "type": "comment"
    },
    "5418": {
        "file_id": 450,
        "content": "        data = results['data']\n        C, T, V, M = data.shape\n        T = self.get_frame_num(data)\n        if T == self.window_size:\n            data_pad = data[:, :self.window_size, :, :]\n        elif T < self.window_size:\n            begin = random.randint(\n                0, self.window_size - T) if self.random_pad else 0\n            data_pad = np.zeros((C, self.window_size, V, M))\n            data_pad[:, begin:begin + T, :, :] = data[:, :T, :, :]\n        else:\n            if self.random_pad:\n                index = np.random.choice(\n                    T, self.window_size, replace=False).astype('int64')\n            else:\n                index = np.linspace(0, T, self.window_size).astype(\"int64\")\n            data_pad = data[:, index, :, :]\n        results['data'] = data_pad\n        return results\n@PIPELINES.register()\nclass SkeletonNorm(object):\n    \"\"\"\n    Normalize skeleton feature.\n    Args:\n        aixs: dimensions of vertex coordinate. 2 for (x,y), 3 for (x,y,z). Default: 2.\n    \"\"\"\n    def __init__(self, axis=2, squeeze=False):",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:102-133"
    },
    "5419": {
        "file_id": 450,
        "content": "Code snippet performs data padding to ensure consistent frame size for skeleton data in the Skeleton Pipeline. It checks the current frame number (T) and pads it with zeroes if T is smaller than the window size, or selects a subset of frames from the original data if T is larger than the window size. The result is then returned as 'results'.",
        "type": "comment"
    },
    "5420": {
        "file_id": 450,
        "content": "        self.axis = axis\n        self.squeeze = squeeze\n    def __call__(self, results):\n        data = results['data']\n        # Centralization\n        data = data - data[:, :, 8:9, :]\n        data = data[:self.axis, :, :, :]  # get (x,y) from (x,y, acc)\n        C, T, V, M = data.shape\n        if self.squeeze:\n            data = data.reshape((C, T, V))  # M = 1\n        results['data'] = data.astype('float32')\n        if 'label' in results:\n            label = results['label']\n            results['label'] = np.expand_dims(label, 0).astype('int64')\n        return results\n@PIPELINES.register()\nclass Iden(object):\n    \"\"\"\n    Wrapper Pipeline\n    \"\"\"\n    def __init__(self, label_expand=True):\n        self.label_expand = label_expand\n    def __call__(self, results):\n        data = results['data']\n        results['data'] = data.astype('float32')\n        if 'label' in results and self.label_expand:\n            label = results['label']\n            results['label'] = np.expand_dims(label, 0).astype('int64')\n        return results",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:134-170"
    },
    "5421": {
        "file_id": 450,
        "content": "This code defines two classes, \"SkeletonPipeline\" and \"Iden\", which are used as PaddleVideo pipeline components. The SkeletonPipeline class is responsible for centralizing the data along a specified axis and reshaping it if squeeze is True. The Iden class simply converts the 'data' to float32 type and expands the shape of 'label' if it exists and label_expand is set to True. Both classes return updated results after processing.",
        "type": "comment"
    },
    "5422": {
        "file_id": 450,
        "content": "@PIPELINES.register()\nclass RandomRotation(object):\n    \"\"\"\n    Random rotation sketeton.\n    Args:\n        argument: bool, if rotation.\n        theta: float, rotation rate.\n    \"\"\"\n    def __init__(self, argument, theta=0.3):\n        self.theta = theta\n        self.argument = argument\n    def _rot(self, rot):\n        \"\"\"\n        rot: T,3\n        \"\"\"\n        cos_r, sin_r = np.cos(rot), np.sin(rot)  # T,3\n        zeros = np.zeros((rot.shape[0], 1))  # T,1\n        ones = np.ones((rot.shape[0], 1))  # T,1\n        r1 = np.stack((ones, zeros, zeros), axis=-1)  # T,1,3\n        rx2 = np.stack((zeros, cos_r[:, 0:1], sin_r[:, 0:1]), axis=-1)  # T,1,3\n        rx3 = np.stack((zeros, -sin_r[:, 0:1], cos_r[:, 0:1]), axis=-1)  # T,1,3\n        rx = np.concatenate((r1, rx2, rx3), axis=1)  # T,3,3\n        ry1 = np.stack((cos_r[:, 1:2], zeros, -sin_r[:, 1:2]), axis=-1)\n        r2 = np.stack((zeros, ones, zeros), axis=-1)\n        ry3 = np.stack((sin_r[:, 1:2], zeros, cos_r[:, 1:2]), axis=-1)\n        ry = np.concatenate((ry1, r2, ry3), axis=1)",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:173-202"
    },
    "5423": {
        "file_id": 450,
        "content": "The code defines a random rotation skeleton class for applying random rotations to input data. It takes arguments for rotation and rotation rate, and has a method for performing the rotation operation on the input data. The method calculates rotation matrices for rotation around the x, y, and z axes using the given rotation rate and applies them to the input data.",
        "type": "comment"
    },
    "5424": {
        "file_id": 450,
        "content": "        rz1 = np.stack((cos_r[:, 2:3], sin_r[:, 2:3], zeros), axis=-1)\n        r3 = np.stack((zeros, zeros, ones), axis=-1)\n        rz2 = np.stack((-sin_r[:, 2:3], cos_r[:, 2:3], zeros), axis=-1)\n        rz = np.concatenate((rz1, rz2, r3), axis=1)\n        rot = np.matmul(np.matmul(rz, ry), rx)\n        return rot\n    def __call__(self, results):\n        # C,T,V,M\n        data = results['data']\n        if self.argument:\n            C, T, V, M = data.shape\n            data_numpy = np.transpose(data, (1, 0, 2, 3)).conjugate().reshape(\n                T, C, V * M)  # T,3,V*M\n            rot = np.random.uniform(-self.theta, self.theta, 3)\n            rot = np.stack(\n                [\n                    rot,\n                ] * T, axis=0)\n            rot = self._rot(rot)  # T,3,3\n            data_numpy = np.matmul(rot, data_numpy)\n            data_numpy = data_numpy.reshape(T, C, V, M)\n            data_numpy = np.transpose(data_numpy, (1, 0, 2, 3))\n            data = data_numpy\n        results['data'] = data.astype(np.float32)",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:204-229"
    },
    "5425": {
        "file_id": 450,
        "content": "This code defines a class with two methods: `_rot` and `__call__`. The `_rot` method takes rotation angles and returns the rotation matrix. The `__call__` method applies random rotations to input data, performs the rotations using `_rot`, and adjusts the shape of the data before returning it.",
        "type": "comment"
    },
    "5426": {
        "file_id": 450,
        "content": "        return results\n@PIPELINES.register()\nclass SketeonCropSample(object):\n    \"\"\"\n    Sketeon Crop Sampler.\n    Args:\n        crop_model: str, crop model, support: ['center'].\n        p_interval: list, crop len\n        window_size: int, sample windows size.\n    \"\"\"\n    def __init__(self, window_size, crop_model='center', p_interval=1):\n        assert crop_model in ['center'], \"Don't support :\" + crop_model\n        self.crop_model = crop_model\n        self.window_size = window_size\n        self.p_interval = p_interval\n    def __call__(self, results):\n        if self.crop_model == 'center':\n            # input: C,T,V,M\n            data = results['data']\n            valid_frame_num = np.sum(data.sum(0).sum(-1).sum(-1) != 0)\n            C, T, V, M = data.shape\n            begin = 0\n            end = valid_frame_num\n            valid_size = end - begin\n            #crop\n            if len(self.p_interval) == 1:\n                p = self.p_interval[0]\n                bias = int((1 - p) * valid_size / 2)\n                data = data[:, begin + bias:end - bias, :, :]  # center_crop",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:230-265"
    },
    "5427": {
        "file_id": 450,
        "content": "This code defines a Pipeline class for cropping sample data using the Sketeon crop model. It supports only the 'center' crop model and takes window size, crop model (default 'center'), and p_interval (default 1) as arguments. The __call__ method is used to apply the crop operation on the input results by selecting a center crop based on the crop model and p_interval values.",
        "type": "comment"
    },
    "5428": {
        "file_id": 450,
        "content": "                cropped_length = data.shape[1]\n            else:\n                p = np.random.rand(1) * (self.p_interval[1] - self.p_interval[0]\n                                         ) + self.p_interval[0]\n                # constraint cropped_length lower bound as 64\n                cropped_length = np.minimum(\n                    np.maximum(int(np.floor(valid_size * p)), 64), valid_size)\n                bias = np.random.randint(0, valid_size - cropped_length + 1)\n                data = data[:, begin + bias:begin + bias + cropped_length, :, :]\n            # resize\n            data = np.transpose(data, (0, 2, 3, 1)).conjugate().reshape(\n                C * V * M, cropped_length)\n            data = data[None, None, :, :]\n            # could perform both up sample and down sample\n            data_tensor = paddle.to_tensor(data)\n            data_tensor = F.interpolate(\n                data_tensor,\n                size=(C * V * M, self.window_size),\n                mode='bilinear',\n                align_corners=False).squeeze()",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:266-286"
    },
    "5429": {
        "file_id": 450,
        "content": "This code randomly selects a cropped length within a specified interval, then applies random bias to the cropped length. It reshapes and transposes the data before performing interpolation on the tensor for up or down sampling using bilinear mode.",
        "type": "comment"
    },
    "5430": {
        "file_id": 450,
        "content": "            data = paddle.transpose(\n                paddle.reshape(data_tensor, (C, V, M, self.window_size)),\n                (0, 3, 1, 2)).numpy()\n        else:\n            raise NotImplementedError\n        results['data'] = data\n        return results\n@PIPELINES.register()\nclass SketeonModalityTransform(object):\n    \"\"\"\n    Sketeon Crop Sampler.\n    Args:\n        crop_model: str, crop model, support: ['center'].\n        p_interval: list, crop len\n        window_size: int, sample windows size.\n    \"\"\"\n    def __init__(self, bone, motion, joint=True, graph='ntu_rgb_d'):\n        self.joint = joint\n        self.bone = bone\n        self.motion = motion\n        self.graph = graph\n        if self.graph == \"ntu_rgb_d\":\n            self.bone_pairs = ((1, 2), (2, 21), (3, 21), (4, 3), (5, 21),\n                               (6, 5), (7, 6), (8, 7), (9, 21), (10, 9),\n                               (11, 10), (12, 11), (13, 1), (14, 13), (15, 14),\n                               (16, 15), (17, 1), (18, 17), (19, 18), (20, 19),",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:287-316"
    },
    "5431": {
        "file_id": 450,
        "content": "This code is part of the PaddleVideo library and appears to be a function or class related to skeleton data transformation for video analysis tasks. The code seems to handle reshaping and transposing data based on certain parameters, such as window size, crop model, and more. This could potentially be used for video processing in computer vision applications like action recognition or pose estimation.",
        "type": "comment"
    },
    "5432": {
        "file_id": 450,
        "content": "                               (22, 23), (21, 21), (23, 8), (24, 25), (25, 12))\n        else:\n            raise NotImplementedError\n    def __call__(self, results):\n        if self.joint:\n            return results\n        data_numpy = results['data']\n        if self.bone:\n            bone_data_numpy = np.zeros_like(data_numpy)\n            for v1, v2 in self.bone_pairs:\n                bone_data_numpy[:, :, v1 -\n                                1] = data_numpy[:, :, v1 -\n                                                1] - data_numpy[:, :, v2 - 1]\n            data_numpy = bone_data_numpy\n        if self.motion:\n            data_numpy[:, :-1] = data_numpy[:, 1:] - data_numpy[:, :-1]\n            data_numpy[:, -1] = 0\n        results['data'] = data_numpy\n        return results\n@PIPELINES.register()\nclass UniformSampleFrames:\n    \"\"\"Uniformly sample frames from the video.\n    To sample an n-frame clip from the video. UniformSampleFrames basically\n    divide the video into n segments of equal length and randomly sample one",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:317-344"
    },
    "5433": {
        "file_id": 450,
        "content": "This code defines a class for skeleton processing in PaddleVideo. If joints are enabled, it returns the results as is. If bones are enabled, it calculates bone data by subtracting corresponding bone vertices from each other. If motion is enabled, it sets the last frame's coordinates to 0. The UniformSampleFrames pipeline uniformly samples frames from a video by dividing it into equal segments and randomly selecting one from each segment.",
        "type": "comment"
    },
    "5434": {
        "file_id": 450,
        "content": "    frame from each segment. To make the testing results reproducible, a\n    random seed is set during testing, to make the sampling results\n    deterministic.\n    Required keys are \"total_frames\", \"start_index\" , added or modified keys\n    are \"frame_inds\", \"clip_len\", \"frame_interval\" and \"num_clips\".\n    Args:\n        clip_len (int): Frames of each sampled output clip.\n        num_clips (int): Number of clips to be sampled. Default: 1.\n        test_mode (bool): Store True when building test or validation dataset.\n            Default: False.\n        seed (int): The random seed used during test time. Default: 255.\n    \"\"\"\n    def __init__(self, clip_len, num_clips=1, test_mode=False, seed=255):\n        self.clip_len = clip_len\n        self.num_clips = num_clips\n        self.test_mode = test_mode\n        self.seed = seed\n    def _get_train_clips(self, num_frames, clip_len):\n        \"\"\"Uniformly sample indices for training clips.\n        Args:\n            num_frames (int): The number of frames.\n            clip_len (int): The length of the clip.",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:345-372"
    },
    "5435": {
        "file_id": 450,
        "content": "This code snippet defines a class with an __init__ method that initializes the clip_len, num_clips, test_mode, and seed. The _get_train_clips method uniformly samples indices for training clips based on the given number of frames and clip length. This is used in PaddleVideo for loading and processing video data.",
        "type": "comment"
    },
    "5436": {
        "file_id": 450,
        "content": "        \"\"\"\n        assert self.num_clips == 1\n        if num_frames < clip_len:\n            start = np.random.randint(0, num_frames)\n            inds = np.arange(start, start + clip_len)\n        elif clip_len <= num_frames < 2 * clip_len:\n            basic = np.arange(clip_len)\n            inds = np.random.choice(\n                clip_len + 1, num_frames - clip_len, replace=False)\n            offset = np.zeros(clip_len + 1, dtype=np.int64)\n            offset[inds] = 1\n            offset = np.cumsum(offset)\n            inds = basic + offset[:-1]\n        else:\n            bids = np.array(\n                [i * num_frames // clip_len for i in range(clip_len + 1)])\n            bsize = np.diff(bids)\n            bst = bids[:clip_len]\n            offset = np.random.randint(bsize)\n            inds = bst + offset\n        return inds\n    def _get_test_clips(self, num_frames, clip_len):\n        \"\"\"Uniformly sample indices for testing clips.\n        Args:\n            num_frames (int): The number of frames.\n            clip_len (int): The length of the clip.",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:373-401"
    },
    "5437": {
        "file_id": 450,
        "content": "This code determines the indices for a skeleton clip from a given number of frames and clip length. It handles three scenarios: when the number of frames is less than the clip length, between the clip length and twice the clip length, or more than twice the clip length. The function returns the sampled indices accordingly.",
        "type": "comment"
    },
    "5438": {
        "file_id": 450,
        "content": "        \"\"\"\n        np.random.seed(self.seed)\n        if num_frames < clip_len:\n            # Then we use a simple strategy\n            if num_frames < self.num_clips:\n                start_inds = list(range(self.num_clips))\n            else:\n                start_inds = [\n                    i * num_frames // self.num_clips\n                    for i in range(self.num_clips)\n                ]\n            inds = np.concatenate(\n                [np.arange(i, i + clip_len) for i in start_inds])\n        elif clip_len <= num_frames < clip_len * 2:\n            all_inds = []\n            for i in range(self.num_clips):\n                basic = np.arange(clip_len)\n                inds = np.random.choice(\n                    clip_len + 1, num_frames - clip_len, replace=False)\n                offset = np.zeros(clip_len + 1, dtype=np.int64)\n                offset[inds] = 1\n                offset = np.cumsum(offset)\n                inds = basic + offset[:-1]\n                all_inds.append(inds)\n            inds = np.concatenate(all_inds)",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:402-427"
    },
    "5439": {
        "file_id": 450,
        "content": "The code handles the random selection of frame indices for a given clip length and total number of frames. It considers three scenarios: when there are fewer frames than the clip length, exactly equal to the clip length, or between the clip length and twice the clip length. It uses list comprehension and numpy functions to generate the desired indices for each case.",
        "type": "comment"
    },
    "5440": {
        "file_id": 450,
        "content": "        else:\n            bids = np.array(\n                [i * num_frames // clip_len for i in range(clip_len + 1)])\n            bsize = np.diff(bids)\n            bst = bids[:clip_len]\n            all_inds = []\n            for i in range(self.num_clips):\n                offset = np.random.randint(bsize)\n                all_inds.append(bst + offset)\n            inds = np.concatenate(all_inds)\n        return inds\n    def __call__(self, results):\n        num_frames = results['total_frames']\n        if self.test_mode:\n            inds = self._get_test_clips(num_frames, self.clip_len)\n        else:\n            inds = self._get_train_clips(num_frames, self.clip_len)\n        inds = np.mod(inds, num_frames)\n        start_index = results['start_index']\n        inds = inds + start_index\n        results['frame_inds'] = inds.astype(np.int)\n        results['clip_len'] = self.clip_len\n        results['frame_interval'] = None\n        results['num_clips'] = self.num_clips\n        return results\n    def __repr__(self):\n        repr_str = (f'{self.__class__.__name__}('",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:428-459"
    },
    "5441": {
        "file_id": 450,
        "content": "This code defines a class for generating frame indices for skeleton data in PaddleVideo. It has methods to generate clips for training or testing, and returns the generated clips as results. The class takes parameters such as clip length, number of clips, total frames, etc. It ensures that the returned frame indices are within the range of total frames and converts them to integers.",
        "type": "comment"
    },
    "5442": {
        "file_id": 450,
        "content": "                    f'clip_len={self.clip_len}, '\n                    f'num_clips={self.num_clips}, '\n                    f'test_mode={self.test_mode}, '\n                    f'seed={self.seed})')\n        return repr_str\n@PIPELINES.register()\nclass PoseDecode:\n    \"\"\"Load and decode pose with given indices.\n    Required keys are \"keypoint\", \"frame_inds\" (optional), \"keypoint_score\"\n    (optional), added or modified keys are \"keypoint\", \"keypoint_score\" (if\n    applicable).\n    \"\"\"\n    @staticmethod\n    def _load_kp(kp, frame_inds):\n        \"\"\"Load keypoints given frame indices.\n        Args:\n            kp (np.ndarray): The keypoint coordinates.\n            frame_inds (np.ndarray): The frame indices.\n        \"\"\"\n        return [x[frame_inds].astype(np.float32) for x in kp]\n    @staticmethod\n    def _load_kpscore(kpscore, frame_inds):\n        \"\"\"Load keypoint scores given frame indices.\n        Args:\n            kpscore (np.ndarray): The confidence scores of keypoints.\n            frame_inds (np.ndarray): The frame indices.",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:460-493"
    },
    "5443": {
        "file_id": 450,
        "content": "This code defines a PoseDecode class that loads and decodes pose with given indices. It requires \"keypoint\" and \"frame_inds\" keys, and optionally \"keypoint_score\". The _load_kp static method loads keypoint coordinates based on frame indices, while the _load_kpscore method loads keypoint scores with frame indices. Both methods return arrays of float32 values for keypoint coordinates or scores respectively. This class is registered at PIPelines for further usage.",
        "type": "comment"
    },
    "5444": {
        "file_id": 450,
        "content": "        \"\"\"\n        return [x[frame_inds].astype(np.float32) for x in kpscore]\n    def __call__(self, results):\n        if 'frame_inds' not in results:\n            results['frame_inds'] = np.arange(results['total_frames'])\n        if results['frame_inds'].ndim != 1:\n            results['frame_inds'] = np.squeeze(results['frame_inds'])\n        offset = results.get('offset', 0)\n        frame_inds = results['frame_inds'] + offset\n        if 'keypoint_score' in results:\n            kpscore = results['keypoint_score']\n            results['keypoint_score'] = kpscore[:, frame_inds].astype(\n                np.float32)\n        if 'keypoint' in results:\n            results['keypoint'] = results['keypoint'][:, frame_inds].astype(\n                np.float32)\n        return results\n    def __repr__(self):\n        repr_str = f'{self.__class__.__name__}()'\n        return repr_str\n@PIPELINES.register()\nclass PoseCompact:\n    \"\"\"Convert the coordinates of keypoints to make it more compact.\n    Specifically, it first find a tight bounding box that surrounds all joints",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:494-528"
    },
    "5445": {
        "file_id": 450,
        "content": "This code defines a PoseCompact class, which is a pipeline for converting keypoint coordinates into a more compact representation. It takes results from previous steps and processes 'keypoint_score' and 'keypoint' keys based on frame indices. If present, it extracts the keypoint scores and keypoint coordinates for the specified frames and converts them to float32 type.",
        "type": "comment"
    },
    "5446": {
        "file_id": 450,
        "content": "    in each frame, then we expand the tight box by a given padding ratio. For\n    example, if 'padding == 0.25', then the expanded box has unchanged center,\n    and 1.25x width and height.\n    Required keys in results are \"img_shape\", \"keypoint\", add or modified keys\n    are \"img_shape\", \"keypoint\", \"crop_quadruple\".\n    Args:\n        padding (float): The padding size. Default: 0.25.\n        threshold (int): The threshold for the tight bounding box. If the width\n            or height of the tight bounding box is smaller than the threshold,\n            we do not perform the compact operation. Default: 10.\n        hw_ratio (float | tuple[float] | None): The hw_ratio of the expanded\n            box. Float indicates the specific ratio and tuple indicates a\n            ratio range. If set as None, it means there is no requirement on\n            hw_ratio. Default: None.\n        allow_imgpad (bool): Whether to allow expanding the box outside the\n            image to meet the hw_ratio requirement. Default: True.",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:529-546"
    },
    "5447": {
        "file_id": 450,
        "content": "This function expands tight bounding boxes by a given padding ratio and adds new key \"crop_quadruple\". It requires keys \"img_shape\", \"keypoint\" and may modify them. The threshold determines if the box is too small to expand, hw_ratio sets the box aspect ratio (optional), and allow_imgpad allows expanding outside image for hw_ratio (optional). Default values are padding=0.25, threshold=10, hw_ratio=None, allow_imgpad=True.",
        "type": "comment"
    },
    "5448": {
        "file_id": 450,
        "content": "    Returns:\n        type: Description of returned object.\n    \"\"\"\n    def __init__(self,\n                 padding=0.25,\n                 threshold=10,\n                 hw_ratio=None,\n                 allow_imgpad=True):\n        self.padding = padding\n        self.threshold = threshold\n        if hw_ratio is not None:\n            hw_ratio = _pair(hw_ratio)\n        self.hw_ratio = hw_ratio\n        self.allow_imgpad = allow_imgpad\n        assert self.padding >= 0\n    def _combine_quadruple(self, a, b):\n        return (a[0] + a[2] * b[0], a[1] + a[3] * b[1], a[2] * b[2],\n                a[3] * b[3])\n    def __call__(self, results):\n        img_shape = results['img_shape']\n        h, w = img_shape\n        kp = results['keypoint']\n        # Make NaN zero\n        kp[np.isnan(kp)] = 0.\n        kp_x = kp[..., 0]\n        kp_y = kp[..., 1]\n        min_x = np.min(kp_x[kp_x != 0], initial=np.Inf)\n        min_y = np.min(kp_y[kp_y != 0], initial=np.Inf)\n        max_x = np.max(kp_x[kp_x != 0], initial=-np.Inf)\n        max_y = np.max(kp_y[kp_y != 0], initial=-np.Inf)",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:548-585"
    },
    "5449": {
        "file_id": 450,
        "content": "This code is initializing a class for skeleton pipeline. It takes parameters such as padding, threshold, hw_ratio, and allow_imgpad. The class also has methods to combine quadruples, apply transformations, and handle keypoints in the image. The code performs various operations like making NaN values zero, finding minimum and maximum keypoint coordinates, and applying padding if needed.",
        "type": "comment"
    },
    "5450": {
        "file_id": 450,
        "content": "        # The compact area is too small\n        if max_x - min_x < self.threshold or max_y - min_y < self.threshold:\n            return results\n        center = ((max_x + min_x) / 2, (max_y + min_y) / 2)\n        half_width = (max_x - min_x) / 2 * (1 + self.padding)\n        half_height = (max_y - min_y) / 2 * (1 + self.padding)\n        if self.hw_ratio is not None:\n            half_height = max(self.hw_ratio[0] * half_width, half_height)\n            half_width = max(1 / self.hw_ratio[1] * half_height, half_width)\n        min_x, max_x = center[0] - half_width, center[0] + half_width\n        min_y, max_y = center[1] - half_height, center[1] + half_height\n        # hot update\n        if not self.allow_imgpad:\n            min_x, min_y = int(max(0, min_x)), int(max(0, min_y))\n            max_x, max_y = int(min(w, max_x)), int(min(h, max_y))\n        else:\n            min_x, min_y = int(min_x), int(min_y)\n            max_x, max_y = int(max_x), int(max_y)\n        kp_x[kp_x != 0] -= min_x\n        kp_y[kp_y != 0] -= min_y",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:587-611"
    },
    "5451": {
        "file_id": 450,
        "content": "This code checks if the compact area is too small and adjusts the bounding box parameters accordingly. It calculates the center, half-width, and half-height of the bounding box. If the aspect ratio should be maintained (hw_ratio), it ensures that by adjusting half_height based on half_width. The code then updates the minimum and maximum x and y values within the constraints of the image's width and height, unless allow_imgpad is True, in which case it doesn't limit the bounding box size. Finally, it adjusts the x and y coordinates of the keypoints by subtracting the new min_x and min_y to maintain their relative positions within the adjusted bounding box.",
        "type": "comment"
    },
    "5452": {
        "file_id": 450,
        "content": "        new_shape = (max_y - min_y, max_x - min_x)\n        results['img_shape'] = new_shape\n        # the order is x, y, w, h (in [0, 1]), a tuple\n        crop_quadruple = results.get('crop_quadruple', (0., 0., 1., 1.))\n        new_crop_quadruple = (min_x / w, min_y / h, (max_x - min_x) / w,\n                              (max_y - min_y) / h)\n        crop_quadruple = self._combine_quadruple(crop_quadruple,\n                                                 new_crop_quadruple)\n        results['crop_quadruple'] = crop_quadruple\n        return results\n    def __repr__(self):\n        repr_str = (f'{self.__class__.__name__}(padding={self.padding}, '\n                    f'threshold={self.threshold}, '\n                    f'hw_ratio={self.hw_ratio}, '\n                    f'allow_imgpad={self.allow_imgpad})')\n        return repr_str\nclass CropBase:\n    @staticmethod\n    def _crop_kps(kps, crop_bbox):\n        return kps - crop_bbox[:2]\n    @staticmethod\n    def _crop_imgs(imgs, crop_bbox):\n        x1, y1, x2, y2 = crop_bbox",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:613-640"
    },
    "5453": {
        "file_id": 450,
        "content": "This code segment is part of a pipeline for skeleton detection in images. It calculates the new image shape based on the cropping region, updates the 'crop_quadruple' in the results dictionary, and defines two static methods for cropping keypoints (_crop_kps) and cropped images (_crop_imgs). The CropBase class provides functionality to crop keypoints based on the provided crop region.",
        "type": "comment"
    },
    "5454": {
        "file_id": 450,
        "content": "        return [img[y1:y2, x1:x2] for img in imgs]\n    @staticmethod\n    def _box_crop(box, crop_bbox):\n        \"\"\"Crop the bounding boxes according to the crop_bbox.\n        Args:\n            box (np.ndarray): The bounding boxes.\n            crop_bbox(np.ndarray): The bbox used to crop the original image.\n        \"\"\"\n        x1, y1, x2, y2 = crop_bbox\n        img_w, img_h = x2 - x1, y2 - y1\n        box_ = box.copy()\n        box_[..., 0::2] = np.clip(box[..., 0::2] - x1, 0, img_w - 1)\n        box_[..., 1::2] = np.clip(box[..., 1::2] - y1, 0, img_h - 1)\n        return box_\n    def _all_box_crop(self, results, crop_bbox):\n        \"\"\"Crop the gt_bboxes and proposals in results according to crop_bbox.\n        Args:\n            results (dict): All information about the sample, which contain\n                'gt_bboxes' and 'proposals' (optional).\n            crop_bbox(np.ndarray): The bbox used to crop the original image.\n        \"\"\"\n        results['gt_bboxes'] = self._box_crop(results['gt_bboxes'], crop_bbox)\n        if 'proposals' in results and results['proposals'] is not None:",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:641-669"
    },
    "5455": {
        "file_id": 450,
        "content": "This code defines a function `_all_box_crop` that crops the gt_bboxes and proposals in results according to the crop_bbox. It first applies the `_box_crop` function to 'gt_bboxes', then if 'proposals' are present and not None, it also applies the `_box_crop` function to them. The `_box_crop` function crops bounding boxes by subtracting the x1, y1 coordinates from their x and y values respectively, ensuring they fall within the new image dimensions.",
        "type": "comment"
    },
    "5456": {
        "file_id": 450,
        "content": "            assert results['proposals'].shape[1] == 4\n            results['proposals'] = self._box_crop(results['proposals'],\n                                                  crop_bbox)\n        return results\n    def __call__(self, results):\n        raise NotImplementedError\n@PIPELINES.register()\nclass RandomResizedCrop_V2(CropBase):\n    \"\"\"Random crop that specifics the area and height-weight ratio range.\n    Required keys in results are \"img_shape\", \"crop_bbox\", \"imgs\" (optional),\n    \"keypoint\" (optional), added or modified keys are \"imgs\", \"keypoint\",\n    \"crop_bbox\" and \"lazy\"; Required keys in \"lazy\" are \"flip\", \"crop_bbox\",\n    added or modified key is \"crop_bbox\".\n    Args:\n        area_range (Tuple[float]): The candidate area scales range of\n            output cropped images. Default: (0.08, 1.0).\n        aspect_ratio_range (Tuple[float]): The candidate aspect ratio range of\n            output cropped images. Default: (3 / 4, 4 / 3).\n        lazy (bool): Determine whether to apply lazy operation. Default: False.",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:670-693"
    },
    "5457": {
        "file_id": 450,
        "content": "This code defines a RandomResizedCrop_V2 pipeline that randomly crops an image to a specified area and height-weight ratio range. The required keys in results are \"img_shape\", \"crop_bbox\", and \"imgs\" (optional). The modified keys are \"imgs\", \"keypoint\", \"crop_bbox\", and \"lazy\". The required keys in \"lazy\" are \"flip\", \"crop_bbox\". It provides an area range and aspect ratio range for the output cropped images.",
        "type": "comment"
    },
    "5458": {
        "file_id": 450,
        "content": "    \"\"\"\n    def __init__(self,\n                 area_range=(0.08, 1.0),\n                 aspect_ratio_range=(3 / 4, 4 / 3),\n                 lazy=False):\n        self.area_range = eval(area_range)\n        self.aspect_ratio_range = aspect_ratio_range\n        self.lazy = lazy\n        if not is_tuple_of(self.area_range, float):\n            raise TypeError(f'Area_range must be a tuple of float, '\n                            f'but got {type(area_range)}')\n        if not is_tuple_of(self.aspect_ratio_range, float):\n            raise TypeError(f'Aspect_ratio_range must be a tuple of float, '\n                            f'but got {type(aspect_ratio_range)}')\n    @staticmethod\n    def get_crop_bbox(img_shape,\n                      area_range,\n                      aspect_ratio_range,\n                      max_attempts=10):\n        \"\"\"Get a crop bbox given the area range and aspect ratio range.\n        Args:\n            img_shape (Tuple[int]): Image shape\n            area_range (Tuple[float]): The candidate area scales range of",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:694-719"
    },
    "5459": {
        "file_id": 450,
        "content": "This code initializes a class with area_range, aspect_ratio_range, and lazy parameters. It checks if the ranges are tuples of floats, and raises TypeError if not. The get_crop_bbox static method takes image shape, area range, aspect ratio range, and max attempts as arguments to return a crop bounding box.",
        "type": "comment"
    },
    "5460": {
        "file_id": 450,
        "content": "                output cropped images. Default: (0.08, 1.0).\n            aspect_ratio_range (Tuple[float]): The candidate aspect\n                ratio range of output cropped images. Default: (3 / 4, 4 / 3).\n                max_attempts (int): The maximum of attempts. Default: 10.\n            max_attempts (int): Max attempts times to generate random candidate\n                bounding box. If it doesn't qualified one, the center bounding\n                box will be used.\n        Returns:\n            (list[int]) A random crop bbox within the area range and aspect\n            ratio range.\n        \"\"\"\n        assert 0 < area_range[0] <= area_range[1] <= 1\n        assert 0 < aspect_ratio_range[0] <= aspect_ratio_range[1]\n        img_h, img_w = img_shape\n        area = img_h * img_w\n        min_ar, max_ar = aspect_ratio_range\n        aspect_ratios = np.exp(\n            np.random.uniform(\n                np.log(min_ar), np.log(max_ar), size=max_attempts))\n        target_areas = np.random.uniform(*area_range, size=max_attempts) * area",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:720-741"
    },
    "5461": {
        "file_id": 450,
        "content": "This function generates a random crop bounding box within a specified area range and aspect ratio range. It takes image shape, area range, and aspect ratio range as input parameters. The function first checks the validity of the ranges, then calculates the image's total area, minimum and maximum aspect ratios from the aspect ratio range. It uses numpy to generate a list of random candidate bounding box aspect ratios and target areas within the specified ranges.",
        "type": "comment"
    },
    "5462": {
        "file_id": 450,
        "content": "        candidate_crop_w = np.round(np.sqrt(\n            target_areas * aspect_ratios)).astype(np.int32)\n        candidate_crop_h = np.round(np.sqrt(\n            target_areas / aspect_ratios)).astype(np.int32)\n        for i in range(max_attempts):\n            crop_w = candidate_crop_w[i]\n            crop_h = candidate_crop_h[i]\n            if crop_h <= img_h and crop_w <= img_w:\n                x_offset = random.randint(0, img_w - crop_w)\n                y_offset = random.randint(0, img_h - crop_h)\n                return x_offset, y_offset, x_offset + crop_w, y_offset + crop_h\n        # Fallback\n        crop_size = min(img_h, img_w)\n        x_offset = (img_w - crop_size) // 2\n        y_offset = (img_h - crop_size) // 2\n        return x_offset, y_offset, x_offset + crop_size, y_offset + crop_size\n    def __call__(self, results):\n        \"\"\"Performs the RandomResizeCrop augmentation.\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:742-766"
    },
    "5463": {
        "file_id": 450,
        "content": "This code calculates random crop sizes based on the aspect ratios and target areas, then attempts to find a suitable crop region within the image. If a suitable crop is found, it returns the offsets and dimensions of that crop. If not, it falls back to a centered crop with minimum size. This function is called as part of a pipeline for image augmentation.",
        "type": "comment"
    },
    "5464": {
        "file_id": 450,
        "content": "        \"\"\"\n        _init_lazy_if_proper(results, self.lazy)\n        if 'keypoint' in results:\n            assert not self.lazy, ('Keypoint Augmentations are not compatible '\n                                   'with lazy == True')\n        img_h, img_w = results['img_shape']\n        left, top, right, bottom = self.get_crop_bbox(\n            (img_h, img_w), self.area_range, self.aspect_ratio_range)\n        new_h, new_w = bottom - top, right - left\n        if 'crop_quadruple' not in results:\n            results['crop_quadruple'] = np.array(\n                [0, 0, 1, 1],  # x, y, w, h\n                dtype=np.float32)\n        x_ratio, y_ratio = left / img_w, top / img_h\n        w_ratio, h_ratio = new_w / img_w, new_h / img_h\n        old_crop_quadruple = results['crop_quadruple']\n        old_x_ratio, old_y_ratio = old_crop_quadruple[0], old_crop_quadruple[1]\n        old_w_ratio, old_h_ratio = old_crop_quadruple[2], old_crop_quadruple[3]\n        new_crop_quadruple = [\n            old_x_ratio + x_ratio * old_w_ratio,",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:767-791"
    },
    "5465": {
        "file_id": 450,
        "content": "This code initializes and adjusts the crop quadruple of an image based on its aspect ratio, area range, and size. It ensures that the 'keypoint' is not applied if lazy augmentation is enabled. The crop quadruple contains x, y, width, and height values representing the image's cropping region, which are updated according to the original image's dimensions and the desired aspect ratio range.",
        "type": "comment"
    },
    "5466": {
        "file_id": 450,
        "content": "            old_y_ratio + y_ratio * old_h_ratio, w_ratio * old_w_ratio,\n            h_ratio * old_h_ratio\n        ]\n        results['crop_quadruple'] = np.array(\n            new_crop_quadruple, dtype=np.float32)\n        crop_bbox = np.array([left, top, right, bottom])\n        results['crop_bbox'] = crop_bbox\n        results['img_shape'] = (new_h, new_w)\n        if not self.lazy:\n            if 'keypoint' in results:\n                results['keypoint'] = self._crop_kps(results['keypoint'],\n                                                     crop_bbox)\n            if 'imgs' in results:\n                results['imgs'] = self._crop_imgs(results['imgs'], crop_bbox)\n        else:\n            lazyop = results['lazy']\n            if lazyop['flip']:\n                raise NotImplementedError('Put Flip at last for now')\n            # record crop_bbox in lazyop dict to ensure only crop once in Fuse\n            lazy_left, lazy_top, lazy_right, lazy_bottom = lazyop['crop_bbox']\n            left = left * (lazy_right - lazy_left) / img_w",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:792-815"
    },
    "5467": {
        "file_id": 450,
        "content": "This code performs cropping on images and bboxes based on given ratios. It updates the results dictionary with cropped quadruple, crop bbox, and new image shape. If not in lazy mode, it crops keypoints and images using these values. If in lazy mode, it stores the left, top, right, and bottom crop positions for later fusion.",
        "type": "comment"
    },
    "5468": {
        "file_id": 450,
        "content": "            right = right * (lazy_right - lazy_left) / img_w\n            top = top * (lazy_bottom - lazy_top) / img_h\n            bottom = bottom * (lazy_bottom - lazy_top) / img_h\n            lazyop['crop_bbox'] = np.array(\n                [(lazy_left + left), (lazy_top + top), (lazy_left + right),\n                 (lazy_top + bottom)],\n                dtype=np.float32)\n        if 'gt_bboxes' in results:\n            assert not self.lazy\n            results = self._all_box_crop(results, results['crop_bbox'])\n        return results\n    def __repr__(self):\n        repr_str = (f'{self.__class__.__name__}('\n                    f'area_range={self.area_range}, '\n                    f'aspect_ratio_range={self.aspect_ratio_range}, '\n                    f'lazy={self.lazy})')\n        return repr_str\ndef is_seq_of(seq, expected_type, seq_type=None):\n    \"\"\"Check whether it is a sequence of some type.\n    Args:\n        seq (Sequence): The sequence to be checked.\n        expected_type (type): Expected type of sequence items.",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:816-843"
    },
    "5469": {
        "file_id": 450,
        "content": "This code is a pipeline for skeleton processing in PaddleVideo. It scales the bounding box and applies it to the lazy operation, performs cropping based on the new bounding box, and if 'gt_bboxes' is present in results, it crops all other boxes accordingly. The class also has a __repr__ method for string representation. There is also a helper function, is_seq_of, which checks whether a sequence contains items of a specific type.",
        "type": "comment"
    },
    "5470": {
        "file_id": 450,
        "content": "        seq_type (type, optional): Expected sequence type.\n    Returns:\n        bool: Whether the sequence is valid.\n    \"\"\"\n    if seq_type is None:\n        exp_seq_type = abc.Sequence\n    else:\n        assert isinstance(seq_type, type)\n        exp_seq_type = seq_type\n    if not isinstance(seq, exp_seq_type):\n        return False\n    for item in seq:\n        if not isinstance(item, expected_type):\n            return False\n    return True\ndef is_tuple_of(seq, expected_type):\n    \"\"\"Check whether it is a tuple of some type.\n    A partial method of :func:`is_seq_of`.\n    \"\"\"\n    return is_seq_of(seq, expected_type, seq_type=tuple)\n@PIPELINES.register()\nclass CenterCrop_V2(CropBase):\n    \"\"\"Crop the center area from images.\n    Required keys are \"img_shape\", \"imgs\" (optional), \"keypoint\" (optional),\n    added or modified keys are \"imgs\", \"keypoint\", \"crop_bbox\", \"lazy\" and\n    \"img_shape\". Required keys in \"lazy\" is \"crop_bbox\", added or modified key\n    is \"crop_bbox\".\n    Args:\n        crop_size (int | tuple[int]): (w, h) of crop size.",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:844-880"
    },
    "5471": {
        "file_id": 450,
        "content": "The code defines a function `is_seq_of` that checks if a given sequence is of the expected type. It also defines two partial methods, `is_tuple_of`, which uses `is_seq_of` to check if a sequence is a tuple of a certain type. Lastly, it registers a new pipeline `CenterCrop_V2` for cropping the center area from images with required keys \"img_shape\", \"imgs\" (optional), and modified or added keys as mentioned in the function description.",
        "type": "comment"
    },
    "5472": {
        "file_id": 450,
        "content": "        lazy (bool): Determine whether to apply lazy operation. Default: False.\n    \"\"\"\n    def __init__(self, crop_size, lazy=False):\n        self.crop_size = _pair(crop_size)\n        self.lazy = lazy\n        if not is_tuple_of(self.crop_size, int):\n            raise TypeError(f'Crop_size must be int or tuple of int, '\n                            f'but got {type(crop_size)}')\n    def __call__(self, results):\n        \"\"\"Performs the CenterCrop augmentation.\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"\n        _init_lazy_if_proper(results, self.lazy)\n        if 'keypoint' in results:\n            assert not self.lazy, ('Keypoint Augmentations are not compatible '\n                                   'with lazy == True')\n        img_h, img_w = results['img_shape']\n        crop_w, crop_h = self.crop_size\n        left = (img_w - crop_w) // 2\n        top = (img_h - crop_h) // 2\n        right = left + crop_w\n        bottom = top + crop_h",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:881-909"
    },
    "5473": {
        "file_id": 450,
        "content": "This code defines a class for CenterCrop augmentation. It initializes with crop size and lazy operation flag, checks the validity of input parameters, performs CenterCrop operation on images, and handles keypoint augmentations if present in results dictionary.",
        "type": "comment"
    },
    "5474": {
        "file_id": 450,
        "content": "        new_h, new_w = bottom - top, right - left\n        crop_bbox = np.array([left, top, right, bottom])\n        results['crop_bbox'] = crop_bbox\n        results['img_shape'] = (new_h, new_w)\n        if 'crop_quadruple' not in results:\n            results['crop_quadruple'] = np.array(\n                [0, 0, 1, 1],  # x, y, w, h\n                dtype=np.float32)\n        x_ratio, y_ratio = left / img_w, top / img_h\n        w_ratio, h_ratio = new_w / img_w, new_h / img_h\n        old_crop_quadruple = results['crop_quadruple']\n        old_x_ratio, old_y_ratio = old_crop_quadruple[0], old_crop_quadruple[1]\n        old_w_ratio, old_h_ratio = old_crop_quadruple[2], old_crop_quadruple[3]\n        new_crop_quadruple = [\n            old_x_ratio + x_ratio * old_w_ratio,\n            old_y_ratio + y_ratio * old_h_ratio, w_ratio * old_w_ratio,\n            h_ratio * old_h_ratio\n        ]\n        results['crop_quadruple'] = np.array(\n            new_crop_quadruple, dtype=np.float32)\n        if not self.lazy:\n            if 'keypoint' in results:",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:910-936"
    },
    "5475": {
        "file_id": 450,
        "content": "This code calculates the new image shape and crop box coordinates based on the provided top, left, right, and bottom values. It then updates the 'crop_bbox' and 'img_shape' in the results dictionary. If 'crop_quadruple' is not already present in the results, it creates and appends it. The code then calculates new crop quadruple coordinates by adjusting the old ones with the ratios of the original and new image widths and heights. Finally, if 'keypoint' is present in the results, the code proceeds further (presumably for lazy mode).",
        "type": "comment"
    },
    "5476": {
        "file_id": 450,
        "content": "                results['keypoint'] = self._crop_kps(results['keypoint'],\n                                                     crop_bbox)\n            if 'imgs' in results:\n                results['imgs'] = self._crop_imgs(results['imgs'], crop_bbox)\n        else:\n            lazyop = results['lazy']\n            if lazyop['flip']:\n                raise NotImplementedError('Put Flip at last for now')\n            # record crop_bbox in lazyop dict to ensure only crop once in Fuse\n            lazy_left, lazy_top, lazy_right, lazy_bottom = lazyop['crop_bbox']\n            left = left * (lazy_right - lazy_left) / img_w\n            right = right * (lazy_right - lazy_left) / img_w\n            top = top * (lazy_bottom - lazy_top) / img_h\n            bottom = bottom * (lazy_bottom - lazy_top) / img_h\n            lazyop['crop_bbox'] = np.array(\n                [(lazy_left + left), (lazy_top + top), (lazy_left + right),\n                 (lazy_top + bottom)],\n                dtype=np.float32)\n        if 'gt_bboxes' in results:",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:937-957"
    },
    "5477": {
        "file_id": 450,
        "content": "This code is handling the case where 'lazyop' contains a flip operation. It records crop_bbox in lazyop to ensure only one crop operation is performed in Fuse. If 'gt_bboxes' is present in results, it indicates ground truth bbox information.",
        "type": "comment"
    },
    "5478": {
        "file_id": 450,
        "content": "            assert not self.lazy\n            results = self._all_box_crop(results, results['crop_bbox'])\n        return results\n    def __repr__(self):\n        repr_str = (f'{self.__class__.__name__}(crop_size={self.crop_size}, '\n                    f'lazy={self.lazy})')\n        return repr_str\n@PIPELINES.register()\nclass Flip_V2:\n    \"\"\"Flip the input images with a probability.\n    Reverse the order of elements in the given imgs with a specific direction.\n    The shape of the imgs is preserved, but the elements are reordered.\n    Required keys are \"img_shape\", \"modality\", \"imgs\" (optional), \"keypoint\"\n    (optional), added or modified keys are \"imgs\", \"keypoint\", \"lazy\" and\n    \"flip_direction\". Required keys in \"lazy\" is None, added or modified key\n    are \"flip\" and \"flip_direction\". The Flip augmentation should be placed\n    after any cropping / reshaping augmentations, to make sure crop_quadruple\n    is calculated properly.\n    Args:\n        flip_ratio (float): Probability of implementing flip. Default: 0.5.",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:958-984"
    },
    "5479": {
        "file_id": 450,
        "content": "This code snippet is registering a pipeline class called \"Flip_V2\". The Flip_V2 class flips the input images with a probability and reverses the order of elements in the given imgs with a specific direction. It requires keys such as \"img_shape\", \"modality\", and \"imgs\" while adding or modifying keys like \"imgs\", \"keypoint\", \"lazy\", and \"flip_direction\". Flip_V2 should be placed after cropping/reshaping augmentations to ensure crop_quadruple is calculated properly. The flip ratio, which determines the probability of implementing flip, is set to 0.5 by default.",
        "type": "comment"
    },
    "5480": {
        "file_id": 450,
        "content": "        direction (str): Flip imgs horizontally or vertically. Options are\n            \"horizontal\" | \"vertical\". Default: \"horizontal\".\n        flip_label_map (Dict[int, int] | None): Transform the label of the\n            flipped image with the specific label. Default: None.\n        left_kp (list[int]): Indexes of left keypoints, used to flip keypoints.\n            Default: None.\n        right_kp (list[ind]): Indexes of right keypoints, used to flip\n            keypoints. Default: None.\n        lazy (bool): Determine whether to apply lazy operation. Default: False.\n    \"\"\"\n    _directions = ['horizontal', 'vertical']\n    def __init__(self,\n                 flip_ratio=0.5,\n                 direction='horizontal',\n                 flip_label_map=None,\n                 left_kp=None,\n                 right_kp=None,\n                 lazy=False):\n        if direction not in self._directions:\n            raise ValueError(f'Direction {direction} is not supported. '\n                             f'Currently support ones are {self._directions}')",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:985-1006"
    },
    "5481": {
        "file_id": 450,
        "content": "This code snippet is for an object called \"SkeletonPipeline\". It has parameters for direction, flip_label_map, left_kp, right_kp, and lazy. The direction parameter can be either 'horizontal' or 'vertical'. Flip_label_map is a dictionary used to transform the label of flipped images. Left_kp and right_kp are indexes used for flipping keypoints. Lazy determines whether to apply lazy operations, default set to False. The function checks if the given direction is within ['horizontal', 'vertical'].",
        "type": "comment"
    },
    "5482": {
        "file_id": 450,
        "content": "        self.flip_ratio = flip_ratio\n        self.direction = direction\n        self.flip_label_map = flip_label_map\n        self.left_kp = left_kp\n        self.right_kp = right_kp\n        self.lazy = lazy\n    def _flip_imgs(self, imgs, modality):\n        _ = [imflip_(img, self.direction) for img in imgs]\n        lt = len(imgs)\n        if modality == 'Flow':\n            # The 1st frame of each 2 frames is flow-x\n            for i in range(0, lt, 2):\n                imgs[i] = iminvert(imgs[i])\n        return imgs\n    def _flip_kps(self, kps, kpscores, img_width):\n        kp_x = kps[..., 0]\n        kp_x[kp_x != 0] = img_width - kp_x[kp_x != 0]\n        new_order = list(range(kps.shape[2]))\n        if self.left_kp is not None and self.right_kp is not None:\n            for left, right in zip(self.left_kp, self.right_kp):\n                new_order[left] = right\n                new_order[right] = left\n        kps = kps[:, :, new_order]\n        if kpscores is not None:\n            kpscores = kpscores[:, :, new_order]\n        return kps, kpscores",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:1007-1034"
    },
    "5483": {
        "file_id": 450,
        "content": "This code is a part of the skeleton_pipeline module in PaddleVideo. It initializes parameters such as flip ratio, direction, and flips label map, and defines a function _flip_imgs for image flipping and another function _flip_kps for keypoints flipping based on the direction provided. The code also includes conditions to handle flow images specifically by inverting the first frame of each two frames.",
        "type": "comment"
    },
    "5484": {
        "file_id": 450,
        "content": "    @staticmethod\n    def _box_flip(box, img_width):\n        \"\"\"Flip the bounding boxes given the width of the image.\n        Args:\n            box (np.ndarray): The bounding boxes.\n            img_width (int): The img width.\n        \"\"\"\n        box_ = box.copy()\n        box_[..., 0::4] = img_width - box[..., 2::4]\n        box_[..., 2::4] = img_width - box[..., 0::4]\n        return box_\n    def __call__(self, results):\n        \"\"\"Performs the Flip augmentation.\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"\n        _init_lazy_if_proper(results, self.lazy)\n        if 'keypoint' in results:\n            assert not self.lazy, ('Keypoint Augmentations are not compatible '\n                                   'with lazy == True')\n            assert self.direction == 'horizontal', (\n                'Only horizontal flips are'\n                'supported for human keypoints')\n        modality = results['modality']\n        if modality == 'Flow':",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:1036-1065"
    },
    "5485": {
        "file_id": 450,
        "content": "This code snippet is from the PaddleVideo library's skeleton pipeline. It defines a function for flipping bounding boxes and a method that applies horizontal flip augmentation to images, unless the image contains keypoints where only horizontal flip is supported. The code checks if the modality of the image is 'Flow'.",
        "type": "comment"
    },
    "5486": {
        "file_id": 450,
        "content": "            assert self.direction == 'horizontal'\n        flip = np.random.rand() < self.flip_ratio\n        results['flip'] = flip\n        results['flip_direction'] = self.direction\n        img_width = results['img_shape'][1]\n        if self.flip_label_map is not None and flip:\n            results['label'] = self.flip_label_map.get(results['label'],\n                                                       results['label'])\n        if not self.lazy:\n            if flip:\n                if 'imgs' in results:\n                    results['imgs'] = self._flip_imgs(results['imgs'], modality)\n                if 'keypoint' in results:\n                    kp = results['keypoint']\n                    kpscore = results.get('keypoint_score', None)\n                    kp, kpscore = self._flip_kps(kp, kpscore, img_width)\n                    results['keypoint'] = kp\n                    if 'keypoint_score' in results:\n                        results['keypoint_score'] = kpscore\n        else:\n            lazyop = results['lazy']",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:1066-1090"
    },
    "5487": {
        "file_id": 450,
        "content": "The code checks if the direction is horizontal, flips the image randomly based on a flip ratio, and updates results accordingly. If the flip label map is not None and flip occurs, it updates the label in the results. If lazy is not set, it flips images and keypoints if necessary, updating results. Otherwise, it stores the operation for later execution.",
        "type": "comment"
    },
    "5488": {
        "file_id": 450,
        "content": "            if lazyop['flip']:\n                raise NotImplementedError('Use one Flip please')\n            lazyop['flip'] = flip\n            lazyop['flip_direction'] = self.direction\n        if 'gt_bboxes' in results and flip:\n            assert not self.lazy and self.direction == 'horizontal'\n            width = results['img_shape'][1]\n            results['gt_bboxes'] = self._box_flip(results['gt_bboxes'], width)\n            if 'proposals' in results and results['proposals'] is not None:\n                assert results['proposals'].shape[1] == 4\n                results['proposals'] = self._box_flip(results['proposals'],\n                                                      width)\n        return results\n    def __repr__(self):\n        repr_str = (\n            f'{self.__class__.__name__}('\n            f'flip_ratio={self.flip_ratio}, direction={self.direction}, '\n            f'flip_label_map={self.flip_label_map}, lazy={self.lazy})')\n        return repr_str\n@PIPELINES.register()\nclass FormatShape:\n    \"\"\"Format final imgs shape to the given input_format.",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:1091-1117"
    },
    "5489": {
        "file_id": 450,
        "content": "This code snippet is part of the \"SkeletonPipeline\" class in PaddleVideo. It checks if the 'flip' parameter is set and applies horizontal flipping to the 'gt_bboxes' and 'proposals' (if present) based on the direction specified. It also defines a __repr__ method for the class, providing information about its attributes, and registers a new pipeline called \"FormatShape\" for use in the system.",
        "type": "comment"
    },
    "5490": {
        "file_id": 450,
        "content": "    Required keys are \"imgs\", \"num_clips\" and \"clip_len\", added or modified\n    keys are \"imgs\" and \"input_shape\".\n    Args:\n        input_format (str): Define the final imgs format.\n        collapse (bool): To collpase input_format N... to ... (NCTHW to CTHW,\n            etc.) if N is 1. Should be set as True when training and testing\n            detectors. Default: False.\n    \"\"\"\n    def __init__(self, input_format, collapse=False):\n        self.input_format = input_format\n        self.collapse = collapse\n        if self.input_format not in ['NCTHW', 'NCHW', 'NCHW_Flow', 'NPTCHW']:\n            raise ValueError(\n                f'The input format {self.input_format} is invalid.')\n    def __call__(self, results):\n        \"\"\"Performs the FormatShape formating.\n        Args:\n            results (dict): The resulting dict to be modified and passed\n                to the next transform in pipeline.\n        \"\"\"\n        if not isinstance(results['imgs'], np.ndarray):\n            results['imgs'] = np.array(results['imgs'])",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:1119-1144"
    },
    "5491": {
        "file_id": 450,
        "content": "The code defines a class for formatting image data in a specific format based on the input_format parameter. The class takes an input_format and a collapse boolean argument, and checks if the input_format is valid (options are 'NCTHW', 'NCHW', 'NCHW_Flow', or 'NPTCHW'). If results['imgs'] is not of type np.ndarray, it converts it to np.ndarray.",
        "type": "comment"
    },
    "5492": {
        "file_id": 450,
        "content": "        imgs = results['imgs']\n        # [M x H x W x C]\n        # M = 1 * N_crops * N_clips * L\n        if self.collapse:\n            assert results['num_clips'] == 1\n        if self.input_format == 'NCTHW':\n            num_clips = results['num_clips']\n            clip_len = results['clip_len']\n            imgs = imgs.reshape((-1, num_clips, clip_len) + imgs.shape[1:])\n            # N_crops x N_clips x L x H x W x C\n            imgs = np.transpose(imgs, (0, 1, 5, 2, 3, 4))\n            # N_crops x N_clips x C x L x H x W\n            imgs = imgs.reshape((-1, ) + imgs.shape[2:])\n            # M' x C x L x H x W\n            # M' = N_crops x N_clips\n        elif self.input_format == 'NCHW':\n            imgs = np.transpose(imgs, (0, 3, 1, 2))\n            # M x C x H x W\n        elif self.input_format == 'NCHW_Flow':\n            num_clips = results['num_clips']\n            clip_len = results['clip_len']\n            imgs = imgs.reshape((-1, num_clips, clip_len) + imgs.shape[1:])\n            # N_crops x N_clips x L x H x W x C",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:1145-1169"
    },
    "5493": {
        "file_id": 450,
        "content": "This code is a part of the SkeletonPipeline class in PaddleVideo. It processes images from results and reshapes them based on the input format specified. If input_format is 'NCTHW', it transposes and reshapes the images accordingly, if it's 'NCHW', it only transposes, and if it's 'NCHW_Flow', it also performs reshaping similar to 'NCTHW'. The 'collapse' check ensures that if results have multiple clips, it won't collapse them.",
        "type": "comment"
    },
    "5494": {
        "file_id": 450,
        "content": "            imgs = np.transpose(imgs, (0, 1, 2, 5, 3, 4))\n            # N_crops x N_clips x L x C x H x W\n            imgs = imgs.reshape((-1, imgs.shape[2] * imgs.shape[3]) +\n                                imgs.shape[4:])\n            # M' x C' x H x W\n            # M' = N_crops x N_clips\n            # C' = L x C\n        elif self.input_format == 'NPTCHW':\n            num_proposals = results['num_proposals']\n            num_clips = results['num_clips']\n            clip_len = results['clip_len']\n            imgs = imgs.reshape((num_proposals, num_clips * clip_len) +\n                                imgs.shape[1:])\n            # P x M x H x W x C\n            # M = N_clips x L\n            imgs = np.transpose(imgs, (0, 1, 4, 2, 3))\n            # P x M x C x H x W\n        if self.collapse:\n            assert imgs.shape[0] == 1\n            imgs = imgs.squeeze(0)\n        results['imgs'] = imgs\n        results['input_shape'] = imgs.shape\n        return results\n    def __repr__(self):\n        repr_str = self.__class__.__name__",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:1170-1197"
    },
    "5495": {
        "file_id": 450,
        "content": "This code transforms image data into various formats depending on the input format specified. It supports 'NHWC', 'NCHW', and 'NPTCHW' formats. If the collapse parameter is True, it squeezes the first dimension of the images array. The results dictionary is updated with the transformed images and their shape.",
        "type": "comment"
    },
    "5496": {
        "file_id": 450,
        "content": "        repr_str += f\"(input_format='{self.input_format}')\"\n        return repr_str\n@PIPELINES.register()\nclass Collect:\n    \"\"\"Collect data from the loader relevant to the specific task.\n    This keeps the items in ``keys`` as it is, and collect items in\n    ``meta_keys`` into a meta item called ``meta_name``.This is usually\n    the last stage of the data loader pipeline.\n    For example, when keys='imgs', meta_keys=('filename', 'label',\n    'original_shape'), meta_name='img_metas', the results will be a dict with\n    keys 'imgs' and 'img_metas', where 'img_metas' is a DataContainer of\n    another dict with keys 'filename', 'label', 'original_shape'.\n    Args:\n        keys (Sequence[str]): Required keys to be collected.\n        meta_name (str): The name of the key that contains meta infomation.\n            This key is always populated. Default: \"img_metas\".\n        meta_keys (Sequence[str]): Keys that are collected under meta_name.\n            The contents of the ``meta_name`` dictionary depends on\n            ``meta_keys``.",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:1198-1220"
    },
    "5497": {
        "file_id": 450,
        "content": "The code defines a Pipeline class called \"Collect\" that collects specific data from the loader relevant to the task. It keeps keys as is and gathers items in meta_keys into a meta item called meta_name, typically used as the last stage of the data loader pipeline. The Collect class takes keys, meta_name, and meta_keys as arguments, with default values provided for meta_name.",
        "type": "comment"
    },
    "5498": {
        "file_id": 450,
        "content": "            By default this includes:\n            - \"filename\": path to the image file\n            - \"label\": label of the image file\n            - \"original_shape\": original shape of the image as a tuple\n                (h, w, c)\n            - \"img_shape\": shape of the image input to the network as a tuple\n                (h, w, c).  Note that images may be zero padded on the\n                bottom/right, if the batch tensor is larger than this shape.\n            - \"pad_shape\": image shape after padding\n            - \"flip_direction\": a str in (\"horiziontal\", \"vertival\") to\n                indicate if the image is fliped horizontally or vertically.\n            - \"img_norm_cfg\": a dict of normalization information:\n                - mean - per channel mean subtraction\n                - std - per channel std divisor\n                - to_rgb - bool indicating if bgr was converted to rgb\n        nested (bool): If set as True, will apply data[x] = [data[x]] to all\n            items in data. The arg is added for compatibility. Default: False.",
        "type": "code",
        "location": "/paddlevideo/loader/pipelines/skeleton_pipeline.py:1221-1238"
    },
    "5499": {
        "file_id": 450,
        "content": "This code defines a dictionary containing default parameters for image data loading. It includes fields such as \"filename\", \"label\", \"original_shape\", \"img_shape\", \"pad_shape\", \"flip_direction\", and \"img_norm_cfg\". The \"nested\" argument determines whether these parameters should be applied recursively to all items within the data dictionary.",
        "type": "comment"
    }
}