{
    "0": {
        "file_id": 0,
        "content": "/MANIFEST.in",
        "type": "filepath"
    },
    "1": {
        "file_id": 0,
        "content": "This code is specifying the files and directories to include in the package distribution for PaddleVideo. It includes important documents like LICENSE and README, utilities scripts like utils.py and ava_predict.py, and key data file Kinetics-400_label_list.txt. Additionally, it uses a recursive-include to incorporate all .py and .txt files within the paddlevideo directory.",
        "type": "summary"
    },
    "2": {
        "file_id": 0,
        "content": "include LICENSE\ninclude README.md\ninclude tools/__init__.py\ninclude tools/utils.py\ninclude tools/ava_predict.py\ninclude tools/wheel.py\ninclude data/k400/Kinetics-400_label_list.txt\nrecursive-include paddlevideo/ *.py *.txt",
        "type": "code",
        "location": "/MANIFEST.in:1-9"
    },
    "3": {
        "file_id": 0,
        "content": "This code is specifying the files and directories to include in the package distribution for PaddleVideo. It includes important documents like LICENSE and README, utilities scripts like utils.py and ava_predict.py, and key data file Kinetics-400_label_list.txt. Additionally, it uses a recursive-include to incorporate all .py and .txt files within the paddlevideo directory.",
        "type": "comment"
    },
    "4": {
        "file_id": 1,
        "content": "/README.md",
        "type": "filepath"
    },
    "5": {
        "file_id": 1,
        "content": "PaddleVideo is a Python library for advanced video processing, featuring industry-specific models and data production to deployment pipeline support. The documentation includes sections on distillation, inference deployment, datasets, application scenarios, and licensing information (Apache 2.0).",
        "type": "summary"
    },
    "6": {
        "file_id": 1,
        "content": "[English](README_en.md) | ä¸­æ–‡\n# PaddleVideo\n![python version](https://img.shields.io/badge/python-3.7+-orange.svg) ![paddle version](https://img.shields.io/badge/PaddlePaddle-2.3.1-blue)\n## ç®€ä»‹\nPaddleVideoæ—¨åœ¨æ‰“é€ ä¸€å¥—ä¸°å¯Œã€é¢†å…ˆä¸”å®ç”¨çš„Videoå·¥å…·åº“ï¼Œæ—¨åœ¨å¸®åŠ©å¼€å‘è€…æ›´å¥½çš„è¿›è¡Œè§†é¢‘é¢†åŸŸçš„å­¦æœ¯ç ”ç©¶å’Œäº§ä¸šå®è·µã€‚\n<div align=\"center\">\n  <img src=\"docs/images/home.gif\" width=\"450px\"/><br>\n</div>\n## è¿‘æœŸæ›´æ–°\n- å¼€æºè§†é¢‘æ ‡æ³¨å·¥å…·ğŸŒŸ[BILS](./docs/zh-CN/annotation_tools.md)ï¼Œæ¬¢è¿ä¸‹è½½å®‰è£…åŒ…ä½“éªŒï½\n- å‘å¸ƒè½»é‡åŒ–è¡Œä¸ºè¯†åˆ«æ¨¡å‹**ğŸ”¥[PP-TSMv2](./docs/zh-CN/model_zoo/recognition/pp-tsm_v2.md)**, Kinetics-400ç²¾åº¦75.16%ï¼Œ25fpsçš„10sè§†é¢‘cpuæ¨ç†æ—¶é—´ä»…éœ€456ms.å„æ¨¡å‹æ€§èƒ½å¯¹æ¯”[benchmark](./docs/zh-CN/benchmark.md).\n- æ–°å¢[çŸ¥è¯†è’¸é¦](./docs/zh-CN/distillation.md)åŠŸèƒ½.\n- æ–°å¢åŸºäºtransformerçš„è¡Œä¸ºè¯†åˆ«æ¨¡å‹[TokenShift](https://github.com/PaddlePaddle/PaddleVideo/blob/develop/docs/zh-CN/model_zoo/recognition/tokenshift_transformer.md).\n- æ–°å¢åŸºäºéª¨éª¼ç‚¹çš„è¡Œä¸ºè¯†åˆ«æ¨¡å‹[2s-ACGN](https://github.com/PaddlePaddle/PaddleVideo/blob/develop/docs/zh-CN/model_zoo/recognition/agcn2s.md)ã€[CTR-GCN](./docs/zh-CN/model_zoo/recognition/ctrgcn.md).\n- æ–°å¢å•é˜¶æ®µæ—¶ç©ºåŠ¨ä½œæ£€æµ‹æ¨¡å‹[YOWO](./docs/zh-CN/model_zoo/localization/yowo.md).",
        "type": "code",
        "location": "/README.md:1-22"
    },
    "7": {
        "file_id": 1,
        "content": "PaddleVideo is a Python library for advanced video processing, providing extensive and cutting-edge tools to assist researchers and industry professionals in the field of computer vision. The recent updates include an open-source video annotation tool (BILS), a lightweight action recognition model (PP-TSMv2), knowledge distillation functionality, transformer-based models, and single-stage action detection models (YOWO).",
        "type": "comment"
    },
    "8": {
        "file_id": 1,
        "content": "ğŸ‘€ ğŸŒŸ  **ã€Šäº§ä¸šçº§è§†é¢‘æŠ€æœ¯ä¸åº”ç”¨æ¡ˆä¾‹ã€‹ç³»åˆ—è¯¾ç¨‹å›æ”¾é“¾æ¥**:  https://aistudio.baidu.com/aistudio/course/introduce/6742 ğŸŒŸ\nâ€‹\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  ğŸ’– **æ¬¢è¿å¤§å®¶æ‰«ç å…¥ç¾¤è®¨è®º** ğŸ’–\n<div align=\"center\">\n  <img src=\"docs/images/user_group.png\" width=250/></div>\n- æ·»åŠ æˆåŠŸåå›å¤ã€è§†é¢‘ã€‘åŠ å…¥äº¤æµç¾¤\n## ç‰¹æ€§\næ”¯æŒå¤šç§Videoç›¸å…³å‰æ²¿ç®—æ³•ï¼Œåœ¨æ­¤åŸºç¡€ä¸Šæ‰“é€ äº§ä¸šçº§ç‰¹è‰²æ¨¡å‹[PP-TSM](docs/zh-CN/model_zoo/recognition/pp-tsm.md)å’Œ[PP-TSMv2](docs/zh-CN/model_zoo/recognition/pp-tsm_v2.md)ï¼Œå¹¶æ‰“é€šæ•°æ®ç”Ÿäº§ã€æ¨¡å‹è®­ç»ƒã€å‹ç¼©ã€é¢„æµ‹éƒ¨ç½²å…¨æµç¨‹ã€‚\n<div align=\"center\">\n    <img src=\"./docs/images/features.png\" width=\"700\">\n</div>\n## å¿«é€Ÿå¼€å§‹\n- ä¸€è¡Œå‘½ä»¤å¿«é€Ÿä½¿ç”¨: [å¿«é€Ÿå¼€å§‹](./docs/zh-CN/quick_start.md)\n## åœºæ™¯åº”ç”¨\nPaddleVideoåœºæ™¯åº”ç”¨è¦†ç›–ä½“è‚²ã€äº’è”ç½‘ã€å·¥ä¸šã€åŒ»ç–—è¡Œä¸šï¼Œåœ¨PP-TSMçš„åŸºç¡€èƒ½åŠ›ä¹‹ä¸Šï¼Œä»¥æ¡ˆä¾‹çš„å½¢å¼å±•ç¤ºåˆ©ç”¨åœºæ™¯æ•°æ®å¾®è°ƒã€æ¨¡å‹ä¼˜åŒ–æ–¹æ³•ã€æ•°æ®å¢å¹¿ç­‰å†…å®¹ï¼Œä¸ºå¼€å‘è€…å®é™…è½åœ°æä¾›ç¤ºèŒƒä¸å¯å‘ã€‚è¯¦æƒ…å¯æŸ¥çœ‹[åº”ç”¨](./applications/)ã€‚\n## æ–‡æ¡£æ•™ç¨‹\n- [å¿«é€Ÿå¼€å§‹](./docs/zh-CN/quick_start.md)\n- [å®‰è£…è¯´æ˜](./docs/zh-CN/install.md)\n- [è®­ç»ƒ/æµ‹è¯•/æ¨ç†å…¨æµç¨‹ä½¿ç”¨æŒ‡å—](./docs/zh-CN/usage.md)\n- [PP-TSMè¡Œä¸ºè¯†åˆ«ğŸ”¥](./docs/zh-CN/model_zoo/recognition/pp-tsm.md)\n  - [æ¨¡å‹åº“](./docs/zh-CN/model_zoo/recognition/pp-tsm.md#7)\n  - [æ¨¡å‹è®­ç»ƒ](./docs/zh-CN/model_zoo/recognition/pp-tsm.md#4)\n  - [æ¨¡å‹å‹ç¼©](./deploy/slim/)\n      - [æ¨¡å‹é‡åŒ–](./deploy/slim/readme.md)",
        "type": "code",
        "location": "/README.md:25-58"
    },
    "9": {
        "file_id": 1,
        "content": "This code is for PaddleVideo, a series of industry-level video technology and application case courses. It supports various video cutting-edge algorithms, creates industry-specific models PP-TSM and PP-TSMv2, and covers the entire data production, model training, compression, and deployment pipeline. The code provides quick start instructions, scene application examples, and documentation for tutorials on different topics such as recognition, model library, and model compression. It also includes links to join discussion groups and course replay.",
        "type": "comment"
    },
    "10": {
        "file_id": 1,
        "content": "      - [çŸ¥è¯†è’¸é¦](./docs/zh-CN/distillation.md)\n  - [æ¨ç†éƒ¨ç½²](./deploy/)\n      - [åŸºäºPythoné¢„æµ‹å¼•æ“æ¨ç†](./docs/zh-CN/model_zoo/recognition/pp-tsm.md#62)\n      - [åŸºäºC++é¢„æµ‹å¼•æ“æ¨ç†](./deploy/cpp_infer/readme.md)\n      - [æœåŠ¡ç«¯éƒ¨ç½²](./deploy/python_serving/readme.md)\n      - [Paddle2ONNXæ¨¡å‹è½¬åŒ–ä¸é¢„æµ‹](./deploy/paddle2onnx/readme.md)\n      - [Benchmark](./docs/zh-CN/benchmark.md)\n- [å‰æ²¿ç®—æ³•ä¸æ¨¡å‹](./docs/zh-CN/model_zoo/README.md)ğŸš€\n- [æ•°æ®é›†](./docs/zh-CN/dataset/README.md)\n- [åœºæ™¯åº”ç”¨](./applications/README.md)\n- [æ•°æ®æ ‡æ³¨](./docs/zh-CN/annotation_tools.md)\n- [èµ›äº‹æ”¯æŒ](./docs/zh-CN/competition.md)\n- [è´¡çŒ®ä»£ç ](./docs/zh-CN/contribute/README.md)\n## è®¸å¯è¯ä¹¦\næœ¬é¡¹ç›®çš„å‘å¸ƒå—[Apache 2.0 license](LICENSE)è®¸å¯è®¤è¯ã€‚",
        "type": "code",
        "location": "/README.md:59-75"
    },
    "11": {
        "file_id": 1,
        "content": "This code provides a table of contents for the PaddleVideo documentation, including sections on distillation, inference deployment using Python and C++ engines, server-side deployment, converting to ONNX models, state-of-the-art algorithms and models, datasets, application scenarios, data labeling tools, competition support, contributing code, and licensing information (Apache 2.0).",
        "type": "comment"
    },
    "12": {
        "file_id": 2,
        "content": "/README_en.md",
        "type": "filepath"
    },
    "13": {
        "file_id": 2,
        "content": "PaddleVideo is a deep learning library for video processing, offering pre-trained models, training, compression, inference, and deployment options, along with installation guides, datasets, and annotation tools under the Apache 2.0 license.",
        "type": "summary"
    },
    "14": {
        "file_id": 2,
        "content": "[ç®€ä½“ä¸­æ–‡](README.md) | English\n# PaddleVideo\n![python version](https://img.shields.io/badge/python-3.7+-orange.svg) ![paddle version](https://img.shields.io/badge/PaddlePaddle-2.0-blue)\n## Introduction\nPaddleVideo is a toolset for video tasks prepared for the industry and academia. This repository provides examples and best practice guildelines for exploring deep learning algorithm in the scene of video area.\n<div align=\"center\">\n  <img src=\"docs/images/home.gif\" width=\"450px\"/><br>\n</div>\n## Update:\n- release **ğŸ”¥[PP-TSMv2](./docs/zh-CN/model_zoo/recognition/pp-tsm.md)**, an lite action recognition model, top1_acc on Kinetics-400 is 74.38%ï¼Œcpu inference time on 10s video with 25fps is only 433ms. [benchmark](./docs/zh-CN/benchmark.md).\n- add [Knowledge Distilltion](./docs/zh-CN/distillation.md) framework code.\n- add [TokenShift](https://github.com/PaddlePaddle/PaddleVideo/blob/develop/docs/zh-CN/model_zoo/recognition/tokenshift_transformer.md), [2s-ACGN](https://github.com/PaddlePaddle/PaddleVideo/b",
        "type": "code",
        "location": "/README_en.md:1-20"
    },
    "15": {
        "file_id": 2,
        "content": "This code is for PaddleVideo, a toolset for video tasks in industry and academia. It provides examples and best practice guidelines for deep learning algorithms in the video domain. Recent updates include the release of PP-TSMv2 (lite action recognition model), addition of Knowledge Distillation framework code, and TokenShift and 2s-ACGN models. Python version required is 3.7+, and it uses PaddlePaddle version 2.0.",
        "type": "comment"
    },
    "16": {
        "file_id": 2,
        "content": "lob/develop/docs/zh-CN/model_zoo/recognition/agcn2s.md) and [CTR-GCN](./docs/zh-CN/model_zoo/recognition/ctrgcn.md) model.\nâ€‹ ğŸ’– **Welcome to scan the code and join the group discussion** ğŸ’–\n<div align=\"center\">\n  <img src=\"docs/images/user_group.png\" width=250/></div>\n- Scan the QR code below with your Wechat and reply \"video\", you can access to official technical exchange group. Look forward to your participation.\n## Features\nPaddleVideo support a variety of cutting-edge algorithms related to video, and developed industrial featured models/solution [PP-TSM](docs/zh-CN/model_zoo/recognition/pp-tsm.md) and [PP-TSMv2](docs/zh-CN/model_zoo/recognition/pp-tsm.md) on this basis, and get through the whole process of data production, model training, compression, inference and deployment.\n<div align=\"center\">\n    <img src=\"./docs/images/features_en.png\" width=\"700\">\n</div>\n## Quick Start\n- One line of code quick use: [Quick Start](./docs/zh-CN/quick_start.md)\n## Tutorials\n- [Quick Start](./docs/zh-CN/quick_start.md)",
        "type": "code",
        "location": "/README_en.md:20-43"
    },
    "17": {
        "file_id": 2,
        "content": "This code is from the \"PaddleVideo\" project's README file. It introduces PaddleVideo as a platform that supports various cutting-edge video algorithms, developed industrial featured models like PP-TSM and PP-TSMv2, and provides a full process of data production, model training, compression, inference, and deployment. The code also mentions the availability of quick start guides and tutorials to make it easier for users to get started with PaddleVideo.",
        "type": "comment"
    },
    "18": {
        "file_id": 2,
        "content": "- [Installation](./docs/zh-CN/install.md)\n- [Usage](./docs/zh-CN/usage.md)\n- [PP-TSMğŸ”¥](./docs/zh-CN/model_zoo/recognition/pp-tsm.md)\n  - [Model Zoo](./docs/zh-CN/model_zoo/recognition/pp-tsm.md#7)\n  - [Model training](./docs/zh-CN/model_zoo/recognition/pp-tsm.md#4)\n  - [Model Compression](./deploy/slim/)\n      - [Model Quantization](./deploy/slim/readme.md)\n      - [Knowledge Distillation](./docs/zh-CN/distillation.md)\n  - [Inference and Deployment](./deploy/)\n      - [Python Inference](./docs/zh-CN/model_zoo/recognition/pp-tsm.md#62)\n      - [C++ Inference](./deploy/cpp_infer/readme.md)\n      - [Serving](./deploy/python_serving/readme.md)\n      - [Paddle2ONNX](./deploy/paddle2onnx/readme.md)\n      - [Benchmark](./docs/zh-CN/benchmark.md)\n- [Academic algorithms](./docs/en/model_zoo/README.md)ğŸš€\n- [Datasets](./docs/en/dataset/README.md)\n- [Data Annotation](./applications/BILS)\n- [Contribute](./docs/zh-CN/contribute/README.md)\n## License\nPaddleVideo is released under the [Apache 2.0 license](LICENSE).",
        "type": "code",
        "location": "/README_en.md:44-65"
    },
    "19": {
        "file_id": 2,
        "content": "This code outlines the main components of PaddleVideo, a deep learning library for video processing. It includes installation instructions, usage guidelines, model zoo (pre-trained models), model training, model compression techniques such as quantization and knowledge distillation, inference and deployment options including Python, C++, and serving, academic algorithms, datasets, data annotation tool (BILS), and licensing information under Apache 2.0 license.",
        "type": "comment"
    },
    "20": {
        "file_id": 3,
        "content": "/__init__.py",
        "type": "filepath"
    },
    "21": {
        "file_id": 3,
        "content": "This code is a Python module with the license and copyright information. It imports a class named PaddleVideo from the tools package, and defines its availability as part of the __all__ list.",
        "type": "summary"
    },
    "22": {
        "file_id": 3,
        "content": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n__all__ = ['PaddleVideo']\nfrom .tools import PaddleVideo",
        "type": "code",
        "location": "/__init__.py:1-16"
    },
    "23": {
        "file_id": 3,
        "content": "This code is a Python module with the license and copyright information. It imports a class named PaddleVideo from the tools package, and defines its availability as part of the __all__ list.",
        "type": "comment"
    },
    "24": {
        "file_id": 4,
        "content": "/applications/AbnormalActionDetection/README.md",
        "type": "filepath"
    },
    "25": {
        "file_id": 4,
        "content": "This PaddleVideo code performs video action detection, using SlowFast+FasterRCNN model for abnormal behavior detection, with data preparation, training, evaluation, inference, and deployment. The code exports, deploys static models, and provides deployment instructions.",
        "type": "summary"
    },
    "26": {
        "file_id": 4,
        "content": "# å¼‚å¸¸è¡Œä¸ºè¯†åˆ«\n## å†…å®¹\n- [æ¨¡å‹ç®€ä»‹](#æ¨¡å‹ç®€ä»‹)\n- [æ•°æ®å‡†å¤‡](#æ•°æ®å‡†å¤‡)\n- [æ¨¡å‹è®­ç»ƒ](#æ¨¡å‹è®­ç»ƒ)\n- [æ¨¡å‹è¯„ä¼°](#æ¨¡å‹è¯„ä¼°)\n- [æ¨¡å‹æ¨ç†](#æ¨¡å‹æ¨ç†)\n- [æ¨¡å‹éƒ¨ç½²](#æ¨¡å‹éƒ¨ç½²)\n- [å‚è€ƒè®ºæ–‡](#å‚è€ƒè®ºæ–‡)\n## æ¨¡å‹ç®€ä»‹\nè¯¥ä»£ç åº“ç”¨äºå¼‚å¸¸è¡Œä¸ºæ£€æµ‹, åŸºäºpaddle2.2ç‰ˆæœ¬å¼€å‘ï¼Œç»“åˆPaddleVideoä¸­çš„SlowFast+FasterRCNNæ¨¡å‹å®ç°7ä¸ªå¼‚å¸¸è¡Œä¸ºçš„æ£€æµ‹ã€‚\nä¸»è¦æ¡†æ¶å¦‚ä¸‹ï¼š\n<div align=\"center\">\n  <img src=\"./images/SlowFast_FasterRCNN.png\" width=\"640px\"/><br>\n</div>\nAIStudioé¡¹ç›®: [åŸºäºæ—¶ç©ºä¿¡æ¯çš„å¼‚å¸¸è¡Œä¸ºæ£€æµ‹](https://aistudio.baidu.com/aistudio/projectdetail/3431613)\n## æ•°æ®å‡†å¤‡\n### Step1 ç¨€ç–æŠ½å–è§†é¢‘å¸§\né¦–å…ˆç¨€ç–æŠ½å–è§†é¢‘å¸§ç”¨äºæ£€æµ‹æ¯å¸§ä¸­äººçš„ä½ç½®ï¼š\n```\ncd data/ava/script && bash extract_video_frames.sh abnormal_action_videos abnormal_action_frames 2\n```\n* ç¬¬ä¸€ä¸ªå‚æ•°abnormal_action_videosï¼šè¢«æŠ½å¸§çš„è§†é¢‘æ ¹ç›®å½•ï¼›\n* ç¬¬äºŒä¸ªå‚æ•°abnormal_action_framesï¼šæŠ½å–çš„è§†é¢‘å¸§å­˜æ”¾ç›®å½•ï¼›\n* ç¬¬ä¸‰ä¸ªå‚æ•°2ï¼šæŠ½å¸§å¸§ç‡ã€‚\n### Step2 ç›®æ ‡æ£€æµ‹\nç”¨æˆç†Ÿçš„å¯æ£€æµ‹äººçš„ç›®æ ‡æ£€æµ‹æ¨¡å‹æ£€æµ‹ä¸Šè¿°æ­¥éª¤æŠ½å¾—çš„è§†é¢‘å¸§ä¸­çš„äººã€‚å¦‚PaddleDetectionå¥—ä»¶ä¸­çš„åŸºäºcocoæ•°æ®é›†è®­ç»ƒå¾—åˆ°çš„[PP-YOLOv2](https://github.com/PaddlePaddle/PaddleDetection/tree/develop/configs/ppyolo)æ¨¡å‹ã€‚\n### Step3 ç”Ÿæˆpklæ–‡ä»¶\nå°†ä¸Šè¿°æ­¥éª¤å¾—åˆ°çš„æ¯ä¸ªè§†é¢‘å¸§çš„æ£€æµ‹ç»“æœè¿›è¡Œè½¬åŒ–ï¼Œå¾—åˆ°SlowFast_FasterRCNNæ¨¡å‹éœ€è¦çš„è¾“å…¥æ ¼å¼ã€‚æ³¨æ„æˆ‘ä»¬åªéœ€è¦äººçš„æ£€æµ‹ç»“æœï¼Œå…¶ä»–ç›®æ ‡ä¸éœ€è¦ã€‚\nSlowFast_FasterRCNNæ¨¡å‹éœ€è¦çš„proposalsæ˜¯pklæ ¼å¼æ–‡ä»¶ï¼Œè¯¥æ–‡ä»¶ä»¥å­—å…¸å½¢å¼å­˜å‚¨æ£€æµ‹ç»“æœï¼Œå­—å…¸çš„keyæ˜¯è§†é¢‘å¸§çš„ç´¢å¼•ï¼ˆvideo_id+frame_idæ‹¼æ¥å¾—åˆ°ï¼‰ï¼Œvalueæ˜¯ä¸€ä¸ªlistï¼Œæ¯ä¸ªå…ƒç´ æ˜¯æ£€æµ‹å¾—åˆ°çš„äººçš„ä½ç½®ä¿¡æ¯å’Œç½®ä¿¡åº¦ã€‚",
        "type": "code",
        "location": "/applications/AbnormalActionDetection/README.md:1-40"
    },
    "27": {
        "file_id": 4,
        "content": "This code is for abnormal behavior detection using the PaddleVideo framework with SlowFast+FasterRCNN model, consisting of 6 steps: data preparation (sparse frame extraction, target detection, generating pkl files), model training, evaluation, inference, and deployment.",
        "type": "comment"
    },
    "28": {
        "file_id": 4,
        "content": "```\n{\n    æ‰“æ¶,0001:\n        [[0.036    0.098    0.55     0.979    0.995518] # x1,y1,x2,y2,score\n        [0.443    0.04     0.99     0.989    0.977824]]\n}\n```\n### Step4 å¯†é›†æŠ½å–è§†é¢‘å¸§\nå¯¹è§†é¢‘æ•°æ®è¿›è¡Œå¯†é›†æŠ½å¸§ã€‚\nSlowFast_FasterRCNNè¾“å…¥çš„è§†é¢‘å¸§æ˜¯å¯†é›†å¸§ï¼Œå› æ­¤éœ€è¦å†æ¬¡å¯¹è§†é¢‘è¿›è¡ŒæŠ½å¸§ã€‚å…·ä½“å‘½ä»¤å¦‚ä¸‹ï¼š\n```\ncd data/ava/script && bash extract_video_frames.sh abnormal_action_videos abnormal_action_frames_30fps 30\n```\nå…·ä½“å‚æ•°åŒæ­¥éª¤1ï¼Œåªä¸è¿‡æ¬¡æ•°æŠ½å¸§ç‡ä¸º30fpsã€‚\n### Step5 å‡†å¤‡æ ‡ç­¾æ•°æ®\næ ‡ç­¾æ•°æ®ä»¥pbtxtæ–‡ä»¶ä¸ªæ•°å­˜å‚¨ï¼Œæœ¬æ¡ˆä¾‹å…·ä½“å¦‚ä¸‹ï¼ˆæ³¨æ„è¡Œä¸ºæ ‡ç­¾idä»1å¼€å§‹ï¼‰ï¼š\n```\nitem {\n  name: \"æŒ¥æ£\"\n  id: 1\n}\nitem {\n  name: \"æ‰“æ¶\"\n  id: 2\n}\nitem {\n  name: \"è¸¢ä¸œè¥¿\"\n  id: 3\n}\nitem {\n  name: \"è¿½é€\"\n  id: 4\n}\nitem {\n  name: \"äº‰åµ\"\n  id: 5\n}\nitem {\n  name: \"å¿«é€Ÿå¥”è·‘\"\n  id: 6\n}\nitem {\n  name: \"æ‘”å€’\"\n  id: 7\n}\n```\n## æ¨¡å‹è®­ç»ƒ\nå¼‚å¸¸è¡Œä¸ºæ£€æµ‹æ¨¡å‹åŸºäºåœ¨AVAæ•°æ®é›†ä¸Šè®­ç»ƒå¾—åˆ°æ¨¡å‹è¿›è¡Œè¿ç§»å­¦ä¹ ã€‚å…·ä½“è®­ç»ƒå‘½ä»¤å¦‚ä¸‹ï¼š\n```\npython main.py --validate -w AVA_SlowFast_FastRcnn_best.pdparams \\\n -c configs/abnoraml_action.yaml\n```\n - w é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„\n - c é…ç½®æ–‡ä»¶è·¯å¾„\n## æ¨¡å‹è¯„ä¼°\n```\npython main.py --test \\\n   -w abnormal_action_SlowFast_FastRcnn.pdparams \\\n   -c configs/abnoraml_action.yaml\n```\n## æ¨¡å‹æ¨ç†\nåŸºäºåŠ¨æ€å›¾çš„æ¨ç†ï¼š\n```\npython tools/ava_predict.py \\\n  -c configs/abnoraml_action.yaml \\\n  -w abnormal_action_SlowFast_FastRcnn.pdparams \\",
        "type": "code",
        "location": "/applications/AbnormalActionDetection/README.md:42-114"
    },
    "29": {
        "file_id": 4,
        "content": "Step 4: Extracts video frames at a rate of 30fps for SlowFast_FasterRCNN input.\nStep 5: Stores label data as pbtxt files with action IDs starting from 1.\nModel training using pre-trained AVA model and config file.\nModel evaluation on abnormal action detection.\nModel inference using dynamic graph execution.",
        "type": "comment"
    },
    "30": {
        "file_id": 4,
        "content": "  --video_path data/wave_9.mp4 \\\n  --detection_model_name 'faster_rcnn/faster_rcnn_r50_fpn_1x_coco' \\\n  --detection_model_weights 'faster_rcnn_r50_fpn_1x_coco.pdparams'\n```\n- video_path è§†é¢‘è·¯å¾„\n- detection_model_name æ£€æµ‹æ¨¡å‹åç§°\n- detection_model_weights æ£€æµ‹æ¨¡å‹æƒé‡è·¯å¾„\nåŸºäºé™æ€å›¾æ¨¡å‹è¿›è¡Œæ¨ç†ï¼š\nå¯¼å‡ºæ¨¡å‹ï¼ŒåŠ¨æ€å›¾æ¨¡å‹è½¬æ¢ä¸ºé™æ€å›¾æ¨¡å‹ï¼š\n```\npython tools/export_model.py \\\n  -c configs/abnoraml_action.yaml \\\n  -o inference_output \\\n  -p abnormal_action_SlowFast_FastRcnn.pdparams\n```\n- o å¯¼å‡ºæ¨¡å‹å­˜æ”¾æ–‡ä»¶å¤¹\n- p è¢«å¯¼å‡ºæ¨¡å‹è·¯å¾„\nåŸºäºå¯¼å‡ºçš„æ¨¡å‹åšæ¨ç†ï¼š\n```\npython tools/predict.py \\\n    -c configs/abnoraml_action.yaml \\\n    --input_file \"data/wave_9.mp4\" \\\n    --model_file \"inference_output/abnormal_action_SlowFast_FastRcnn.pdmodel\" \\\n    --params_file \"inference_output/abnormal_action_SlowFast_FastRcnn.pdiparams\" \\\n    --use_gpu=True \\\n    --use_tensorrt=False\n```\n## æ¨¡å‹éƒ¨ç½²\nè¯·å‚è€ƒ[Paddle Inferenceç¤ºä¾‹](https://paddle-inference.readthedocs.io/en/latest/quick_start/python_demo.html)\n## å‚è€ƒè®ºæ–‡\n- [SlowFast Networks for Video Recognition](https://arxiv.org/pdf/1812.03982.pdf), Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, Kaiming He",
        "type": "code",
        "location": "/applications/AbnormalActionDetection/README.md:115-153"
    },
    "31": {
        "file_id": 4,
        "content": "This code is for video action detection using PaddleVideo. It exports a static model, converts dynamic to static model, performs inference, and deploys the model. The parameters include video path, detection model name, and weights path. Deployment instructions are provided with a reference to a relevant paper.",
        "type": "comment"
    },
    "32": {
        "file_id": 5,
        "content": "/applications/Anti-UAV/README.md",
        "type": "filepath"
    },
    "33": {
        "file_id": 5,
        "content": "This code provides instructions to detect UAVs in restricted areas using PaddleDetection, with data preparation and dependency installation steps. Users can customize the configuration file and trained model for specific use cases.",
        "type": "summary"
    },
    "34": {
        "file_id": 5,
        "content": "# Paddle-Anti-UAV\nAnti-UAV base on PaddleDetection\n## Background\nUAVs are very popular and we can see them in many public spaces, such as parks and playgrounds. Most people use UAVs for taking photos.\nHowever, many areas like airport forbiden UAVs since they are potentially dangerous. In this case, we need to detect the flying UAVs in\nthese areas.\nIn this repository, we show how to train a detection model using [PaddleDetection](https://github.com/PaddlePaddle/PaddleDetection).\n## Data preparation\nThe dataset can be found [here](https://anti-uav.github.io/dataset/). We direcly download the ```test-dev``` split composed of 140 videos\ntrain the detection model.\n* Download the ```test-dev``` dataset.\n* Run `unzip Anti_UAV_test_dev.zip -d Anti_UAV`.\n* Run `python get_image_label.py`. In this step, you may change the path to the videos and the value of `interval`.\nAfter the above steps, you will get a MSCOCO-style datasst for object detection.\n## Install PaddleDetection\nPlease refer to this [link](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.3/docs/tutorials/INSTALL.md).",
        "type": "code",
        "location": "/applications/Anti-UAV/README.md:1-21"
    },
    "35": {
        "file_id": 5,
        "content": "This code is for the Paddle-Anti-UAV application that uses PaddleDetection to detect flying UAVs in restricted areas. It provides details on data preparation, where to download and unzip the dataset, and how to install PaddleDetection.",
        "type": "comment"
    },
    "36": {
        "file_id": 5,
        "content": "We use `python=3.7`, `Paddle=2.2.1`, `CUDA=10.2`.\n## Train PP-YOLO\nWe use [PP-YOLO](https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.3/configs/ppyolo) as the detector.\n* Run `git clone https://github.com/PaddlePaddle/PaddleDetection.git`. Note that you should finish this step when you install PaddleDetection.\n* Move the anti-UAV dataset to `dataset`.\n* Move `anti_uav.yml` to `configs/datasets`, move `ppyolo_r50vd_dcn_1x_antiuav.yml` to `configs/ppyolo` and move `ppyolo_r50vd_dcn_antiuav.yml`\nto `configs/ppyolo/_base`.\n* Keep the value of `anchors` in `configs/ppyolo/_base/ppyolo_reader.yml` the same as `ppyolo_r50vd_dcn_antiuav.yml`.\n* Run `python -m paddle.distributed.launch --log_dir=./ppyolo_dygraph/ --gpus 0,1,2,3,4,5,6,7 tools/train.py -c configs/ppyolo/ppyolo_r50vd_dcn_1x_antiuav.yml &>ppyolo_dygraph.log 2>&1 &`.\nNote that you may change the arguments, such as `batch_size` and `gups`.\n## Inference\nPlease refer to the infernce section on this [webpage](https://github.com/Paddle",
        "type": "code",
        "location": "/applications/Anti-UAV/README.md:23-36"
    },
    "37": {
        "file_id": 5,
        "content": "The code outlines the process to train and use PP-YOLO for UAV detection using PaddleDetection in a specific environment. It involves cloning a repository, moving dataset files, adjusting configurations, and running training and inference commands with specific arguments.",
        "type": "comment"
    },
    "38": {
        "file_id": 5,
        "content": "Paddle/PaddleDetection/blob/release/2.3/docs/tutorials/GETTING_STARTED.md). You can just switch the configeration file and trained model to your own files.\n![](https://github.com/qingzwang/Paddle-Anti-UAV/blob/main/demo1.gif)\n![](https://github.com/qingzwang/Paddle-Anti-UAV/blob/main/demo.gif)",
        "type": "code",
        "location": "/applications/Anti-UAV/README.md:36-39"
    },
    "39": {
        "file_id": 5,
        "content": "The code snippet is referring to the README file of an anti-UAV application based on PaddleVideo. It demonstrates two GIFs showing the demo in action and mentions that users can customize the configuration file and trained model for their own use cases.",
        "type": "comment"
    },
    "40": {
        "file_id": 6,
        "content": "/applications/Anti-UAV/get_image_label.py",
        "type": "filepath"
    },
    "41": {
        "file_id": 6,
        "content": "The code initializes paths, defines train_info and val_info dictionaries, sets interval variables, processes video frames, draws rectangles around objects based on labels, saves images with label information, and writes data to train.json and val.json files after processing all data from given folders.",
        "type": "summary"
    },
    "42": {
        "file_id": 6,
        "content": "import cv2\nimport os\nimport json\n# please change it to your path\npath = '/workspace/wangqingzhong/Anti_UAV'\nannotation_path = 'annotations'\ntrain_img_path = 'train_imgs'\nval_img_path = 'val_imgs'\nif not os.path.exists(annotation_path):\n    os.makedirs(annotation_path)\nif not os.path.exists(train_img_path):\n    os.makedirs(train_img_path)\nif not os.path.exists(val_img_path):\n    os.makedirs(val_img_path)\ntrain_info = {\n    'images': [],\n    'type':\n    'instances',\n    'annotations': [],\n    'categories': [{\n        \"supercategory\": \"none\",\n        \"id\": 1,\n        \"name\": \"drone\"\n    }, {\n        \"supercategory\": \"none\",\n        \"id\": 2,\n        \"name\": \"noise\"\n    }]\n}\nval_info = {\n    'images': [],\n    'type':\n    'instances',\n    'annotations': [],\n    'categories': [{\n        \"supercategory\": \"none\",\n        \"id\": 1,\n        \"name\": \"drone\"\n    }, {\n        \"supercategory\": \"none\",\n        \"id\": 2,\n        \"name\": \"noise\"\n    }]\n}\n# you can change it\ninterval = 5\ndirs = os.listdir(path)\ntrain_img_id = 0\nval_img_id = 0\nfor d in dirs:",
        "type": "code",
        "location": "/applications/Anti-UAV/get_image_label.py:1-53"
    },
    "43": {
        "file_id": 6,
        "content": "The code initializes necessary paths and directories for annotation, training, and validation image paths. It creates the required directories if they do not exist. The code defines train_info and val_info as dictionaries containing information about images, annotations, and categories. It sets the interval variable for file processing and then iterates through each directory in the given path.",
        "type": "comment"
    },
    "44": {
        "file_id": 6,
        "content": "    if 'new' in d:\n        video_file = os.path.join(path, d, 'IR.mp4')\n        label_file = os.path.join(path, d, 'IR_label.json')\n        labels = json.load(open(label_file, 'r'))\n        exits = labels['exist']\n        gt_bbox = labels['gt_rect']\n        assert len(exits) == len(gt_bbox)\n        videocap = cv2.VideoCapture(video_file)\n        i = 0\n        while True:\n            success, frame = videocap.read()\n            if success:\n                if i % interval == 0:\n                    img_name = d + '_' + str(i) + '.jpg'\n                    cv2.imwrite(os.path.join(val_img_path, img_name), frame)\n                    height, width, depth = frame.shape\n                    x, y, w, h = gt_bbox[i]\n                    isexist = exits[i]\n                    if isexist:\n                        category_id = 1\n                    else:\n                        category_id = 2\n                    draw_frame = cv2.rectangle(frame, (x, y), (x + w, y + h),\n                                               (0, 255, 0), 2)",
        "type": "code",
        "location": "/applications/Anti-UAV/get_image_label.py:54-77"
    },
    "45": {
        "file_id": 6,
        "content": "This code reads an image file and its label from a specified path. It then processes each frame of the video, drawing a rectangle around the object in the frame based on the provided labels. If the object exists, category_id is set to 1; otherwise, it's set to 2. Each processed frame is saved as an image file with its corresponding label information.",
        "type": "comment"
    },
    "46": {
        "file_id": 6,
        "content": "                    img_name_draw = d + '_' + str(i) + 'draw.jpg'\n                    cv2.imwrite(os.path.join(val_img_path, img_name_draw),\n                                draw_frame)\n                    img_info = {\n                        'file_name': img_name,\n                        'height': float(height),\n                        'width': float(width),\n                        'id': val_img_id\n                    }\n                    ann_info = {\n                        'area': float(w) * float(h),\n                        'iscrowd': 0,\n                        'bbox': [float(x),\n                                 float(y),\n                                 float(w),\n                                 float(h)],\n                        'category_id': category_id,\n                        'ignore': 0,\n                        'image_id': val_img_id,\n                        'id': val_img_id + 1\n                    }\n                    val_info['images'].append(img_info)\n                    val_info['annotations'].append(ann_info)",
        "type": "code",
        "location": "/applications/Anti-UAV/get_image_label.py:78-101"
    },
    "47": {
        "file_id": 6,
        "content": "This code writes an image, creates image information (file name, height, width, and id), and annotation information (area, iscrowd, bbox coordinates, category_id, ignore, image_id, and id). It then appends the image and annotation information to the existing val_info data structure.",
        "type": "comment"
    },
    "48": {
        "file_id": 6,
        "content": "                    val_img_id += 1\n                i += 1\n            else:\n                print('finish {}'.format(d))\n                break\n    else:\n        video_file = os.path.join(path, d, 'IR.mp4')\n        label_file = os.path.join(path, d, 'IR_label.json')\n        labels = json.load(open(label_file, 'r'))\n        exits = labels['exist']\n        gt_bbox = labels['gt_rect']\n        assert len(exits) == len(gt_bbox)\n        videocap = cv2.VideoCapture(video_file)\n        i = 0\n        while True:\n            success, frame = videocap.read()\n            if success:\n                if i % interval == 0:\n                    img_name = d + '_' + str(i) + '.jpg'\n                    cv2.imwrite(os.path.join(train_img_path, img_name), frame)\n                    height, width, depth = frame.shape\n                    x, y, w, h = gt_bbox[i]\n                    isexist = exits[i]\n                    if isexist:\n                        category_id = 1\n                    else:\n                        category_id = 2",
        "type": "code",
        "location": "/applications/Anti-UAV/get_image_label.py:102-128"
    },
    "49": {
        "file_id": 6,
        "content": "Code reads a video and its corresponding label file, then extracts frames based on labels and saves them. If the object exists in the frame, it is labeled as category_id 1, otherwise as category_id 2. The process continues until all frames have been processed or a \"finish\" message is encountered.",
        "type": "comment"
    },
    "50": {
        "file_id": 6,
        "content": "                    draw_frame = cv2.rectangle(frame, (x, y), (x + w, y + h),\n                                               (0, 255, 0), 2)\n                    img_name_draw = d + '_' + str(i) + 'draw.jpg'\n                    cv2.imwrite(os.path.join(train_img_path, img_name_draw),\n                                draw_frame)\n                    img_info = {\n                        'file_name': img_name,\n                        'height': height,\n                        'width': width,\n                        'id': train_img_id\n                    }\n                    ann_info = {\n                        'area': float(w) * float(h),\n                        'iscrowd': 0,\n                        'bbox': [float(x),\n                                 float(y),\n                                 float(w),\n                                 float(h)],\n                        'category_id': category_id,\n                        'ignore': 0,\n                        'image_id': train_img_id,\n                        'id': train_img_id + 1",
        "type": "code",
        "location": "/applications/Anti-UAV/get_image_label.py:129-151"
    },
    "51": {
        "file_id": 6,
        "content": "This code draws a rectangle around the detected object in an image, saves the image with the drawn rectangle, and creates two dictionaries (image and annotation information) to be used for training purposes.",
        "type": "comment"
    },
    "52": {
        "file_id": 6,
        "content": "                    }\n                    train_info['images'].append(img_info)\n                    train_info['annotations'].append(ann_info)\n                    train_img_id += 1\n                i += 1\n            else:\n                print('finish {}'.format(d))\n                break\nwith open('annotations/train.json', 'w') as f:\n    json.dump(train_info, f)\nwith open('annotations/val.json', 'w') as f:\n    json.dump(val_info, f)",
        "type": "code",
        "location": "/applications/Anti-UAV/get_image_label.py:152-164"
    },
    "53": {
        "file_id": 6,
        "content": "Code writes image and annotation information to train.json and val.json files after processing all data from given folders, ending the loop when done.",
        "type": "comment"
    },
    "54": {
        "file_id": 7,
        "content": "/applications/BasketballAction/README.md",
        "type": "filepath"
    },
    "55": {
        "file_id": 7,
        "content": "PaddleVideo's BasketballAction app uses PaddlePaddle 2.0 and models for basketball action detection, achieving an F1-score of 80.14%. Developed by authors including hari and Joonseok Lee, it optimizes based on speed, time distribution, and feature fusion methods.",
        "type": "summary"
    },
    "56": {
        "file_id": 7,
        "content": "# ç¯®çƒåŠ¨ä½œæ£€æµ‹æ¨¡å‹\n## å†…å®¹\n- [æ¨¡å‹ç®€ä»‹](#æ¨¡å‹ç®€ä»‹)\n- [æ•°æ®å‡†å¤‡](#æ•°æ®å‡†å¤‡)\n- [æ¨¡å‹è®­ç»ƒ](#æ¨¡å‹è®­ç»ƒ)\n- [æ¨¡å‹è¯„ä¼°](#æ¨¡å‹è¯„ä¼°)\n- [æ¨¡å‹æ¨ç†](#æ¨¡å‹æ¨ç†)\n- [æ¨¡å‹ä¼˜åŒ–](#æ¨¡å‹ä¼˜åŒ–)\n- [æ¨¡å‹éƒ¨ç½²](#æ¨¡å‹éƒ¨ç½²)\n- [å‚è€ƒè®ºæ–‡](#å‚è€ƒè®ºæ–‡)\n## æ¨¡å‹ç®€ä»‹\nè¯¥ä»£ç åº“ç”¨äºç¯®çƒåŠ¨ä½œæ£€æµ‹+è¯†åˆ«, åŸºäºpaddle2.0ç‰ˆæœ¬å¼€å‘ï¼Œç»“åˆPaddleVideoä¸­çš„ppTSM, BMN, attentionLSTMçš„å¤šä¸ªè§†é¢‘æ¨¡å‹è¿›è¡Œè§†é¢‘æ—¶ç©ºäºŒé˜¶æ®µæ£€æµ‹ç®—æ³•ã€‚\nä¸»è¦åˆ†ä¸ºå¦‚ä¸‹å‡ æ­¥\n - ç‰¹å¾æŠ½å–\n    - å›¾åƒç‰¹æ€§ï¼ŒppTSM\n    - éŸ³é¢‘ç‰¹å¾ï¼ŒVggsound\n - proposalæå–ï¼ŒBMN\n - LSTMï¼ŒåŠ¨ä½œåˆ†ç±» + å›å½’\n## æ•°æ®å‡†å¤‡\næ•°æ®é›†å¤„ç†ä»£ç \n```\nå‚è€ƒhttps://github.com/PaddlePaddle/PaddleVideo/tree/application/FootballAction/datasets\n```\n- æ•°æ®é›†labelæ ¼å¼\n```\n{\n    \"0\": \"èƒŒæ™¯\",\n    \"1\": \"å›æ”¾\",\n    \"2\": \"è¿›çƒ-ä¸‰åˆ†çƒ\",\n    \"3\": \"è¿›çƒ-ä¸¤åˆ†çƒ\",\n    \"4\": \"è¿›çƒ-æ‰£ç¯®\",\n    \"5\": \"ç½šçƒ\",\n    \"6\": \"è·³çƒ\"\n}\n```\n- æ•°æ®é›†gtså¤„ç†, å°†åŸå§‹æ ‡æ³¨æ•°æ®å¤„ç†æˆå¦‚ä¸‹jsonæ ¼å¼\n```\n{\n    'fps': 5,\n    'gts': [\n        {\n            'url': 'xxx.mp4',\n            'total_frames': 6341,\n            'actions': [\n                {\n                    \"label_ids\": [6],\n                    \"label_names\": [\"è·³çƒ\"],\n                    \"start_id\": 395,\n                    \"end_id\": 399\n                },\n                ...\n            ]\n        },\n        ...\n    ]\n}\n```\n- æ•°æ®é›†æŠ½å¸§, ç”±mp4, å¾—åˆ°frameså’Œpcm, è¿™é‡Œéœ€è¦æ·»åŠ ffmpegç¯å¢ƒ\n```\ncd datasets/script && python get_frames_pcm.py",
        "type": "code",
        "location": "/applications/BasketballAction/README.md:1-69"
    },
    "57": {
        "file_id": 7,
        "content": "This code is for basketball action detection, using PaddlePaddle 2.0 and incorporating various video models from PaddleVideo (ppTSM, BMN, attentionLSTM). The process includes image feature extraction with ppTSM, proposal extraction with BMN, and LSTM-based action classification and regression. Dataset preparation involves data handling, label format specification, gts processing to JSON format, and abstracting frames from mp4 files using ffmpeg.",
        "type": "comment"
    },
    "58": {
        "file_id": 7,
        "content": "```\n- æ•°æ®é¢„å¤„ç†åä¿å­˜æ ¼å¼å¦‚ä¸‹\n```\n   |--  datasets                   # è®­ç»ƒæ•°æ®é›†å’Œå¤„ç†è„šæœ¬\n        |--  basketball            # xxæ•°æ®é›†\n            |--  mp4               # åŸå§‹è§†é¢‘.mp4\n            |--  frames            # å›¾åƒå¸§, fps=5, '.jpg'æ ¼å¼\n            |--  pcm               # éŸ³é¢‘pcm, éŸ³é¢‘é‡‡æ ·ç‡16000ï¼Œé‡‡ç”¨é€šé“æ•°1\n            |--  url.list          # è§†é¢‘åˆ—è¡¨\n            |--  label_train.json  # è®­ç»ƒé›†åŸå§‹gts\n            |--  label_val.json    # éªŒè¯é›†åŸå§‹gts\n```\n## æ¨¡å‹è®­ç»ƒ\nä»£ç å‚è€ƒè¶³çƒåŠ¨ä½œæ£€æµ‹ï¼šhttps://github.com/PaddlePaddle/PaddleVideo/tree/application/FootballAction\nå°†è¯¥ä»£ç åº“çš„æ–‡ä»¶å¤¹ [datasets](https://github.com/PaddlePaddle/PaddleVideo/tree/application/FootballAction/datasets)ï¼Œ[extractor](https://github.com/PaddlePaddle/PaddleVideo/tree/application/FootballAction/extractor)ï¼Œ[train_lstm](https://github.com/PaddlePaddle/PaddleVideo/tree/application/FootballAction/train_lstm)ï¼Œ æ‹·è´åˆ°æœ¬ä»£ç åº“å¤ç”¨ã€‚\n - image é‡‡æ ·é¢‘ç‡fps=5ï¼Œå¦‚æœæœ‰äº›åŠ¨ä½œæ—¶é—´è¾ƒçŸ­ï¼Œå¯ä»¥é€‚å½“æé«˜é‡‡æ ·é¢‘ç‡\n - BMN windows=200ï¼Œå³40sï¼Œæ‰€ä»¥æµ‹è¯•è‡ªå·±çš„æ•°æ®æ—¶ï¼Œè§†é¢‘æ—¶é•¿éœ€å¤§äº40s\n### åŸºç¡€é•œåƒ\n```\ndocker pull tmtalgo/paddleaction:action-detection-v2\n```\n### step1 ppTSMè®­ç»ƒ\næˆ‘ä»¬æä¾›äº†ç¯®çƒæ•°æ®è®­ç»ƒçš„æ¨¡å‹ï¼Œå‚è€ƒcheckpoints_basketballã€‚å¦‚æœä½¿ç”¨æä¾›çš„pptsmæ¨¡å‹ï¼Œå¯ç›´æ¥è·³è¿‡ä¸‹è¾¹çš„pptsmè®­ç»ƒæ•°æ®å¤„ç†å’Œè®­ç»ƒæ­¥éª¤ã€‚",
        "type": "code",
        "location": "/applications/BasketballAction/README.md:70-99"
    },
    "59": {
        "file_id": 7,
        "content": "This code describes the storage location and structure of a basketball action dataset, including video files (mp4), image frames, audio files (pcm), and JSON files containing ground truth data. It also references the PaddleVideo footbal",
        "type": "comment"
    },
    "60": {
        "file_id": 7,
        "content": "å¦‚æœéœ€è¦åœ¨è‡ªå·±çš„æ•°æ®ä¸Šè®­ç»ƒï¼ŒppTSMè®­ç»ƒä»£ç ä¸ºï¼šhttps://github.com/PaddlePaddle/PaddleVideo/tree/release/2.0\nppTSMæ–‡æ¡£å‚è€ƒï¼šhttps://github.com/PaddlePaddle/PaddleVideo/blob/develop/docs/zh-CN/model_zoo/recognition/pp-tsm.md\n#### step1.1  ppTSM è®­ç»ƒæ•°æ®å¤„ç†\nç”±framesç»“åˆgtsç”Ÿæˆè®­ç»ƒæ‰€éœ€è¦çš„æ­£è´Ÿæ ·æœ¬\n```\ncd ${BasketballAction}\ncd datasets/script && python get_instance_for_tsn.py\n# æ–‡ä»¶åæŒ‰ç…§å¦‚ä¸‹æ ¼å¼\n'{}_{}_{}_{}'.format(video_basename, start_id, end_id, label)\n```\nå®Œæˆè¯¥æ­¥éª¤åï¼Œæ•°æ®å­˜å‚¨ä½ç½®\n```\n   |--  datasets                   # è®­ç»ƒæ•°æ®é›†å’Œå¤„ç†è„šæœ¬\n        |--  basketball           # xxæ•°æ®é›†\n            |--  input_for_tsn     # tsn/tsmè®­ç»ƒçš„æ•°æ®\n```\n#### step1.2 ppTSMæ¨¡å‹è®­ç»ƒ\n```\n# https://github.com/PaddlePaddle/PaddleVideo/tree/release/2.0\ncd ${PaddleVideo}\n# ä¿®æ”¹config.yamlå‚æ•°ä¿®æ”¹ä¸º ${BasketballAcation}/configs_train/pptsm_basketball.yaml\npython -B -m paddle.distributed.launch \\\n    --gpus=\"0,1,2,3\" \\\n    --log_dir=$save_dir/logs \\\n    main.py  \\\n    --validate \\\n    -c {BasketballAcation}/configs_train/pptsm_basketball.yaml \\\n    -o output_dir=$save_dir\n```\n#### step1.3 ppTSMæ¨¡å‹è½¬ä¸ºé¢„æµ‹æ¨¡å¼\n```\n# https://github.com/PaddlePaddle/PaddleVideo/tree/release/2.0",
        "type": "code",
        "location": "/applications/BasketballAction/README.md:100-135"
    },
    "61": {
        "file_id": 7,
        "content": "Step 1.1: Prepare ppTSM training data by combining frames and gts to generate positive and negative samples, following the format '{}_{}_{}_{}'.format(video_basename, start_id, end_id, label).\n\nStep 1.2: Train ppTSM model using the prepared dataset by modifying config.yaml parameters and running main.py with distributed launch script.\n\nStep 1.3: Convert trained ppTSM model to prediction mode for inference.",
        "type": "comment"
    },
    "62": {
        "file_id": 7,
        "content": "$cd {PaddleVideo}\npython tools/export_model.py -c ${BasketballAcation}/configs_train/pptsm_basketball.yaml \\\n                               -p ${pptsm_train_dir}/checkpoints/models_pptsm/ppTSM_epoch_00057.pdparams \\\n                               -o {BasketballAcation}/checkpoints/ppTSM\n```\n####  step1.4 åŸºäºppTSMè§†é¢‘ç‰¹å¾æå–\nimage and audioç‰¹å¾æå–ï¼Œä¿å­˜åˆ°datasets featuresæ–‡ä»¶å¤¹ä¸‹\n```\ncd ${BasketballAcation}\ncd extractor && python extract_feat.py\n# ç‰¹å¾ç»´åº¦, image(2048) + audio(1024) + pcm(640)\n# ç‰¹å¾ä¿å­˜æ ¼å¼å¦‚ä¸‹ï¼Œå°†å¦‚ä¸‹dictä¿å­˜åœ¨pklæ ¼å¼ï¼Œç”¨äºæ¥ä¸‹æ¥çš„BMNè®­ç»ƒ\nvideo_features = {'image_feature': np_image_features,\n                  'audio_feature': np_audio_features\n                  'pcm_feature': np_pcm_features}\n```\nå®Œæˆè¯¥æ­¥éª¤åï¼Œæ•°æ®å­˜å‚¨ä½ç½®\n```\n   |--  datasets                   # è®­ç»ƒæ•°æ®é›†å’Œå¤„ç†è„šæœ¬\n        |--  basketball            # xxæ•°æ®é›†\n            |--  features          # è§†é¢‘çš„å›¾åƒ+éŸ³é¢‘ç‰¹å¾\n```\n### step2 BMNè®­ç»ƒ\nBMNè®­ç»ƒä»£ç ä¸ºï¼šhttps://github.com/PaddlePaddle/PaddleVideo/tree/release/2.0\nBMNæ–‡æ¡£å‚è€ƒï¼šhttps://github.com/PaddlePaddle/PaddleVideo/blob/develop/docs/zh-CN/model_zoo/localization/bmn.md",
        "type": "code",
        "location": "/applications/BasketballAction/README.md:136-163"
    },
    "63": {
        "file_id": 7,
        "content": "The provided code is related to a PaddleVideo application called BasketballAction. It performs two steps - extracting features from images and audio, and training a BMN model. The extracted features are stored in the datasets/basketball/features directory. The BMN training code can be found at this GitHub link, and more information about BMN can be found in this documentation.",
        "type": "comment"
    },
    "64": {
        "file_id": 7,
        "content": "#### step2.1 BMNè®­ç»ƒæ•°æ®å¤„ç†\nç”¨äºæå–äºŒåˆ†ç±»çš„proposalï¼Œwindows=40ï¼Œæ ¹æ®gtså’Œç‰¹å¾å¾—åˆ°BMNè®­ç»ƒæ‰€éœ€è¦çš„æ•°æ®é›†\n```\ncd ${BasketballAcation}\ncd datasets/script && python get_instance_for_bmn.py\n# æ•°æ®æ ¼å¼\n{\n    \"719b0a4bcb1f461eabb152298406b861_753_793\": {\n        \"duration_second\": 40.0,\n        \"duration_frame\": 200,\n        \"feature_frame\": 200,\n        \"subset\": \"train\",\n        \"annotations\": [\n            {\n                \"segment\": [\n                    15.0,\n                    22.0\n                ],\n                \"label\": \"6.0\",\n                \"label_name\": \"è·³çƒ\"\n            }\n        ]\n    },\n    ...\n}\n```\nå®Œæˆè¯¥æ­¥éª¤åï¼Œæ•°æ®å­˜å‚¨ä½ç½®\n```\n   |--  datasets                   # è®­ç»ƒæ•°æ®é›†å’Œå¤„ç†è„šæœ¬\n        |--  basketball            # xxæ•°æ®é›†\n            |--  input_for_bmn     # bmnè®­ç»ƒçš„proposal         \n```\n#### step2.2  BMNæ¨¡å‹è®­ç»ƒ\n```\n# https://github.com/PaddlePaddle/PaddleVideo/tree/release/2.0\ncd ${PaddleVideo}\n# ä¿®æ”¹config.yamlå‚æ•°ä¿®ä¸º${BasketballAcation}/configs_train/bmn_basketball.yaml\npython -B -m paddle.distributed.launch \\\n     --gpus=\"0,1\" \\\n     --log_dir=$out_dir/logs \\\n     main.py  \\",
        "type": "code",
        "location": "/applications/BasketballAction/README.md:165-206"
    },
    "65": {
        "file_id": 7,
        "content": "Step 2.1 involves processing the Basketball Action dataset to generate binary proposals for BMN training, with a window size of 40. This is done using the get_instance_for_bmn.py script in the datasets/script directory. The resulting data format consists of instance identifiers, duration and feature frame numbers, subset information (train or test), and annotations containing segment locations and labels. Step 2.2 involves training the BMN model, requiring modification of the config.yaml file with Basketball Action-specific parameters and launching the main.py script using PaddlePaddle's distributed training functionality.",
        "type": "comment"
    },
    "66": {
        "file_id": 7,
        "content": "     --validate \\\n     -c ${BasketballAcation}/configs_train/bmn_basketball.yaml \\\n     -o output_dir=$out_dir\n```\n#### step2.3 BMNæ¨¡å‹è½¬ä¸ºé¢„æµ‹æ¨¡å¼\n```\n# https://github.com/PaddlePaddle/PaddleVideo/tree/release/2.0\n${PaddleVideo}\npython tools/export_model.py -c $${BasketballAcation}/configs_train/bmn_basketball.yaml \\\n                               -p ${bmn_train_dir}/checkpoints/models_bmn/bmn_epoch16.pdparams \\\n                               -o {BasketballAcation}/checkpoints/BMN\n```\n#### step2.4  BMNæ¨¡å‹é¢„æµ‹\nå¾—åˆ°åŠ¨ä½œproposalä¿¡æ¯ï¼š start_id, end_id, score\n```\ncd ${BasketballAcation}\ncd extractor && python extract_bmn.py\n# æ•°æ®æ ¼å¼\n[\n    {\n        \"video_name\": \"c9516c903de3416c97dae91a59e968d7\",\n        \"num_proposal\": 5534,\n        \"bmn_results\": [\n            {\n                \"start\": 7850.0,\n                \"end\": 7873.0,\n                \"score\": 0.77194699622342\n            },\n            {\n                \"start\": 4400.0,\n                \"end\": 4443.0,\n                \"score\": 0.7663803287641536\n            },\n            ...\n        ]",
        "type": "code",
        "location": "/applications/BasketballAction/README.md:207-243"
    },
    "67": {
        "file_id": 7,
        "content": "The code is used in the BasketballAction application of the PaddleVideo library. It converts the BMN model to prediction mode (step2.3), exports it, and then performs BMN-based predictions (step2.4) for obtaining action proposal information like start, end, and score. This helps identify basketball actions from given videos.",
        "type": "comment"
    },
    "68": {
        "file_id": 7,
        "content": "    },\n    ...\n]\n```\nå®Œæˆè¯¥æ­¥éª¤åï¼Œæ•°æ®å­˜å‚¨ä½ç½®\n```\n   |--  datasets                   # è®­ç»ƒæ•°æ®é›†å’Œå¤„ç†è„šæœ¬\n        |--  basketball            # xxæ•°æ®é›†\n            |--  feature_bmn\n                 |--  prop.json    # bmn é¢„æµ‹ç»“æœ\n```\n### step3 LSTMè®­ç»ƒ\nLSTMè®­ç»ƒä»£ç ä¸ºï¼štrain_lstm\n#### step3.1  LSTMè®­ç»ƒæ•°æ®å¤„ç†\nå°†BMNå¾—åˆ°çš„proposalæˆªæ–­å¹¶å¤„ç†æˆLSTMè®­ç»ƒæ‰€éœ€æ•°æ®é›†\n```\ncd ${BasketballAcation}\ncd datasets/script && python get_instance_for_lstm.py\n# æ•°æ®æ ¼å¼1ï¼Œlabel_info\n{\n    \"fps\": 5,\n    \"results\": [\n        {\n            \"url\": \"https://xxx.mp4\",\n            \"mode\": \"train\",        # train or validation\n            \"total_frames\": 6128,\n            \"num_gts\": 93,\n            \"num_proposals\": 5043,\n            \"proposal_actions\": [\n                {\n                    \"label\": 6,\n                    \"norm_iou\": 0.7575757575757576,\n                    \"norm_ioa\": 0.7575757575757576,\n                    \"norm_start\": -0.32,\n                    \"proposal\": {\n                        \"start\": 5011,\n                        \"end\": 5036,\n                        \"score\": 0.7723643666324231\n                    },",
        "type": "code",
        "location": "/applications/BasketballAction/README.md:244-284"
    },
    "69": {
        "file_id": 7,
        "content": "This code represents a JSON object containing information about a dataset for LSTM training. It includes the frame rate (fps), whether it's for training or validation, the total number of frames, the number of ground truth (gt) instances, and the number of proposals. The proposals contain details like label, normalized IOU, start time, end time, and score. This data is used to train a LSTM model in BasketballAction application.",
        "type": "comment"
    },
    "70": {
        "file_id": 7,
        "content": "                    \"hit_gts\": {\n                        \"label_ids\": [\n                            6\n                        ],\n                        \"label_names\": [\n                            \"è·³çƒ\"\n                        ],\n                        \"start_id\": 5003,\n                        \"end_id\": 5036\n                    }\n                },\n                ...\n        },\n        ...\n}\n# æ•°æ®æ ¼å¼2ï¼ŒLSTMè®­ç»ƒæ‰€éœ€è¦çš„feature\n{\n    'features': np.array(feature_hit, dtype=np.float32),    # TSM audio and pcm ç‰¹å¾, å¯æ ¹æ®éœ€æ±‚é€‰æ‹©ç»„åˆ\n    'feature_fps': 5,                                       # fps = 5\n    'label_info': {'norm_iou': 0.5, 'label': 3, ...},       # æ•°æ®æ ¼å¼1ä¸­çš„'proposal_actions'\n    'video_name': 'c9516c903de3416c97dae91a59e968d7'        # video_name\n}\n# æ•°æ®æ ¼å¼3ï¼ŒLSTMè®­ç»ƒæ‰€éœ€label.txt\n'{} {}'.format(filename, label)\n```\nå®Œæˆè¯¥æ­¥éª¤åï¼Œæ•°æ®å­˜å‚¨ä½ç½®\n```\n   |--  datasets                   # è®­ç»ƒæ•°æ®é›†å’Œå¤„ç†è„šæœ¬\n        |--  basketball            # xxæ•°æ®é›†\n            |--  input_for_lstm    # LSTMè®­ç»ƒæ•°æ®é›†\n```\n#### step3.2  LSTMè®­ç»ƒ\n```\n#conf.yamlä¿®æ”¹ä¸º ${BasketballAcation}/configs_train/lstm_basketball.yaml",
        "type": "code",
        "location": "/applications/BasketballAction/README.md:285-319"
    },
    "71": {
        "file_id": 7,
        "content": "This code is from the PaddleVideo library's BasketballAction application and provides information on data formats for LSTM training. The first format contains label information, start and end IDs in a JSON object. The second format includes features like audio, pcm, fps, and label info in a NumPy array. The third format is the label.txt file for LSTM training. After completing these steps, the trained data will be stored in the \"input_for_lstm\" folder within the BasketballAction dataset folder.",
        "type": "comment"
    },
    "72": {
        "file_id": 7,
        "content": "cd ${BasketballAcation}\npython -u scenario_lib/train.py \\\n    --model_name=ActionNet \\\n    --config=${BasketballAcation}/configs_train/lstm_basketball.yaml \\\n    --save_dir=${out_dir}\"/models_lstm/\" \\\n    --log_interval=5 \\\n    --valid_interval=1\n```\n#### step3.3 LSTMæ¨¡å‹è½¬ä¸ºé¢„æµ‹æ¨¡å¼\n```\n${BasketballAcation}\npython tools/export_model.py -c ${BasketballAction}/train_lstm/conf/conf.yaml \\\n                               -p ${lstm_train_dir}/checkpoints/models_lstm/bmn_epoch29.pdparams \\\n                               -o {BasketballAcation}/checkpoints/LSTM\n```\n## æ¨¡å‹æ¨ç†\næµ‹è¯•æ•°æ®æ ¼å¼ï¼Œå¯å‚è€ƒä½¿ç”¨æ ·ä¾‹\n```\nwget https://videotag.bj.bcebos.com/Applications/basketball/datasets.tar.gz\n```\næµ‹è¯•æ¨¡å‹ï¼Œå¯ä½¿ç”¨æˆ‘ä»¬æä¾›çš„æ¨¡å‹\n```\nwget https://videotag.bj.bcebos.com/Applications/basketball/checkpoints_basketball.tar.gz\n```\nè¿è¡Œé¢„æµ‹ä»£ç \n```\ncd ${BasketballAction}\ncd predict\n# å¦‚æœä½¿ç”¨è‡ªå·±è®­ç»ƒçš„æ¨¡å‹ï¼Œè¯·å°†å„è®­ç»ƒè¿‡ç¨‹ä¸­è½¬æ¢çš„inferenceæ¨¡å‹æ”¾åˆ°predictåº“\n# cp -rf ../checkpoints checkpoints_basketball\npython predict.py\n```\näº§å‡ºæ–‡ä»¶\n```\n${BasketballAction}/predict/results.json\n```\n## æ¨¡å‹è¯„ä¼°\n```\ncd ${BasketballAction}\ncd predict\npython eval.py results.json",
        "type": "code",
        "location": "/applications/BasketballAction/README.md:320-365"
    },
    "73": {
        "file_id": 7,
        "content": "The code is converting the trained LSTM model to prediction mode, running model inference on testing data, and evaluating the results. This process involves using pre-prepared datasets and models from provided URLs for easy execution of prediction and evaluation.",
        "type": "comment"
    },
    "74": {
        "file_id": 7,
        "content": "```\n## æ¨¡å‹ä¼˜åŒ–\nåœ¨å®é™…ä½¿ç”¨åœºæ™¯ä¸­å¯æ ¹æ®è§†é¢‘å†…å®¹å°è¯•ä¼˜åŒ–ç­–ç•¥\n- å¯æ ¹æ®åŠ¨ä½œè¿åŠ¨é€Ÿåº¦ï¼Œè°ƒæ•´æŠ½å¸§é‡‡æ ·ç‡ï¼Œæœ¬ä»£ç é»˜è®¤ä¸ºfps=5\n- ç»Ÿè®¡åŠ¨ä½œçš„æ—¶é—´åˆ†å¸ƒï¼Œè°ƒæ•´bmné‡‡æ ·çª—å£\n- æ ¹æ®å›¾åƒå’ŒéŸ³é¢‘çš„å…³è”ç¨‹åº¦ï¼Œè°ƒæ•´å›¾åƒå’ŒéŸ³é¢‘ç‰¹å¾çš„èåˆæ–¹å¼ï¼šæœ¬ä»£ç å°†å›¾åƒç‰¹å¾å’ŒéŸ³é¢‘åœ¨æ—¶é—´ç»´åº¦å¯¹é½ï¼Œèåˆåå†è¿›å…¥æ¨¡å‹è®­ç»ƒã€‚ä¹Ÿå¯å°è¯•åˆ†åˆ«æ¨¡å‹è®­ç»ƒåï¼ŒåŠ æƒèåˆç­‰\n- æœ¬ä»£ç çš„è§£å†³æ–¹æ¡ˆä¹Ÿå¯ç”¨äºå…¶ä»–åŠ¨ä½œæ£€æµ‹ã€‚å˜æ¢åœºæ™¯åï¼Œå›¾åƒç‰¹å¾é‡æ–°è®­ç»ƒæ•ˆæœæ›´å¥½ã€‚éŸ³é¢‘ç‰¹å¾é‡‡ç”¨çš„VGGSoundè®­ç»ƒï¼Œå¦‚æœä½¿ç”¨åœºæ™¯ä»ä¸ºç”Ÿæ´»åœºæ™¯ï¼Œå¯ç›´æ¥å¤ç”¨ã€‚\n## æ¨¡å‹éƒ¨ç½²\næœ¬ä»£ç è§£å†³æ–¹æ¡ˆåœ¨åŠ¨ä½œçš„æ£€æµ‹å’Œå¬å›æŒ‡æ ‡F1-score=80.14%\n<div align=\"center\">\n  <img src=\"images/BasketballAction_demo.gif\" width=\"640px\"/><br>\n</div>\n## å‚è€ƒè®ºæ–‡\n- [TSM: Temporal Shift Module for Efficient Video Understanding](https://arxiv.org/pdf/1811.08383.pdf), Ji Lin, Chuang Gan, Song Han\n- [BMN: Boundary-Matching Network for Temporal Action Proposal Generation](https://arxiv.org/abs/1907.09702), Tianwei Lin, Xiao Liu, Xin Li, Errui Ding, Shilei Wen.\n- [Attention Clusters: Purely Attention Based Local Feature Integration for Video Classification](https://arxiv.org/abs/1711.09550), Xiang Long, Chuang Gan, Gerard de Melo, Jiajun Wu, Xiao Liu, Shilei Wen\n- [YouTube-8M: A Large-Scale Video Classification Benchmark](https://arxiv.org/abs/1609.08675), Sami Abu-El-Haija, Nisarg Kot",
        "type": "code",
        "location": "/applications/BasketballAction/README.md:366-389"
    },
    "75": {
        "file_id": 7,
        "content": "This code uses the TSM and BMN models for efficient video understanding, allowing for action detection with a F1-score of 80.14%. The code can optimize based on motion speed, time distribution, and feature fusion methods, and is applicable to other action detection scenarios.",
        "type": "comment"
    },
    "76": {
        "file_id": 7,
        "content": "hari, Joonseok Lee, Paul Natsev, George Toderici, Balakrishnan Varadarajan, Sudheendra Vijayanarasimhan",
        "type": "code",
        "location": "/applications/BasketballAction/README.md:389-389"
    },
    "77": {
        "file_id": 7,
        "content": "Code represents authors of a paper or contributors to the project, including hari, Joonseok Lee, Paul Natsev, George Toderici, Balakrishnan Varadarajan, and Sudheendra Vijayanarasimhan.",
        "type": "comment"
    },
    "78": {
        "file_id": 8,
        "content": "/applications/BasketballAction/predict/action_detect/action.py",
        "type": "filepath"
    },
    "79": {
        "file_id": 8,
        "content": "The Python code initializes a ModelPredict class for basketball action detection using image, audio, and property features. It infers actions by extracting features and utilizing prepared models for classification before saving the output in 'results.json'.",
        "type": "summary"
    },
    "80": {
        "file_id": 8,
        "content": "#!./python27-gcc482/bin/python\n# coding: utf-8\n\"\"\"\nBAIDU CLOUD action\n\"\"\"\nimport os\nimport sys\nimport pickle\nimport json\nimport time\nimport functools\nimport numpy as np\nfrom utils.preprocess import get_images\nfrom utils.config_utils import parse_config, print_configs\nimport mfcc.feature_extractor as mfcc_extractor\nimport models.pptsm_infer as image_model\nimport models.audio_infer as audio_model\nimport models.bmn_infer as prop_model\nimport models.lstm_infer as classify_model\nimport logger\nlogger = logger.Logger()\ndef record_time_info(func):\n    \"\"\"decorator func to log cost time for func\n    \"\"\"\n    @functools.wraps(func)\n    def timer(*args):\n        \"\"\"log cost time for func\n        \"\"\"\n        logger.info(\"function [{}] processing ...\".format(func.__name__))\n        start_time = time.time()\n        retval = func(*args)\n        cost_time = round(time.time() - start_time, 5)\n        logger.info(\"function [{}] run time: {:.2f} min\".format(func.__name__, cost_time / 60))\n        return retval\n    return timer\nclass ActionDetection(object):",
        "type": "code",
        "location": "/applications/BasketballAction/predict/action_detect/action.py:1-44"
    },
    "81": {
        "file_id": 8,
        "content": "The code is a Python file containing a class for performing basketball action detection using a combination of image, audio, and property features. It uses various models for feature extraction and classification. The `record_time_info` function is a decorator to log the processing time of different functions.",
        "type": "comment"
    },
    "82": {
        "file_id": 8,
        "content": "    \"\"\"ModelPredict\"\"\"\n    def __init__(self, cfg_file=\"configs/configs.yaml\"):\n        cfg = parse_config(cfg_file)\n        self.configs = cfg\n        print_configs(self.configs, \"Infer\")\n        name = 'COMMON'\n        self.DEBUG          = cfg[name]['DEBUG']\n        self.BMN_ONLY       = cfg[name]['BMN_ONLY']\n        self.LSTM_ONLY      = cfg[name]['LSTM_ONLY']\n        self.PCM_ONLY       = cfg[name]['PCM_ONLY']\n        if self.LSTM_ONLY:\n            self.prop_dict = {}\n            for dataset in ['EuroCup2016']:\n                prop_json = '/home/work/datasets/{}/feature_bmn/prop.json'.format(dataset)\n                json_data = json.load(open(prop_json, 'r'))\n                for item in json_data:\n                    basename = prop_json.replace('feature_bmn/prop.json', 'mp4')\n                    basename = basename + '/' + item['video_name'] + '.mp4'\n                    self.prop_dict[basename] = item['bmn_results']\n    @record_time_info\n    def load_model(self):\n        \"\"\"\n        load_model\n        \"\"\"",
        "type": "code",
        "location": "/applications/BasketballAction/predict/action_detect/action.py:45-71"
    },
    "83": {
        "file_id": 8,
        "content": "The code initializes a ModelPredict class by parsing a configuration file, setting instance variables based on the configurations, and loading a model. The configurations include settings for debugging, whether to use only BMN (Basketball Motion Network), LSTM (Long Short-Term Memory) or PCM (Prediction of Coming Movement) models, and a dictionary of properties for specific datasets. The load_model method is decorated with @record_time_info, which suggests it records the time taken to execute this function.",
        "type": "comment"
    },
    "84": {
        "file_id": 8,
        "content": "        if not self.DEBUG:\n            self.image_model = image_model.InferModel(self.configs)\n            if not self.PCM_ONLY:\n                self.audio_model = audio_model.InferModel(self.configs)\n        if not self.LSTM_ONLY:\n            self.prop_model = prop_model.InferModel(self.configs)\n        if not self.BMN_ONLY:\n            self.classify_model = classify_model.InferModel(self.configs)\n        logger.info(\"==> Action Detection prepared.\")\n    @record_time_info\n    def infer(self, imgs_path, pcm_path, fps=5):\n        \"\"\"\n        extract_feature\n        \"\"\"\n        print(\"imgs_path  = \", imgs_path)\n        self.imgs_path = imgs_path\n        self.pcm_path = pcm_path\n        self.configs['COMMON']['fps'] = fps\n        logger.info(\"==> input video {}\".format(os.path.basename(self.imgs_path)))\n        # step 1: extract feature\n        video_features = self.extract_feature()\n        # step2: get proposal\n        bmn_results = self.extract_proposal(video_features)\n        # step3: classify \n        material = {'feature': video_features, 'proposal': bmn_results}",
        "type": "code",
        "location": "/applications/BasketballAction/predict/action_detect/action.py:72-104"
    },
    "85": {
        "file_id": 8,
        "content": "Code creates InferModels for image, audio, and classification tasks depending on the configurations. The main function infers action by extracting features from images and audio, then classifying them using the prepared models.",
        "type": "comment"
    },
    "86": {
        "file_id": 8,
        "content": "        action_results = self.video_classify(material)\n        return bmn_results, action_results\n    @record_time_info\n    def video_classify(self, material):\n        \"\"\"video classify\"\"\"\n        if self.BMN_ONLY:\n            return []\n        action_results = self.classify_model.predict(self.configs, material=material) \n        logger.info('action shape {}'.format(np.array(action_results).shape))\n        return action_results\n    @record_time_info\n    def extract_proposal(self, video_features):\n        \"\"\"extract proposal\"\"\"\n        if self.LSTM_ONLY:\n            basename = self.imgs_path.replace('frames', 'mp4') + '.mp4'\n            bmn_results = self.prop_dict[basename]\n            return bmn_results\n        bmn_results = self.prop_model.predict(self.configs, material=video_features)\n        logger.info('proposal shape {}'.format(np.array(bmn_results).shape))\n        return bmn_results\n    @record_time_info\n    def extract_feature(self):\n        \"\"\"extract feature\"\"\"\n        if not self.DEBUG:\n            image_path_list = get_images(self.imgs_path)",
        "type": "code",
        "location": "/applications/BasketballAction/predict/action_detect/action.py:105-133"
    },
    "87": {
        "file_id": 8,
        "content": "This code defines classes for video feature extraction, proposal generation, and action detection. It utilizes model prediction with configured parameters and logs the shapes of results.",
        "type": "comment"
    },
    "88": {
        "file_id": 8,
        "content": "            self.configs['PPTSM']['frame_list'] = image_path_list\n            self.configs['AUDIO']['pcm_file'] = self.pcm_path\n            image_features = self.image_model.predict(self.configs)\n            if self.PCM_ONLY:\n                sample_rate = self.configs['AUDIO']['sample_rate']\n                pcm_features = mfcc_extractor.extract_pcm(self.pcm_path, sample_rate)\n                audio_features = []\n            else:\n                audio_features, pcm_features = self.audio_model.predict(self.configs)\n            np_image_features = np.array(image_features, dtype=np.float32)\n            np_audio_features = np.array(audio_features, dtype=np.float32)\n            np_pcm_features = np.array(pcm_features, dtype=np.float32)\n            video_features = {'image_feature': np_image_features,\n                              'audio_feature': np_audio_features,\n                              'pcm_feature': np_pcm_features}\n        else:\n            feature_path = self.imgs_path.replace(\"frames\", \"features\") + '.pkl'",
        "type": "code",
        "location": "/applications/BasketballAction/predict/action_detect/action.py:134-152"
    },
    "89": {
        "file_id": 8,
        "content": "The code configures the model inputs, predicts image and audio features, and stores them in the video_features dictionary. If PCM_ONLY is true, it extracts pcm_features separately. Otherwise, it predicts audio_features along with image_features. If no features are available, it sets feature_path to the image path's corresponding features file.",
        "type": "comment"
    },
    "90": {
        "file_id": 8,
        "content": "            video_features = pickle.load(open(feature_path, 'rb'))\n        logger.info(\"feature shape {} {} {}\".format(video_features['image_feature'].shape,\n                                                    video_features['audio_feature'].shape,\n                                                    video_features['pcm_feature'].shape))\n        return video_features\nif __name__ == '__main__':\n    model_predict = ActionDetection(cfg_file=\"../configs/configs.yaml\")\n    model_predict.load_model()\n    imgs_path = \"/home/work/datasets/EuroCup2016/frames/1be705a8f67648da8ec4b4296fa80895\"\n    pcm_path = \"/home/work/datasets/EuroCup2016/pcm/1be705a8f67648da8ec4b4296fa80895.pcm\"\n    bmn_results, action_results = model_predict.infer(imgs_path, pcm_path)\n    results = {'bmn_results': bmn_results, 'action_results': action_results}\n    with open('results.json', 'w', encoding='utf-8') as f:\n       data = json.dumps(results, indent=4, ensure_ascii=False)\n       f.write(data)",
        "type": "code",
        "location": "/applications/BasketballAction/predict/action_detect/action.py:153-174"
    },
    "91": {
        "file_id": 8,
        "content": "This code loads video features from file, checks the shape of image_feature, audio_feature, and pcm_feature arrays, and returns the video features. It then calls the ActionDetection model to infer on given image and audio paths, storing the results in bmn_results and action_results variables. Finally, it saves these results in a 'results.json' file.",
        "type": "comment"
    },
    "92": {
        "file_id": 9,
        "content": "/applications/BasketballAction/predict/action_detect/logger.py",
        "type": "filepath"
    },
    "93": {
        "file_id": 9,
        "content": "This code defines a custom logger class for the news stripper application. It checks if the 'logs' directory exists, creates it if not, and sets up a file handler for the logger. The handler is configured to log INFO level messages and uses a specific log format and date format.",
        "type": "summary"
    },
    "94": {
        "file_id": 9,
        "content": "\"\"\"\nlogger\n\"\"\"\nimport os\nimport logging\nclass Logger(logging.Logger):\n    \"\"\"Customized logger for news stripper\n    \"\"\"\n    def __init__(self):\n        super(Logger, self).__init__(self)\n        if not os.path.exists('logs'):\n            os.mkdir('logs')\n        handler = logging.FileHandler(\"logs/action_detect.log\")\n        # handler.setLevel(logging.DEBUG)\n        handler.setLevel(logging.INFO)\n        format = \"%(levelname)s: %(asctime)s: %(filename)s:%(lineno)d %(message)s\"\n        datefmt = \"%y-%m-%d %H:%M:%S\"\n        formatter = logging.Formatter(format, datefmt)\n        handler.setFormatter(formatter)\n        self.addHandler(handler)",
        "type": "code",
        "location": "/applications/BasketballAction/predict/action_detect/logger.py:1-23"
    },
    "95": {
        "file_id": 9,
        "content": "This code defines a custom logger class for the news stripper application. It checks if the 'logs' directory exists, creates it if not, and sets up a file handler for the logger. The handler is configured to log INFO level messages and uses a specific log format and date format.",
        "type": "comment"
    },
    "96": {
        "file_id": 10,
        "content": "/applications/BasketballAction/predict/action_detect/mfcc/feature_extractor.py",
        "type": "filepath"
    },
    "97": {
        "file_id": 10,
        "content": "This code extracts audio features, converts data to [-1.0, +1.0] range, applies log mel spectrogram, frames into examples for further processing, reads pcm data as bytes, and prints the shape of resulting examples batch.",
        "type": "summary"
    },
    "98": {
        "file_id": 10,
        "content": "\"\"\"\naudio feature extract\n\"\"\"\n# coding: utf-8\nimport os\nimport numpy as np\nimport pickle\nimport mfcc.vgg_params as vgg_params\ndef frame(data, window_length, hop_length):\n    \"\"\"\n    frame\n    \"\"\"\n    num_samples = data.shape[0]\n    num_frames = 1 + int(np.floor((num_samples - window_length) / hop_length))\n    shape = (num_frames, window_length) + data.shape[1:]\n    strides = (data.strides[0] * hop_length, ) + data.strides\n    return np.lib.stride_tricks.as_strided(data, shape=shape, strides=strides)\ndef periodic_hann(window_length):\n    \"\"\"\n    periodic_hann\n    \"\"\"\n    return 0.5 - (0.5 *\n                  np.cos(2 * np.pi / window_length * np.arange(window_length)))\ndef stft_magnitude(signal, fft_length, hop_length=None, window_length=None):\n    \"\"\"\n    stft_magnitude\n    \"\"\"\n    frames = frame(signal, window_length, hop_length)\n    window = periodic_hann(window_length)\n    windowed_frames = frames * window\n    return np.abs(np.fft.rfft(windowed_frames, int(fft_length)))\n_MEL_BREAK_FREQUENCY_HERTZ = 700.0\n_MEL_HIGH_FREQUENCY_Q = 1127.0",
        "type": "code",
        "location": "/applications/BasketballAction/predict/action_detect/mfcc/feature_extractor.py:1-41"
    },
    "99": {
        "file_id": 10,
        "content": "This code defines functions for audio feature extraction, including framing the audio data, applying a window function, and computing the short-time Fourier transform (STFT) magnitude. The _MEL_BREAK_FREQUENCY_HERTZ and _MEL_HIGH_FREQUENCY_Q variables are used for converting frequency values to Mel scale.",
        "type": "comment"
    }
}