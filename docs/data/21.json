{
    "2100": {
        "file_id": 164,
        "content": "            size=[self._voc_size, self._emb_size],\n            dtype=self._emb_dtype,\n            param_attr=paddle.ParamAttr(\n                name=self._word_emb_name, initializer=self._param_initializer),\n            is_sparse=False)\n        position_emb_out = static.nn.embedding(\n            input=position_ids,\n            size=[self._max_position_seq_len, self._emb_size],\n            dtype=self._emb_dtype,\n            param_attr=paddle.ParamAttr(\n                name=self._pos_emb_name, initializer=self._param_initializer))\n        sent_emb_out = static.nn.embedding(\n            sentence_ids,\n            size=[self._sent_types, self._emb_size],\n            dtype=self._emb_dtype,\n            param_attr=paddle.ParamAttr(\n                name=self._sent_emb_name, initializer=self._param_initializer))\n        # emb_out = emb_out + position_emb_out\n        # emb_out = emb_out + sent_emb_out\n        emb_out = paddle.add(x=emb_out, y=position_emb_out)\n        emb_out = paddle.add(x=emb_out, y=sent_emb_out)\n        if self._use_task_id:",
        "type": "code",
        "location": "/applications/MultimodalVideoTag/scenario_lib/models/ernie.py:133-158"
    },
    "2101": {
        "file_id": 164,
        "content": "This code initializes and concatenates three embeddings - word, position, and sentence - in a multimodal video tagging model. The embeddings are defined with specific sizes and data types. Two embeddings (position_emb_out and sent_emb_out) are added to the original embedding (emb_out), and then these combined embeddings are returned.",
        "type": "comment"
    },
    "2102": {
        "file_id": 164,
        "content": "            task_emb_out = static.nn.embedding(\n                task_ids,\n                size=[self._task_types, self._emb_size],\n                dtype=self._emb_dtype,\n                param_attr=paddle.ParamAttr(\n                    name=self._task_emb_name,\n                    initializer=self._param_initializer))\n            emb_out = emb_out + task_emb_out\n        emb_out = pre_process_layer(\n            emb_out, 'nd', self._prepostprocess_dropout, name='pre_encoder')\n        if self._dtype == \"float16\":\n            emb_out = paddle.cast(x=emb_out, dtype=self._dtype)\n            input_mask = paddle.cast(x=input_mask, dtype=self._dtype)\n        self_attn_mask = paddle.matmul(\n            x=input_mask, y=input_mask, transpose_y=True)\n        self_attn_mask = paddle.scale(\n            x=self_attn_mask, scale=10000.0, bias=-1.0, bias_after_scale=False)\n        n_head_self_attn_mask = paddle.stack(\n            x=[self_attn_mask] * self._n_head, axis=1)\n        n_head_self_attn_mask.stop_gradient = True\n        self._enc_out = encoder(",
        "type": "code",
        "location": "/applications/MultimodalVideoTag/scenario_lib/models/ernie.py:159-184"
    },
    "2103": {
        "file_id": 164,
        "content": "This code initializes an embedding layer for task types, adds it to the embeddings, applies pre-processing with dropout if necessary, and casts the embeddings to the desired dtype. It also creates a self-attention mask, stacks it for each attention head, sets its gradient to stop during backpropagation, and passes the embeddings through an encoder.",
        "type": "comment"
    },
    "2104": {
        "file_id": 164,
        "content": "            enc_input=emb_out,\n            attn_bias=n_head_self_attn_mask,\n            n_layer=self._n_layer,\n            n_head=self._n_head,\n            d_key=self._emb_size // self._n_head,\n            d_value=self._emb_size // self._n_head,\n            d_model=self._emb_size,\n            d_inner_hid=self._emb_size * 4,\n            prepostprocess_dropout=self._prepostprocess_dropout,\n            attention_dropout=self._attention_dropout,\n            relu_dropout=0,\n            hidden_act=self._hidden_act,\n            preprocess_cmd=\"\",\n            postprocess_cmd=\"dan\",\n            param_initializer=self._param_initializer,\n            name='encoder')\n        if self._dtype == \"float16\":\n            self._enc_out = paddle.cast(\n                x=self._enc_out, dtype=self._emb_dtype)\n    def get_sequence_output(self):\n        \"\"\"\n        get sequence output\n        \"\"\"\n        return self._enc_out\n    def get_sequence_textcnn_output(self, sequence_feature, input_mask):\n        \"\"\"\n        get sequence output\n        \"\"\"",
        "type": "code",
        "location": "/applications/MultimodalVideoTag/scenario_lib/models/ernie.py:185-215"
    },
    "2105": {
        "file_id": 164,
        "content": "This code is defining and initializing a model for an encoder layer in a deep learning application. The model takes several parameters such as embedding size, number of layers and heads, dropout rates, activation function, etc. It then casts the output to the specified data type if necessary. The `get_sequence_output` method returns the sequence output from the encoder layer and `get_sequence_textcnn_output` takes in a feature sequence and an input mask to generate the output.",
        "type": "comment"
    },
    "2106": {
        "file_id": 164,
        "content": "        seq_len = paddle.sum(x=input_mask, axis=[1, 2])\n        seq_len = paddle.cast(seq_len, 'int64')\n        sequence_feature = paddle.static.nn.sequence_unpad(sequence_feature, seq_len)\n        return self.textcnn(sequence_feature)\n    def get_pooled_output(self):\n        \"\"\"Get the first feature of each sequence for classification\"\"\"\n        next_sent_feat = paddle.slice(\n            input=self._enc_out, axes=[1], starts=[0], ends=[1])\n        next_sent_feat = static.nn.fc(\n            x=next_sent_feat,\n            size=self._emb_size,\n            activation=\"tanh\",\n            weight_attr=paddle.ParamAttr(\n                name=\"pooled_fc.w_0\", initializer=self._param_initializer),\n            bias_attr=\"pooled_fc.b_0\")\n        return next_sent_feat\n    def textcnn(self, feature, name='text_cnn'):\n        \"\"\"\n        TextCNN sequence feature extraction\n        \"\"\"\n        win_sizes = [2, 3, 4]\n        hid_dim = 256\n        convs = []\n        for win_size in win_sizes:\n            conv_h = paddle.fluid.nets.sequence_conv_pool(input=feature,",
        "type": "code",
        "location": "/applications/MultimodalVideoTag/scenario_lib/models/ernie.py:216-243"
    },
    "2107": {
        "file_id": 164,
        "content": "This code defines a TextCNN model for sequence feature extraction. It pads the input sequence, applies convolutions with various window sizes, and pools the results. The get_pooled_output function extracts the first feature of each sequence for classification by applying an FC layer with tanh activation. The textcnn function initializes a TextCNN model with specified window sizes and hidden dimensions.",
        "type": "comment"
    },
    "2108": {
        "file_id": 164,
        "content": "                                                   num_filters=hid_dim,\n                                                   filter_size=win_size,\n                                                   act=\"tanh\",\n                                                   pool_type=\"max\")\n            convs.append(conv_h)\n        convs_out = paddle.concat(x=convs, axis=1)\n        return convs_out",
        "type": "code",
        "location": "/applications/MultimodalVideoTag/scenario_lib/models/ernie.py:244-250"
    },
    "2109": {
        "file_id": 164,
        "content": "This code is creating a 1D convolutional layer with specified parameters, including the number of filters, filter size, activation function, and pooling type. The resulting convolutional layers are appended to the `convs` list, and then concatenated along axis 1 to form `convs_out`. Finally, the function returns `convs_out`.",
        "type": "comment"
    },
    "2110": {
        "file_id": 165,
        "content": "/applications/MultimodalVideoTag/scenario_lib/models/transformer_encoder.py",
        "type": "filepath"
    },
    "2111": {
        "file_id": 165,
        "content": "This code defines a PaddlePaddle transformer encoder layer for normalization and training, including residual connections, dropout, self-attention mechanism, and position-wise feed-forward networks. It creates a Transformer Encoder with Scaled Dot-Product Attention for NLP tasks.",
        "type": "summary"
    },
    "2112": {
        "file_id": 165,
        "content": "#   Copyright (c) 2019 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Transformer encoder.\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom functools import partial\nimport paddle\nimport paddle.static as static\ndef multi_head_attention(queries,\n                         keys,\n                         values,\n                         attn_bias,\n                         d_key,\n                         d_value,\n                         d_model,",
        "type": "code",
        "location": "/applications/MultimodalVideoTag/scenario_lib/models/transformer_encoder.py:1-32"
    },
    "2113": {
        "file_id": 165,
        "content": "This code defines a function called \"multi_head_attention\" which performs multi-head attention operations on queries, keys, and values. The function takes in additional parameters such as attn_bias, d_key, d_value, d_model. This is part of the Transformer encoder model implementation in PaddlePaddle framework.",
        "type": "comment"
    },
    "2114": {
        "file_id": 165,
        "content": "                         n_head=1,\n                         dropout_rate=0.,\n                         cache=None,\n                         param_initializer=None,\n                         name='multi_head_att'):\n    \"\"\"\n    Multi-Head Attention. Note that attn_bias is added to the logit before\n    computing softmax activiation to mask certain selected positions so that\n    they will not considered in attention weights.\n    \"\"\"\n    keys = queries if keys is None else keys\n    values = keys if values is None else values\n    if not (len(queries.shape) == len(keys.shape) == len(values.shape) == 3):\n        raise ValueError(\n            \"Inputs: quries, keys and values should all be 3-D tensors.\")\n    def __compute_qkv(queries, keys, values, n_head, d_key, d_value):\n        \"\"\"\n        Add linear projection to queries, keys, and values.\n        \"\"\"\n        q = static.nn.fc(x=queries,\n                      size=d_key * n_head,\n                      num_flatten_dims=2,\n                      weight_attr=paddle.ParamAttr(",
        "type": "code",
        "location": "/applications/MultimodalVideoTag/scenario_lib/models/transformer_encoder.py:33-57"
    },
    "2115": {
        "file_id": 165,
        "content": "This code snippet defines a Multi-Head Attention layer. It takes in queries, keys (optional), and values (optional) as inputs, and performs linear projections on the queries before computing the attention weights. The function __compute_qkv also handles the case when keys or values are None by setting them to be equal to queries if needed. The inputs should all be 3-D tensors.",
        "type": "comment"
    },
    "2116": {
        "file_id": 165,
        "content": "                          name=name + '_query_fc.w_0',\n                          initializer=param_initializer),\n                      bias_attr=name + '_query_fc.b_0')\n        k = static.nn.fc(x=keys,\n                      size=d_key * n_head,\n                      num_flatten_dims=2,\n                      weight_attr=paddle.ParamAttr(\n                          name=name + '_key_fc.w_0',\n                          initializer=param_initializer),\n                      bias_attr=name + '_key_fc.b_0')\n        v = static.nn.fc(x=values,\n                      size=d_value * n_head,\n                      num_flatten_dims=2,\n                      weight_attr=paddle.ParamAttr(\n                          name=name + '_value_fc.w_0',\n                          initializer=param_initializer),\n                      bias_attr=name + '_value_fc.b_0')\n        return q, k, v\n    def __split_heads(x, n_head):\n        \"\"\"\n        Reshape the last dimension of inpunt tensor x so that it becomes two\n        dimensions and then transpose. Specifically, input a tensor with shape",
        "type": "code",
        "location": "/applications/MultimodalVideoTag/scenario_lib/models/transformer_encoder.py:58-80"
    },
    "2117": {
        "file_id": 165,
        "content": "This code defines a function for the Transformer Encoder layer. It includes functions for multi-head attention, position-wise feed-forward network layers, and splits heads of input tensors. Parameters such as d_key, n_head, and param_initializer are used to define the dimensions and initialization methods for weights. The code uses Paddle's static nn library and defines the names for different FC layers within the function.",
        "type": "comment"
    },
    "2118": {
        "file_id": 165,
        "content": "        [bs, max_sequence_length, n_head * hidden_dim] then output a tensor\n        with shape [bs, n_head, max_sequence_length, hidden_dim].\n        \"\"\"\n        hidden_size = x.shape[-1]\n        # The value 0 in shape attr means copying the corresponding dimension\n        # size of the input as the output dimension size.\n        reshaped = paddle.reshape(\n            x=x, shape=[0, 0, n_head, hidden_size // n_head])\n        # permuate the dimensions into:\n        # [batch_size, n_head, max_sequence_len, hidden_size_per_head]\n        return paddle.transpose(x=reshaped, perm=[0, 2, 1, 3])\n    def __combine_heads(x):\n        \"\"\"\n        Transpose and then reshape the last two dimensions of inpunt tensor x\n        so that it becomes one dimension, which is reverse to __split_heads.\n        \"\"\"\n        if len(x.shape) == 3: return x\n        if len(x.shape) != 4:\n            raise ValueError(\"Input(x) should be a 4-D Tensor.\")\n        trans_x = paddle.transpose(x, perm=[0, 2, 1, 3])\n        # The value 0 in shape attr means copying the corresponding dimension",
        "type": "code",
        "location": "/applications/MultimodalVideoTag/scenario_lib/models/transformer_encoder.py:81-104"
    },
    "2119": {
        "file_id": 165,
        "content": "This code is performing tensor reshaping and transposing operations to split the input tensor into multiple smaller tensors, representing different attention heads. The `__split_heads` function splits the tensor into a shape of [bs, n_head, max_sequence_length, hidden_dim], while the `__combine_heads` function reverses this process by transposing and reshaping the last two dimensions to combine the attention heads back into one dimension.",
        "type": "comment"
    },
    "2120": {
        "file_id": 165,
        "content": "        # size of the input as the output dimension size.\n        return paddle.reshape(\n            x=trans_x,\n            shape=[0, 0, trans_x.shape[2] * trans_x.shape[3]])\n    def scaled_dot_product_attention(q, k, v, attn_bias, d_key, dropout_rate):\n        \"\"\"\n        Scaled Dot-Product Attention\n        \"\"\"\n        scaled_q = paddle.scale(x=q, scale=d_key**-0.5)\n        product = paddle.matmul(x=scaled_q, y=k, transpose_y=True)\n        if attn_bias:\n            # product += attn_bias\n            product = paddle.add(x=product, y=attn_bias)\n        weights = paddle.nn.functional.softmax(x=product)\n        if dropout_rate:\n            weights = paddle.nn.functional.dropout(weights, p=dropout_rate, mode=\"upscale_in_train\", training=True)\n        out = paddle.matmul(x=weights, y=v)\n        return out\n    q, k, v = __compute_qkv(queries, keys, values, n_head, d_key, d_value)\n    if cache is not None:  # use cache and concat time steps\n        # Since the inplace reshape in __split_heads changes the shape of k and",
        "type": "code",
        "location": "/applications/MultimodalVideoTag/scenario_lib/models/transformer_encoder.py:105-128"
    },
    "2121": {
        "file_id": 165,
        "content": "This code defines a function that performs Scaled Dot-Product Attention. It first scales the query vector by dividing it with the square root of the key dimension, then takes the dot product between scaled query and key matrices after transposing the key matrix. If attention bias is provided, it adds it to the product. It applies softmax activation on the result to get weights, which are optionally dropout masked if a dropout rate is specified. Finally, it computes the output vector by taking the weighted sum of value vectors. This function is used in the context of Transformer Encoder layers.",
        "type": "comment"
    },
    "2122": {
        "file_id": 165,
        "content": "        # v, which is the cache input for next time step, reshape the cache\n        # input from the previous time step first.\n        k = cache[\"k\"] = paddle.concat(\n            x=[paddle.reshape(\n                x=cache[\"k\"], shape=[0, 0, d_model]), k], axis=1)\n        v = cache[\"v\"] = paddle.concat(\n            x=[paddle.reshape(\n                x=cache[\"v\"], shape=[0, 0, d_model]), v], axis=1)\n    q = __split_heads(q, n_head)\n    k = __split_heads(k, n_head)\n    v = __split_heads(v, n_head)\n    ctx_multiheads = scaled_dot_product_attention(q, k, v, attn_bias, d_key,\n                                                  dropout_rate)\n    out = __combine_heads(ctx_multiheads)\n    # Project back to the model size.\n    proj_out = static.nn.fc(x=out,\n                         size=d_model,\n                         num_flatten_dims=2,\n                         weight_attr=paddle.ParamAttr(\n                             name=name + '_output_fc.w_0',\n                             initializer=param_initializer),\n                         bias_attr=name + '_output_fc.b_0')",
        "type": "code",
        "location": "/applications/MultimodalVideoTag/scenario_lib/models/transformer_encoder.py:129-154"
    },
    "2123": {
        "file_id": 165,
        "content": "This code is reshaping the cache input for the next time step and splitting the inputs into multiple heads. It performs scaled dot product attention, combines the outputs of each head, and projects the result back to the model size using a fully connected layer.",
        "type": "comment"
    },
    "2124": {
        "file_id": 165,
        "content": "    return proj_out\ndef positionwise_feed_forward(x,\n                              d_inner_hid,\n                              d_hid,\n                              dropout_rate,\n                              hidden_act,\n                              param_initializer=None,\n                              name='ffn'):\n    \"\"\"\n    Position-wise Feed-Forward Networks.\n    This module consists of two linear transformations with a ReLU activation\n    in between, which is applied to each position separately and identically.\n    \"\"\"\n    hidden = static.nn.fc(x=x,\n                       size=d_inner_hid,\n                       num_flatten_dims=2,\n                       activation=hidden_act,\n                       weight_attr=paddle.ParamAttr(\n                           name=name + '_fc_0.w_0',\n                           initializer=param_initializer),\n                       bias_attr=name + '_fc_0.b_0')\n    if dropout_rate:\n        hidden = paddle.nn.functional.dropout(\n            hidden,\n            p=dropout_rate,\n            mode=\"upscale_in_train\",",
        "type": "code",
        "location": "/applications/MultimodalVideoTag/scenario_lib/models/transformer_encoder.py:155-182"
    },
    "2125": {
        "file_id": 165,
        "content": "This code defines the position-wise feed-forward network used in a transformer encoder. It consists of two linear transformations with a ReLU activation applied to each position separately and identically. The hidden layer is passed through a dropout if dropout_rate is specified.",
        "type": "comment"
    },
    "2126": {
        "file_id": 165,
        "content": "            training=True)\n    out = static.nn.fc(x=hidden,\n                    size=d_hid,\n                    num_flatten_dims=2,\n                    weight_attr=paddle.ParamAttr(\n                        name=name + '_fc_1.w_0', initializer=param_initializer),\n                    bias_attr=name + '_fc_1.b_0')\n    return out\ndef pre_post_process_layer(prev_out, out, process_cmd, dropout_rate=0.,\n                           name=''):\n    \"\"\"\n    Add residual connection, layer normalization and droput to the out tensor\n    optionally according to the value of process_cmd.\n    This will be used before or after multi-head attention and position-wise\n    feed-forward networks.\n    \"\"\"\n    for cmd in process_cmd:\n        if cmd == \"a\":  # add residual connection\n            # out = out + prev_out if prev_out else out\n            out = paddle.add(x=out, y=prev_out) if prev_out else out\n        elif cmd == \"n\":  # add layer normalization\n            out_dtype = out.dtype\n            if out_dtype == \"float16\":\n                out = paddle.cast(x=out, dtype=\"float32\")",
        "type": "code",
        "location": "/applications/MultimodalVideoTag/scenario_lib/models/transformer_encoder.py:183-208"
    },
    "2127": {
        "file_id": 165,
        "content": "This code defines a function for a transformer encoder layer in the PaddleVideo MultimodalVideoTag application. The layer includes a multi-head attention mechanism and a position-wise feed-forward network, with residual connections and layer normalization added before or after these operations, as specified by the process_cmd argument.",
        "type": "comment"
    },
    "2128": {
        "file_id": 165,
        "content": "            out = static.nn.layer_norm(\n                out,\n                begin_norm_axis=len(out.shape) - 1,\n                param_attr=paddle.ParamAttr(\n                    name=name + '_layer_norm_scale',\n                    initializer=paddle.nn.initializer.Constant(value=1.)),\n                bias_attr=paddle.ParamAttr(\n                    name=name + '_layer_norm_bias',\n                    initializer=paddle.nn.initializer.Constant(value=0.)))\n            if out_dtype == \"float16\":\n                out = paddle.cast(x=out, dtype=\"float16\")\n        elif cmd == \"d\":  # add dropout\n            if dropout_rate:\n                out = paddle.nn.functional.dropout(\n                    out,\n                    p=dropout_rate,\n                    dropout_implementation=\"upscale_in_train\",\n                    training=True)\n    return out\npre_process_layer = partial(pre_post_process_layer, None)\npost_process_layer = pre_post_process_layer\ndef encoder_layer(enc_input,\n                  attn_bias,\n                  n_head,",
        "type": "code",
        "location": "/applications/MultimodalVideoTag/scenario_lib/models/transformer_encoder.py:209-236"
    },
    "2129": {
        "file_id": 165,
        "content": "This code is part of a transformer encoder layer implementation in PaddlePaddle. It applies layer normalization, optional float16 casting, and optionally dropout for training. The pre_process_layer and post_process_layer are partial functions used for data pre-processing and post-processing respectively. The encoder_layer function takes input, attention bias, and number of heads as inputs to create a transformer encoder layer.",
        "type": "comment"
    },
    "2130": {
        "file_id": 165,
        "content": "                  d_key,\n                  d_value,\n                  d_model,\n                  d_inner_hid,\n                  prepostprocess_dropout,\n                  attention_dropout,\n                  relu_dropout,\n                  hidden_act,\n                  preprocess_cmd=\"n\",\n                  postprocess_cmd=\"da\",\n                  param_initializer=None,\n                  name=''):\n    \"\"\"The encoder layers that can be stacked to form a deep encoder.\n    This module consits of a multi-head (self) attention followed by\n    position-wise feed-forward networks and both the two components companied\n    with the post_process_layer to add residual connection, layer normalization\n    and droput.\n    \"\"\"\n    attn_output = multi_head_attention(\n        pre_process_layer(\n            enc_input,\n            preprocess_cmd,\n            prepostprocess_dropout,\n            name=name + '_pre_att'),\n        None,\n        None,\n        attn_bias,\n        d_key,\n        d_value,\n        d_model,\n        n_head,\n        attention_dropout,",
        "type": "code",
        "location": "/applications/MultimodalVideoTag/scenario_lib/models/transformer_encoder.py:237-268"
    },
    "2131": {
        "file_id": 165,
        "content": "This code defines a transformer encoder layer that stacks multiple layers to form a deep encoder. The encoder consists of a multi-head self-attention mechanism followed by position-wise feed-forward networks, all with residual connections and layer normalization to add dropout.",
        "type": "comment"
    },
    "2132": {
        "file_id": 165,
        "content": "        param_initializer=param_initializer,\n        name=name + '_multi_head_att')\n    attn_output = post_process_layer(\n        enc_input,\n        attn_output,\n        postprocess_cmd,\n        prepostprocess_dropout,\n        name=name + '_post_att')\n    ffd_output = positionwise_feed_forward(\n        pre_process_layer(\n            attn_output,\n            preprocess_cmd,\n            prepostprocess_dropout,\n            name=name + '_pre_ffn'),\n        d_inner_hid,\n        d_model,\n        relu_dropout,\n        hidden_act,\n        param_initializer=param_initializer,\n        name=name + '_ffn')\n    return post_process_layer(\n        attn_output,\n        ffd_output,\n        postprocess_cmd,\n        prepostprocess_dropout,\n        name=name + '_post_ffn')\ndef encoder(enc_input,\n            attn_bias,\n            n_layer,\n            n_head,\n            d_key,\n            d_value,\n            d_model,\n            d_inner_hid,\n            prepostprocess_dropout,\n            attention_dropout,\n            relu_dropout,\n            hidden_act,",
        "type": "code",
        "location": "/applications/MultimodalVideoTag/scenario_lib/models/transformer_encoder.py:269-308"
    },
    "2133": {
        "file_id": 165,
        "content": "This code defines a transformer encoder model. It utilizes an attention mechanism to process input sequences, followed by position-wise feed forward layers. The function takes input sequences, attention bias, number of layers, number of heads, and other parameters as inputs and returns the processed output.",
        "type": "comment"
    },
    "2134": {
        "file_id": 165,
        "content": "            preprocess_cmd=\"n\",\n            postprocess_cmd=\"da\",\n            param_initializer=None,\n            name=''):\n    \"\"\"\n    The encoder is composed of a stack of identical layers returned by calling\n    encoder_layer.\n    \"\"\"\n    for i in range(n_layer):\n        enc_output = encoder_layer(\n            enc_input,\n            attn_bias,\n            n_head,\n            d_key,\n            d_value,\n            d_model,\n            d_inner_hid,\n            prepostprocess_dropout,\n            attention_dropout,\n            relu_dropout,\n            hidden_act,\n            preprocess_cmd,\n            postprocess_cmd,\n            param_initializer=param_initializer,\n            name=name + '_layer_' + str(i))\n        enc_input = enc_output\n    enc_output = pre_process_layer(\n        enc_output, preprocess_cmd, prepostprocess_dropout, name=\"post_encoder\")\n    return enc_output",
        "type": "code",
        "location": "/applications/MultimodalVideoTag/scenario_lib/models/transformer_encoder.py:309-338"
    },
    "2135": {
        "file_id": 165,
        "content": "This code defines a function to create an encoder consisting of multiple layers, where each layer is generated by calling the \"encoder_layer\" function. The encoder takes in input, attention bias, number of heads, dimensionality of keys and values, model dimensions, inner hidden dimensions, and dropout rates for preprocessing and postprocessing. The function applies each layer to the input sequentially, updating the input with each iteration. Finally, it applies a pre-processing layer to the output using specified preprocessing command and prepostprocess_dropout.",
        "type": "comment"
    },
    "2136": {
        "file_id": 166,
        "content": "/applications/MultimodalVideoTag/scenario_lib/train.py",
        "type": "filepath"
    },
    "2137": {
        "file_id": 166,
        "content": "This code initializes a PaddlePaddle model for video training, sets up feeds and outputs, configures loss and optimizer, builds the model, prepares programs, trains, logs, and saves it. The main function handles arguments, checks save directory, and executes the training process.",
        "type": "summary"
    },
    "2138": {
        "file_id": 166,
        "content": "\"\"\"\ntrain main\n\"\"\"\n#  Copyright (c) 2019 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\nimport sys\nimport time\nimport argparse\nimport logging\nimport numpy as np\nimport paddle\npaddle.enable_static()\nimport paddle.static as static\nfrom accuracy_metrics import MetricsCalculator\nfrom datareader import get_reader\nfrom config import print_configs, merge_configs, parse_config\nfrom models.attention_lstm_ernie import AttentionLstmErnie\nfrom utils import init_pretraining_params, train_with_pyreader",
        "type": "code",
        "location": "/applications/MultimodalVideoTag/scenario_lib/train.py:1-34"
    },
    "2139": {
        "file_id": 166,
        "content": "The code imports necessary libraries and modules, enables static mode for PaddlePaddle, initializes a model (AttentionLstmErnie), defines train_with_pyreader function, and handles config file operations. It follows the Apache License 2.0 and provides information for obtaining the license.",
        "type": "comment"
    },
    "2140": {
        "file_id": 166,
        "content": "logging.root.handlers = []\nFORMAT = '[%(levelname)s: %(filename)s: %(lineno)4d]: %(message)s'\nlogging.basicConfig(level=logging.INFO, format=FORMAT, stream=sys.stdout)\nlogger = logging.getLogger(__name__)\ndef parse_args():\n    \"\"\"parse_args\n    \"\"\"\n    parser = argparse.ArgumentParser(\"Paddle Video train script\")\n    parser.add_argument(\n        '--model_name',\n        type=str,\n        default='BaiduNet',\n        help='name of model to train.')\n    parser.add_argument(\n        '--config',\n        type=str,\n        default='configs/conf.txt',\n        help='path to config file of model')\n    parser.add_argument(\n        '--batch_size',\n        type=int,\n        default=None,\n        help='training batch size. None to use config file setting.')\n    parser.add_argument(\n        '--learning_rate',\n        type=float,\n        default=None,\n        help='learning rate use for training. None to use config file setting.')\n    parser.add_argument(\n        '--pretrain',\n        type=str,\n        default=None,\n        help='path to pretrain weights. None to use default weights path in  ~/.paddle/weights.'",
        "type": "code",
        "location": "/applications/MultimodalVideoTag/scenario_lib/train.py:37-71"
    },
    "2141": {
        "file_id": 166,
        "content": "This code sets up the logging configuration and parses command-line arguments for training a video model using Paddle Video. The default model name is 'BaiduNet', and the config file path is 'configs/conf.txt'. It also allows setting the batch size, learning rate, and pretrain weights through command-line flags.",
        "type": "comment"
    },
    "2142": {
        "file_id": 166,
        "content": "    )\n    parser.add_argument(\n        '--resume',\n        type=str,\n        default=None,\n        help='path to resume training based on previous checkpoints. '\n        'None for not resuming any checkpoints.')\n    parser.add_argument(\n        '--use_gpu', type=bool, default=True, help='default use gpu.')\n    parser.add_argument(\n        '--no_use_pyreader',\n        action='store_true',\n        default=False,\n        help='whether to use pyreader')\n    parser.add_argument(\n        '--no_memory_optimize',\n        action='store_true',\n        default=False,\n        help='whether to use memory optimize in train')\n    parser.add_argument(\n        '--epoch_num',\n        type=int,\n        default=0,\n        help='epoch number, 0 for read from config file')\n    parser.add_argument(\n        '--valid_interval',\n        type=int,\n        default=1,\n        help='validation epoch interval, 0 for no validation.')\n    parser.add_argument(\n        '--save_dir',\n        type=str,\n        default='checkpoints',\n        help='directory name to save train snapshoot')",
        "type": "code",
        "location": "/applications/MultimodalVideoTag/scenario_lib/train.py:72-105"
    },
    "2143": {
        "file_id": 166,
        "content": "This code snippet from the PaddleVideo library's MultimodalVideoTag application defines command line argument options for training. Options include resuming training, GPU usage, disabling pyreader, memory optimization during training, epoch number, validation interval, and saving directory.",
        "type": "comment"
    },
    "2144": {
        "file_id": 166,
        "content": "    parser.add_argument(\n        '--log_interval',\n        type=int,\n        default=10,\n        help='mini-batch interval to log.')\n    parser.add_argument(\n        '--save_log_name',\n        type=str,\n        default='train_val',\n        help='save to tensorboard filename recommand model name.')\n    args = parser.parse_args()\n    return args\ndef train(args):\n    \"\"\"train main\n    \"\"\"\n    # parse config\n    config = parse_config(args.config)\n    train_config = merge_configs(config, 'train', vars(args))\n    valid_config = merge_configs(config, 'valid', vars(args))\n    print_configs(train_config, 'Train')\n    train_model = AttentionLstmErnie(args.model_name, train_config, mode='train')\n    valid_model = AttentionLstmErnie(args.model_name, valid_config, mode='valid')\n    max_train_steps = train_config.TRAIN.epoch * train_config.TRAIN.num_samples // train_config.TRAIN.batch_size\n    print('max train steps %d' % (max_train_steps))\n    # build model\n    startup = static.Program()\n    train_prog = static.Program()\n    with static.program_guard(train_prog, startup):",
        "type": "code",
        "location": "/applications/MultimodalVideoTag/scenario_lib/train.py:106-136"
    },
    "2145": {
        "file_id": 166,
        "content": "This code defines command-line arguments for the mini-batch interval to log and the save filename, parses the configuration file, creates train and valid models based on the model name and configurations, sets the maximum number of training steps, and prepares static programs for building the model.",
        "type": "comment"
    },
    "2146": {
        "file_id": 166,
        "content": "        paddle.disable_static()\n        train_model.build_input(use_pyreader=True)\n        train_model.build_model()\n            # for the input, has the form [data1, data2,..., label], so train_feeds[-1] is label\n        train_feeds = train_model.feeds()\n        train_feeds[-1].persistable = True\n            # for the output of classification model, has the form [pred]\n        train_outputs = train_model.outputs()\n        for output in train_outputs:\n            output.persistable = True\n        train_loss = train_model.loss()\n        train_loss.persistable = True\n            # outputs, loss, label should be fetched, so set persistable to be true\n        optimizer = train_model.optimizer()\n        optimizer.minimize(train_loss)\n        train_pyreader = train_model.pyreader()\n        paddle.enable_static()\n    if not args.no_memory_optimize:\n        paddle.distributed.transpiler.memory_optimize(train_prog)\n    valid_prog = static.Program()\n    with static.program_guard(valid_prog, startup):\n        paddle.disable_static()",
        "type": "code",
        "location": "/applications/MultimodalVideoTag/scenario_lib/train.py:137-160"
    },
    "2147": {
        "file_id": 166,
        "content": "This code snippet prepares the model for training by setting up feeds, outputs, loss, and optimizer. It also enables memory optimization if specified by arguments.",
        "type": "comment"
    },
    "2148": {
        "file_id": 166,
        "content": "        valid_model.build_input(True)\n        valid_model.build_model()\n        valid_feeds = valid_model.feeds()\n        valid_outputs = valid_model.outputs()\n        valid_loss = valid_model.loss()\n        valid_pyreader = valid_model.pyreader()\n        paddle.enable_static()\n    place = paddle.CUDAPlace(0) if args.use_gpu else paddle.CPUPlace()\n    exe = static.Executor(place)\n    exe.run(startup)\n    if args.resume:\n        # if resume weights is given, load resume weights directly\n        assert os.path.exists(args.resume), \\\n            \"Given resume weight dir {} not exist.\".format(args.resume)\n        def if_exist(var):\n            \"\"\"if_exist\n            \"\"\"\n            return os.path.exists(os.path.join(args.resume, var.name))\n        print('resuming ,,,,,,,,,,,,,,')\n        paddle.fluid.io.load_persistables(\n                    exe, '', main_program=train_prog, filename=args.resume)\n    else:\n        # load ernie pretrain model\n        init_pretraining_params(exe,\n                                train_config.TRAIN.ernie_pretrain_dict_path,",
        "type": "code",
        "location": "/applications/MultimodalVideoTag/scenario_lib/train.py:161-190"
    },
    "2149": {
        "file_id": 166,
        "content": "The code is building the model, setting up executor and place (CPU or GPU), checking if resume weights exist to load them if necessary, and initializing pre-trained parameters for Ernie model.",
        "type": "comment"
    },
    "2150": {
        "file_id": 166,
        "content": "                                main_program=train_prog)\n        # if not in resume mode, load pretrain weights\n        # this pretrain may be only audio or video\n        if args.pretrain:\n            assert os.path.exists(args.pretrain), \\\n                \"Given pretrain weight dir {} not exist.\".format(args.pretrain)\n        if args.pretrain:\n            train_model.load_test_weights_file(exe, args.pretrain, train_prog, place)\n    build_strategy = paddle.static.BuildStrategy()\n    build_strategy.enable_inplace = True\n    compiled_train_prog = static.CompiledProgram(\n        train_prog).with_data_parallel(loss_name=train_loss.name,\n                                       build_strategy=build_strategy)\n    compiled_valid_prog = static.CompiledProgram(\n        valid_prog).with_data_parallel(share_vars_from=compiled_train_prog,\n                                       build_strategy=build_strategy)\n    # get reader\n    bs_denominator = 1\n    if (not args.no_use_pyreader) and args.use_gpu:\n        dev_list = static.cuda_places()",
        "type": "code",
        "location": "/applications/MultimodalVideoTag/scenario_lib/train.py:191-213"
    },
    "2151": {
        "file_id": 166,
        "content": "Loading pre-trained weights if provided, enabling inplace for faster execution and creating compiled programs with data parallelism for both training and validation programs. If not using PyReader and GPU is enabled, it sets the device list to use CUDA places.",
        "type": "comment"
    },
    "2152": {
        "file_id": 166,
        "content": "        bs_denominator = len(dev_list)\n    train_config.TRAIN.batch_size = int(train_config.TRAIN.batch_size /\n                                        bs_denominator)\n    valid_config.VALID.batch_size = int(valid_config.VALID.batch_size /\n                                        bs_denominator)\n    train_reader = get_reader(args.model_name.upper(), 'train', train_config)\n    valid_reader = get_reader(args.model_name.upper(), 'valid', valid_config)\n    exe_places = static.cuda_places() if args.use_gpu else static.cpu_places()\n    train_pyreader.decorate_sample_list_generator(train_reader,\n                                                  places=exe_places)\n    valid_pyreader.decorate_sample_list_generator(valid_reader,\n                                                  places=exe_places)\n    # get metrics\n    train_metrics = MetricsCalculator(args.model_name.upper(), 'train', train_config)\n    valid_metrics = MetricsCalculator(args.model_name.upper(), 'valid', valid_config)\n    # print(\"****************************valid_metrics\", valid_metrics.get())",
        "type": "code",
        "location": "/applications/MultimodalVideoTag/scenario_lib/train.py:214-231"
    },
    "2153": {
        "file_id": 166,
        "content": "This code sets the batch size for training and validation based on the length of the development list. It initializes train and valid readers with these batch sizes, decorates them with specified places, and creates MetricsCalculator objects to get metrics for training and validation.",
        "type": "comment"
    },
    "2154": {
        "file_id": 166,
        "content": "    train_fetch_list = [train_loss.name] + [x.name for x in train_outputs\n                                            ] + [train_feeds[-1].name]\n    valid_fetch_list = [valid_loss.name] + [x.name for x in valid_outputs\n                                            ] + [valid_feeds[-1].name]\n    epochs = args.epoch_num or train_model.epoch_num()\n    train_with_pyreader(\n        exe,\n        train_prog,\n        compiled_train_prog,\n        train_pyreader,\n        train_fetch_list,\n        train_metrics,\n        epochs=epochs,\n        log_interval=args.log_interval,\n        valid_interval=args.valid_interval,\n        save_dir=args.save_dir,\n        save_model_name=args.model_name,\n        test_exe=compiled_valid_prog,\n        test_pyreader=valid_pyreader,\n        test_fetch_list=valid_fetch_list,\n        test_metrics=valid_metrics)\nif __name__ == \"__main__\":\n    args = parse_args()\n    logger.info(args)\n    if not os.path.exists(args.save_dir):\n        os.makedirs(args.save_dir)\n    train(args)",
        "type": "code",
        "location": "/applications/MultimodalVideoTag/scenario_lib/train.py:232-263"
    },
    "2155": {
        "file_id": 166,
        "content": "The code initializes training and validation fetch lists, sets the number of epochs based on argument or model default, then trains the model using the specified executor, programs, feeds, and fetch lists. It also handles logging intervals, valid intervals, save directory, and save model name. The main function parses arguments, checks if the save directory exists, and calls the train function to execute the training process.",
        "type": "comment"
    },
    "2156": {
        "file_id": 167,
        "content": "/applications/MultimodalVideoTag/scenario_lib/utils.py",
        "type": "filepath"
    },
    "2157": {
        "file_id": 167,
        "content": "The `test_with_pyreader` and `train_with_pyreader` functions are used in a framework to execute tests with `pyreader`, evaluate metrics, log intervals, train models, handle options like testing, saving, early stopping, measure processing time, and update metrics. The code snippet defines model saving functions, deletes directories, implements early stopping, initializes pre-trained parameters, and uses AttrDict for getter/setter functionality.",
        "type": "summary"
    },
    "2158": {
        "file_id": 167,
        "content": "\"\"\"\nutils\n\"\"\"\n#  Copyright (c) 2018 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\nimport sys\nimport time\nimport traceback\nimport logging\nimport shutil\nimport numpy as np\nimport paddle\nimport paddle.static as static\nimport static as static\nlogger = logging.getLogger(__name__)\ndef test_with_pyreader(exe,\n                       compiled_test_prog,\n                       test_pyreader,\n                       test_fetch_list,\n                       test_metrics,\n                       log_interval=0):",
        "type": "code",
        "location": "/applications/MultimodalVideoTag/scenario_lib/utils.py:1-39"
    },
    "2159": {
        "file_id": 167,
        "content": "The code defines a function `test_with_pyreader` which takes several parameters like `exe`, `compiled_test_prog`, etc., and appears to be part of a larger framework. It seems to execute a test with the help of `pyreader` for input data, fetch list for outputs, and metrics for evaluation. The function runs on an interval specified by `log_interval`.",
        "type": "comment"
    },
    "2160": {
        "file_id": 167,
        "content": "    \"\"\"test_with_pyreader\n    \"\"\"\n    if not test_pyreader:\n        logger.error(\"[TEST] get pyreader failed.\")\n    test_metrics.reset()\n    test_iter = 0\n    label_all = []\n    pred_all = []\n    try:\n        for data in test_pyreader():\n            test_outs = exe.run(compiled_test_prog,\n                                fetch_list=test_fetch_list,\n                                feed=data)\n            loss = np.array(test_outs[0])\n            pred = np.array(test_outs[1])\n            label = np.array(test_outs[-1])\n            pred_all.extend(pred)\n            label_all.extend(label)\n            test_metrics.accumulate(loss, pred, label)\n            test_iter += 1\n        test_metrics.finalize_and_log_out(\"[TEST] Finish\")\n    except Exception as e:\n        logger.warn(\n            \"[TEST] fail to execute test or calculate metrics: {}\".format(e))\n        traceback.print_exc()\n    metrics_dict, test_loss = test_metrics.get_computed_metrics()\n    metrics_dict['label_all'] = label_all\n    metrics_dict['pred_all'] = pred_all",
        "type": "code",
        "location": "/applications/MultimodalVideoTag/scenario_lib/utils.py:40-67"
    },
    "2161": {
        "file_id": 167,
        "content": "The code tests a PaddleVideo application, using the \"test_pyreader\" to read data and runs it through a neural network. It accumulates and logs test metrics, handles exceptions and provides a final result with computed metrics.",
        "type": "comment"
    },
    "2162": {
        "file_id": 167,
        "content": "    return test_loss, metrics_dict\ndef train_with_pyreader(exe, train_prog, compiled_train_prog, train_pyreader,\n                        train_fetch_list, train_metrics, epochs=10,\n                        log_interval=0, valid_interval=0,\n                        save_dir='./', save_model_name='model',\n                        test_exe=None, test_pyreader=None,\n                        test_fetch_list=None, test_metrics=None):\n    \"\"\"train_with_pyreader\n    \"\"\"\n    if not train_pyreader:\n        logger.error(\"[TRAIN] get pyreader failed.\")\n    EARLY_STOP_NUM = 20\n    early_stop = EARLY_STOP_NUM\n    global_iter = 0\n    train_iter = 0\n    iter_all = 0\n    best_test_acc1 = 0\n    for epoch in range(epochs):\n        lr = static.global_scope().find_var(\"learning_rate\").get_tensor()\n        logger.info(\n            \"------- learning rate {}, learning rate counter  -----\".format(\n                np.array(lr)))\n        if early_stop < 0:\n            logger.info('Earyly Stop !!!')\n            break\n        train_metrics.reset()",
        "type": "code",
        "location": "/applications/MultimodalVideoTag/scenario_lib/utils.py:68-96"
    },
    "2163": {
        "file_id": 167,
        "content": "The function `train_with_pyreader` trains a model for the specified number of epochs and returns the test loss and metrics dictionary. It also includes options for testing and saving the model, as well as early stopping based on a defined threshold. The code initializes variables and enters a loop over the number of epochs, resetting training metrics and checking if early stopping should occur before each iteration.",
        "type": "comment"
    },
    "2164": {
        "file_id": 167,
        "content": "        global_iter += train_iter\n        epoch_periods = []\n        for data in train_pyreader():\n            try:\n                cur_time = time.time()\n                train_outs = exe.run(compiled_train_prog,\n                                     fetch_list=train_fetch_list,\n                                     feed=data)\n                iter_all += 1\n                period = time.time() - cur_time\n                epoch_periods.append(period)\n                loss = np.array(train_outs[0])\n                pred = np.array(train_outs[1])\n                label = np.array(train_outs[-1])\n                train_metrics.accumulate(loss, pred, label)\n                if log_interval > 0 and (train_iter % log_interval == 0):\n                    # eval here\n                    train_metrics.finalize_and_log_out(\n                                info='[TRAIN] Epoch {} iter {} everage: '.format(epoch, train_iter))\n                train_iter += 1\n            except Exception as e:\n                logger.info(\n                    \"[TRAIN] Epoch {}, iter {} data training failed: {}\".",
        "type": "code",
        "location": "/applications/MultimodalVideoTag/scenario_lib/utils.py:97-119"
    },
    "2165": {
        "file_id": 167,
        "content": "This code is part of a training loop for a machine learning model. It keeps track of the current iteration, measures the time taken for processing each data batch, and updates loss, prediction, and label metrics. If logging interval is met, it finalizes and logs the metrics for the current epoch's iteration.",
        "type": "comment"
    },
    "2166": {
        "file_id": 167,
        "content": "                    format(epoch, train_iter, str(e)))\n        if len(epoch_periods) < 1:\n            logger.info(\n                'No iteration was executed, please check the data reader')\n            sys.exit(1)\n        logger.info(\n            '[TRAIN] Epoch {} training finished, average time: {}'.format(\n                epoch, np.mean(epoch_periods)))\n        train_metrics.finalize_and_log_out( \\\n            info='[TRAIN] Finished ... Epoch {} all iters average: '.format(epoch))\n        # save models of min loss in best acc epochs\n        if test_exe and valid_interval > 0 and (epoch +\n                                                1) % valid_interval == 0:\n            # metrics_dict,loss = train_metrics.calculator.get_computed_metrics()\n            loss, metrics_dict_test = test_with_pyreader(\n                exe, test_exe, test_pyreader, test_fetch_list, test_metrics,\n                log_interval)\n            test_acc1 = metrics_dict_test['avg_acc1']\n            if test_acc1 > best_test_acc1:\n                best_test_acc1 = test_acc1",
        "type": "code",
        "location": "/applications/MultimodalVideoTag/scenario_lib/utils.py:120-141"
    },
    "2167": {
        "file_id": 167,
        "content": "The code finishes an epoch of training, logs the average time taken, and finalizes training metrics. If testing is enabled and a valid interval is set, it performs testing after each valid interval iteration and saves models with the best test accuracy.",
        "type": "comment"
    },
    "2168": {
        "file_id": 167,
        "content": "                save_model(exe, train_prog, save_dir, save_model_name,\n                           \"_epoch{}_acc{}\".format(epoch, best_test_acc1))\n                early_stop = EARLY_STOP_NUM\n            else:\n                early_stop -= 1\ndef save_model(exe, program, save_dir, model_name, postfix=None):\n    \"\"\"save_model\n    \"\"\"\n    model_path = os.path.join(save_dir, model_name + postfix)\n    if os.path.isdir(model_path):\n        shutil.rmtree(model_path)\n    # fluid.io.save_persistables(exe, model_path, main_program=program)\n    save_vars = [x for x in program.list_vars() \\\n                                 if isinstance(x, paddle.framework.Parameter)]\n    static.save_vars(exe,\n                       dirname=model_path,\n                       main_program=program,\n                       vars=save_vars,\n                       filename=\"param\")\ndef save_model_persist(exe, program, save_dir, model_name, postfix=None):\n    \"\"\"save_model\"\"\"\n    model_path = os.path.join(save_dir, model_name + postfix)\n    if os.path.isdir(model_path):",
        "type": "code",
        "location": "/applications/MultimodalVideoTag/scenario_lib/utils.py:142-169"
    },
    "2169": {
        "file_id": 167,
        "content": "This code snippet defines functions for saving the model at specific epochs and after training has stopped. It checks if a directory with the model name exists, deletes it if necessary, and then saves the model using either fluid.io or static methods. The save_model function takes in execution context, program, save directory, model name, and optional postfix for the file name. The save_model_persist function is similar but uses the save_model method to save the model. The code also includes a check to stop training if the early stopping condition is met.",
        "type": "comment"
    },
    "2170": {
        "file_id": 167,
        "content": "        shutil.rmtree(model_path)\n    paddle.fluid.io.save_persistables(exe,\n                               save_dir,\n                               main_program=program,\n                               filename=model_path)\ndef init_pretraining_params(exe,\n                            pretraining_params_path,\n                            main_program,\n                            use_fp16=False):\n    \"\"\"\n    init pretrain_params\n    \"\"\"\n    assert os.path.exists(pretraining_params_path\n                          ), \"[%s] cann't be found.\" % pretraining_params_path\n    def existed_params(var):\n        \"\"\"\n        Load existed params\n        \"\"\"\n        if not isinstance(var, paddle.framework.Parameter):\n            return False\n        flag = os.path.exists(os.path.join(pretraining_params_path, var.name))\n        return flag\n    static.load_vars(exe,\n                       pretraining_params_path,\n                       main_program=main_program,\n                       predicate=existed_params)\n    logger.info(\n        \"Load pretraining parameters from {}.\".format(pretraining_params_path))",
        "type": "code",
        "location": "/applications/MultimodalVideoTag/scenario_lib/utils.py:170-201"
    },
    "2171": {
        "file_id": 167,
        "content": "This function initializes the pre-trained parameters for a model. It first checks if the pretraining_params_path exists, and then loads any existing variables in the main program using static.load_vars(). If var is not a Parameter instance, it will return False. Finally, it logs that the pretraining parameters were loaded from the given path.",
        "type": "comment"
    },
    "2172": {
        "file_id": 167,
        "content": "class AttrDict(dict):\n    \"\"\"AttrDict\n    \"\"\"\n    def __getattr__(self, key):\n        \"\"\"getter\n        \"\"\"\n        return self[key]\n    def __setattr__(self, key, value):\n        \"\"\"setter\n        \"\"\"\n        if key in self.__dict__:\n            self.__dict__[key] = value\n        else:\n            self[key] = value",
        "type": "code",
        "location": "/applications/MultimodalVideoTag/scenario_lib/utils.py:204-218"
    },
    "2173": {
        "file_id": 167,
        "content": "The code defines a subclass of Python's dictionary, named AttrDict. It overrides the `__getattr__` and `__setattr__` methods to provide getter and setter functionality for dictionary keys as attributes, similar to regular class attributes.",
        "type": "comment"
    },
    "2174": {
        "file_id": 168,
        "content": "/applications/MultimodalVideoTag/train.sh",
        "type": "filepath"
    },
    "2175": {
        "file_id": 168,
        "content": "This script exports CUDA device settings and FLAGS for efficient GPU usage, then executes a Python file to train an Attention LSTM Ernie model using the specified configuration file. The logs are saved at specific intervals, with pre-trained checkpoints used as well.",
        "type": "summary"
    },
    "2176": {
        "file_id": 168,
        "content": "export CUDA_VISIBLE_DEVICES=0,1\nexport FLAGS_eager_delete_tensor_gb=0.0\nexport FLAGS_sync_nccl_allreduce=1\nexport FLAGS_fast_eager_deletion_mode=1\nexport FLAGS_fraction_of_gpu_memory_to_use=0.5\nexport FLAGS_reallocate_gpu_memory_in_mb=0\nexport FLAGS_memory_fraction_of_eager_deletion=1\npython scenario_lib/train.py --model_name=AttentionLstmErnie \\\n--config=./conf/conf.txt \\\n--log_interval=20 \\\n--valid_interval=1 \\\n--save_dir=checkpoints_save_new/ \\\n--pretrain=checkpoints_save/",
        "type": "code",
        "location": "/applications/MultimodalVideoTag/train.sh:1-13"
    },
    "2177": {
        "file_id": 168,
        "content": "This script exports CUDA device settings and FLAGS for efficient GPU usage, then executes a Python file to train an Attention LSTM Ernie model using the specified configuration file. The logs are saved at specific intervals, with pre-trained checkpoints used as well.",
        "type": "comment"
    },
    "2178": {
        "file_id": 169,
        "content": "/applications/PP-Care/Readme.md",
        "type": "filepath"
    },
    "2179": {
        "file_id": 169,
        "content": "The code initializes a pre-trained PaddleVideo model for PP-Care using TSM and ResNet50 weights, executes the application, and provides accuracy metrics while referencing relevant research papers on video understanding.",
        "type": "summary"
    },
    "2180": {
        "file_id": 169,
        "content": "# Video models for 3DMRI\n## \n- [](#)\n- [](#)\n- [](#)\n- [](#)\n- [](#)\n- [](#)\n- [](#)\n\n```bash\npython -m pip install SimpleITK\n```\n## \n3DMRI2D3D3D3DPaddleVideo3DMRIPP-TSNPP-TSMSlowfastTimesformer3DMRI\n## \n(PD)(Con)378case=30078*neurocon*, *taowu*, *PPMI**OASIS-1**name.nii**name.nii.gz*labeltxt[](https://pan.baidu.com/s/1eIsHHqnkKNG5x9CGjRONEA?pwd=avug)\n- label\n```\n{\n   \"0\": \"Con\",\n   \"1\": \"PD\"\n}\n```\n- \n```\n{\n   path1 label1\n   path2 label2\n   ...\n}\n```\n- \n```\n{\n   |--  datasets\n      |--  neurocon\n      |--  taowu\n      |--  PPMI\n      |--  OASIS-1\n}\n```\n## \n#### \n1. PP-TSNPP-TSMImageNet1000[PP-TSN](../../../docs/zh-CN/model_zoo/recognition/pp-tsn.md)[PP-",
        "type": "code",
        "location": "/applications/PP-Care/Readme.md:1-55"
    },
    "2181": {
        "file_id": 169,
        "content": "Introduction to video models for 3DMRI, data preparation, model training, testing, inference details, and references. Install SimpleITK dependency. Uses PaddleVideo models for 3DMRI classification. Dataset includes PD and Con cases; train/test split is 300:78. Format as *.nii or *.nii.gz. Downloaded from a Baidu link.",
        "type": "comment"
    },
    "2182": {
        "file_id": 169,
        "content": "TSM](../../../docs/zh-CN/model_zoo/recognition/pp-tsm.md))MRIResNet50Backbone: [](https://pan.baidu.com/s/1eIsHHqnkKNG5x9CGjRONEA?pwd=avug)SlowfastTimeSformer[Slowfast](../../../docs/zh-CN/model_zoo/recognition/slowfast.md)[Timesformer](../../../docs/zh-CN/model_zoo/recognition/timesformer.md)\n2. `PaddleVideo/applications/PP-Care/configs/XXX.yaml``pretrained:`pptsn_MRI\n   ```yaml\n   MODEL:\n       framework: \"RecognizerMRI\"\n       backbone:\n           name: \"ResNetTSN_MRI\"\n           pretrained: \n   ```\n#### \n- PP-TSN_MRI4\n  ```bash\n  python3.7 -B -m paddle.distributed.launch --gpus=\"0,1,2,3\" --log_dir=log_pptsn_MRI main.py  --validate -c applications/PP-Care/configs/pptsn_MRI.yaml\n  ```\n## \n`topk Acc`PP-TSN_MRI\n```bash\npython3.7 -B -m paddle.distributed.laun",
        "type": "code",
        "location": "/applications/PP-Care/Readme.md:55-81"
    },
    "2183": {
        "file_id": 169,
        "content": "This code provides instructions on how to initialize a pre-trained model for PaddleVideo's PP-Care application. It mentions downloading a pre-trained TSM model, initializing the backbone with ResNet50 weights trained on MRI data, and filling in the weight path in the YAML configuration file. The code also explains how to train and test the model using specific commands for PP-TSN_MRI as an example.",
        "type": "comment"
    },
    "2184": {
        "file_id": 169,
        "content": "ch --gpus=\"0,1,2,3\" --log_dir=log_pptsn_MRI main.py  --test -c applications/PP-Care/configs/pptsn_MRI.yaml -w \"output/ppTSN_MRI/ppTSN_MRI_best.pdparams\"\n```\n.yaml3DMRIvalidation\n|      backbone      |     head     |  Acc  |\n| :----------------: | :----------: | :---: |\n|      ResNet50      |    PP-TSN    | 91.07 |\n|      ResNet50      |    PP-TSM    | 90.83 |\n|     3DResNet50     |   Slowfast   | 91.07 |\n| Vision Transformer |  Timesformer | 88.33 |\n[](https://pan.baidu.com/s/1eIsHHqnkKNG5x9CGjRONEA?pwd=avug)\n## \n\n- MRI\n- \n- 3DResNet3D\n## \n- [Temporal Segment Networks: Towards Good Practices for Deep Action Recognition](https://arxiv.org/pdf/1608.00859.pdf), Limin Wang, Yuanjun Xiong, Zhe Wang\n- [TSM: Temporal Shift Module for Efficient Video Understanding](https://arxiv.org/pdf/1811.08383.pdf), Ji Lin, Chuang Gan, Song Han",
        "type": "code",
        "location": "/applications/PP-Care/Readme.md:81-106"
    },
    "2185": {
        "file_id": 169,
        "content": "The given code executes a PaddleVideo application, PP-Care, using specific configurations and trained model weights. It tests the ResNet50 backbone with PP-TSN and PP-TSM heads on 3DMRI validation data and reports accuracy metrics. The optimized models can be downloaded from a Baidu disk link provided.",
        "type": "comment"
    },
    "2186": {
        "file_id": 169,
        "content": "- [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531), Geoffrey Hinton, Oriol Vinyals, Jeff Dean\n- [SlowFast Networks for Video Recognition](https://arxiv.org/abs/1812.03982), Feichtenhofer C, Fan H, Malik J, et al.\n- [A Multigrid Method for Efficiently Training Video Models](https://arxiv.org/abs/1912.00998), Chao-Yuan Wu, Ross Girshick, et al.\n- [Is Space-Time Attention All You Need for Video Understanding?](https://arxiv.org/pdf/2102.05095.pdf), Gedas Bertasius, Heng Wang, Lorenzo Torresani",
        "type": "code",
        "location": "/applications/PP-Care/Readme.md:107-110"
    },
    "2187": {
        "file_id": 169,
        "content": "This code snippet contains the references for various important research papers related to video understanding using neural networks. These papers cover topics like knowledge distillation, slow-fast networks, and efficient training methods for video models.",
        "type": "comment"
    },
    "2188": {
        "file_id": 170,
        "content": "/applications/PPHuman/README.md",
        "type": "filepath"
    },
    "2189": {
        "file_id": 170,
        "content": "This code converts PaddleVideo's JSON files to training data, exports a model for PP-Human, and organizes it in directories suitable for behavior recognition inference.",
        "type": "summary"
    },
    "2190": {
        "file_id": 170,
        "content": "# PP-Human \n[PP-Human](https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.4/deploy/pphuman)[PaddleVideo](https://github.com/PaddlePaddle/PaddleVideo/)\n## \n[ST-GCN](https://arxiv.org/abs/1801.07455)[PaddleVideo](https://github.com/PaddlePaddle/PaddleVideo/blob/develop/docs/zh-CN/model_zoo/recognition/stgcn.md)\n### \nSTGCNPaddleVideo`.npy``Numpy``.npy``.pkl``(N,C,T,V,M)`\nPPhuman\n|  |  |  |\n| ---- | ---- | ---------- |\n| N |  |  |\n| C | 2 | (x, y) |\n| T | 50 | |\n| V | 17 | `COCO`[](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.4/docs/tutorials/PrepareKeypointDataSet_cn.md#COCO%E6%95%B0%E6%8D%AE%E9%9B%86) |\n| M | 1 |  |\n#### 1. \n",
        "type": "code",
        "location": "/applications/PPHuman/README.md:1-21"
    },
    "2191": {
        "file_id": 170,
        "content": "Training behavior recognition model using ST-GCN on PaddleVideo.\nPrepare training data in Numpy format with dimensions (N,C,T,V,M).",
        "type": "comment"
    },
    "2192": {
        "file_id": 170,
        "content": "- [PaddleDetection KeyPoint](https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.4/configs/keypoint) `3 -  - +keypoint top-down`17\n- \n[](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.4/deploy/pphuman/pipe_utils.py#L352-L363)\n#### 2. \nPP-Human50\n- 50\n- 050\n-  \n\n#### 3. PaddleVideo\n`all_kpts`(T, V, C) (50, 17, 2)), PaddleVideo\n-  `np.transpose``np.expand_dims`(C, T, V, M)\n- \n`class_id``int``0 1`\n`.npy``.pkl`",
        "type": "code",
        "location": "/applications/PPHuman/README.md:22-42"
    },
    "2193": {
        "file_id": 170,
        "content": "The code describes the process of preparing data for PP-Human, a human action detection model. It involves obtaining key points from pre-trained models or manual annotations, normalizing the coordinates, setting a uniform sequence length, and saving the data in PaddleVideo compatible format.",
        "type": "comment"
    },
    "2194": {
        "file_id": 170,
        "content": "#### UR Fall Detection Dataset\n[UR Fall Detection Dataset](http://fenix.univ.rzeszow.pl/~mkepski/ds/uf.html)camera 0RGB\n1[PaddleDetection](https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.4/configs/keypoint)\n```bash\n# current path is under root of PaddleDetection\n# Step 1: download pretrained inference models.\nwget https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_l_36e_pipeline.zip\nwget https://bj.bcebos.com/v1/paddledet/models/pipeline/dark_hrnet_w32_256x192.zip\nunzip -d output_inference/ mot_ppyoloe_l_36e_pipeline.zip\nunzip -d output_inference/ dark_hrnet_w32_256x192.zip\n# Step 2: Get the keypoint coordinarys\n# if your data is image sequence\npython deploy/python/det_keypoint_unite_infer.py --det_model_dir=output_inference/mot_ppyoloe_l_36e_pipeline/ --keypoint_model_dir=output_inference/dark_hrnet_w32_256x192 --image_dir={your image directory path} --device=GPU --save_res=True",
        "type": "code",
        "location": "/applications/PPHuman/README.md:44-60"
    },
    "2195": {
        "file_id": 170,
        "content": "This code is downloading pretrained models for keypoint detection using PaddleDetection and then using them to get the keypoint coordinates for an image sequence.",
        "type": "comment"
    },
    "2196": {
        "file_id": 170,
        "content": "# if your data is video\npython deploy/python/det_keypoint_unite_infer.py --det_model_dir=output_inference/mot_ppyoloe_l_36e_pipeline/ --keypoint_model_dir=output_inference/dark_hrnet_w32_256x192 --video_file={your video file path} --device=GPU --save_res=True\n```\n`det_keypoint_unite_image_results.json`[](https://github.com/PaddlePaddle/PaddleDetection/blob/release/2.4/deploy/python/det_keypoint_unite_infer.py#L108)\nUR Fall\n```bash\nmkdir {root of PaddleVideo}/applications/PPHuman/datasets/annotations\nmv det_keypoint_unite_image_results.json {root of PaddleVideo}/applications/PPHuman/datasets/annotations/det_keypoint_unite_image_results_{video_id}_{camera_id}.json\n```\n2\n\n```\nannotations/\n det_keypoint_unite_image_results_fall-01-cam0-rgb.json\n det_keypoint_unite_image_results_fall-02-cam0-rgb.json\n det_keypoint_unite_image_results_fall-03-cam0-rgb.json\n det_keypoint_unite_image_results_fall-04-cam0-rgb.json",
        "type": "code",
        "location": "/applications/PPHuman/README.md:62-83"
    },
    "2197": {
        "file_id": 170,
        "content": "The provided code is a command line instruction for running the PaddleVideo's PPHuman application on video data. It uses pre-trained models to detect human keypoints in the video, resulting in a `det_keypoint_unite_image_results.json` file containing the detection results. These steps are repeated for each segment of UR Fall data. The JSON files are then saved into a specific directory structure with a naming convention based on video and camera IDs.",
        "type": "comment"
    },
    "2198": {
        "file_id": 170,
        "content": "    ...\n det_keypoint_unite_image_results_fall-28-cam0-rgb.json\n det_keypoint_unite_image_results_fall-29-cam0-rgb.json\n det_keypoint_unite_image_results_fall-30-cam0-rgb.json\n```\n, `train_data.npy`, `train_label.pkl`json\n```bash\n# current path is {root of PaddleVideo}/applications/PPHuman/datasets/\npython prepare_dataset.py\n```\n\n- UR Fall100\n- 10050\n- label1\n[](https://bj.bcebos.com/v1/paddledet/data/PPhuman/fall_data.zip)\n### \nPaddleVideo\n```bash\n# current path is under root of PaddleVideo\npython main.py -c applications/PPHuman/configs/stgcn_pphuman.yaml\n# ,\npython main.py --validate -c applications/PPHuman/configs/stgcn_pphuman.yaml\n```\n\n```bash\npython main.py --test -c applications/PPHuman/configs/stgcn_pphuman.yaml  -w output/STGCN/STGCN_best.pdparams",
        "type": "code",
        "location": "/applications/PPHuman/README.md:84-114"
    },
    "2199": {
        "file_id": 170,
        "content": "Code snippet represents a list of json files in a PaddleVideo application called \"PPHuman\". These JSON files contain image results for different actions. The code suggests using a provided script to convert these data into training data, resulting in two new files: \"train_data.npy\" and \"train_label.pkl\". It mentions that some data preparation steps include parsing the JSON content and organizing the training data. There is a link for more comprehensive data available for download. The code also provides instructions on how to train and test the model using PaddleVideo's main script with specific configurations.",
        "type": "comment"
    }
}