{
    "6100": {
        "file_id": 487,
        "content": "        return x.transpose((0, 2, 1, 3))\n    def forward(self, hidden_states, attention_mask):\n        mixed_query_layer = self.query(hidden_states)\n        mixed_key_layer = self.key(hidden_states)\n        mixed_value_layer = self.value(hidden_states)\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n        key_layer = self.transpose_for_scores(mixed_key_layer)\n        value_layer = self.transpose_for_scores(mixed_value_layer)\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        attention_scores = paddle.matmul(query_layer,\n                                         key_layer.transpose((0, 1, 3, 2)))\n        attention_scores = attention_scores / math.sqrt(\n            self.attention_head_size)\n        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n        attention_scores = attention_scores + attention_mask\n        # Normalize the attention scores to probabilities.\n        attention_probs = nn.Softmax(axis=-1)(attention_scores)",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/actbert.py:124-144"
    },
    "6101": {
        "file_id": 487,
        "content": "This code performs multi-head attention in an attention mechanism. It transposes the query, key, and value layers before calculating raw attention scores via dot product. The results are then normalized into probabilities using softmax. The attention mask is applied to the attention scores for masked self-attention.",
        "type": "comment"
    },
    "6102": {
        "file_id": 487,
        "content": "        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs = self.dropout(attention_probs)\n        context_layer = paddle.matmul(attention_probs, value_layer)\n        context_layer = context_layer.transpose((0, 2, 1, 3))\n        new_context_layer_shape = context_layer.shape[:-2] + [\n            self.all_head_size\n        ]\n        context_layer = context_layer.reshape(new_context_layer_shape)\n        return context_layer, attention_probs\nclass BertSelfOutput(nn.Layer):\n    def __init__(self, hidden_size, hidden_dropout_prob):\n        super(BertSelfOutput, self).__init__()\n        self.dense = nn.Linear(hidden_size, hidden_size)\n        self.LayerNorm = nn.LayerNorm(hidden_size, epsilon=1e-12)\n        self.dropout = nn.Dropout(hidden_dropout_prob)\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/actbert.py:146-169"
    },
    "6103": {
        "file_id": 487,
        "content": "This code defines a BertSelfOutput layer that takes input hidden states, applies linear transformation and dropout for regularization, then passes the output through layer normalization.",
        "type": "comment"
    },
    "6104": {
        "file_id": 487,
        "content": "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\nclass BertAttention(nn.Layer):\n    def __init__(self, hidden_size, hidden_dropout_prob, num_attention_heads,\n                 attention_probs_dropout_prob):\n        super(BertAttention, self).__init__()\n        self.self = BertSelfAttention(hidden_size, num_attention_heads,\n                                      attention_probs_dropout_prob)\n        self.output = BertSelfOutput(hidden_size, hidden_dropout_prob)\n    def forward(self, input_tensor, attention_mask):\n        self_output, attention_probs = self.self(input_tensor, attention_mask)\n        attention_output = self.output(self_output, input_tensor)\n        return attention_output, attention_probs\nclass BertIntermediate(nn.Layer):\n    def __init__(self, hidden_size, intermediate_size, hidden_act):\n        super(BertIntermediate, self).__init__()\n        self.dense = nn.Linear(hidden_size, intermediate_size)\n        if isinstance(hidden_act, str) or (sys.version_info[0] == 2",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/actbert.py:170-192"
    },
    "6105": {
        "file_id": 487,
        "content": "This code defines three classes: `ActBert`, `BertAttention`, and `BertIntermediate`. \n\n`ActBert` appears to be a model that includes `BertAttention` and `BertIntermediate` as its layers. The `BertAttention` class defines forward function for attention mechanism, which takes in an input tensor and attention mask, and returns output and attention probabilities. The `BertIntermediate` class appears to be a dense layer with linear activation function.",
        "type": "comment"
    },
    "6106": {
        "file_id": 487,
        "content": "                                           and isinstance(hidden_act, str)):\n            self.intermediate_act_fn = ACT2FN[hidden_act]\n        else:\n            self.intermediate_act_fn = hidden_act\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.intermediate_act_fn(hidden_states)\n        return hidden_states\nclass BertOutput(nn.Layer):\n    def __init__(self, intermediate_size, hidden_size, hidden_dropout_prob):\n        super(BertOutput, self).__init__()\n        self.dense = nn.Linear(intermediate_size, hidden_size)\n        self.LayerNorm = nn.LayerNorm(hidden_size, epsilon=1e-12)\n        self.dropout = nn.Dropout(hidden_dropout_prob)\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\nclass BertEntAttention(nn.Layer):\n    \"\"\"Core mudule of tangled transformer.",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/actbert.py:193-219"
    },
    "6107": {
        "file_id": 487,
        "content": "Code defines a class for an attention-based transformer model. It includes an intermediate activation function and forward pass layers for processing input, applying dropout regularization, and normalizing outputs. The BertEntAttention layer is the core module for the transformer.",
        "type": "comment"
    },
    "6108": {
        "file_id": 487,
        "content": "    \"\"\"\n    def __init__(\n        self,\n        hidden_size,\n        v_hidden_size,\n        a_hidden_size,\n        bi_hidden_size,\n        attention_probs_dropout_prob,\n        v_attention_probs_dropout_prob,\n        a_attention_probs_dropout_prob,\n        av_attention_probs_dropout_prob,\n        at_attention_probs_dropout_prob,\n        bi_num_attention_heads,\n    ):\n        super(BertEntAttention, self).__init__()\n        if bi_hidden_size % bi_num_attention_heads != 0:\n            raise ValueError(\n                \"The hidden size (%d) is not a multiple of the number of attention \"\n                \"heads (%d)\" % (bi_hidden_size, bi_num_attention_heads))\n        self.num_attention_heads = bi_num_attention_heads\n        self.attention_head_size = int(bi_hidden_size / bi_num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n        # self attention layers for vision input\n        self.query1 = nn.Linear(v_hidden_size, self.all_head_size)\n        self.key1 = nn.Linear(v_hidden_size, self.all_head_size)",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/actbert.py:220-246"
    },
    "6109": {
        "file_id": 487,
        "content": "This code defines a BertEntAttention class with parameters for hidden size, vision input hidden size, attention probabilities dropout probabilities, and bi-directional hidden size. It also checks if the hidden size is a multiple of the number of attention heads. The class initializes attributes such as the number of attention heads, attention head size, all head size, and linear layers for self-attention in vision input.",
        "type": "comment"
    },
    "6110": {
        "file_id": 487,
        "content": "        self.value1 = nn.Linear(v_hidden_size, self.all_head_size)\n        self.dropout1 = nn.Dropout(v_attention_probs_dropout_prob)\n        # self attention layers for text input\n        self.query2 = nn.Linear(hidden_size, self.all_head_size)\n        self.key2 = nn.Linear(hidden_size, self.all_head_size)\n        self.value2 = nn.Linear(hidden_size, self.all_head_size)\n        self.dropout2 = nn.Dropout(attention_probs_dropout_prob)\n        # self attention layers for action input\n        self.query3 = nn.Linear(a_hidden_size, self.all_head_size)\n        self.key3 = nn.Linear(a_hidden_size, self.all_head_size)\n        self.value3 = nn.Linear(a_hidden_size, self.all_head_size)\n        self.dropout3 = nn.Dropout(a_attention_probs_dropout_prob)\n        # self attention layers for action_text\n        self.key_at = nn.Linear(bi_hidden_size, self.all_head_size)\n        self.value_at = nn.Linear(bi_hidden_size, self.all_head_size)\n        self.dropout_at = nn.Dropout(av_attention_probs_dropout_prob)\n        # self attention layers for action_vision",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/actbert.py:247-267"
    },
    "6111": {
        "file_id": 487,
        "content": "This code defines layers for self-attention in the ACTBERT model, including linear and dropout layers for text, action, action_text, and action_vision inputs.",
        "type": "comment"
    },
    "6112": {
        "file_id": 487,
        "content": "        self.key_av = nn.Linear(bi_hidden_size, self.all_head_size)\n        self.value_av = nn.Linear(bi_hidden_size, self.all_head_size)\n        self.dropout_av = nn.Dropout(at_attention_probs_dropout_prob)\n    def transpose_for_scores(self, x):\n        new_x_shape = x.shape[:-1] + [\n            self.num_attention_heads,\n            self.attention_head_size,\n        ]\n        x = x.reshape(new_x_shape)\n        return x.transpose((0, 2, 1, 3))\n    def forward(\n        self,\n        input_tensor1,\n        attention_mask1,\n        input_tensor2,\n        attention_mask2,\n        input_tensor3,\n        attention_mask3,\n    ):\n        # for vision input.\n        mixed_query_layer1 = self.query1(input_tensor1)\n        mixed_key_layer1 = self.key1(input_tensor1)\n        mixed_value_layer1 = self.value1(input_tensor1)\n        query_layer1 = self.transpose_for_scores(mixed_query_layer1)\n        key_layer1 = self.transpose_for_scores(mixed_key_layer1)\n        value_layer1 = self.transpose_for_scores(mixed_value_layer1)\n        # for text input:",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/actbert.py:268-299"
    },
    "6113": {
        "file_id": 487,
        "content": "This code defines a model for attention mechanism in a transformer architecture, used for both vision and text inputs. The key steps involve creating linear layers for keys and values, applying dropout, transposing the input tensors for scoring, and forwarding the input through these operations for both vision and text inputs separately.",
        "type": "comment"
    },
    "6114": {
        "file_id": 487,
        "content": "        mixed_query_layer2 = self.query2(input_tensor2)\n        mixed_key_layer2 = self.key2(input_tensor2)\n        mixed_value_layer2 = self.value2(input_tensor2)\n        query_layer2 = self.transpose_for_scores(mixed_query_layer2)\n        key_layer2 = self.transpose_for_scores(mixed_key_layer2)\n        value_layer2 = self.transpose_for_scores(mixed_value_layer2)\n        # for action input:\n        mixed_query_layer3 = self.query3(input_tensor3)\n        mixed_key_layer3 = self.key3(input_tensor3)\n        mixed_value_layer3 = self.value3(input_tensor3)\n        query_layer3 = self.transpose_for_scores(mixed_query_layer3)\n        key_layer3 = self.transpose_for_scores(mixed_key_layer3)\n        value_layer3 = self.transpose_for_scores(mixed_value_layer3)\n        def do_attention(query_layer, key_layer, value_layer, attention_mask,\n                         dropout):\n            \"\"\" compute attention \"\"\"\n            attention_scores = paddle.matmul(query_layer,\n                                             key_layer.transpose((0, 1, 3, 2)))",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/actbert.py:300-321"
    },
    "6115": {
        "file_id": 487,
        "content": "This code is performing multi-head attention operation. It first separates the input tensor into two parts, with each part going through its own set of linear layers to create query, key, and value tensors. Then it transposes the query and key tensors before computing attention scores by taking the dot product of the transposed query and key tensors.",
        "type": "comment"
    },
    "6116": {
        "file_id": 487,
        "content": "            attention_scores = attention_scores / math.sqrt(\n                self.attention_head_size)\n            attention_scores = attention_scores + attention_mask\n            # Normalize the attention scores to probabilities.\n            attention_probs = nn.Softmax(axis=-1)(attention_scores)\n            # This is actually dropping out entire tokens to attend to, which might\n            # seem a bit unusual, but is taken from the original Transformer paper.\n            attention_probs = dropout(attention_probs)\n            context_layer = paddle.matmul(attention_probs, value_layer)\n            context_layer = context_layer.transpose((0, 2, 1, 3))\n            new_context_layer_shape = context_layer.shape[:-2] + [\n                self.all_head_size\n            ]\n            context_layer = context_layer.reshape(new_context_layer_shape)\n            return context_layer\n        context_av = do_attention(query_layer3, key_layer1, value_layer1,\n                                  attention_mask1, self.dropout_av)",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/actbert.py:322-342"
    },
    "6117": {
        "file_id": 487,
        "content": "This code calculates attention scores between queries, keys, and values, normalizes them to probabilities using softmax, applies dropout, performs matrix multiplication with the values, transposes the result, reshapes it, and returns the context layer. It follows the Transformer paper's approach of dropping out entire tokens to attend to.",
        "type": "comment"
    },
    "6118": {
        "file_id": 487,
        "content": "        context_at = do_attention(query_layer3, key_layer2, value_layer2,\n                                  attention_mask2, self.dropout_at)\n        context_key_av = self.key_av(context_av).transpose((0, 2, 1))\n        # interpolate only support 4-D tensor now.\n        context_key_av = F.interpolate(context_key_av.unsqueeze(-1),\n                                       size=(key_layer2.shape[2],\n                                             1)).squeeze(-1)\n        context_key_av = self.transpose_for_scores(\n            context_key_av.transpose((0, 2, 1)))\n        key_layer2 = key_layer2 + context_key_av\n        context_key_at = self.key_at(context_at).transpose((0, 2, 1))\n        context_key_at = F.interpolate(context_key_at.unsqueeze(-1),\n                                       size=(key_layer1.shape[2],\n                                             1)).squeeze(-1)\n        context_key_at = self.transpose_for_scores(\n            context_key_at.transpose((0, 2, 1)))\n        key_layer1 = key_layer1 + context_key_at",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/actbert.py:343-361"
    },
    "6119": {
        "file_id": 487,
        "content": "This code is performing attention mechanism for multi-scale context fusion. It uses interpolation to resize the context features, then adds them to the original key layers (key_layer2 and key_layer1). The purpose of this is to incorporate contextual information from different scales into the model's understanding.",
        "type": "comment"
    },
    "6120": {
        "file_id": 487,
        "content": "        context_val_av = self.value_at(context_av).transpose((0, 2, 1))\n        context_val_av = F.interpolate(context_val_av.unsqueeze(-1),\n                                       size=(value_layer2.shape[2],\n                                             1)).squeeze(-1)\n        context_val_av = self.transpose_for_scores(\n            context_val_av.transpose((0, 2, 1)))\n        value_layer2 = value_layer2 + context_val_av\n        context_val_at = self.value_at(context_at).transpose((0, 2, 1))\n        context_val_at = F.interpolate(context_val_at.unsqueeze(-1),\n                                       size=(value_layer1.shape[2],\n                                             1)).squeeze(-1)\n        context_val_at = self.transpose_for_scores(\n            context_val_at.transpose((0, 2, 1)))\n        value_layer1 = value_layer1 + context_val_at\n        context_layer1 = do_attention(query_layer1, key_layer1, value_layer1,\n                                      attention_mask1, self.dropout1)\n        context_layer2 = do_attention(query_layer2, key_layer2, value_layer2,",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/actbert.py:363-381"
    },
    "6121": {
        "file_id": 487,
        "content": "This code snippet is performing cross-attention in a transformer model. It first interpolates and adds context vectors to value layers, then applies attention mechanisms to compute context layers. This process helps to capture dependencies between different parts of the input data effectively.",
        "type": "comment"
    },
    "6122": {
        "file_id": 487,
        "content": "                                      attention_mask2, self.dropout2)\n        context_layer3 = do_attention(query_layer3, key_layer3, value_layer3,\n                                      attention_mask3, self.dropout3)\n        return context_layer1, context_layer2, context_layer3  # vision, text, action\nclass BertEntOutput(nn.Layer):\n    def __init__(\n        self,\n        bi_hidden_size,\n        hidden_size,\n        v_hidden_size,\n        v_hidden_dropout_prob,\n        hidden_dropout_prob,\n    ):\n        super(BertEntOutput, self).__init__()\n        self.dense1 = nn.Linear(bi_hidden_size, v_hidden_size)\n        self.LayerNorm1 = nn.LayerNorm(v_hidden_size, epsilon=1e-12)\n        self.dropout1 = nn.Dropout(v_hidden_dropout_prob)\n        self.dense2 = nn.Linear(bi_hidden_size, hidden_size)\n        self.LayerNorm2 = nn.LayerNorm(hidden_size, epsilon=1e-12)\n        self.dropout2 = nn.Dropout(hidden_dropout_prob)\n        self.dense3 = nn.Linear(bi_hidden_size, hidden_size)\n        self.LayerNorm3 = nn.LayerNorm(hidden_size, epsilon=1e-12)",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/actbert.py:382-409"
    },
    "6123": {
        "file_id": 487,
        "content": "The code defines a class \"BertEntOutput\" with several layers including dense and dropout. It uses bi-hidden size, hidden size, v_hidden_size, and corresponding dropout probabilities for initializing the layers. This class seems to be part of a model architecture where it performs layer normalization, applies dropout regularization, and linear transformations to process input data.",
        "type": "comment"
    },
    "6124": {
        "file_id": 487,
        "content": "        self.dropout3 = nn.Dropout(hidden_dropout_prob)\n    def forward(\n        self,\n        hidden_states1,\n        input_tensor1,\n        hidden_states2,\n        input_tensor2,\n        hidden_states3,\n        input_tensor3,\n    ):\n        context_state1 = self.dense1(hidden_states1)\n        context_state1 = self.dropout1(context_state1)\n        context_state2 = self.dense2(hidden_states2)\n        context_state2 = self.dropout2(context_state2)\n        context_state3 = self.dense3(hidden_states3)\n        context_state3 = self.dropout3(context_state3)\n        hidden_states1 = self.LayerNorm1(context_state1 + input_tensor1)\n        hidden_states2 = self.LayerNorm2(context_state2 + input_tensor2)\n        hidden_states3 = self.LayerNorm3(context_state3 + input_tensor3)\n        return hidden_states1, hidden_states2, hidden_states3\nclass BertLayer(nn.Layer):\n    def __init__(self, hidden_size, intermediate_size, hidden_act,\n                 hidden_dropout_prob, num_attention_heads,\n                 attention_probs_dropout_prob):",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/actbert.py:410-440"
    },
    "6125": {
        "file_id": 487,
        "content": "This code defines a BertLayer class with Dropout layers and dense layers, performing attention mechanism. The forward method applies these layers to hidden_states1,2,3, and returns the updated hidden states after adding them with their corresponding input tensors.",
        "type": "comment"
    },
    "6126": {
        "file_id": 487,
        "content": "        super(BertLayer, self).__init__()\n        self.attention = BertAttention(hidden_size, hidden_dropout_prob,\n                                       num_attention_heads,\n                                       attention_probs_dropout_prob)\n        self.intermediate = BertIntermediate(hidden_size, intermediate_size,\n                                             hidden_act)\n        self.output = BertOutput(intermediate_size, hidden_size,\n                                 hidden_dropout_prob)\n    def forward(self, hidden_states, attention_mask):\n        attention_output, attention_probs = self.attention(\n            hidden_states, attention_mask)\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output)\n        return layer_output, attention_probs\nclass BertConnectionLayer(nn.Layer):\n    def __init__(self, hidden_size, v_hidden_size, a_hidden_size,\n                 bi_hidden_size, bi_num_attention_heads,\n                 attention_probs_dropout_prob, v_attention_probs_dropout_prob,",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/actbert.py:441-461"
    },
    "6127": {
        "file_id": 487,
        "content": "This code defines a BertLayer class and a BertConnectionLayer class. The BertLayer class includes an attention layer, an intermediate layer, and an output layer. It has a forward function that performs the calculations for these layers. The BertConnectionLayer class is a subclass of nn.Layer with various parameters for hidden sizes and attention dropout probabilities.",
        "type": "comment"
    },
    "6128": {
        "file_id": 487,
        "content": "                 a_attention_probs_dropout_prob,\n                 av_attention_probs_dropout_prob,\n                 at_attention_probs_dropout_prob, intermediate_size,\n                 v_intermediate_size, a_intermediate_size, hidden_act,\n                 v_hidden_act, a_hidden_act, hidden_dropout_prob,\n                 v_hidden_dropout_prob, a_hidden_dropout_prob):\n        super(BertConnectionLayer, self).__init__()\n        self.ent_attention = BertEntAttention(\n            hidden_size,\n            v_hidden_size,\n            a_hidden_size,\n            bi_hidden_size,\n            attention_probs_dropout_prob,\n            v_attention_probs_dropout_prob,\n            a_attention_probs_dropout_prob,\n            av_attention_probs_dropout_prob,\n            at_attention_probs_dropout_prob,\n            bi_num_attention_heads,\n        )\n        self.ent_output = BertEntOutput(\n            bi_hidden_size,\n            hidden_size,\n            v_hidden_size,\n            v_hidden_dropout_prob,\n            hidden_dropout_prob,",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/actbert.py:462-487"
    },
    "6129": {
        "file_id": 487,
        "content": "This code initializes a BertConnectionLayer object with various parameters including hidden size, attention probabilities dropout probability, and intermediate size. It also initializes two other objects: BertEntAttention and BertEntOutput. The BertEntAttention object is responsible for performing entity-based attention while the BertEntOutput object is responsible for producing the output of the connection layer.",
        "type": "comment"
    },
    "6130": {
        "file_id": 487,
        "content": "        )\n        self.v_intermediate = BertIntermediate(v_hidden_size,\n                                               v_intermediate_size,\n                                               v_hidden_act)\n        self.v_output = BertOutput(v_intermediate_size, v_hidden_size,\n                                   v_hidden_dropout_prob)\n        self.t_intermediate = BertIntermediate(hidden_size, intermediate_size,\n                                               hidden_act)\n        self.t_output = BertOutput(intermediate_size, hidden_size,\n                                   hidden_dropout_prob)\n        self.a_intermediate = BertIntermediate(a_hidden_size,\n                                               a_intermediate_size,\n                                               a_hidden_act)\n        self.a_output = BertOutput(a_intermediate_size, a_hidden_size,\n                                   a_hidden_dropout_prob)\n    def forward(\n        self,\n        input_tensor1,\n        attention_mask1,\n        input_tensor2,\n        attention_mask2,",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/actbert.py:488-512"
    },
    "6131": {
        "file_id": 487,
        "content": "This code defines a model with three input streams (v, t, and a) and initializes intermediate layers and output layers for each stream. The forward function takes in two pairs of input tensors and attention masks for each stream.",
        "type": "comment"
    },
    "6132": {
        "file_id": 487,
        "content": "        input_tensor3,\n        attention_mask3,\n    ):\n        ent_output1, ent_output2, ent_output3 = self.ent_attention(\n            input_tensor1, attention_mask1, input_tensor2, attention_mask2,\n            input_tensor3, attention_mask3)\n        attention_output1, attention_output2, attention_output3 = self.ent_output(\n            ent_output1, input_tensor1, ent_output2, input_tensor2, ent_output3,\n            input_tensor3)\n        intermediate_output1 = self.v_intermediate(attention_output1)\n        layer_output1 = self.v_output(intermediate_output1, attention_output1)\n        intermediate_output2 = self.t_intermediate(attention_output2)\n        layer_output2 = self.t_output(intermediate_output2, attention_output2)\n        intermediate_output3 = self.a_intermediate(attention_output3)\n        layer_output3 = self.a_output(intermediate_output3, attention_output3)\n        return layer_output1, layer_output2, layer_output3\nclass BertEncoder(nn.Layer):\n    \"\"\"\n    ActBert Encoder, consists 3 pathway of multi-BertLayers and BertConnectionLayer.",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/actbert.py:513-539"
    },
    "6133": {
        "file_id": 487,
        "content": "This function computes the layer outputs for three different pathways using the ActBert Encoder. It utilizes attention masks and input tensors for each pathway, passing them through various intermediate layers before returning the final layer outputs. The BertEncoder class represents a combination of three multi-BertLayers and BertConnectionLayer.",
        "type": "comment"
    },
    "6134": {
        "file_id": 487,
        "content": "    \"\"\"\n    def __init__(\n        self,\n        v_ent_attention_id,\n        t_ent_attention_id,\n        a_ent_attention_id,\n        fixed_t_layer,\n        fixed_v_layer,\n        hidden_size,\n        v_hidden_size,\n        a_hidden_size,\n        bi_hidden_size,\n        intermediate_size,\n        v_intermediate_size,\n        a_intermediate_size,\n        hidden_act,\n        v_hidden_act,\n        a_hidden_act,\n        hidden_dropout_prob,\n        v_hidden_dropout_prob,\n        a_hidden_dropout_prob,\n        attention_probs_dropout_prob,\n        v_attention_probs_dropout_prob,\n        a_attention_probs_dropout_prob,\n        av_attention_probs_dropout_prob,\n        at_attention_probs_dropout_prob,\n        num_attention_heads,\n        v_num_attention_heads,\n        a_num_attention_heads,\n        bi_num_attention_heads,\n        num_hidden_layers,\n        v_num_hidden_layers,\n        a_num_hidden_layers,\n    ):\n        super(BertEncoder, self).__init__()\n        self.v_ent_attention_id = v_ent_attention_id\n        self.t_ent_attention_id = t_ent_attention_id",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/actbert.py:540-576"
    },
    "6135": {
        "file_id": 487,
        "content": "This code defines the `BertEncoder` class, which initializes various parameters for the BERT model's encoder. These parameters include attention IDs, fixed layer positions, hidden sizes, and dropout probabilities for different components of the model. The class extends `super(BertEncoder, self).__init__()`, indicating it inherits from another class.",
        "type": "comment"
    },
    "6136": {
        "file_id": 487,
        "content": "        self.a_ent_attention_id = a_ent_attention_id\n        self.fixed_t_layer = fixed_t_layer\n        self.fixed_v_layer = fixed_v_layer\n        layer = BertLayer(hidden_size, intermediate_size, hidden_act,\n                          hidden_dropout_prob, num_attention_heads,\n                          attention_probs_dropout_prob)\n        v_layer = BertLayer(v_hidden_size, v_intermediate_size, v_hidden_act,\n                            v_hidden_dropout_prob, v_num_attention_heads,\n                            v_attention_probs_dropout_prob)\n        a_layer = BertLayer(a_hidden_size, a_intermediate_size, a_hidden_act,\n                            a_hidden_dropout_prob, a_num_attention_heads,\n                            a_attention_probs_dropout_prob)\n        connect_layer = BertConnectionLayer(\n            hidden_size, v_hidden_size, a_hidden_size, bi_hidden_size,\n            bi_num_attention_heads, attention_probs_dropout_prob,\n            v_attention_probs_dropout_prob, a_attention_probs_dropout_prob,\n            av_attention_probs_dropout_prob, at_attention_probs_dropout_prob,",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/actbert.py:577-594"
    },
    "6137": {
        "file_id": 487,
        "content": "This code initializes three BertLayer objects and one BertConnectionLayer object with different hidden sizes, intermediate sizes, activation functions, dropout probabilities, and attention head numbers. These layers will be used for encoding input sequences in the Actor-Critic Transformer (ACT) model.",
        "type": "comment"
    },
    "6138": {
        "file_id": 487,
        "content": "            intermediate_size, v_intermediate_size, a_intermediate_size,\n            hidden_act, v_hidden_act, a_hidden_act, hidden_dropout_prob,\n            v_hidden_dropout_prob, a_hidden_dropout_prob)\n        self.layer = nn.LayerList(\n            [copy.deepcopy(layer) for _ in range(num_hidden_layers)])  #12\n        self.v_layer = nn.LayerList(\n            [copy.deepcopy(v_layer) for _ in range(v_num_hidden_layers)])  #2\n        self.a_layer = nn.LayerList(\n            [copy.deepcopy(a_layer) for _ in range(a_num_hidden_layers)])  #3\n        self.c_layer = nn.LayerList([\n            copy.deepcopy(connect_layer) for _ in range(len(v_ent_attention_id))\n        ]  #2  [0,1]\n                                    )\n    def forward(\n        self,\n        txt_embedding,\n        image_embedding,\n        action_embedding,\n        txt_attention_mask,\n        image_attention_mask,\n        action_attention_mask,\n        output_all_encoded_layers=True,\n    ):\n        v_start, a_start, t_start = 0, 0, 0\n        count = 0\n        all_encoder_layers_t = []",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/actbert.py:595-622"
    },
    "6139": {
        "file_id": 487,
        "content": "This code defines a model with separate layers for text (txt_layer), vision (v_layer), and action (a_layer) embeddings. It also includes a connect_layer to combine the visual and action information for attention masks. The forward method takes input embeddings, attention masks, and an optional parameter to output all encoded layers.",
        "type": "comment"
    },
    "6140": {
        "file_id": 487,
        "content": "        all_encoder_layers_v = []\n        all_encoder_layers_a = []\n        for v_layer_id, a_layer_id, t_layer_id in zip(self.v_ent_attention_id,\n                                                      self.a_ent_attention_id,\n                                                      self.t_ent_attention_id):\n            v_end = v_layer_id\n            a_end = a_layer_id\n            t_end = t_layer_id\n            assert self.fixed_t_layer <= t_end\n            assert self.fixed_v_layer <= v_end\n            ### region embedding\n            for idx in range(v_start,\n                             self.fixed_v_layer):  #两次训练，这个循环都没有进去  #前面的层固定住\n                with paddle.no_grad():\n                    image_embedding, image_attention_probs = self.v_layer[idx](\n                        image_embedding, image_attention_mask)\n                    v_start = self.fixed_v_layer\n            for idx in range(v_start, v_end):\n                image_embedding, image_attention_probs = self.v_layer[idx](\n                    image_embedding, image_attention_mask)",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/actbert.py:623-645"
    },
    "6141": {
        "file_id": 487,
        "content": "This code initializes empty lists for all encoder layers in vision and audio. It then iterates through the given layer IDs, splitting them into vision (v), audio (a), and time (t) layers. The code asserts that the fixed time layer is less than or equal to the last time layer, and the fixed vision layer is less than or equal to the last vision layer. Next, it iterates through all vision layers from the start index up to but not including the fixed vision layer. Inside this loop, it applies the corresponding vision layer to the image embedding and attention mask using Paddle's no_grad context manager. Finally, it loops over all vision layers from the start index to the end index (fixed vision layer excluded), applying the corresponding vision layer to the image embedding and attention mask.",
        "type": "comment"
    },
    "6142": {
        "file_id": 487,
        "content": "            ### action embedding\n            for idx in range(a_start, a_end):\n                action_embedding, action_attention_probs = self.a_layer[idx](\n                    action_embedding, action_attention_mask)\n            ### text embedding\n            for idx in range(t_start, self.fixed_t_layer):\n                with paddle.no_grad():\n                    txt_embedding, txt_attention_probs = self.layer[idx](\n                        txt_embedding, txt_attention_mask)\n                    t_start = self.fixed_t_layer\n            for idx in range(t_start, t_end):\n                txt_embedding, txt_attention_probs = self.layer[idx](\n                    txt_embedding, txt_attention_mask)\n            image_embedding, txt_embedding, action_embedding = self.c_layer[\n                count](image_embedding, image_attention_mask, txt_embedding,\n                       txt_attention_mask, action_embedding,\n                       action_attention_mask)\n            v_start = v_end\n            t_start = t_end\n            a_start = a_end",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/actbert.py:647-669"
    },
    "6143": {
        "file_id": 487,
        "content": "This code is performing multi-modal embedding by separately handling action, text, and image embeddings. It iterates through layers for each modality to compute the embeddings and attention probs. Finally, it combines the embeddings in a specific order before potentially updating start/end indices.",
        "type": "comment"
    },
    "6144": {
        "file_id": 487,
        "content": "            count += 1\n            if output_all_encoded_layers:\n                all_encoder_layers_t.append(txt_embedding)\n                all_encoder_layers_v.append(image_embedding)\n                all_encoder_layers_a.append(action_embedding)\n        for idx in range(v_start, len(self.v_layer)):  # 1\n            image_embedding, image_attention_probs = self.v_layer[idx](\n                image_embedding, image_attention_mask)\n        for idx in range(a_start, len(self.a_layer)):\n            action_embedding, action_attention_probs = self.a_layer[idx](\n                action_embedding, action_attention_mask)\n        for idx in range(t_start, len(self.layer)):\n            txt_embedding, txt_attention_probs = self.layer[idx](\n                txt_embedding, txt_attention_mask)\n        # add the end part to finish.\n        if not output_all_encoded_layers:\n            all_encoder_layers_t.append(txt_embedding)  #8, 36, 768\n            all_encoder_layers_v.append(image_embedding)  #8, 37, 1024\n            all_encoder_layers_a.append(action_embedding)  #8, 5, 768",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/actbert.py:670-693"
    },
    "6145": {
        "file_id": 487,
        "content": "This code is responsible for encoding text, image, and action inputs in a neural network model. It iterates over the layers of each input type to produce their respective encoded representations. If output_all_encoded_layers is set to True, it appends all intermediate encoded layers to separate lists; otherwise, only the final encoded layer is stored. This allows for flexibility in selecting which encoded layers to use in further processing.",
        "type": "comment"
    },
    "6146": {
        "file_id": 487,
        "content": "        return all_encoder_layers_t, all_encoder_layers_v, all_encoder_layers_a\nclass BertPooler(nn.Layer):\n    \"\"\" \"Pool\" the model by simply taking the hidden state corresponding\n        to the first token.\n    \"\"\"\n    def __init__(self, hidden_size, bi_hidden_size):\n        super(BertPooler, self).__init__()\n        self.dense = nn.Linear(hidden_size, bi_hidden_size)\n        self.activation = nn.ReLU()\n    def forward(self, hidden_states):\n        first_token_tensor = hidden_states[:, 0]  #8, 768\n        pooled_output = self.dense(first_token_tensor)\n        pooled_output = self.activation(pooled_output)\n        return pooled_output\nclass BertModel(nn.Layer):\n    def __init__(\n        self,\n        vocab_size,\n        max_position_embeddings,\n        type_vocab_size,\n        v_feature_size,\n        a_feature_size,\n        num_hidden_layers,\n        v_num_hidden_layers,\n        a_num_hidden_layers,\n        v_ent_attention_id,\n        t_ent_attention_id,\n        a_ent_attention_id,\n        fixed_t_layer,\n        fixed_v_layer,",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/actbert.py:695-729"
    },
    "6147": {
        "file_id": 487,
        "content": "The code defines a BertPooler class that pools the model by taking the hidden state corresponding to the first token. It also includes a BertModel class with various parameters for initializing the BERT model, including vocab size, max position embeddings, type vocab size, and feature sizes for different entities (v, a). The code also defines attention IDs and fixed layers for tokens and aspects.",
        "type": "comment"
    },
    "6148": {
        "file_id": 487,
        "content": "        hidden_size,\n        v_hidden_size,\n        a_hidden_size,\n        bi_hidden_size,\n        intermediate_size,\n        v_intermediate_size,\n        a_intermediate_size,\n        hidden_act,\n        v_hidden_act,\n        a_hidden_act,\n        hidden_dropout_prob,\n        v_hidden_dropout_prob,\n        a_hidden_dropout_prob,\n        attention_probs_dropout_prob,\n        v_attention_probs_dropout_prob,\n        a_attention_probs_dropout_prob,\n        av_attention_probs_dropout_prob,\n        at_attention_probs_dropout_prob,\n        num_attention_heads,\n        v_num_attention_heads,\n        a_num_attention_heads,\n        bi_num_attention_heads,\n    ):\n        super(BertModel, self).__init__()\n        # initilize word embedding\n        self.embeddings = BertEmbeddings(vocab_size, max_position_embeddings,\n                                         type_vocab_size, hidden_size,\n                                         hidden_dropout_prob)\n        # initlize the region embedding\n        self.v_embeddings = BertImageEmbeddings(v_feature_size, v_hidden_size,",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/actbert.py:730-759"
    },
    "6149": {
        "file_id": 487,
        "content": "This code is initializing a BertModel class with various parameters including hidden size, attention-related parameters, and embedding types. It uses the superclass constructor to initialize the base model and then further customizes it by adding word and image embeddings.",
        "type": "comment"
    },
    "6150": {
        "file_id": 487,
        "content": "                                                v_hidden_dropout_prob)\n        # initlize the action embedding\n        self.a_embeddings = BertActionEmbeddings(a_feature_size, a_hidden_size,\n                                                 a_hidden_dropout_prob)\n        self.encoder = BertEncoder(\n            v_ent_attention_id, t_ent_attention_id, a_ent_attention_id,\n            fixed_t_layer, fixed_v_layer, hidden_size, v_hidden_size,\n            a_hidden_size, bi_hidden_size, intermediate_size,\n            v_intermediate_size, a_intermediate_size, hidden_act, v_hidden_act,\n            a_hidden_act, hidden_dropout_prob, v_hidden_dropout_prob,\n            a_hidden_dropout_prob, attention_probs_dropout_prob,\n            v_attention_probs_dropout_prob, a_attention_probs_dropout_prob,\n            av_attention_probs_dropout_prob, at_attention_probs_dropout_prob,\n            num_attention_heads, v_num_attention_heads, a_num_attention_heads,\n            bi_num_attention_heads, num_hidden_layers, v_num_hidden_layers,",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/actbert.py:760-775"
    },
    "6151": {
        "file_id": 487,
        "content": "This code is initializing a model for the ACTBERT backbone, which includes an encoder and action embedding. The model has parameters for various hidden sizes, dropout probabilities, attention head numbers, and hidden layer counts for both textual (v), visual (t), and action (a) components. The model also uses different activation functions for each component.",
        "type": "comment"
    },
    "6152": {
        "file_id": 487,
        "content": "            a_num_hidden_layers)\n        self.t_pooler = BertPooler(hidden_size, bi_hidden_size)\n        self.v_pooler = BertPooler(v_hidden_size, bi_hidden_size)\n        self.a_pooler = BertPooler(a_hidden_size, bi_hidden_size)\n    def forward(\n        self,\n        text_ids,\n        action_feat,\n        image_feat,\n        image_loc,\n        token_type_ids=None,\n        text_mask=None,\n        image_mask=None,\n        action_mask=None,\n        output_all_encoded_layers=False,\n    ):\n        \"\"\"\n        text_ids: input text ids. Shape: [batch_size, seqence_length]\n        action_feat: input action feature. Shape: [batch_size, action_length, action_feature_dim]\n        image_feat: input image feature. Shape: [batch_size, region_length, image_feature_dim]]\n        image_loc: input region location. Shape: [batch_size, region_length, region_location_dim]\n        token_type_ids: segment ids of each video clip. Shape: [batch_size, seqence_length]\n        text_mask: text mask, 1 for real tokens and 0 for padding tokens. Shape: [batch_size, seqence_length]",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/actbert.py:776-800"
    },
    "6153": {
        "file_id": 487,
        "content": "This code defines a class for a model that takes in text, action, and image features as inputs. It initializes three poolers for text, action, and visual features. The forward function processes the input data and returns encoded layers based on the inputs received. The output_all_encoded_layers parameter allows getting all encoded layers if set to True.",
        "type": "comment"
    },
    "6154": {
        "file_id": 487,
        "content": "        image_mask: image mask, 1 for real tokens and 0 for padding tokens. Shape: [batch_size, region_length]\n        action_mask: action mask, 1 for real tokens and 0 for padding tokens. Shape: [batch_size, action_length]\n        output_all_encoded_layers: is output encoded layers feature or not. Type: Bool.\n        \"\"\"\n        if text_mask is None:\n            text_mask = paddle.ones_like(text_ids)\n        if token_type_ids is None:\n            token_type_ids = paddle.zeros_like(text_ids)\n        if image_mask is None:\n            image_mask = paddle.ones(image_feat.shape[0],\n                                     image_feat.shape[1]).astype(text_ids.dtype)\n        if action_mask is None:\n            action_mask = paddle.ones(action_feat.shape[0],\n                                      action_feat.shape[1]).astype(\n                                          text_ids.dtype)\n        # We create a 3D attention mask from a 2D tensor mask.\n        # Sizes are [batch_size, 1, 1, to_seq_length]\n        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length].",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/actbert.py:801-819"
    },
    "6155": {
        "file_id": 487,
        "content": "This code checks if the input masks for text, token_type, image, and action are None. If any of them are None, it generates a mask with the same shape as the corresponding feature tensor and fills it with ones (real tokens) or zeros (padding tokens). The attention mask is created from the 2D tensor mask to be used in the multi-head attention mechanism, which broadcasts to [batch_size, num_heads, from_seq_length, to_seq_length].",
        "type": "comment"
    },
    "6156": {
        "file_id": 487,
        "content": "        extended_text_mask = text_mask.unsqueeze(1).unsqueeze(2)\n        extended_image_mask = image_mask.unsqueeze(1).unsqueeze(2)\n        extended_action_mask = action_mask.unsqueeze(1).unsqueeze(2)\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -10000.0 for masked positions.\n        # Since we are adding it to the raw scores before the softmax, this is\n        # effectively the same as removing these entirely.\n        def set_mask(extended_attention_mask):\n            extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n            return extended_attention_mask\n        extended_text_mask = set_mask(extended_text_mask)\n        extended_image_mask = set_mask(extended_image_mask)\n        extended_action_mask = set_mask(extended_action_mask)\n        t_embedding_output = self.embeddings(text_ids, token_type_ids)\n        v_embedding_output = self.v_embeddings(image_feat, image_loc)",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/actbert.py:820-838"
    },
    "6157": {
        "file_id": 487,
        "content": "This code segment is part of a backbone model for the ACTBERT. It creates extended masks for text, image, and action inputs by unsqueezing the existing masks along dimensions 1 and 2. The function set_mask is then used to multiply each mask with -10000.0 at positions where we want to attend, effectively removing those positions from the attention process. This is done for all three input types: text, image, and action. Finally, the code applies the embeddings to the text inputs using self.embeddings.",
        "type": "comment"
    },
    "6158": {
        "file_id": 487,
        "content": "        a_embedding_output = self.a_embeddings(action_feat)\n        # var = [t_embedding_output, v_embedding_output, a_embedding_output]\n        # import numpy as np\n        # for i, item in enumerate(var):\n        #     np.save('tmp/' + str(i)+'.npy', item.numpy())\n        encoded_layers_t, encoded_layers_v, encoded_layers_a = self.encoder(\n            t_embedding_output,\n            v_embedding_output,\n            a_embedding_output,\n            extended_text_mask,\n            extended_image_mask,\n            extended_action_mask,\n            output_all_encoded_layers=output_all_encoded_layers,\n        )\n        sequence_output_t = encoded_layers_t[-1]  #get item from list\n        sequence_output_v = encoded_layers_v[-1]\n        sequence_output_a = encoded_layers_a[-1]\n        pooled_output_t = self.t_pooler(sequence_output_t)\n        pooled_output_v = self.v_pooler(sequence_output_v)\n        pooled_output_a = self.a_pooler(sequence_output_a)\n        if not output_all_encoded_layers:\n            encoded_layers_t = encoded_layers_t[-1]",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/actbert.py:839-865"
    },
    "6159": {
        "file_id": 487,
        "content": "This code is part of a backbone for a multimodal model, specifically ACTBERT. It first computes the embedding outputs for text (t), vision (v), and action (a) features. Then it passes these embeddings to an encoder to obtain encoded layers for each modality. The last hidden state from each encoder is used as a sequence output, and a pooled output is also computed using separate poolers for each modality. If output_all_encoded_layers is False, the code reduces the encoded layers to their last hidden states.",
        "type": "comment"
    },
    "6160": {
        "file_id": 487,
        "content": "            encoded_layers_v = encoded_layers_v[-1]\n            encoded_layers_a = encoded_layers_a[-1]\n        return encoded_layers_t, encoded_layers_v, encoded_layers_a, \\\n            pooled_output_t, pooled_output_v, pooled_output_a\n# For Head\nclass BertPredictionHeadTransform(nn.Layer):\n    def __init__(self, hidden_size, hidden_act):\n        super(BertPredictionHeadTransform, self).__init__()\n        self.dense = nn.Linear(hidden_size, hidden_size)\n        if isinstance(hidden_act, str) or (sys.version_info[0] == 2\n                                           and isinstance(hidden_act, str)):\n            self.transform_act_fn = ACT2FN[hidden_act]\n        else:\n            self.transform_act_fn = hidden_act\n        self.LayerNorm = nn.LayerNorm(hidden_size, epsilon=1e-12)\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.transform_act_fn(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states)\n        return hidden_states\nclass BertLMPredictionHead(nn.Layer):",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/actbert.py:866-892"
    },
    "6161": {
        "file_id": 487,
        "content": "This code defines two classes: `BertPredictionHeadTransform` and `BertLMPredictionHead`. The former is a transform layer used in BERT's prediction heads. It applies a dense layer followed by an activation function and layer normalization. The latter is the prediction head itself, which takes input hidden states, applies the transform defined in `BertPredictionHeadTransform`, and returns output.",
        "type": "comment"
    },
    "6162": {
        "file_id": 487,
        "content": "    def __init__(self, hidden_size, hidden_act, bert_model_embedding_weights):\n        super(BertLMPredictionHead, self).__init__()\n        self.transform = BertPredictionHeadTransform(hidden_size, hidden_act)\n        # The output weights are the same as the input embeddings, but there is\n        # an output-only bias for each token.\n        assert bert_model_embedding_weights.shape[1] == hidden_size\n        vocab_size = bert_model_embedding_weights.shape[0]\n        # another implementation which would create another big params:\n        # self.decoder = nn.Linear(hidden_size, vocab_size)   # NOTE bias default: constant 0.0\n        # self.decoder.weight = self.create_parameter(shape=[hidden_size, vocab_size],\n        #                                             default_initializer=nn.initializer.Assign(\n        #                                                 bert_model_embedding_weights.t()))  # transpose\n        self.decoder_weight = bert_model_embedding_weights\n        self.decoder_bias = self.create_parameter(",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/actbert.py:893-909"
    },
    "6163": {
        "file_id": 487,
        "content": "This code initializes the BertLMPredictionHead class, which is a part of the BERT model. It takes in hidden_size, hidden_act, and bert_model_embedding_weights as parameters. The class uses these to initialize its transform and decoder components. The decoder component has a weight equal to the input embedding weights with an output-only bias for each token. This implementation avoids creating additional large parameters by directly assigning the input embedding weights to the decoder's weight attribute.",
        "type": "comment"
    },
    "6164": {
        "file_id": 487,
        "content": "            shape=[vocab_size],\n            dtype=bert_model_embedding_weights.dtype,\n            is_bias=True)  # NOTE bias default: constant 0.0\n    def forward(self, hidden_states):\n        hidden_states = self.transform(hidden_states)\n        hidden_states = paddle.tensor.matmul(\n            hidden_states, self.decoder_weight,\n            transpose_y=True) + self.decoder_bias\n        return hidden_states\nclass BertImageActionPredictionHead(nn.Layer):\n    def __init__(self, hidden_size, hidden_act, target_size):\n        super(BertImageActionPredictionHead, self).__init__()\n        self.transform = BertPredictionHeadTransform(hidden_size, hidden_act)\n        self.decoder = nn.Linear(hidden_size, target_size)\n    def forward(self, hidden_states):\n        hidden_states = self.transform(hidden_states)\n        hidden_states = self.decoder(hidden_states)\n        return hidden_states\nclass BertPreTrainingHeads(nn.Layer):\n    def __init__(self, hidden_size, v_hidden_size, a_hidden_size,\n                 bi_hidden_size, hidden_act, v_hidden_act, a_hidden_act,",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/actbert.py:910-937"
    },
    "6165": {
        "file_id": 487,
        "content": "This code defines three classes: ActBert, BertImageActionPredictionHead, and BertPreTrainingHeads. ActBert is a layer that performs attention and feedforward operations in BERT's transformer layers. The BertImageActionPredictionHead class is responsible for the image action prediction task in BERT. Finally, the BertPreTrainingHeads class includes multiple layers for pre-training tasks such as masked language modeling, next sentence prediction, and SQUAD question answering.",
        "type": "comment"
    },
    "6166": {
        "file_id": 487,
        "content": "                 v_target_size, a_target_size, fusion_method,\n                 bert_model_embedding_weights):\n        super(BertPreTrainingHeads, self).__init__()\n        self.predictions = BertLMPredictionHead(hidden_size, hidden_act,\n                                                bert_model_embedding_weights)\n        self.seq_relationship = nn.Linear(bi_hidden_size, 2)\n        self.imagePredictions = BertImageActionPredictionHead(\n            v_hidden_size, v_hidden_act, v_target_size)  # visual class number\n        self.actionPredictions = BertImageActionPredictionHead(\n            a_hidden_size, a_hidden_act, a_target_size)  # action class number\n        self.fusion_method = fusion_method\n        self.dropout = nn.Dropout(0.1)\n    def forward(self, sequence_output_t, sequence_output_v, sequence_output_a,\n                pooled_output_t, pooled_output_v, pooled_output_a):\n        if self.fusion_method == 'sum':\n            pooled_output = self.dropout(pooled_output_t + pooled_output_v +\n                                         pooled_output_a)",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/actbert.py:938-956"
    },
    "6167": {
        "file_id": 487,
        "content": "The code defines a class BertPreTrainingHeads that extends an existing class. It initializes the necessary modules for prediction and fusion. The forward function performs pooling and fusion operations based on the specified fusion method ('sum').",
        "type": "comment"
    },
    "6168": {
        "file_id": 487,
        "content": "        elif self.fusion_method == 'mul':\n            pooled_output = self.dropout(pooled_output_t * pooled_output_v +\n                                         pooled_output_a)\n        else:\n            assert False\n        prediction_scores_t = self.predictions(\n            sequence_output_t)  # 8， 36 ，30522\n        seq_relationship_score = self.seq_relationship(pooled_output)  # 8, 2\n        prediction_scores_v = self.imagePredictions(\n            sequence_output_v)  # 8, 37, 1601\n        prediction_scores_a = self.actionPredictions(\n            sequence_output_a)  # 8, 5, 401\n        return prediction_scores_t, prediction_scores_v, prediction_scores_a, seq_relationship_score\n@BACKBONES.register()\nclass BertForMultiModalPreTraining(nn.Layer):\n    \"\"\"BERT model with multi modal pre-training heads.\n    \"\"\"\n    def __init__(\n        self,\n        vocab_size=30522,\n        max_position_embeddings=512,\n        type_vocab_size=2,\n        v_target_size=1601,\n        a_target_size=700,\n        v_feature_size=2048,\n        a_feature_size=2048,",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/actbert.py:957-986"
    },
    "6169": {
        "file_id": 487,
        "content": "In this code snippet, the model is returning prediction scores for text (t), video (v), and action (a) inputs along with a sequence relationship score. The model is a multi-modal pre-training BERT model with fusion method 'mul'. If the fusion method is not 'mul', it will raise an assertion error.",
        "type": "comment"
    },
    "6170": {
        "file_id": 487,
        "content": "        num_hidden_layers=12,\n        v_num_hidden_layers=2,\n        a_num_hidden_layers=3,\n        t_ent_attention_id=[10, 11],\n        v_ent_attention_id=[0, 1],\n        a_ent_attention_id=[0, 1],\n        fixed_t_layer=0,\n        fixed_v_layer=0,\n        hidden_size=768,\n        v_hidden_size=1024,\n        a_hidden_size=768,\n        bi_hidden_size=1024,\n        intermediate_size=3072,\n        v_intermediate_size=1024,\n        a_intermediate_size=3072,\n        hidden_act=\"gelu\",\n        v_hidden_act=\"gelu\",\n        a_hidden_act=\"gelu\",\n        hidden_dropout_prob=0.1,\n        v_hidden_dropout_prob=0.1,\n        a_hidden_dropout_prob=0.1,\n        attention_probs_dropout_prob=0.1,\n        v_attention_probs_dropout_prob=0.1,\n        a_attention_probs_dropout_prob=0.1,\n        av_attention_probs_dropout_prob=0.1,\n        at_attention_probs_dropout_prob=0.1,\n        num_attention_heads=12,\n        v_num_attention_heads=8,\n        a_num_attention_heads=12,\n        bi_num_attention_heads=8,\n        fusion_method=\"mul\",\n        pretrained=None,",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/actbert.py:987-1018"
    },
    "6171": {
        "file_id": 487,
        "content": "This code defines a custom transformer backbone model for ACT-BERT, with specific configurations for the text (t), video (v), and audio (a) modalities. It includes parameters such as hidden layer numbers, sizes, activation functions, dropout rates, attention heads, and fusion method. The pretrained parameter is set to None.",
        "type": "comment"
    },
    "6172": {
        "file_id": 487,
        "content": "    ):\n        \"\"\"\n        vocab_size: vocabulary size. Default: 30522.\n        max_position_embeddings: max position id. Default: 512.\n        type_vocab_size: max segment id. Default: 2.\n        v_target_size: class number of visual word. Default: 1601.\n        a_target_size: class number of action word. Default: 700.\n        v_feature_size: input visual feature dimension. Default: 2048.\n        a_feature_size: input action feature dimension. Default: 2048.\n        num_hidden_layers: number of BertLayer in text transformer. Default: 12.\n        v_num_hidden_layers: number of BertLayer in visual transformer. Default: 2.\n        a_num_hidden_layers: number of BertLayer in action transformer. Default:3.\n        t_ent_attention_id: index id of BertConnectionLayer in text transformer. Default: [10, 11].\n        v_ent_attention_id: index id of BertConnectionLayer in visual transformer. Default:[0, 1].\n        a_ent_attention_id: index id of BertConnectionLayer in action transformer. Default:[0, 1].\n        fixed_t_layer: index id of fixed BertLayer in text transformer. Default: 0.",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/actbert.py:1019-1034"
    },
    "6173": {
        "file_id": 487,
        "content": "This function defines the input parameters for an ActBERT model, including vocabulary size, maximum position embedding, type vocab size, visual and action target sizes, feature sizes for vision and actions, number of hidden layers in text, visual, and action transformers, index IDs for BertConnectionLayer, and a fixed layer index for the text transformer.",
        "type": "comment"
    },
    "6174": {
        "file_id": 487,
        "content": "        fixed_v_layer: index id of fixed BertLayer in visual transformer. Default: 0.\n        hidden_size: hidden size in text BertLayer. Default: 768.\n        v_hidden_size: hidden size in visual BertLayer. Default: 1024.\n        a_hidden_size: hidden size in action BertLayer. Default: 768.\n        bi_hidden_size: hidden size in BertConnectionLayer. Default: 1024,\n        intermediate_size: intermediate size in text BertLayer. Default: 3072.\n        v_intermediate_size: intermediate size in visual BertLayer. Default: 1024.\n        a_intermediate_size: intermediate size in text BertLayer. Default: 3072.\n        hidden_act: hidden activation function in text BertLayer. Default: \"gelu\".\n        v_hidden_act: hidden activation function in visual BertLayer. Default: \"gelu\".\n        a_hidden_act: hidden activation function in action BertLayer. Default: \"gelu\".\n        hidden_dropout_prob: hidden dropout probability in text Embedding Layer. Default: 0.1\n        v_hidden_dropout_prob: hidden dropout probability in visual Embedding Layer. Default: 0.1",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/actbert.py:1035-1047"
    },
    "6175": {
        "file_id": 487,
        "content": "This code defines parameters for different BertLayers in a model. Fixed_v_layer is the index of a fixed BertLayer in the visual transformer, hidden_size is the hidden size for text and other BERT layers, v_hidden_size is the hidden size for visual BERT layer, a_hidden_size is the hidden size for action BERT layer, bi_hidden_size is the hidden size for BertConnectionLayer, intermediate_size is the intermediate size for text and other BERT layers, v_intermediate_size is the intermediate size for visual BERT layer, a_intermediate_size is the intermediate size for action BERT layer, hidden_act is the activation function for text BERT layer, v_hidden_act is the activation function for visual BERT layer, a_hidden_act is the activation function for action BERT layer, and hidden_dropout_prob is the dropout probability for text embedding layer. All default values are provided in case no other values are specified.",
        "type": "comment"
    },
    "6176": {
        "file_id": 487,
        "content": "        a_hidden_dropout_prob: hidden dropout probability in action Embedding Layer. Default: 0.1\n        attention_probs_dropout_prob: attention dropout probability in text BertLayer. Default: 0.1\n        v_attention_probs_dropout_prob: attention dropout probability in visual BertLayer. Default: 0.1\n        a_attention_probs_dropout_prob: attention dropout probability in action BertLayer. Default: 0.1\n        av_attention_probs_dropout_prob: attention dropout probability in action-visual BertConnectionLayer. Default: 0.1\n        at_attention_probs_dropout_prob: attention dropout probability in action-text BertConnectionLayer. Default: 0.1\n        num_attention_heads: number of heads in text BertLayer. Default: 12.\n        v_num_attention_heads: number of heads in visual BertLayer. Default: 8.\n        a_num_attention_heads: number of heads in action BertLayer. Default: 12.\n        bi_num_attention_heads: number of heads in BertConnectionLayer. Default: 8.\n        fusion_method: methods of fusing pooled output from 3 transformer. Default: \"mul\".",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/actbert.py:1048-1058"
    },
    "6177": {
        "file_id": 487,
        "content": "This code snippet defines default values for various parameters in a transformer model. These parameters include hidden dropout probabilities, attention dropout probabilities, number of attention heads, and fusion methods. The default values are provided for the text, visual, and action BertLayers as well as the BertConnectionLayer.",
        "type": "comment"
    },
    "6178": {
        "file_id": 487,
        "content": "        \"\"\"\n        super(BertForMultiModalPreTraining, self).__init__()\n        self.pretrained = pretrained\n        self.vocab_size = vocab_size\n        self.a_target_size = a_target_size\n        self.bert = BertModel(\n            vocab_size,\n            max_position_embeddings,\n            type_vocab_size,\n            v_feature_size,\n            a_feature_size,\n            num_hidden_layers,\n            v_num_hidden_layers,\n            a_num_hidden_layers,\n            v_ent_attention_id,\n            t_ent_attention_id,\n            a_ent_attention_id,\n            fixed_t_layer,\n            fixed_v_layer,\n            hidden_size,\n            v_hidden_size,\n            a_hidden_size,\n            bi_hidden_size,\n            intermediate_size,\n            v_intermediate_size,\n            a_intermediate_size,\n            hidden_act,\n            v_hidden_act,\n            a_hidden_act,\n            hidden_dropout_prob,\n            v_hidden_dropout_prob,\n            a_hidden_dropout_prob,\n            attention_probs_dropout_prob,",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/actbert.py:1059-1092"
    },
    "6179": {
        "file_id": 487,
        "content": "This code initializes an instance of the BertForMultiModalPreTraining class, which is a pre-trained model for multi-modal tasks. It takes in various parameters such as vocab_size, max_position_embeddings, type_vocab_size, v_feature_size, a_feature_size, num_hidden_layers, v_num_hidden_layers, a_num_hidden_layers, v_ent_attention_id, t_ent_attention_id, a_ent_attention_id, fixed_t_layer, fixed_v_layer, hidden_size, v_hidden_size, a_hidden_size, bi_hidden_size, intermediate_size, v_intermediate_size, a_intermediate_size, hidden_act, v_hidden_act, a_hidden_act, hidden_dropout_prob, v_hidden_dropout_prob, and a_hidden_dropout_prob. These parameters define the architecture and behavior of the model. The super() function is used to call a method from the parent class, in this case, BertModel. The pretrained variable indicates whether the model should use pre-trained weights or not.",
        "type": "comment"
    },
    "6180": {
        "file_id": 487,
        "content": "            v_attention_probs_dropout_prob,\n            a_attention_probs_dropout_prob,\n            av_attention_probs_dropout_prob,\n            at_attention_probs_dropout_prob,\n            num_attention_heads,\n            v_num_attention_heads,\n            a_num_attention_heads,\n            bi_num_attention_heads,\n        )\n        self.cls = BertPreTrainingHeads(\n            hidden_size, v_hidden_size, a_hidden_size, bi_hidden_size,\n            hidden_act, v_hidden_act, a_hidden_act, v_target_size,\n            a_target_size, fusion_method,\n            self.bert.embeddings.word_embeddings.weight)\n    def init_weights(self):\n        \"\"\"Initiate the parameters.\n        \"\"\"\n        if isinstance(self.pretrained, str) and self.pretrained.strip() != \"\":\n            load_ckpt(self, self.pretrained)\n        elif self.pretrained is None or self.pretrained.strip() == \"\":\n            for layer in self.sublayers():\n                if isinstance(layer, (nn.Linear, nn.Embedding)):\n                    weight_init_(layer, 'Normal', std=0.02)",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/actbert.py:1093-1116"
    },
    "6181": {
        "file_id": 487,
        "content": "This code initializes the parameters of a pre-trained ACTBERT model. It checks if the model has been pre-trained and, if not, initializes the weights for the layers (using normal distribution with standard deviation 0.02).",
        "type": "comment"
    },
    "6182": {
        "file_id": 487,
        "content": "                elif isinstance(layer, nn.LayerNorm):\n                    weight_init_(layer, 'Constant', value=1)\n    def forward(\n            self,\n            text_ids,  #8,36\n            action_feat,  #8,5,2048\n            image_feat,  #8,37,2048\n            image_loc,  #8,37,5\n            token_type_ids=None,  #8,36\n            text_mask=None,  #8,36\n            image_mask=None,  #8,37\n            action_mask=None,  #8,5\n    ):\n        \"\"\"\n        text_ids: input text ids. Shape: [batch_size, seqence_length]\n        action_feat: input action feature. Shape: [batch_size, action_length, action_feature_dim]\n        image_feat: input image feature. Shape: [batch_size, region_length+1, image_feature_dim]], add 1 for image global feature.\n        image_loc: input region location. Shape: [batch_size, region_length+1, region_location_dim], add 1 for image global feature location.\n        token_type_ids: segment ids of each video clip. Shape: [batch_size, seqence_length]\n        text_mask: text mask, 1 for real tokens and 0 for padding tokens. Shape: [batch_size, seqence_length]",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/actbert.py:1117-1137"
    },
    "6183": {
        "file_id": 487,
        "content": "The code defines a function \"forward\" that takes text_ids, action_feat, image_feat, image_loc, token_type_ids, text_mask, image_mask, and action_mask as input. The text_ids represent input text ids of shape [batch_size, sequence_length]. Action_feat is the input action feature of shape [batch_size, action_length, action_feature_dim], while image_feat is the input image feature of shape [batch_size, region_length+1, image_feature_dim] (adding 1 for global image feature). Image_loc represents the input region location of shape [batch_size, region_length+1, region_location_dim] (adding 1 for global image feature location). Token_type_ids represent segment ids of each video clip and are of shape [batch_size, sequence_length]. Text_mask is a binary mask representing real tokens as 1 and padding tokens as 0 with shape [batch_size, sequence_length]. Image_mask and action_mask also serve similar functions but for image and action respectively.",
        "type": "comment"
    },
    "6184": {
        "file_id": 487,
        "content": "        image_mask: image mask, 1 for real tokens and 0 for padding tokens. Shape: [batch_size, region_length]\n        action_mask: action mask, 1 for real tokens and 0 for padding tokens. Shape: [batch_size, action_length]\n        \"\"\"\n        sequence_output_t, sequence_output_v, sequence_output_a, \\\n        pooled_output_t, pooled_output_v, pooled_output_a = self.bert(\n            text_ids,\n            action_feat,\n            image_feat,\n            image_loc,\n            token_type_ids,\n            text_mask,\n            image_mask,\n            action_mask,\n            output_all_encoded_layers=False,\n        )\n        prediction_scores_t, prediction_scores_v, prediction_scores_a, seq_relationship_score = self.cls(\n            sequence_output_t, sequence_output_v, sequence_output_a,\n            pooled_output_t, pooled_output_v, pooled_output_a)\n        return prediction_scores_t, prediction_scores_v, prediction_scores_a, seq_relationship_score",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/actbert.py:1138-1158"
    },
    "6185": {
        "file_id": 487,
        "content": "This code is a function that takes in text IDs, action feature, image feature, image location, token type IDs, text mask, image mask, and action mask as inputs. It uses the BERT model to process these inputs and returns prediction scores for each input (text, vision, action) and sequence relationship score.",
        "type": "comment"
    },
    "6186": {
        "file_id": 488,
        "content": "/paddlevideo/modeling/backbones/adds.py",
        "type": "filepath"
    },
    "6187": {
        "file_id": 488,
        "content": "The code imports necessary libraries, registers object detection backbones, performs vector transformations, defines network creation functions, computes depth prediction, initializes PaddleVideo backbone, includes Project3D layer, calculates SSIM loss, and creates a deep learning model for image processing with ResNet V1.5, DepthDecoder, and PoseDecoder classes. The pose estimation model supports diverse inputs, handles day/night scenarios, computes parameters, generates warped images, and selects data based on conditions.",
        "type": "summary"
    },
    "6188": {
        "file_id": 488,
        "content": "# Copyright (c) 2021  PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport math\nfrom collections import OrderedDict\nimport numpy as np\nimport paddle\nimport paddle.nn as nn\nimport paddle.nn.functional as F\nfrom paddle.nn import BatchNorm2D, Conv2D\nfrom paddle.nn.initializer import Constant, Normal\nfrom paddle.vision.models import ResNet\nfrom ...utils import load_ckpt\nfrom ..registry import BACKBONES\nfrom ..weight_init import kaiming_normal_, _calculate_fan_in_and_fan_out\nzeros_ = Constant(value=0.)",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/adds.py:1-30"
    },
    "6189": {
        "file_id": 488,
        "content": "This code imports necessary libraries, defines constants, and registers backbones in a PaddlePaddle's object detection model library. It also includes comments for licensing and copyright information as well as function definitions for weight initialization and calculating fan-in and fan-out of layers.",
        "type": "comment"
    },
    "6190": {
        "file_id": 488,
        "content": "ones_ = Constant(value=1.)\nnormal_ = Normal(mean=0, std=1e-3)\ndef disp_to_depth(disp, min_depth, max_depth):\n    \"\"\"Convert network's sigmoid output into depth prediction\n    The formula for this conversion is given in the 'additional considerations'\n    section of the paper.\n    \"\"\"\n    min_disp = 1 / max_depth\n    max_disp = 1 / min_depth\n    scaled_disp = min_disp + (max_disp - min_disp) * disp\n    depth = 1 / scaled_disp\n    return scaled_disp, depth\ndef gram_matrix(y):\n    (b, ch, h, w) = y.shape\n    features = y.reshape([b, ch, w * h])\n    features_t = paddle.transpose(features, [0, 2, 1])\n    gram = features.bmm(features_t) / (ch * h * w)\n    return gram\ndef convt_bn_relu(in_channels,\n                  out_channels,\n                  kernel_size,\n                  stride=1,\n                  padding=0,\n                  output_padding=0,\n                  bn=True,\n                  relu=True):\n    bias = not bn\n    layers = []\n    layers.append(\n        nn.Conv2DTranspose(in_channels,\n                           out_channels,",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/adds.py:31-67"
    },
    "6191": {
        "file_id": 488,
        "content": "Code snippet defines three functions - \"disp_to_depth\" for converting network's sigmoid output into depth prediction, \"gram_matrix\" for computing the Gram matrix of feature maps and \"convt_bn_relu\" for creating a convolution layer with batch normalization and ReLU activation.",
        "type": "comment"
    },
    "6192": {
        "file_id": 488,
        "content": "                           kernel_size,\n                           stride,\n                           padding,\n                           output_padding,\n                           bias_attr=bias))\n    if bn:\n        layers.append(nn.BatchNorm2D(out_channels))\n    if relu:\n        layers.append(nn.LeakyReLU(0.2))\n    layers = nn.Sequential(*layers)\n    # initialize the weights\n    for m in layers.sublayers(include_self=True):\n        if isinstance(m, nn.Conv2DTranspose):\n            normal_(m.weight)\n            if m.bias is not None:\n                zeros_(m.bias)\n        elif isinstance(m, nn.BatchNorm2D):\n            ones_(m.weight)\n            zeros_(m.bias)\n    return layers\ndef transformation_from_parameters(axisangle, translation, invert=False):\n    \"\"\"Convert the network's (axisangle, translation) output into a 4x4 matrix\n    \"\"\"\n    R = rot_from_axisangle(axisangle)\n    t = translation.clone()\n    if invert:\n        R = R.transpose([0, 2, 1])\n        t *= -1\n    T = get_translation_matrix(t)\n    if invert:",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/adds.py:68-104"
    },
    "6193": {
        "file_id": 488,
        "content": "The code defines a function for creating a convolutional transpose layer, adding batch normalization and Leaky ReLU activation if specified. It also includes weight initialization for the created layers. The second function converts network's (axisangle, translation) output into a 4x4 matrix based on parameters and an optional invert flag.",
        "type": "comment"
    },
    "6194": {
        "file_id": 488,
        "content": "        M = paddle.matmul(R, T)\n    else:\n        M = paddle.matmul(T, R)\n    return M\ndef get_translation_matrix(translation_vector):\n    \"\"\"Convert a translation vector into a 4x4 transformation matrix\n    \"\"\"\n    t = translation_vector.reshape([-1, 3, 1])\n    gather_object = paddle.stack([\n        paddle.zeros([\n            translation_vector.shape[0],\n        ], paddle.float32),\n        paddle.ones([\n            translation_vector.shape[0],\n        ], paddle.float32),\n        paddle.squeeze(t[:, 0], axis=-1),\n        paddle.squeeze(t[:, 1], axis=-1),\n        paddle.squeeze(t[:, 2], axis=-1),\n    ])\n    gather_index = paddle.to_tensor([\n        [1],\n        [0],\n        [0],\n        [2],\n        [0],\n        [1],\n        [0],\n        [3],\n        [0],\n        [0],\n        [1],\n        [4],\n        [0],\n        [0],\n        [0],\n        [1],\n    ])\n    T = paddle.gather_nd(gather_object, gather_index)\n    T = T.reshape([4, 4, -1]).transpose((2, 0, 1))\n    return T\ndef rot_from_axisangle(vec):\n    \"\"\"Convert an axisangle rotation into a 4x4 transformation matrix",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/adds.py:105-151"
    },
    "6195": {
        "file_id": 488,
        "content": "get_translation_matrix: Converts translation vector to a 4x4 transformation matrix.\nrot_from_axisangle: Converts axis-angle rotation into a 4x4 transformation matrix.",
        "type": "comment"
    },
    "6196": {
        "file_id": 488,
        "content": "    (adapted from https://github.com/Wallacoloo/printipi)\n    Input 'vec' has to be Bx1x3\n    \"\"\"\n    angle = paddle.norm(vec, 2, 2, True)\n    axis = vec / (angle + 1e-7)\n    ca = paddle.cos(angle)\n    sa = paddle.sin(angle)\n    C = 1 - ca\n    x = axis[..., 0].unsqueeze(1)\n    y = axis[..., 1].unsqueeze(1)\n    z = axis[..., 2].unsqueeze(1)\n    xs = x * sa\n    ys = y * sa\n    zs = z * sa\n    xC = x * C\n    yC = y * C\n    zC = z * C\n    xyC = x * yC\n    yzC = y * zC\n    zxC = z * xC\n    gather_object = paddle.stack([\n        paddle.squeeze(x * xC + ca, axis=(-1, -2)),\n        paddle.squeeze(xyC - zs, axis=(-1, -2)),\n        paddle.squeeze(zxC + ys, axis=(-1, -2)),\n        paddle.squeeze(xyC + zs, axis=(-1, -2)),\n        paddle.squeeze(y * yC + ca, axis=(-1, -2)),\n        paddle.squeeze(yzC - xs, axis=(-1, -2)),\n        paddle.squeeze(zxC - ys, axis=(-1, -2)),\n        paddle.squeeze(yzC + xs, axis=(-1, -2)),\n        paddle.squeeze(z * zC + ca, axis=(-1, -2)),\n        paddle.ones([\n            vec.shape[0],\n        ], dtype=paddle.float32),",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/adds.py:152-188"
    },
    "6197": {
        "file_id": 488,
        "content": "This code performs rotation operations on a 3D vector 'vec'. It calculates the angle and axis of rotation, then applies trigonometry to compute rotation matrices. Finally, it gathers transformed vectors using stacked tensor operations.",
        "type": "comment"
    },
    "6198": {
        "file_id": 488,
        "content": "        paddle.zeros([\n            vec.shape[0],\n        ], dtype=paddle.float32)\n    ])\n    gather_index = paddle.to_tensor([\n        [0],\n        [1],\n        [2],\n        [10],\n        [3],\n        [4],\n        [5],\n        [10],\n        [6],\n        [7],\n        [8],\n        [10],\n        [10],\n        [10],\n        [10],\n        [9],\n    ])\n    rot = paddle.gather_nd(gather_object, gather_index)\n    rot = rot.reshape([4, 4, -1]).transpose((2, 0, 1))\n    return rot\ndef upsample(x):\n    \"\"\"Upsample input tensor by a factor of 2\n    \"\"\"\n    return F.interpolate(x, scale_factor=2, mode=\"nearest\")\ndef get_smooth_loss(disp, img):\n    \"\"\"Computes the smoothness loss for a disparity image\n    The color image is used for edge-aware smoothness\n    \"\"\"\n    grad_disp_x = paddle.abs(disp[:, :, :, :-1] - disp[:, :, :, 1:])\n    grad_disp_y = paddle.abs(disp[:, :, :-1, :] - disp[:, :, 1:, :])\n    grad_img_x = paddle.mean(paddle.abs(img[:, :, :, :-1] - img[:, :, :, 1:]),\n                             1,\n                             keepdim=True)",
        "type": "code",
        "location": "/paddlevideo/modeling/backbones/adds.py:189-231"
    },
    "6199": {
        "file_id": 488,
        "content": "Code defines three functions: \"get_rot\", \"upsample\", and \"get_smooth_loss\". get_rot performs a gather operation on a tensor, reshapes the result, then transposes it. upsample interpolates an input tensor by doubling its size. get_smooth_loss computes the smoothness loss for disparity images using gradients of disparities and color image edges.",
        "type": "comment"
    }
}