{
    "4700": {
        "file_id": 402,
        "content": "Second, to implenment DI, we build two components:\n- Register, to regist a class\n- Builder, to new an instance\n1. Register\nWe implenment a getter and a setter function to map string to an instance.\n[source code](../../paddlevideo/utils/registry.py)\n```python\n#excerpt from source code.\nclass Registry():\n    def __init__(self, name):\n        self._name = name\n        self._obj_map = {}\n    #mapping name -> object\n    def register(self,  obj, name):\n        self._obj_map[name] = obj\n    #get object\n    def get(self, name):\n        ret = self._obj_map.get(name)\n        return ret\n```\nIt provides name -> object mapping. For example, To register an object:\n```python\n    BACKBONES = Registry('backbone')\n    class ResNet:\n        pass\n    BACKBONES.register(ResNet)\n```\nOr, use a decorator\n```python\n    BACKBONES = Registry('backbone') #new a Register\n    @BACKBONES.register() #regist resnet as a backbone.\n    class ResNet:\n        pass\n```\n2. Builder\nTo obtain a registed module.\n```python\n    # Usage: To build a module.\n    backbone_name = \"ResNet\"",
        "type": "code",
        "location": "/english_documents/tutorials/config.md:39-89"
    },
    "4701": {
        "file_id": 402,
        "content": "This code demonstrates the implementation of Dependency Injection (DI) using a Register and Builder. The Register provides name-to-object mapping, allowing objects to be registered with a specific name. The Builder facilitates obtaining registered modules by accepting a module's name and returning the corresponding instance.",
        "type": "comment"
    },
    "4702": {
        "file_id": 402,
        "content": "    b = BACKBONES.get(backbone_name)()\n```\nso that we can new(register) an instance in **where it declared**, not **where it called**, a basic DI sub-system has been created now.\nWe apply this design on many places, such as: PIPELINE, BACKBONE, HEAD, LOSS, METRIC and so on.\nFinally, We build all of the framework components from config yaml which matches the source code one by one, **It means the attributes in a configuration field is same as the init atrributes of the mathced class**, and to indicate a specified class, we always use ```name``` to mark it. like:\n```yaml\nhead:\n    name: \"TSMHead\"  # class name\n    num_classes: 400 # TSMHead class init attributes\n    ...\n```\n---\n## config yaml details\nWe separate the config to several parts, in high level:\n- **MODEL:** Architecture configuration, such as HEAD module, BACKBONE module.\n- **DATASET:** DATASET and dataloader configuration.\n- **PIPELINE:** pipeline of processing configuration.\n- **OPTIMIZER:** Optimizer configuration.\nand some unique global configurations, like",
        "type": "code",
        "location": "/english_documents/tutorials/config.md:90-117"
    },
    "4703": {
        "file_id": 402,
        "content": "This code snippet is creating an instance of a class based on its name specified in the configuration file. It applies this design to various components like PIPELINE, BACKBONE, HEAD, LOSS, and METRIC for dependency injection. The attributes in the configuration field match the initialization attributes of the corresponding class. The config file separates architecture, dataset, pipeline, and optimizer configurations, along with global settings.",
        "type": "comment"
    },
    "4704": {
        "file_id": 402,
        "content": "- model_name\n- log_interval\n- epochs\n- resume_epoch\n- log_level\n...\nTraining script args\n-  **--validate**: switch validate mode on or not\n-  **--test**: switch test mode on or not\n-  **--weights**: weights path\n-  **-c**: config yaml path\n-  **-o**: override args, one can use it like: -o DATASET.batch_size=16",
        "type": "code",
        "location": "/english_documents/tutorials/config.md:118-131"
    },
    "4705": {
        "file_id": 402,
        "content": "This code snippet is describing command-line arguments for a training script. The user can switch validate or test mode on/off, provide weights and config paths, and override specific args using \"-o\" option. It also mentions the available commands for each argument.",
        "type": "comment"
    },
    "4706": {
        "file_id": 403,
        "content": "/english_documents/tutorials/customized_usage.md",
        "type": "filepath"
    },
    "4707": {
        "file_id": 403,
        "content": "This code provides information about customizing different aspects of the PaddleVideo framework, including dataset, network, solvers, metrics, and debug tools. It discusses finetuning, adding new augmentations and batch augments in the pipeline, modular design, changing frameworks, initializing functions, loss functions, step/epoch decay, creating customized solvers, adding new data processing, records, and metrics, as well as using debug levels and FAQ.",
        "type": "summary"
    },
    "4708": {
        "file_id": 403,
        "content": "[简体中文](../../zh-CN/tutorials/customized_usage.md) | English\n# Customized Usage\n## Customized Dataset\n1. finetune\nPlease refer to [finetune](../start.md#model_finetune) if only change a \"regular\" dataset.\n2. customized pipeline\n  - add new augments\n  - add new batch augments\n  **Note**: Be care of checking the difference of different modes.\n## Customized Network\n1. module function\nPlease refer to [modular desigh](modular_design.md) for more information.\n2. customized framework\n  - change framework\n  - change initialized function\n  - customized loss\n## Customized Solvers\n1. step decay and epoch decay\n2. customized solvers\n## Customized metrics\n  - add new data processing\n  - add new record\n  - add new metrics\n## Debug tools\n1. Debug level\n2. FAQ",
        "type": "code",
        "location": "/english_documents/tutorials/customized_usage.md:1-44"
    },
    "4709": {
        "file_id": 403,
        "content": "This code provides information about customizing different aspects of the PaddleVideo framework, including dataset, network, solvers, metrics, and debug tools. It discusses finetuning, adding new augmentations and batch augments in the pipeline, modular design, changing frameworks, initializing functions, loss functions, step/epoch decay, creating customized solvers, adding new data processing, records, and metrics, as well as using debug levels and FAQ.",
        "type": "comment"
    },
    "4710": {
        "file_id": 404,
        "content": "/english_documents/tutorials/demos",
        "type": "filepath"
    },
    "4711": {
        "file_id": 404,
        "content": "The code outlines six different tasks in action recognition and detection using various algorithms, including TSN (Two-Stream Convolutional Networks), TSM (Temporal Shift Module), SlowFast Networks, LSTM (Long Short-Term Memory), and BNM (Boundary-aware Multi-scale Network). The tasks include single-class action recognition, multi-class action recognition, action localization, spatio-temporal action detection, 3000-class tagging application, and highlights detection application.",
        "type": "summary"
    },
    "4712": {
        "file_id": 404,
        "content": "some useful demo todo.  \n1、single-class action recognition， tsn/tsm/slowfast  \n2、multi-class action recognition，lstm  \n3、action localization，bmn  \n4、spatio temporal action detection，todo  \n5、3000-class tagging application(videotag)：tsn+lstm  \n6、Highlights detection application：bmn+tsn+lstm  ",
        "type": "code",
        "location": "/english_documents/tutorials/demos:1-8"
    },
    "4713": {
        "file_id": 404,
        "content": "The code outlines six different tasks in action recognition and detection using various algorithms, including TSN (Two-Stream Convolutional Networks), TSM (Temporal Shift Module), SlowFast Networks, LSTM (Long Short-Term Memory), and BNM (Boundary-aware Multi-scale Network). The tasks include single-class action recognition, multi-class action recognition, action localization, spatio-temporal action detection, 3000-class tagging application, and highlights detection application.",
        "type": "comment"
    },
    "4714": {
        "file_id": 405,
        "content": "/english_documents/tutorials/deployment.md",
        "type": "filepath"
    },
    "4715": {
        "file_id": 405,
        "content": "This code explains how to convert dygraph models to static models for inference and deployment using PaddleInference, and provides examples on video inference testing with predict.py and benchmarking. Support for C++ infer and PaddleHub Serving deploy are coming soon.",
        "type": "summary"
    },
    "4716": {
        "file_id": 405,
        "content": "[简体中文](../../zh-CN/tutorials/deployment.md) | English\n# Inference\n## How to convert dygraph model to static model?\nTo infer and deploy a model, we need export an inference model, or called to_static: `convert dygraph model to static model`, at first.\n```python\npython3.7 tools/export_model.py -c config_file -o output_path -p params_file\n```\nNote: In `export_model.py`, It will build a model again, and then loading the prarams. But some init params in the infer phase is different from the train phase.\nwe add `num_seg` for TSM in advanced, please add more params or modify them if it is necessary.\nplease refer to [official documents](https://www.paddlepaddle.org.cn/documentation/docs/zh/develop/guides/04_dygraph_to_static/index_cn.html) for more information.\n## How to test the export model?\nPaddleVideo supports a test script to test the exported model.\n```python\npython3.7 tools/test_export_model.py -p params_file -i inference_folder -c config_file\n```\nWe just print the output shape, please feel free to ex",
        "type": "code",
        "location": "/english_documents/tutorials/deployment.md:1-24"
    },
    "4717": {
        "file_id": 405,
        "content": "The code provides instructions on how to convert a dygraph model to a static model for inference and deployment, as well as testing the exported model using PaddleVideo's test script. The conversion is done using the \"export_model.py\" script with appropriate arguments, and some additional parameters are added for TSM. Refer to official documents for more information.",
        "type": "comment"
    },
    "4718": {
        "file_id": 405,
        "content": "tend it. Avtually, only test a video file by PaddleInference can make sure the exported model is right.\n## How to use PaddleInference?\nPaddleVideo supports ```tools/predict.py``` to infer\n```python\npython3.7 tools/predict.py -v example.avi --model_file \"./inference/example.pdmodel\" --params_file \"./inference/example.pdiparams\" --enable_benchmark=False --model=\"example\" --num_seg=8\n ```\n## How to test inference speed?\nPaddleVideo support a script to test inference speed\n```python\npython3.7 tools/predict.py --enable_benchmark=True --model_file=模型文件 --params_file=参数文件\n```\n## How to use C++ infer?\n<sup> coming soon</sup>\n# Deployment\n## How to use PaddleHub Serving deploy?\n<sup> coming soon</sup>\n## How to use PaddleLite deploy?\n<sup> coming soon</sup>",
        "type": "code",
        "location": "/english_documents/tutorials/deployment.md:24-48"
    },
    "4719": {
        "file_id": 405,
        "content": "This code explains how to use the PaddleInference tool for testing video inference, providing examples on using predict.py and enabling benchmarking. It also mentions that support for C++ infer is coming soon, as well as instructions on using PaddleHub Serving deploy and PaddleLite deploy, which will be added later.",
        "type": "comment"
    },
    "4720": {
        "file_id": 406,
        "content": "/english_documents/tutorials/modular_design.md",
        "type": "filepath"
    },
    "4721": {
        "file_id": 406,
        "content": "This code provides a link to the Chinese version of the modular design tutorial and the English version, allowing users to switch between languages based on their preference.",
        "type": "summary"
    },
    "4722": {
        "file_id": 406,
        "content": "[简体中文](../../zh-CN/tutorials/modular_design.md) | English",
        "type": "code",
        "location": "/english_documents/tutorials/modular_design.md:1-1"
    },
    "4723": {
        "file_id": 406,
        "content": "This code provides a link to the Chinese version of the modular design tutorial and the English version, allowing users to switch between languages based on their preference.",
        "type": "comment"
    },
    "4724": {
        "file_id": 407,
        "content": "/english_documents/tutorials/pp-tsm.md",
        "type": "filepath"
    },
    "4725": {
        "file_id": 407,
        "content": "This code introduces the PP-TSM, a high-performance and efficient video recognition model optimized based on TSM in PaddleVideo. It outlines various strategies like ImageNet pretraining, data augmentation, and optimizer improvements to enhance performance and achieve fast inference speed on V101 GPU with top-1 accuracy on UCF101 and Kinetics400 datasets.",
        "type": "summary"
    },
    "4726": {
        "file_id": 407,
        "content": "# High performance recognition 2D architecture PP-TSM\nPP-TSM：An Effective and Efficient video-recognition model   \nPP-TSM is an optimized model based on TSM in PaddleVideo,   \nwhose performance (top-1 on UCF101 and Kinetics400) and inference spped   \nare better than TSM paper(https://arxiv.org/abs/1811.08383 ) and   \nother open source TSM，PaddlePaddle2.0(available on pip now) or   \nDaily Version( https://www.paddlepaddle.org.cn/documentation/docs/zh/install/Tables.html#whl-dev )   \nis required to run PP-TSM.    \nWhen only use ImageNet for pretrain and only use 8X1 sample，  \nPP-TSM’s top1 reached to 89.5% and 73.5% on UCF101 and Kinetics400,   \nand inference speed of FP32 on single V100 is 147 VPS on Kinectics400 dataset.  \ninference speed of FP16 with TensorRT on single V100 isTODO.  \nAs far as we know, under the same conditions,    \ntop1=73.5% on Kinetics400 is the best performance for 2D video model until now.  \nPP-TSM improved performance and speed of TSM with following methods:   \n1、Model Tweaks: ResNet50vd  ，+2.5%  ",
        "type": "code",
        "location": "/english_documents/tutorials/pp-tsm.md:1-22"
    },
    "4727": {
        "file_id": 407,
        "content": "This code describes the PP-TSM, a high-performance and efficient video recognition model optimized based on TSM in PaddleVideo. It mentions its better performance and inference speed compared to TSM paper and other open source TSM models. Requires PaddlePaddle2.0 for execution. When using ImageNet for pretrain and 8X1 sample, it achieves high top-1 accuracy on UCF101 and Kinetics400 datasets with fast inference speed on V100 GPU.",
        "type": "comment"
    },
    "4728": {
        "file_id": 407,
        "content": "2、ImageNet pretrain weights based on Knowledge Distillation  ， +1.3%    \n3、beter batch size  ，+0.2%   \n4、beter L2  ，+0.3%  \n5、label_smoothing  ，+0.2%  \n6、beter lr decay  ，+0.15%  \n7、Data augmentation  ，+0.3%  \n8、beter epoch num  ，+0.15%  \n9、bn strategy  ，+0.4%  \n10、integrated PaddleInference  \n11、more strategies todo: Knowledge Distillation、optimizer and so on.  ",
        "type": "code",
        "location": "/english_documents/tutorials/pp-tsm.md:23-32"
    },
    "4729": {
        "file_id": 407,
        "content": "This code outlines several strategies implemented to improve the performance of a model, including ImageNet pretraining, better batch size and L2 values, label smoothing, better learning rate decay, data augmentation, and updated epoch numbers. The code also mentions using Knowledge Distillation, optimizer improvements, and plans for integrating PaddleInference.",
        "type": "comment"
    },
    "4730": {
        "file_id": 408,
        "content": "/english_documents/tutorials/summarize.md",
        "type": "filepath"
    },
    "4731": {
        "file_id": 408,
        "content": "Video classification tasks involve recognizing actions through RGB images and skeleton data. Concepts include temporal action localization, dense-captioning events, popular datasets, feature extraction, motion representation, and classification using deep learning methods since 2014.",
        "type": "summary"
    },
    "4732": {
        "file_id": 408,
        "content": "[简体中文](../../zh-CN/tutorials/summarize.md) | English\n# Introduction for video classification(action recognition)\n## Wide range of application scenarios\nVideo classification has a wide range of applications in many fields, such as online video platforms such as short videos, offline such as security, transportation, quality inspection and other fields。\n## Multiple subtasks\nSimilar to image tasks, video tasks can also be divided into two categories: **classification (recognition) and detection**, and these two types of tasks can be specifically subdivided by combining different scenes：\n+ Task1：Trimmed Action Recognition. Users input a trimmed video,which contains only single action,then a video tag will be output by model as depicted in fig below:\n<p align=\"center\">\n<img src=\"../../images/action_classification.png\" height=300 width=700 hspace='10'/> <br />\n Action Classification\n</p>\n  In terms of the data modality used, classification tasks can be further subdivided into classification based on si",
        "type": "code",
        "location": "/english_documents/tutorials/summarize.md:1-18"
    },
    "4733": {
        "file_id": 408,
        "content": "Introduction to video classification (action recognition) with various applications in different fields, including online platforms and offline sectors like security, transportation, and quality inspection. Tasks include classification/recognition and detection, further subdivided by combining different scenes.",
        "type": "comment"
    },
    "4734": {
        "file_id": 408,
        "content": "ngle modality data, classification based on multi-modality data, classification based on RGB images and classification based on human skeleton, etc, as shown in the figure below:\n  <p align=\"center\">\n  <img src=\"../../images/multimodality.png\" height=300 width=500 hspace='10'/> <br />\n multi-modality\n  </p>\nIn terms of the perspective of video, it can also be divided into first-person action recognition, \nthird-person action recognition, single perspective action recognition and multi-perspective fusion action recognition. \nUsers who are interested in these fields can refer to relevant literatures.\n+ Task2：Untrimmed Video Classification. \nUnlike trimmed videos, untrimmed videos often contain multiple actions and have a long time span. \nThere are a lot of movements that we may need not paying attention to. Through the global analysis of the input long video, and then make a soft classify to mutiple categories.\n+ Task3：Temporal Action Proposal. It is similar to the ROI extraction in the image detection task. ",
        "type": "code",
        "location": "/english_documents/tutorials/summarize.md:18-32"
    },
    "4735": {
        "file_id": 408,
        "content": "This code is describing different types of classification tasks in video analysis. It covers multi-modality data, RGB images, human skeleton data, and various perspectives such as first-person, third-person, and multiple perspectives. Additionally, it mentions untrimmed videos, temporal action proposals, and ROI extraction in image detection tasks.",
        "type": "comment"
    },
    "4736": {
        "file_id": 408,
        "content": "The task is to find the video clips that may contain action in a long video with a lot of actions.\n+ Task4：Temporal Action Localization. Compared with the temporal action proposal task as mentioned above, \ntemporal action localization task is more consistent with detection task in the field of imgae, \nit requires not only to find the video segments with possible actions from the video but also to classify them,\nas shown in the figure below\n <p align=\"center\">\n<img src=\"../../images/action_detection.png\" height=200 width=1000 hspace='10'/> <br />\n Action Detection\n</p>\n+ Task5：Dense-Captioning Events. The reason why it is called dense captioning events is mainly \nbecause that this task requires video action description on the basis of temporal action localization \n(detection). That is to say, the task needs to locate the actions in a **untrimmed** video,in **temporal \ndimension** and describe the behavior of the **whole video** after obtaining many video segments which contain actions.\n## Introduction of datasets",
        "type": "code",
        "location": "/english_documents/tutorials/summarize.md:33-49"
    },
    "4737": {
        "file_id": 408,
        "content": "Task 4: Temporal Action Localization - find video segments with possible actions, classify them.\nTask 5: Dense-Captioning Events - describe untrimmed videos' actions in temporal dimension.",
        "type": "comment"
    },
    "4738": {
        "file_id": 408,
        "content": "### Classification datasets\nThe training and validation of the model cannot be done without comprehensive, \nlarge and well annotated datasets. With the deepening of research on video action recognition, \nmore and more datasets are applied to the research in this field. \nTypical datasets are as follows:\n+ KTH[<sup>1</sup>](#1)\nKTH dataset is an early small action recognition dataset, \nincluding 599 videos of 6 types of actions (walking, jumping, running, punching, waving and clapping). \nThe background is relatively still, except for the zoom in and out of the camera, \nthe camera movement is relatively slight. Since this data set is relatively small, \nit is easy to overfit when training heavy 3D networks, \nso most current researches are not based on this it.\n+ UCF10[<sup>2</sup>](#2)\nUCF101 is a medium-size dataset in which most videos are from YouTube. \nIt contains 13,320 videos with 101 types of actions. \nEach type of action is performed by 25 people, each of whom performs 4-7 sets of actions. \nThe UCF101 and HMDB51 datasets used to be the benchmarks to evaluate the effectiveness of action ",
        "type": "code",
        "location": "/english_documents/tutorials/summarize.md:51-72"
    },
    "4739": {
        "file_id": 408,
        "content": "The code provides a brief overview of popular video action recognition datasets, such as KTH and UCF101. It mentions that the datasets are essential for training and validating models, but overfitting may occur with larger 3D networks on smaller datasets like KTH.",
        "type": "comment"
    },
    "4740": {
        "file_id": 408,
        "content": "recognition model for a long time before the Kinetics dataset was released.\n+ HMDB51[<sup>3</sup>](#3)\nBrown University's proposed dataset named HMDB51 was released in 2011. \nMost of the videos come from movies, \nbut some come from public databases and online video libraries such as YouTube. \nThe datasets contains 6849 samples divided into 51 classes, \neach of which contains at least 101 samples.\n+ Kinetics[<sup>4</sup>](#4)\nKinetics is the most important large-scale action recognition dataset, which was proposed by Google's DeepMind team in 2017. The video data also comes from YouTube, with 400 categories (now expanded to 700 categories) and more than 300,000 videos (now expanded to 600,000 videos), each lasting about 10 seconds. \nThe action categories are mainly divided into three categories: \"human\", \"human and animal\", \"human and human interaction\". Kinetics can train 3D-RESNET up to 152 layers without over-fitting, \nwhich solves the problem that the previous training dataset is too small to train deep 3D network. ",
        "type": "code",
        "location": "/english_documents/tutorials/summarize.md:73-87"
    },
    "4741": {
        "file_id": 408,
        "content": "HMDB51 is a dataset proposed by Brown University in 2011, consisting of movie and online video sources. It contains 6849 samples across 51 classes with at least 101 samples each. Kinetics is the largest action recognition dataset, created by Google's DeepMind team in 2017. It uses YouTube videos, now expanded to 600k videos in 700 categories. The categories are divided into human, human and animal, and human and human interaction. Kinetics can train deep networks like 3D-RESNET up to 152 layers without overfitting, solving the issue of small training datasets.",
        "type": "comment"
    },
    "4742": {
        "file_id": 408,
        "content": "Kinetics has replaced UCF101 and HMDB51 as the benchmark in the field of action recognition. \nAt present, most studies use this dataset for evaluation and pre-training.\n+ Something-Something[<sup>5</sup>](#5)\nSomethingV1 contains 108,499 annotated videos (V2 has expanded to 220,847), each of which last two to six seconds. These videos contain 174 kinds of actions. Different from the previous dataset, \nthe identification of this data set requires stronger time information, \nso this dataset has a very important reference value in testing the temporal modeling ability of the model.\nIn addition to the above datasets, there are Charades[<sup>6</sup>](#6) dataset for complex Action recognition, Breakfast Action[<sup>7</sup>](#7), and Sports 1M[<sup>8</sup>](#8).\n### Detection datasets\n+ THUMOS 2014\nThis dataset is from THUMOS Challenge 2014, Its training set is UCF101, validation set and test set include 1010 and 1574 undivided video clips respectively. In the action detection task, only 20 kinds of unsegmented videos of actions were labeled with sequential action fragments, ",
        "type": "code",
        "location": "/english_documents/tutorials/summarize.md:88-104"
    },
    "4743": {
        "file_id": 408,
        "content": "Kinetics is the benchmark for action recognition, replacing UCF101 and HMDB51. Most studies use this dataset for evaluation and pre-training. SomethingV1 has 108,499 annotated videos with 174 kinds of actions, requiring strong temporal modeling ability. Other datasets include Charades (complex action recognition), Breakfast Action, Sports 1M, THUMOS 2014 (action detection).",
        "type": "comment"
    },
    "4744": {
        "file_id": 408,
        "content": "including 200 validation sets (3007 action fragments) and 213 test sets (3358 action fragments).\n+ MEXaction2\nThe Mexaction2 dataset contains two types of action: horse riding and bullfighting. \nThe dataset consists of three parts: YouTube videos, horseback riding videos in UCF101, and INA videos. \nYouTube clips and horseback riding videos in UCF101 are short segmented video clips that are used as training sets. \nThe INA video is a long unsegmented video with a total length of 77 hours, \nand it is divided into three parts: training, validation and test. \nThere are 1336 action segments in the training set, 310 in the validation set and 329 in the test set. \nMoreover, the Mexaction2 dataset is characterized by very long unsegmented video lengths, \nand marked action segments only account for a very low proportion of the total video length.\n+ ActivityNet\nAt present the largest database, also contains two tasks of classification and detection. \nThis dataset only provides a YouTube link to the video, not a direct download of the video, ",
        "type": "code",
        "location": "/english_documents/tutorials/summarize.md:105-121"
    },
    "4745": {
        "file_id": 408,
        "content": "The code describes two datasets, Mexaction2 and ActivityNet. Mexaction2 has horse riding and bullfighting actions, split into training, validation, and test sets. It includes YouTube clips, UCF101 horseback riding videos, and an unsegmented 77-hour INA video with low marked action proportions. ActivityNet is the largest database, including classification and detection tasks, but only provides YouTube links without direct downloads.",
        "type": "comment"
    },
    "4746": {
        "file_id": 408,
        "content": "so you also need to use the YouTube download tool in Python to automatically download the videos. \nThe dataset contains 200 action categories, 20,000 (training + verification + test set) videos, \nand a total of about 700 hours of video.\n## Introduction of classic models\nAs shown in the figure, \nthe action recognition framework mainly includes three steps: \nfeature extraction, motion representation and classification. \nHow to extract spatiotemporal features of video is the core problem of action recognition and video classification.\n <p align=\"center\">\n<img src=\"../../images/action_framework.png\" height=300 width=700 hspace='10'/> <br />\nFramework of action recognition\n</p>\nAccording to different methods, action recognition (video classification) methods can be generally summarized into two stages: \nmanual feature-based method and deep learning-based method. \nTypical motion descriptors in the manual feature-based method stage include DTP and IDT, \nwhich are also the most excellent motion descriptors accepted by most researchers before deep-learning is applied in this field. ",
        "type": "code",
        "location": "/english_documents/tutorials/summarize.md:122-138"
    },
    "4747": {
        "file_id": 408,
        "content": "The code discusses the process of feature extraction, motion representation and classification in action recognition. It highlights two stages - manual feature-based method and deep learning-based method. It mentions DTP and IDT as typical motion descriptors used before deep-learning was applied. The code also shows a framework diagram for action recognition.",
        "type": "comment"
    },
    "4748": {
        "file_id": 408,
        "content": "Interinterested readers may refer to the relevant references at the end of this paper. \nSince 2014, deep learning methods have been gradually applied to the field of video classification. \nAt present, deep learning-based methods have become a hotspot of research in both academic and the practice, and the  effect is far beyond the motion features of manual design. \nSince 2014, many classic network structures have been put forward by the researchers regarding the problem of how to represent motion characteristics, \nas shown in the figure below:\n <p align=\"center\">\n<img src=\"../../images/classic_model.png\" height=300 width=700 hspace='10'/> <br />\nClassic Models\n</p>\nAt present,Paddlevideo has contained several classic models such as:TSN[<sup>9</sup>](#9),TSM[<sup>10</sup>](#10),slowfast[<sup>11</sup>](#11),et al.In the future,\nwe will analyze the classic models and papers in these fields. Please look forward to it\n## Introduction of competetion\n+ [ActivityNet](http://activity-net.org/challenges/2020/challenge.html)",
        "type": "code",
        "location": "/english_documents/tutorials/summarize.md:139-154"
    },
    "4749": {
        "file_id": 408,
        "content": "The code discusses the application of deep learning methods in video classification since 2014, highlighting their effectiveness beyond manual motion design. It mentions various classic network structures proposed by researchers for representing motion characteristics and includes images to illustrate these models. The code also introduces PaddleVideo's inclusion of such models like TSN, TSM, slowfast, etc., and anticipates future analysis of these classical models and papers in the field. Additionally, it references an ActivityNet competition for further context.",
        "type": "comment"
    },
    "4750": {
        "file_id": 408,
        "content": "ActivityNet is a large-scale action recognition competition. Since 2016, \nit has been held simultaneously with CVPR every year. Up to this year, \nit has been held for 4 consecutive sessions. It focuses on identifying everyday, high-level, goal-oriented activities from \nuser-generated videos taken from the Internet video portal YouTube. \nAt present, ActivityNet competition has become the most influential competition in the field of action recognition.\n## Reference\n<div id='1'>\n[1] Schuldt C, Laptev I, Caputo B.Recognizing Human Actions: A Local SVM Approach Proceedings of International Conference on Pattern Recognition. Piscataway, NJ: IEEE, 2004:23-26\n</div>\n<br/>\n<div id='2'>\n[2] Soomro K, Zamir A R, Shah M. UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild. arXiv:1212.0402,2012.\n</div>\n<br/>\n<div id='3'>\n[3] Kuehne H, Jhuang H, Garrote E, et al. HMDB: a large video database for human motion recognition Proceedings of IEEE International Conference on Computer Vision. Piscataway, NJ: IEEE, 2011:2556-2563.",
        "type": "code",
        "location": "/english_documents/tutorials/summarize.md:156-173"
    },
    "4751": {
        "file_id": 408,
        "content": "This code snippet provides information about the ActivityNet competition, which is a large-scale action recognition event held annually since 2016. It focuses on identifying everyday activities from user-generated YouTube videos and has become the most influential in the field of action recognition. The code also includes references to relevant research papers for further reading.",
        "type": "comment"
    },
    "4752": {
        "file_id": 408,
        "content": "</div>\n<br/>\n<div id='4'>\n[4] Carreira J , Zisserman A . Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset Proceedings of IEEE Conference on Computer Vision and Pattern Recognition. Piscataway, NJ: IEEE, 2017:6299-6308.\n</div>\n<br/>\n<div id='5'>\n[5] Goyal R, Kahou S E, Michalski V. The “something something” video database for learning and evaluating visual common sense. arXiv:1706.04261,2017.\n</div>\n<br/>\n<div id='6'>\n[6] Sigurdsson G A , Varol Gül, Wang Xiaolong, et al. Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding. arXiv: 604.01753,2016\n</div>\n<br/>\n<div id='7'>\n[7] Kuehne H, Arslan A, Serre T. The Language of Actions Recovering the Syntax and Semantics of Goal-Directed Human Activities  Proceedings of IEEE Conference on Computer Vision and Pattern Recognition. Piscataway, NJ: IEEE, 2014.\n</div>\n<br/>\n<div id='8'>\n[8] Karpathy A , Toderici G , Shetty S , et al. Large-Scale Video Classification with Convolutional Neural Networks Proceedings of IEEE Conference on Computer Vision and Pattern Recognition. Piscataway, NJ: IEEE, 2014:1725-1732.",
        "type": "code",
        "location": "/english_documents/tutorials/summarize.md:174-193"
    },
    "4753": {
        "file_id": 408,
        "content": "This code provides references to various research papers related to action recognition and video classification, such as the \"Quo Vadis, Action Recognition?\" paper by Carreira and Zisserman, and the \"Hollywood in Homes\" paper by Sigurdsson et al. These references are from well-known conferences like IEEE Conference on Computer Vision and Pattern Recognition (CVPR) and arXiv preprints.",
        "type": "comment"
    },
    "4754": {
        "file_id": 408,
        "content": "</div>\n<br/>\n<div id='9'>\n[9] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoo Tang,and Luc Van Gool. Temporal segment networks for action recognition in videos? In Proceedings of the European Conference on Computer Vision,pages 20–36. Springer, 2016.\n</div>\n<br/>\n<div id='10'>\n[10] Lin Ji , Gan Chuang , Han Song . TSM: Temporal Shift Module for Efficient Video Understanding. arXiv:1811.08383,2018.\n</div>\n<br/>\n<div id='11'>\n[11] Feichtenhofer C , Fan Haoqi , Malik J , et al. SlowFast Networks for Video Recognition. arXiv:1812.03982,2018.\n</div>",
        "type": "code",
        "location": "/english_documents/tutorials/summarize.md:194-206"
    },
    "4755": {
        "file_id": 408,
        "content": "The code represents a list of references for papers related to video recognition. Each reference has an identifier (id) and the corresponding paper details like authors, title, and publication information.",
        "type": "comment"
    },
    "4756": {
        "file_id": 409,
        "content": "/english_documents/usage.md",
        "type": "filepath"
    },
    "4757": {
        "file_id": 409,
        "content": "This code sets up PaddleVideo on Linux, supports multi-card training and testing with PaddlePaddle. It provides log format for phases, resumes sessions, fine-tunes with pretrained params and best accuracy achieved. The code launches PaddleVideo in distributed mode with 4 GPUs, tests, exports model, introduces `use_gpu` parameter, and benchmark results are available in benchmark document.",
        "type": "summary"
    },
    "4758": {
        "file_id": 409,
        "content": "[简体中文](../zh-CN/usage.md) | English\n# Usage\n---\nPlease refer to [installation documents](./install.md) to prepare the enviroment, and follow the steps mentioned in the [data preparation documents](./dataset/) to construct dataset, we will take you through the basic functions supported by PaddleVideo, all of it takes the ucf101 dataset with frame format as example.\nPaddleVideo only support linux operation system and GPU running time environment now.\nDefault detination folder of PaddleVideo files. running the [example config](../../configs/example.yaml) as example.\n```\nPaddleVideo\n    ├── paddlevideo\n    ├── ... #other source codes\n    ├── output #ouput destination\n    |    ├── example\n    |    |   ├── example_best.pdparams #path_to_weights\n    |    |   └── ...  \n    |    └── ...  \n    ├── log  #log file destination.\n    |    ├── worker.0\n    |    ├── worker.1\n    |    └── ...  \n    └── inference #inference files destination.\n         ├── .pdiparams file\n         ├── .pdimodel file\n         └── .pdiparmas.info file",
        "type": "code",
        "location": "/english_documents/usage.md:1-28"
    },
    "4759": {
        "file_id": 409,
        "content": "This code provides instructions for setting up the environment, preparing data using the PaddleVideo library, and explains its supported functions. It also mentions that it only supports Linux operation systems with GPU environments and gives an example of how to run the library. The code outlines the default destination folders for output, log files, and inference files.",
        "type": "comment"
    },
    "4760": {
        "file_id": 409,
        "content": "```\n<a name=\"1\"></a>\n## 1. Train and Test\nStart running multi-cards training scripts or test scripts by `paddle.distributed.launch`, or run the `run.sh` directly.\n```bash\nsh run.sh\n```\nWe put all the start commands in advanced in the ```run.sh```, please uncomment the selected one to run.\n<a name=\"model_train\"></a>\n### 1.1 Train\nSwitch `--validate` on to validating while training.\n```bash\nexport CUDA_VISIBLE_DEVICES=0,1,2,3\npython3 -m paddle.distributed.launch \\\n    --gpus=\"0,1,2,3\" \\\n    main.py \\\n    --validate \\\n    -c ./configs/example.yaml\n```\nIndicating `-c` to set configuration, and one can flexible add `-o` in the script to update it.\n```bash\npython -m paddle.distributed.launch \\\n    --gpus=\"0,1,2,3\" \\\n    main.py \\\n    -c ./configs/example.yaml \\\n    --validate \\\n    -o DATASET.batch_size=16\n```\nIndicating `-o DATASET.batch_size=16` can update batch size to 16, please refer to [configuration](tutorials/config.md#config-yaml-details) for more information.\nAfter starting training, log files will generated, ",
        "type": "code",
        "location": "/english_documents/usage.md:29-71"
    },
    "4761": {
        "file_id": 409,
        "content": "This code demonstrates how to train and test a model using PaddlePaddle, a popular deep learning framework. The training process involves running multi-card training scripts or tests by executing the `paddle.distributed.launch` command with appropriate arguments such as GPU selection, script path, and optional configuration file. The configuration file allows for flexible updates like changing batch sizes on the fly. After starting the training, log files are generated for tracking progress and analysis.",
        "type": "comment"
    },
    "4762": {
        "file_id": 409,
        "content": "and its format is shown as below, it will output to both the screen and files. Default destination of log is under the `.log/` folder, and stored in the files named like `worker.0`, `worker.1` ...\n[train phase] current time, current epoch/ total epoch, batch id, metrics, elapse time, ips, etc.:\n    [12/28 17:31:26] epoch:[ 1/80 ] train step:0   loss: 0.04656 lr: 0.000100 top1: 1.00000 top5: 1.00000 elapse: 0.326 reader: 0.001s ips: 98.22489 instance/sec.\n[eval phase] current time, current epoch/ total epoch, batch id, metrics, elapse time, ips, etc.:\n    [12/28 17:31:32] epoch:[ 80/80 ] val step:0    loss: 0.20538 top1: 0.88281 top5: 0.99219 elapse: 1.589 reader: 0.000s ips: 20.14003 instance/sec.\n[epoch end] current time, metrics, elapse time, ips, etc.\n    [12/28 17:31:38] END epoch:80  val loss_avg: 0.52208 top1_avg: 0.84398 top5_avg: 0.97393 elapse_avg: 0.234 reader_avg: 0.000 elapse_sum: 7.021s ips: 136.73686 instance/sec.\n[the best Acc]  \n    [12/28 17:28:42] Already save the best model (top1 acc)0.8494",
        "type": "code",
        "location": "/english_documents/usage.md:71-89"
    },
    "4763": {
        "file_id": 409,
        "content": "The code shows log output format for training and validation phases, including time, epoch, batch ID, metrics, elapse time (execution time), and ips (instances per second). It also displays the best accuracy achieved during training.",
        "type": "comment"
    },
    "4764": {
        "file_id": 409,
        "content": "<a name=\"model_resume\"></a>\n### 1.2 Resume\nIndicate `-o resume_epoch` to resume, It will training from ```resume_epoch``` epoch, PaddleVideo will auto load optimizers parameters and checkpoints from `./output` folder, as it is the default output destination.\n```bash\nexport CUDA_VISIBLE_DEVICES=0,1,2,3\npython3 -m paddle.distributed.launch \\\n    --gpus=\"0,1,2,3\" \\\n    main.py \\\n    -c ./configs/example.yaml \\\n    --validate \\\n    -o resume_epoch=5\n```\n<a name=\"model_finetune\"></a>\n### 1.3 Finetune\nIndicate `--weights` to load pretrained parameters, PaddleVideo will auto treat it as a finetune mission.\n```bash\nexport CUDA_VISIBLE_DEVICES=0,1,2,3\npython3 -m paddle.distributed.launch \\\n    --gpus=\"0,1,2,3\" \\\n    main.py \\\n    -c ./configs/example.yaml \\\n    --validate \\\n    --weights=./outputs/example/path_to_weights\n```\nNote: PaddleVideo will NOT load shape unmatched parameters.\n<a name=\"model_test\"></a>\n### 1.4 Test\nSwitch `--test` on to start test mode, and indicate `--weights` to load pretrained model.\n```bash\nexport CUDA_VISIBLE_DEVICES=0,1,2,3",
        "type": "code",
        "location": "/english_documents/usage.md:91-132"
    },
    "4765": {
        "file_id": 409,
        "content": "The code provides instructions on how to use PaddleVideo for three different tasks: resuming a training session, finetuning with pretrained parameters, and testing. In the resume task, the user should indicate \"-o resume_epoch\" to continue from a specific epoch, while in finetuning, \"--weights\" is used to load pretrained parameters. The test mode is activated using \"--test\". PaddleVideo will not load unmatched parameters.",
        "type": "comment"
    },
    "4766": {
        "file_id": 409,
        "content": "python3 -m paddle.distributed.launch \\\n    --gpus=\"0,1,2,3\" \\\n    main.py \\\n    -c ./configs/example.yaml \\\n    --test \\\n    --weights=./output/example/path_to_weights\n```\n<a name=\"model_inference\"></a>\n## 2. Infer\nFirst, export model.\nIndicate `-c` to set configuration, `-p` to load pretrained model, `-o` to set inference files destination.\n```bash\npython tools/export_model.py \\\n    -c ./configs/example.yaml \\\n    -p ./output/example/path_to_weights \\\n    -o ./inference\n```\nIt will generate `model_name.pdmodel` , `model_name.pdiparams` and `model_name.pdiparames.info`.\nSecond, start PaddleInference engine to infer a video.\n```bash\npython tools/predict.py \\\n    --input_file \"data/example.avi\" \\\n    --model_file \"./inference/example.pdmodel\" \\\n    --params_file \"./inference/example.pdiparams\" \\\n    --use_gpu=True \\\n    --use_tensorrt=False\n```\nAttributes:\n+ `input_file`: input file path or input directory, which contains input files(s).\n+ `model_file`: pdmodel file path.\n+ `params_file`: pdiparams file path.\n+ `use_tensorrt`: use tensorrt to acclerate or not, default: False.",
        "type": "code",
        "location": "/english_documents/usage.md:134-174"
    },
    "4767": {
        "file_id": 409,
        "content": "This code is launching PaddleVideo in distributed mode with four GPUs, running the main.py script with a specified configuration file, and performing testing using weights from a particular path. Then it exports the model for inference by specifying the configuration file, pretrained weights, and output directory. Lastly, it uses the PaddleInference engine to infer a video using the exported model files, input video file, and optional TensorRT acceleration.",
        "type": "comment"
    },
    "4768": {
        "file_id": 409,
        "content": "+ `use_gpu`: use gpu to infer or not, default: True.\nbenchmark results are shown in th [benchmark](./benchmark.md).",
        "type": "code",
        "location": "/english_documents/usage.md:175-177"
    },
    "4769": {
        "file_id": 409,
        "content": "This code snippet is referring to the `use_gpu` parameter in PaddleVideo, which enables or disables GPU usage for inferencing. The default setting is set to True and benchmark results are available in the [benchmark](./benchmark.md) document.",
        "type": "comment"
    },
    "4770": {
        "file_id": 410,
        "content": "/main.py",
        "type": "filepath"
    },
    "4771": {
        "file_id": 410,
        "content": "This code imports libraries, defines functions for PaddleVideo model training, and allows users to specify command-line arguments. It uses the Apache License 2.0 and enables parallel execution with distributed environments. The method to be executed is determined by the command line arguments.",
        "type": "summary"
    },
    "4772": {
        "file_id": 410,
        "content": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport argparse\nimport random\nimport numpy as np\nimport paddle\nfrom paddlevideo.tasks import (test_model, train_dali, train_model,\n                               train_model_multigrid)\nfrom paddlevideo.utils import get_config, get_dist_info\ndef parse_args():\n    parser = argparse.ArgumentParser(\"PaddleVideo train script\")\n    parser.add_argument('-c',\n                        '--config',\n                        type=str,",
        "type": "code",
        "location": "/main.py:1-29"
    },
    "4773": {
        "file_id": 410,
        "content": "This code imports necessary libraries and defines functions for training a PaddleVideo model. It also handles command line arguments using argparse. The script is licensed under the Apache License, Version 2.0.",
        "type": "comment"
    },
    "4774": {
        "file_id": 410,
        "content": "                        default='configs/example.yaml',\n                        help='config file path')\n    parser.add_argument('-o',\n                        '--override',\n                        action='append',\n                        default=[],\n                        help='config options to be overridden')\n    parser.add_argument('--test',\n                        action='store_true',\n                        help='whether to test a model')\n    parser.add_argument('--train_dali',\n                        action='store_true',\n                        help='whether to use dali to speed up training')\n    parser.add_argument('--multigrid',\n                        action='store_true',\n                        help='whether to use multigrid training')\n    parser.add_argument('-w',\n                        '--weights',\n                        type=str,\n                        help='weights for finetuning or testing')\n    parser.add_argument('--fleet',\n                        action='store_true',\n                        help='whether to use fleet run distributed training')",
        "type": "code",
        "location": "/main.py:30-52"
    },
    "4775": {
        "file_id": 410,
        "content": "This code segment is defining command line arguments using the 'argparse' library for a PaddleVideo program. It allows users to set configuration file paths, override config options, test models, enable DALI for training, use multigrid training, specify weights for finetuning or testing, and utilize fleet run distributed training.",
        "type": "comment"
    },
    "4776": {
        "file_id": 410,
        "content": "    parser.add_argument('--amp',\n                        action='store_true',\n                        help='whether to open amp training.')\n    parser.add_argument(\n        '--amp_level',\n        type=str,\n        default=None,\n        help=\"optimize level when open amp training, can only be 'O1' or 'O2'.\")\n    parser.add_argument(\n        '--validate',\n        action='store_true',\n        help='whether to evaluate the checkpoint during training')\n    parser.add_argument(\n        '--seed',\n        type=int,\n        default=1234,\n        help='fixed all random seeds when the program is running')\n    parser.add_argument(\n        '--max_iters',\n        type=int,\n        default=None,\n        help='max iterations when training(this arg only used in test_tipc)')\n    parser.add_argument(\n        '-p',\n        '--profiler_options',\n        type=str,\n        default=None,\n        help='The option of profiler, which should be in format '\n        '\\\"key1=value1;key2=value2;key3=value3\\\".')\n    args = parser.parse_args()\n    return args",
        "type": "code",
        "location": "/main.py:53-84"
    },
    "4777": {
        "file_id": 410,
        "content": "The code adds command-line arguments for AMP (automatic mixed precision) training, validation, random seed, maximum iterations, and profiler options. It then parses these arguments to customize the program's behavior during training.",
        "type": "comment"
    },
    "4778": {
        "file_id": 410,
        "content": "def main():\n    args = parse_args()\n    cfg = get_config(args.config, overrides=args.override)\n    # enable to use npu if paddle is built with npu\n    if paddle.is_compiled_with_custom_device('npu') :\n        cfg.__setattr__(\"use_npu\", True)\n    elif paddle.device.is_compiled_with_xpu():\n        cfg.__setattr__(\"use_xpu\", True)\n    # set seed if specified\n    seed = args.seed\n    if seed is not None:\n        assert isinstance(\n            seed, int), f\"seed must be a integer when specified, but got {seed}\"\n        random.seed(seed)\n        np.random.seed(seed)\n        paddle.seed(seed)\n    # set amp_level if amp is enabled\n    if args.amp:\n        if args.amp_level is None:\n            args.amp_level = 'O1'  # set defaualt amp_level to 'O1'\n        else:\n            assert args.amp_level in [\n                'O1', 'O2'\n            ], f\"amp_level must be 'O1' or 'O2' when amp enabled, but got {args.amp_level}.\"\n    _, world_size = get_dist_info()\n    parallel = world_size != 1\n    if parallel:\n        paddle.distributed.init_parallel_env()",
        "type": "code",
        "location": "/main.py:87-118"
    },
    "4779": {
        "file_id": 410,
        "content": "This code snippet defines a `main` function that parses arguments, configures settings based on provided overrides and device availability (NPU or XPU), sets seed for random number generation if specified, and enables parallel execution using Paddle's distributed environment.",
        "type": "comment"
    },
    "4780": {
        "file_id": 410,
        "content": "    if args.test:\n        test_model(cfg, weights=args.weights, parallel=parallel)\n    elif args.train_dali:\n        train_dali(cfg, weights=args.weights, parallel=parallel)\n    elif args.multigrid:\n        train_model_multigrid(cfg,\n                              world_size=world_size,\n                              validate=args.validate)\n    else:\n        train_model(cfg,\n                    weights=args.weights,\n                    parallel=parallel,\n                    validate=args.validate,\n                    use_fleet=args.fleet,\n                    use_amp=args.amp,\n                    amp_level=args.amp_level,\n                    max_iters=args.max_iters,\n                    profiler_options=args.profiler_options)\nif __name__ == '__main__':\n    main()",
        "type": "code",
        "location": "/main.py:120-141"
    },
    "4781": {
        "file_id": 410,
        "content": "This code determines the method to be executed based on command line arguments. If '--test' is given, it executes 'test_model'. If '--train_dali' is given, it executes 'train_dali'. If '--multigrid' is given, it executes 'train_model_multigrid'. Otherwise, it executes 'train_model', passing the necessary parameters to perform model training or validation.",
        "type": "comment"
    },
    "4782": {
        "file_id": 411,
        "content": "/paddlevideo/__init__.py",
        "type": "filepath"
    },
    "4783": {
        "file_id": 411,
        "content": "This code snippet is importing the paddlevideo_version from the version module. This suggests that this file is serving as an initialization point for the PaddleVideo library, potentially setting up necessary imports or defining constants and functions to be used throughout the library.",
        "type": "summary"
    },
    "4784": {
        "file_id": 411,
        "content": "# Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom .version import paddlevideo_version",
        "type": "code",
        "location": "/applications/EIVideo/EIVideo/paddlevideo/__init__.py:1-15"
    },
    "4785": {
        "file_id": 411,
        "content": "This code snippet is importing the paddlevideo_version from the version module. This suggests that this file is serving as an initialization point for the PaddleVideo library, potentially setting up necessary imports or defining constants and functions to be used throughout the library.",
        "type": "comment"
    },
    "4786": {
        "file_id": 412,
        "content": "/paddlevideo/loader/__init__.py",
        "type": "filepath"
    },
    "4787": {
        "file_id": 412,
        "content": "This code is part of the PaddleVideo library, containing imports and definitions for functions related to dataset building and data loading. It allows users to build datasets, dataloaders, batch pipelines, and utilize the TSN_Dali_loader and get_input_data functions.",
        "type": "summary"
    },
    "4788": {
        "file_id": 412,
        "content": "# Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom .builder import build_dataset, build_dataloader, build_batch_pipeline\nfrom .dataset import VideoDataset\nfrom .dali_loader import TSN_Dali_loader, get_input_data\n__all__ = [\n    'build_dataset', 'build_dataloader', 'build_batch_pipeline', 'VideoDataset',\n    'TSN_Dali_loader', 'get_input_data'\n]",
        "type": "code",
        "location": "/paddlevideo/loader/__init__.py:1-22"
    },
    "4789": {
        "file_id": 412,
        "content": "This code is part of the PaddleVideo library, containing imports and definitions for functions related to dataset building and data loading. It allows users to build datasets, dataloaders, batch pipelines, and utilize the TSN_Dali_loader and get_input_data functions.",
        "type": "comment"
    },
    "4790": {
        "file_id": 413,
        "content": "/paddlevideo/loader/builder.py",
        "type": "filepath"
    },
    "4791": {
        "file_id": 413,
        "content": "The code constructs a PaddleVideo pipeline for preprocessing data and builds a data loader for distributed model training, handling variable batch sizes and using mix_collate_fn to collate data.",
        "type": "summary"
    },
    "4792": {
        "file_id": 413,
        "content": "# Copyright (c) 2020  PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport signal\nimport os\nimport paddle\nfrom paddle.io import DataLoader, DistributedBatchSampler\nfrom .registry import DATASETS, PIPELINES\nfrom ..utils.build_utils import build\nfrom .pipelines.compose import Compose\nfrom paddlevideo.utils import get_logger\nfrom paddlevideo.utils.multigrid import DistributedShortSampler\nimport numpy as np\nlogger = get_logger(\"paddlevideo\")\ndef build_pipeline(cfg):\n    \"\"\"Build pipeline.",
        "type": "code",
        "location": "/paddlevideo/loader/builder.py:1-29"
    },
    "4793": {
        "file_id": 413,
        "content": "The code is building a pipeline for PaddleVideo. It imports necessary libraries and classes, utilizes function to build the pipeline, logs information using get_logger from paddlevideo.utils, and adheres to Apache License 2.0. The purpose of this code seems to be related to data preprocessing and possibly model training in a distributed environment.",
        "type": "comment"
    },
    "4794": {
        "file_id": 413,
        "content": "    Args:\n        cfg (dict): root config dict.\n    \"\"\"\n    if cfg == None:\n        return\n    return Compose(cfg)\ndef build_dataset(cfg):\n    \"\"\"Build dataset.\n    Args:\n        cfg (dict): root config dict.\n    Returns:\n        dataset: dataset.\n    \"\"\"\n    #XXX: ugly code here!\n    cfg_dataset, cfg_pipeline = cfg\n    cfg_dataset.pipeline = build_pipeline(cfg_pipeline)\n    dataset = build(cfg_dataset, DATASETS, key=\"format\")\n    return dataset\ndef build_batch_pipeline(cfg):\n    batch_pipeline = build(cfg, PIPELINES)\n    return batch_pipeline\ndef build_dataloader(dataset,\n                     batch_size,\n                     num_workers,\n                     places,\n                     shuffle=True,\n                     drop_last=True,\n                     multigrid=False,\n                     collate_fn_cfg=None,\n                     **kwargs):\n    \"\"\"Build Paddle Dataloader.\n    XXX explain how the dataloader work!\n    Args:\n        dataset (paddle.dataset): A PaddlePaddle dataset object.\n        batch_size (int): batch size on single card.",
        "type": "code",
        "location": "/paddlevideo/loader/builder.py:30-74"
    },
    "4795": {
        "file_id": 413,
        "content": "build_dataset: Builds a dataset using provided config dictionary, building pipeline first.\nbuild_batch_pipeline: Constructs the batch pipeline using config from the PIPELINES module.\nbuild_dataloader: Creates Paddle Dataloader object using specified parameters and dataset.",
        "type": "comment"
    },
    "4796": {
        "file_id": 413,
        "content": "        num_worker (int): num_worker\n        shuffle(bool): whether to shuffle the data at every epoch.\n    \"\"\"\n    if multigrid:\n        sampler = DistributedShortSampler(dataset,\n                                          batch_sizes=batch_size,\n                                          shuffle=True,\n                                          drop_last=True)\n    else:\n        sampler = DistributedBatchSampler(dataset,\n                                          batch_size=batch_size,\n                                          shuffle=shuffle,\n                                          drop_last=drop_last)\n    #NOTE(shipping): when switch the mix operator on, such as: mixup, cutmix.\n    # batch like: [[img, label, attibute, ...], [imgs, label, attribute, ...], ...] will recollate to:\n    # [[img, img, ...], [label, label, ...], [attribute, attribute, ...], ...] as using numpy.transpose.\n    def mix_collate_fn(batch):\n        pipeline = build_batch_pipeline(collate_fn_cfg)\n        batch = pipeline(batch)\n        slots = []",
        "type": "code",
        "location": "/paddlevideo/loader/builder.py:75-96"
    },
    "4797": {
        "file_id": 413,
        "content": "This code appears to be part of a data loading and processing function for a machine learning or deep learning model. It uses a sampler to manage the data, with options for shuffling and dropping the last batch if needed. The mix_collate_fn function is defined to collate the data in a specific way using a pipeline built from collate_fn_cfg.",
        "type": "comment"
    },
    "4798": {
        "file_id": 413,
        "content": "        for items in batch:\n            for i, item in enumerate(items):\n                if len(slots) < len(items):\n                    slots.append([item])\n                else:\n                    slots[i].append(item)\n        return [np.stack(slot, axis=0) for slot in slots]\n    #if collate_fn_cfg is not None:\n    #ugly code here. collate_fn is mix op config\n    #    collate_fn = mix_collate_fn(collate_fn_cfg)\n    data_loader = DataLoader(\n        dataset,\n        batch_sampler=sampler,\n        places=places,\n        num_workers=num_workers,\n        collate_fn=mix_collate_fn if collate_fn_cfg is not None else None,\n        return_list=True,\n        **kwargs)\n    return data_loader\ndef term_mp(sig_num, frame):\n    \"\"\" kill all child processes\n    \"\"\"\n    pid = os.getpid()\n    pgid = os.getpgid(os.getpid())\n    logger.info(\"main proc {} exit, kill process group \" \"{}\".format(pid, pgid))\n    os.killpg(pgid, signal.SIGKILL)\n    return\nsignal.signal(signal.SIGINT, term_mp)\nsignal.signal(signal.SIGTERM, term_mp)",
        "type": "code",
        "location": "/paddlevideo/loader/builder.py:97-132"
    },
    "4799": {
        "file_id": 413,
        "content": "This code appears to create a data loader that can handle batches of varying lengths. It iterates through each batch and organizes the items into slots based on their length, either creating a new slot for longer items or appending them to existing ones. The DataLoader class is then instantiated with this collate_fn for processing the dataset, using the provided parameters. Additionally, signal handlers are set up to handle SIGINT and SIGTERM signals to terminate the process group if needed.",
        "type": "comment"
    }
}