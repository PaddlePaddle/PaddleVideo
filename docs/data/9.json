{
    "900": {
        "file_id": 83,
        "content": "/applications/EIVideo/QEIVideo/ui/demo.py",
        "type": "filepath"
    },
    "901": {
        "file_id": 83,
        "content": "The code creates a PyQt5 video player UI with QGraphicsView, QFrame button, and main window layout. It includes interactive buttons, sliders, QProgressBar, QLabel, and tabs in the tab widget for status and configuration.",
        "type": "summary"
    },
    "902": {
        "file_id": 83,
        "content": "# -*- coding: utf-8 -*-\n# Form implementation generated from reading ui file '/Users/zhanghongji/PycharmProjects/EIVideo/resources/QT/demo.ui'\n#\n# Created by: PyQt5 UI code generator 5.15.6\n#\n# WARNING: Any manual changes made to this file will be lost when pyuic5 is\n# run again.  Do not edit this file unless you know what you are doing.\nfrom PyQt5 import QtCore, QtGui, QtWidgets\nclass Ui_MainWindow(object):\n    def setupUi(self, MainWindow):\n        MainWindow.setObjectName(\"MainWindow\")\n        MainWindow.resize(800, 486)\n        MainWindow.setMinimumSize(QtCore.QSize(800, 486))\n        MainWindow.setMaximumSize(QtCore.QSize(800, 486))\n        self.centralwidget = QtWidgets.QWidget(MainWindow)\n        self.centralwidget.setObjectName(\"centralwidget\")\n        self.video_frame = QtWidgets.QFrame(self.centralwidget)\n        self.video_frame.setGeometry(QtCore.QRect(20, 20, 761, 361))\n        self.video_frame.setFrameShape(QtWidgets.QFrame.StyledPanel)\n        self.video_frame.setFrameShadow(QtWidgets.QFrame.Raised)",
        "type": "code",
        "location": "/applications/EIVideo/QEIVideo/ui/demo.py:1-25"
    },
    "903": {
        "file_id": 83,
        "content": "The code is a form implementation generated by PyQt5 UI code generator. It defines the `Ui_MainWindow` class which has a method `setupUi` that sets up the properties and widgets of the `MainWindow`. The main window has a central widget, containing a video frame, with dimensions 761x361 pixels.",
        "type": "comment"
    },
    "904": {
        "file_id": 83,
        "content": "        self.video_frame.setObjectName(\"video_frame\")\n        self.graphicsView = QtWidgets.QGraphicsView(self.video_frame)\n        self.graphicsView.setGeometry(QtCore.QRect(0, 0, 761, 321))\n        self.graphicsView.setObjectName(\"graphicsView\")\n        self.frame_2 = QtWidgets.QFrame(self.video_frame)\n        self.frame_2.setGeometry(QtCore.QRect(0, 320, 761, 41))\n        self.frame_2.setFrameShape(QtWidgets.QFrame.StyledPanel)\n        self.frame_2.setFrameShadow(QtWidgets.QFrame.Raised)\n        self.frame_2.setObjectName(\"frame_2\")\n        self.horizontalLayoutWidget = QtWidgets.QWidget(self.frame_2)\n        self.horizontalLayoutWidget.setGeometry(QtCore.QRect(-1, -1, 761, 41))\n        self.horizontalLayoutWidget.setObjectName(\"horizontalLayoutWidget\")\n        self.horizontalLayout = QtWidgets.QHBoxLayout(self.horizontalLayoutWidget)\n        self.horizontalLayout.setContentsMargins(0, 0, 0, 0)\n        self.horizontalLayout.setObjectName(\"horizontalLayout\")\n        self.open_btn = QtWidgets.QPushButton(self.horizontalLayoutWidget)",
        "type": "code",
        "location": "/applications/EIVideo/QEIVideo/ui/demo.py:26-41"
    },
    "905": {
        "file_id": 83,
        "content": "This code sets up the user interface elements for a video player. It creates a QGraphicsView for displaying video frames, and a QFrame with a horizontal layout widget containing a PushButton for opening videos. The QGraphicsView is set to take up most of the video frame, while the QFrame and its widgets sit at the bottom.",
        "type": "comment"
    },
    "906": {
        "file_id": 83,
        "content": "        self.open_btn.setObjectName(\"open_btn\")\n        self.horizontalLayout.addWidget(self.open_btn)\n        self.save_btn = QtWidgets.QPushButton(self.horizontalLayoutWidget)\n        self.save_btn.setObjectName(\"save_btn\")\n        self.horizontalLayout.addWidget(self.save_btn)\n        self.horizontalSlider = QtWidgets.QSlider(self.horizontalLayoutWidget)\n        self.horizontalSlider.setOrientation(QtCore.Qt.Horizontal)\n        self.horizontalSlider.setObjectName(\"horizontalSlider\")\n        self.horizontalLayout.addWidget(self.horizontalSlider)\n        self.select_btn = QtWidgets.QPushButton(self.horizontalLayoutWidget)\n        self.select_btn.setObjectName(\"select_btn\")\n        self.horizontalLayout.addWidget(self.select_btn)\n        self.clean_btn = QtWidgets.QPushButton(self.horizontalLayoutWidget)\n        self.clean_btn.setObjectName(\"clean_btn\")\n        self.horizontalLayout.addWidget(self.clean_btn)\n        self.start_btn = QtWidgets.QPushButton(self.horizontalLayoutWidget)\n        self.start_btn.setObjectName(\"start_btn\")",
        "type": "code",
        "location": "/applications/EIVideo/QEIVideo/ui/demo.py:42-58"
    },
    "907": {
        "file_id": 83,
        "content": "Creates UI buttons and a slider for video player interaction, sets their object names.",
        "type": "comment"
    },
    "908": {
        "file_id": 83,
        "content": "        self.horizontalLayout.addWidget(self.start_btn)\n        self.draw_frame = QtWidgets.QFrame(self.video_frame)\n        self.draw_frame.setGeometry(QtCore.QRect(0, 10, 751, 301))\n        self.draw_frame.setFrameShape(QtWidgets.QFrame.StyledPanel)\n        self.draw_frame.setFrameShadow(QtWidgets.QFrame.Raised)\n        self.draw_frame.setObjectName(\"draw_frame\")\n        self.menu_tab = QtWidgets.QTabWidget(self.centralwidget)\n        self.menu_tab.setGeometry(QtCore.QRect(20, 380, 761, 81))\n        self.menu_tab.setObjectName(\"menu_tab\")\n        self.tab = QtWidgets.QWidget()\n        self.tab.setObjectName(\"tab\")\n        self.act_label = QtWidgets.QLabel(self.tab)\n        self.act_label.setEnabled(True)\n        self.act_label.setGeometry(QtCore.QRect(10, 30, 71, 21))\n        self.act_label.setObjectName(\"act_label\")\n        self.act_info_label = QtWidgets.QLabel(self.tab)\n        self.act_info_label.setEnabled(True)\n        self.act_info_label.setGeometry(QtCore.QRect(80, 30, 81, 21))\n        self.act_info_label.setObjectName(\"act_info_label\")",
        "type": "code",
        "location": "/applications/EIVideo/QEIVideo/ui/demo.py:59-77"
    },
    "909": {
        "file_id": 83,
        "content": "The code is creating a GUI layout for a video player application. It adds widgets to a main window, sets the geometry and styling of some elements, and creates a tabbed interface with two labels.",
        "type": "comment"
    },
    "910": {
        "file_id": 83,
        "content": "        self.act_progressbar = QtWidgets.QProgressBar(self.tab)\n        self.act_progressbar.setGeometry(QtCore.QRect(170, 32, 521, 21))\n        self.act_progressbar.setProperty(\"value\", 24)\n        self.act_progressbar.setObjectName(\"act_progressbar\")\n        self.label_3 = QtWidgets.QLabel(self.tab)\n        self.label_3.setEnabled(True)\n        self.label_3.setGeometry(QtCore.QRect(680, 30, 60, 21))\n        self.label_3.setLayoutDirection(QtCore.Qt.LeftToRight)\n        self.label_3.setAlignment(QtCore.Qt.AlignRight|QtCore.Qt.AlignTrailing|QtCore.Qt.AlignVCenter)\n        self.label_3.setObjectName(\"label_3\")\n        self.menu_tab.addTab(self.tab, \"\")\n        self.tab_2 = QtWidgets.QWidget()\n        self.tab_2.setObjectName(\"tab_2\")\n        self.menu_tab.addTab(self.tab_2, \"\")\n        MainWindow.setCentralWidget(self.centralwidget)\n        self.statusbar = QtWidgets.QStatusBar(MainWindow)\n        self.statusbar.setObjectName(\"statusbar\")\n        MainWindow.setStatusBar(self.statusbar)\n        self.retranslateUi(MainWindow)",
        "type": "code",
        "location": "/applications/EIVideo/QEIVideo/ui/demo.py:78-97"
    },
    "911": {
        "file_id": 83,
        "content": "Code snippet creates a QProgressBar and QLabel, sets their properties and positions, adds them to the tab widget, and sets tabs for the main window.",
        "type": "comment"
    },
    "912": {
        "file_id": 83,
        "content": "        self.menu_tab.setCurrentIndex(0)\n        QtCore.QMetaObject.connectSlotsByName(MainWindow)\n    def retranslateUi(self, MainWindow):\n        _translate = QtCore.QCoreApplication.translate\n        MainWindow.setWindowTitle(_translate(\"MainWindow\", \"MainWindow\"))\n        self.open_btn.setText(_translate(\"MainWindow\", \"打开视频\"))\n        self.save_btn.setText(_translate(\"MainWindow\", \"保存标注\"))\n        self.select_btn.setText(_translate(\"MainWindow\", \"选择目标\"))\n        self.clean_btn.setText(_translate(\"MainWindow\", \"清空目标\"))\n        self.start_btn.setText(_translate(\"MainWindow\", \"开始推理\"))\n        self.act_label.setText(_translate(\"MainWindow\", \"当前状态：\"))\n        self.act_info_label.setText(_translate(\"MainWindow\", \"-------------\"))\n        self.label_3.setText(_translate(\"MainWindow\", \"12%\"))\n        self.menu_tab.setTabText(self.menu_tab.indexOf(self.tab), _translate(\"MainWindow\", \"状态\"))\n        self.menu_tab.setTabText(self.menu_tab.indexOf(self.tab_2), _translate(\"MainWindow\", \"属性配置\"))",
        "type": "code",
        "location": "/applications/EIVideo/QEIVideo/ui/demo.py:98-113"
    },
    "913": {
        "file_id": 83,
        "content": "This code is a part of the user interface (UI) definition for a MainWindow in the QEIVideo application. It sets the window title and button texts, translates strings using QtCore.QCoreApplication.translate, and updates tab labels using self.menu_tab.setTabText. The UI consists of several tabs: one for displaying the current status, another for configuring attributes. The code also connects slots to signals in this MainWindow class.",
        "type": "comment"
    },
    "914": {
        "file_id": 84,
        "content": "/applications/EIVideo/QEIVideo/version.py",
        "type": "filepath"
    },
    "915": {
        "file_id": 84,
        "content": "This code snippet is the version information for the EIVideo application, created by Acer Zhang on January 11th, 2022. It has a version number \"0.1a\" and the author requests proper attribution if reusing the code.",
        "type": "summary"
    },
    "916": {
        "file_id": 84,
        "content": "# Author: Acer Zhang\n# Datetime: 2022/1/11 \n# Copyright belongs to the author.\n# Please indicate the source for reprinting.\n__version__ = \"0.1a\"",
        "type": "code",
        "location": "/applications/EIVideo/EIVideo/version.py:1-6"
    },
    "917": {
        "file_id": 84,
        "content": "This code snippet is the version information for the EIVideo application, created by Acer Zhang on January 11th, 2022. It has a version number \"0.1a\" and the author requests proper attribution if reusing the code.",
        "type": "comment"
    },
    "918": {
        "file_id": 85,
        "content": "/applications/EIVideo/QEIVideo/widget/PaintBoard.py",
        "type": "filepath"
    },
    "919": {
        "file_id": 85,
        "content": "The PaintBoard class, derived from QWidget, handles data initialization and view size, provides methods for clearing board, changing pen attributes, painting events, and retrieving content. The code implements a mouse event handler for drawing on the board in PaintBoard mode or eraser mode based on user selection.",
        "type": "summary"
    },
    "920": {
        "file_id": 85,
        "content": "from PyQt5.QtWidgets import QWidget\nfrom PyQt5.Qt import QPixmap, QPainter, QPoint, QPaintEvent, QMouseEvent, QPen, \\\n    QColor, QSize\nfrom PyQt5.QtCore import Qt\nclass PaintBoard(QWidget):\n    def __init__(self, parent=None):\n        '''\n        Constructor\n        '''\n        super().__init__(parent)\n        self.__init_data()  # 先初始化数据，再初始化界面\n        self.__init_view()\n    def __init_data(self):\n        self.__size = QSize(810, 458)\n        # 新建QPixmap作为画板，尺寸为__size\n        self.__board = QPixmap(self.__size)\n        self.__board.fill(Qt.transparent)  # 用透明填充画板\n        self.__IsEmpty = True  # 默认为空画板\n        self.EraserMode = False  # 默认为禁用橡皮擦模式\n        self.__lastPos = QPoint(0, 0)  # 上一次鼠标位置\n        self.__currentPos = QPoint(0, 0)  # 当前的鼠标位置\n        self.__painter = QPainter()  # 新建绘图工具\n        self.__thickness = 15  # 默认画笔粗细为10px\n        self.__penColor = QColor(\"black\")  # 设置默认画笔颜色为黑色\n        self.__colorList = QColor.colorNames()  # 获取颜色列表\n    def __init_view(self):\n        # 设置界面的尺寸为__size\n        self.setFixedSize(self.__size)",
        "type": "code",
        "location": "/applications/EIVideo/QEIVideo/widget/PaintBoard.py:1-40"
    },
    "921": {
        "file_id": 85,
        "content": "The code defines a class `PaintBoard` which inherits from `QWidget`. It initializes data such as the size of the board, an empty QPixmap, a boolean for EraserMode, and variables to store the position and pen attributes. Then it sets the view's fixed size based on the initialized data.",
        "type": "comment"
    },
    "922": {
        "file_id": 85,
        "content": "    def clear(self):\n        # 清空画板\n        # self.__board.fill(Qt.white)\n        self.__board = QPixmap(self.__size)\n        self.__board.fill(Qt.transparent)  # 用透明填充画板\n        self.update()\n        self.__IsEmpty = True\n    def change_pen_color(self, color=\"black\"):\n        # 改变画笔颜色\n        # rgbaColor = QColor(255, 255, 0, 100)\n        self.__penColor = QColor(color)\n    def change_pen_thickness(self, thickness=10):\n        # 改变画笔粗细\n        self.__thickness = thickness\n    def is_empty(self):\n        # 返回画板是否为空\n        return self.__IsEmpty\n    def get_content_as_q_image(self):\n        # 获取画板内容（返回QImage）\n        image = self.__board.toImage()\n        return image\n    def paintEvent(self, paint_event):\n        # 绘图事件\n        # 绘图时必须使用QPainter的实例，此处为__painter\n        # 绘图在begin()函数与end()函数间进行\n        # begin(param)的参数要指定绘图设备，即把图画在哪里\n        # drawPixmap用于绘制QPixmap类型的对象\n        self.__painter.begin(self)\n        # 0,0为绘图的左上角起点的坐标，__board即要绘制的图\n        self.__painter.drawPixmap(0, 0, self.__board)\n        self.__painter.end()",
        "type": "code",
        "location": "/applications/EIVideo/QEIVideo/widget/PaintBoard.py:42-78"
    },
    "923": {
        "file_id": 85,
        "content": "This code defines a class with methods to clear the paint board, change pen color and thickness, check if the board is empty, retrieve content as QImage, and handle painting events. The paintEvent method utilizes QPainter to draw the pixmap on the board in the correct location.",
        "type": "comment"
    },
    "924": {
        "file_id": 85,
        "content": "    def mousePressEvent(self, mouse_event):\n        # 鼠标按下时，获取鼠标的当前位置保存为上一次位置\n        self.__currentPos = mouse_event.pos()\n        self.__lastPos = self.__currentPos\n    def mouseMoveEvent(self, mouse_event):\n        # 鼠标移动时，更新当前位置，并在上一个位置和当前位置间画线\n        self.__currentPos = mouse_event.pos()\n        self.__painter.begin(self.__board)\n        if self.EraserMode == False:\n            # 非橡皮擦模式\n            self.__painter.setPen(QPen(self.__penColor, self.__thickness))  # 设置画笔颜色，粗细\n        else:\n            # 橡皮擦模式下画笔为纯白色，粗细为10\n            self.__painter.setPen(QPen(Qt.transparent, 10))\n        # 画线\n        # print(self.__lastPos + self.__currentPos)\n        self.__painter.drawLine(self.__lastPos, self.__currentPos)\n        self.__painter.end()\n        self.__lastPos = self.__currentPos\n        self.update()  # 更新显示\n    def mouseReleaseEvent(self, mouseEvent):\n        self.__IsEmpty = False  # 画板不再为空",
        "type": "code",
        "location": "/applications/EIVideo/QEIVideo/widget/PaintBoard.py:80-106"
    },
    "925": {
        "file_id": 85,
        "content": "This code implements a mouse event handler for drawing on a PaintBoard. When the mouse is pressed, the current position is saved as the previous position. As the mouse moves, it draws lines between the last and current positions based on whether eraser mode is enabled or not. Upon mouse release, the board is marked as not empty. The drawing is updated to reflect the changes.",
        "type": "comment"
    },
    "926": {
        "file_id": 86,
        "content": "/applications/EIVideo/README.md",
        "type": "filepath"
    },
    "927": {
        "file_id": 86,
        "content": "EIVideo is a Windows-based video annotation tool using Baidu Paddle MA-Net model, maintained by QPT-Family on GitHub and available in pre-release/stable versions with customization, usage instructions, updates, and licensing details.",
        "type": "summary"
    },
    "928": {
        "file_id": 86,
        "content": "# EIVideo - 交互式智能视频标注工具\n[![Downloads](https://static.pepy.tech/personalized-badge/eivideo?period=total&units=international_system&left_color=grey&right_color=orange&left_text=EIVideo%20User)](https://pepy.tech/project/eivideo)\n[![Downloads](https://static.pepy.tech/personalized-badge/qeivideo?period=total&units=international_system&left_color=grey&right_color=orange&left_text=QEIVideo%20User)](https://pepy.tech/project/qeivideo)\n![GitHub release (latest by date including pre-releases)](https://img.shields.io/github/v/release/QPT-Family/EIVideo?include_prereleases)\n![GitHub forks](https://img.shields.io/github/forks/QPT-Family/EIVideo)\n![GitHub Repo stars](https://img.shields.io/github/stars/QPT-Family/EIVideo)\n![GitHub](https://img.shields.io/github/license/QPT-Family/EIVideo)\n![](https://img.shields.io/badge/%E6%B7%B1%E5%BA%A6%E9%80%82%E9%85%8D->Win7-9cf)\n---\n<div align=\"center\">\n<img width=\"600\" alt=\"图片\" src=\"https://user-images.githubusercontent.com/46156734/148925774-a04b641c-6a71-43ed-a7c0-d4b66e8d6e8a.png\">",
        "type": "code",
        "location": "/applications/EIVideo/README.md:1-15"
    },
    "929": {
        "file_id": 86,
        "content": "EIVideo is an interactive intelligent video annotation tool, available for Windows systems starting from Win7. It has downloadable packages for both EIVideo and QEIVideo users, with options to choose pre-releases or the latest stable version. The tool features a user-friendly interface and is actively maintained under the QPT-Family organization on GitHub, with an open license for use.",
        "type": "comment"
    },
    "930": {
        "file_id": 86,
        "content": "</div>\nEIVideo，基于百度飞桨MA-Net交互式视频分割模型打造的交互式**智能视频**标注工具箱，只需简单标注几帧，即可完成全视频标注，若自动标注结果未达要求还可通过多次和视频交互而不断提升视频分割质量，直至对分割质量满意。  \n戳 -> 了解相关[技术文章&模型原理](等待微信公众号)\n<div align=\"center\">\n<img width=\"300\" alt=\"图片\" src=\"https://ai-studio-static-online.cdn.bcebos.com/f792bac0dd3b4f44ade7d744b58e908e2a85ed8718b541cfb6b2ce9fc8ad4374\">\n</div>\n> 为了更好的解放双手，我们还提供了图形化界面工具QEIVideo，通过它我们可以不使用繁杂的命令方式来完成视频的智能标注工作。\n---\n### README目录\n- [EAP - The Early Access Program 早期访问计划](#eap---the-early-access-program-早期访问计划)\n- [使用方式](#使用方式)\n  - [安装&运行](#安装运行)\n    - [QPT包 - 适合无Python基础用户](#qpt包---适合无python基础用户)\n    - [标准Python包 - 适合普通Python开发者](#标准python包---适合普通python开发者)\n    - [开发版本 - 适合高阶开发者进行开发/社区贡献](#开发版本---适合高阶开发者进行开发社区贡献)\n- [(Q)EIVideo产品规划安排](#qeivideo产品规划安排)\n- [开源协议](#开源协议)\n---\n### EAP - The Early Access Program 早期访问计划\n> Warning 当前图形化界面QEIVideo处于**极其初阶**的...建设阶段，并不能保证程序稳定性。\n<div align=\"center\"> <img width=\"100\" alt=\"图片\" src=\"https://user-images.githubusercontent.com/46156734/148927601-791362c0-0286-4fb9-b9d1-c193f7485de1.png\"> </div>\n当您选择使用QEIVideo作为图形化界面时，即可视为同意使用“可能会存在大量体验不佳”的EAP产品。",
        "type": "code",
        "location": "/applications/EIVideo/README.md:16-49"
    },
    "931": {
        "file_id": 86,
        "content": "EIVideo: Interactive intelligent video annotation toolbox, based on the Baidu Paddle MA-Net interactive video segmentation model. Can complete full video annotation with simple frame tagging. Improves video segmentation quality through multiple interactions with the video.",
        "type": "comment"
    },
    "932": {
        "file_id": 86,
        "content": "同样，您可选择借助基于[PaddleVideo](https://github.com/PaddlePaddle/PaddleVideo) 实现的\n交互式视频标注模型[EIVideo](https://github.com/QPT-Family/EIVideo/EIVideo) 进行二次开发，在此之上也可完成您需要的自定义图形化界面，后续也将提供二次开发指南。\n<div align=\"center\"> <img width=\"100\" alt=\"图片\" src=\"https://user-images.githubusercontent.com/46156734/148928046-b1490080-52f0-4a15-b7ff-11d54b135039.png\"> </div>\n> 如果您愿意参与到EIVideo或QEIVideo的建设中来，欢迎您与PMC取得联系 -> WX:GT_ZhangAcer  \n## 使用方式\n### 安装&运行\n#### QPT包 - 适合无Python基础用户\n自动化配置相关Python环境，但仅支持Windows7/10/11操作系统，且不对盗版Windows7做任何适配。  \n下载地址：暂未上传\n> 自动化部署工具由[QPT - 自动封装工具](https://github.com/QPT-Family/QPT) 支持  \n#### 标准Python包 - 适合普通Python开发者\n* 国际方式：\n  ```shell\n  python -m pip install eivideo\n  python qeivideo\n  ```\n* 国内推荐：\n  ```shell\n  python -m pip install eivideo -i https://mirrors.bfsu.edu.cn/pypi/web/simple\n  python qeivideo\n  ```\n> 上述命令仅适用于常规情况，若您安装了多个Python或修改了相关开发工具与配置，请自行修改相关命令使其符合您的开发环境。\n#### 开发版本 - 适合高阶开发者进行开发/社区贡献\n* 国际方式：\n  ```shell\n  git clone https://github.com/QPT-Family/EIVideo.git\n  python -m pip install -r requirements.txt\n  ```",
        "type": "code",
        "location": "/applications/EIVideo/README.md:51-85"
    },
    "933": {
        "file_id": 86,
        "content": "Code is introducing the user to EIVideo, a customizable interactive video annotation model based on PaddleVideo, with instructions for installation and usage.",
        "type": "comment"
    },
    "934": {
        "file_id": 86,
        "content": "* 国内推荐：\n  ```shell\n  # 请勿用于Push！！！\n  git clone https://hub.fastgit.org/QPT-Family/EIVideo.git\n  python -m pip install -r requirements.txt -i https://mirrors.bfsu.edu.cn/pypi/web/simple\n  ```\n* 运行程序\n  ```shell\n  # 进入工作目录\n  cd 此处填写EIVideo所在的目录的绝对路径，且该目录下拥有EIVideo与QEIVideo两文件夹。\n  # 运行\n  python QEIVideo/start.py\n  # 如运行时无法找到对应包，可选择下述方式添加环境变量来调整索引次序后执行python\n  # Windows\n  set PYTHONPATH=$pwd:$PYTHONPATH\n  # Linux\n  export PYTHONPATH=$pwd:$PYTHONPATH\n  ```\n> 上述命令仅适用于常规情况，若您安装了多个Python或修改了相关开发工具与配置，请自行修改相关命令使其符合您的开发环境。\n## (Q)EIVideo产品规划安排  \n> 由于QEIVideo由飞桨开源社区学生爱好者构成，所以在项目的产出过程中将会以学习为主进行开源贡献，如您原因与我们一同建设，我们也将非常欢迎~\n<div align=\"center\"> <img width=\"100\" alt=\"图片\" src=\"https://user-images.githubusercontent.com/46156734/148928475-b5b340b7-241d-4ddc-8155-70d98c6384a9.png\"> </div>\n- [x] EIVideo与Demo版QEIVideo发布0.1.0Alpha版本\n- [ ] 完善QEIVideo，丰富基础标注功能，于Q1升级至1.0Alpha版本\n- [ ] 回归QEIVideo稳定性，于Q2完成1.0正式版本发版\n- [ ] 增加视频目标检测、分类任务的交互式标注功能。\n### 开源协议\n本项目使用GNU LESSER GENERAL PUBLIC LICENSE(LGPL)开源协议。  \n> 因所使用的模型与数据集等原因，本项目中任一代码、参数均不可直接进行商用，如需商用请与我们取得联系。",
        "type": "code",
        "location": "/applications/EIVideo/README.md:86-119"
    },
    "935": {
        "file_id": 86,
        "content": "This code provides instructions for cloning the EIVideo repository, installing necessary dependencies, and running the QEIVideo application. It also mentions that these commands are suitable for regular cases, and users might need to modify them according to their specific development environment. The code discusses the product roadmap of (Q)EIVideo, including planned features and versions. It also specifies the open-source license used for this project and clarifies that the code and parameters cannot be directly used for commercial purposes without prior consent from the developers.",
        "type": "comment"
    },
    "936": {
        "file_id": 86,
        "content": "### 引用来源\n1. EIVideo模型以及相关源码、论文与项目 - [PaddleVideo](https://github.com/PaddlePaddle/PaddleVideo)\n2. 部分表情包来源 - [甘城なつき](https://www.pixiv.net/users/3036679)",
        "type": "code",
        "location": "/applications/EIVideo/README.md:121-123"
    },
    "937": {
        "file_id": 86,
        "content": "This code block provides the reference sources for the EIVideo model and its related resources, as well as mentioning the origin of some emoji used in the project.",
        "type": "comment"
    },
    "938": {
        "file_id": 87,
        "content": "/applications/EIVideo/resources/QT/demo.ui",
        "type": "filepath"
    },
    "939": {
        "file_id": 87,
        "content": "The code creates a Qt application UI with video display, QGraphicsView and push button, along with complex design elements including buttons, sliders, panels, tabs, labels, and progress bars.",
        "type": "summary"
    },
    "940": {
        "file_id": 87,
        "content": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<ui version=\"4.0\">\n <class>MainWindow</class>\n <widget class=\"QMainWindow\" name=\"MainWindow\">\n  <property name=\"geometry\">\n   <rect>\n    <x>0</x>\n    <y>0</y>\n    <width>800</width>\n    <height>486</height>\n   </rect>\n  </property>\n  <property name=\"minimumSize\">\n   <size>\n    <width>800</width>\n    <height>486</height>\n   </size>\n  </property>\n  <property name=\"maximumSize\">\n   <size>\n    <width>800</width>\n    <height>486</height>\n   </size>\n  </property>\n  <property name=\"windowTitle\">\n   <string>MainWindow</string>\n  </property>\n  <widget class=\"QWidget\" name=\"centralwidget\">\n   <widget class=\"QFrame\" name=\"video_frame\">\n    <property name=\"geometry\">\n     <rect>\n      <x>20</x>\n      <y>20</y>\n      <width>761</width>\n      <height>361</height>\n     </rect>\n    </property>\n    <property name=\"frameShape\">\n     <enum>QFrame::StyledPanel</enum>\n    </property>\n    <property name=\"frameShadow\">\n     <enum>QFrame::Raised</enum>\n    </property>\n    <widget class=\"QGraphicsView\" name=\"graphicsView\">",
        "type": "code",
        "location": "/applications/EIVideo/resources/QT/demo.ui:1-44"
    },
    "941": {
        "file_id": 87,
        "content": "This code defines a user interface for a main window with a central widget, a frame for displaying video content, and a QGraphicsView to render the video. The window has a fixed size of 800x486 pixels.",
        "type": "comment"
    },
    "942": {
        "file_id": 87,
        "content": "     <property name=\"geometry\">\n      <rect>\n       <x>0</x>\n       <y>0</y>\n       <width>761</width>\n       <height>321</height>\n      </rect>\n     </property>\n    </widget>\n    <widget class=\"QFrame\" name=\"frame_2\">\n     <property name=\"geometry\">\n      <rect>\n       <x>0</x>\n       <y>320</y>\n       <width>761</width>\n       <height>41</height>\n      </rect>\n     </property>\n     <property name=\"frameShape\">\n      <enum>QFrame::StyledPanel</enum>\n     </property>\n     <property name=\"frameShadow\">\n      <enum>QFrame::Raised</enum>\n     </property>\n     <widget class=\"QWidget\" name=\"horizontalLayoutWidget\">\n      <property name=\"geometry\">\n       <rect>\n        <x>-1</x>\n        <y>-1</y>\n        <width>761</width>\n        <height>41</height>\n       </rect>\n      </property>\n      <layout class=\"QHBoxLayout\" name=\"horizontalLayout\">\n       <item>\n        <widget class=\"QPushButton\" name=\"open_btn\">\n         <property name=\"text\">\n          <string>打开视频</string>\n         </property>\n        </widget>\n       </item>\n       <item>",
        "type": "code",
        "location": "/applications/EIVideo/resources/QT/demo.ui:45-86"
    },
    "943": {
        "file_id": 87,
        "content": "The code represents a UI layout design for a user interface with a frame, a horizontal layout widget containing a push button labeled \"打开视频\", and possibly other UI elements. The frame is styled as raised panel, has a specified geometry, and the push button serves to open a video file.",
        "type": "comment"
    },
    "944": {
        "file_id": 87,
        "content": "        <widget class=\"QPushButton\" name=\"save_btn\">\n         <property name=\"text\">\n          <string>保存标注</string>\n         </property>\n        </widget>\n       </item>\n       <item>\n        <widget class=\"QSlider\" name=\"horizontalSlider\">\n         <property name=\"orientation\">\n          <enum>Qt::Horizontal</enum>\n         </property>\n        </widget>\n       </item>\n       <item>\n        <widget class=\"QPushButton\" name=\"select_btn\">\n         <property name=\"text\">\n          <string>选择目标</string>\n         </property>\n        </widget>\n       </item>\n       <item>\n        <widget class=\"QPushButton\" name=\"clean_btn\">\n         <property name=\"text\">\n          <string>清空目标</string>\n         </property>\n        </widget>\n       </item>\n       <item>\n        <widget class=\"QPushButton\" name=\"start_btn\">\n         <property name=\"text\">\n          <string>开始推理</string>\n         </property>\n        </widget>\n       </item>\n      </layout>\n     </widget>\n    </widget>\n    <widget class=\"QFrame\" name=\"draw_frame\">\n     <property name=\"geometry\">",
        "type": "code",
        "location": "/applications/EIVideo/resources/QT/demo.ui:87-125"
    },
    "945": {
        "file_id": 87,
        "content": "This code defines a user interface layout with various widgets, including QPushButtons and QSlider. The buttons have text labels in Chinese for \"保存标注\", \"选择目标\", \"清空目标\", and \"开始推理\". The layout is nested within other widgets to create a complex UI design.",
        "type": "comment"
    },
    "946": {
        "file_id": 87,
        "content": "      <rect>\n       <x>0</x>\n       <y>10</y>\n       <width>751</width>\n       <height>301</height>\n      </rect>\n     </property>\n     <property name=\"frameShape\">\n      <enum>QFrame::StyledPanel</enum>\n     </property>\n     <property name=\"frameShadow\">\n      <enum>QFrame::Raised</enum>\n     </property>\n    </widget>\n   </widget>\n   <widget class=\"QTabWidget\" name=\"menu_tab\">\n    <property name=\"geometry\">\n     <rect>\n      <x>20</x>\n      <y>380</y>\n      <width>761</width>\n      <height>81</height>\n     </rect>\n    </property>\n    <property name=\"currentIndex\">\n     <number>0</number>\n    </property>\n    <widget class=\"QWidget\" name=\"tab\">\n     <attribute name=\"title\">\n      <string>状态</string>\n     </attribute>\n     <widget class=\"QLabel\" name=\"act_label\">\n      <property name=\"enabled\">\n       <bool>true</bool>\n      </property>\n      <property name=\"geometry\">\n       <rect>\n        <x>10</x>\n        <y>30</y>\n        <width>71</width>\n        <height>21</height>\n       </rect>\n      </property>\n      <property name=\"text\">",
        "type": "code",
        "location": "/applications/EIVideo/resources/QT/demo.ui:126-169"
    },
    "947": {
        "file_id": 87,
        "content": "The code defines a user interface layout with a panel, tab widget, and label. The panel has dimensions, frame shape, and shadow properties set. The tab widget contains a single enabled tab named \"状态\" (Chinese for \"Status\") and has a label inside it with specific geometry and text settings.",
        "type": "comment"
    },
    "948": {
        "file_id": 87,
        "content": "       <string>当前状态：</string>\n      </property>\n     </widget>\n     <widget class=\"QLabel\" name=\"act_info_label\">\n      <property name=\"enabled\">\n       <bool>true</bool>\n      </property>\n      <property name=\"geometry\">\n       <rect>\n        <x>80</x>\n        <y>30</y>\n        <width>81</width>\n        <height>21</height>\n       </rect>\n      </property>\n      <property name=\"text\">\n       <string>-------------</string>\n      </property>\n     </widget>\n     <widget class=\"QProgressBar\" name=\"act_progressbar\">\n      <property name=\"geometry\">\n       <rect>\n        <x>170</x>\n        <y>32</y>\n        <width>521</width>\n        <height>21</height>\n       </rect>\n      </property>\n      <property name=\"value\">\n       <number>24</number>\n      </property>\n     </widget>\n     <widget class=\"QLabel\" name=\"label_3\">\n      <property name=\"enabled\">\n       <bool>true</bool>\n      </property>\n      <property name=\"geometry\">\n       <rect>\n        <x>680</x>\n        <y>30</y>\n        <width>60</width>\n        <height>21</height>\n       </rect>",
        "type": "code",
        "location": "/applications/EIVideo/resources/QT/demo.ui:170-212"
    },
    "949": {
        "file_id": 87,
        "content": "This code snippet represents the UI layout for a user interface, using QT framework. It includes labels, progress bars and their respective properties such as position, size, text and value. The labels display current status, act information, and potentially other relevant data. The progress bar shows progress with a specific value and is likely used to represent the completion of certain tasks or actions.",
        "type": "comment"
    },
    "950": {
        "file_id": 87,
        "content": "      </property>\n      <property name=\"layoutDirection\">\n       <enum>Qt::LeftToRight</enum>\n      </property>\n      <property name=\"text\">\n       <string>12%</string>\n      </property>\n      <property name=\"alignment\">\n       <set>Qt::AlignRight|Qt::AlignTrailing|Qt::AlignVCenter</set>\n      </property>\n     </widget>\n    </widget>\n    <widget class=\"QWidget\" name=\"tab_2\">\n     <attribute name=\"title\">\n      <string>属性配置</string>\n     </attribute>\n    </widget>\n   </widget>\n  </widget>\n  <widget class=\"QStatusBar\" name=\"statusbar\"/>\n </widget>\n <resources/>\n <connections/>\n</ui>",
        "type": "code",
        "location": "/applications/EIVideo/resources/QT/demo.ui:213-236"
    },
    "951": {
        "file_id": 87,
        "content": "This code represents the user interface layout for a Qt application. It includes various widgets such as labels, buttons and tabs arranged in a specific order with their respective properties and alignment set. The code also specifies the title of each tab.",
        "type": "comment"
    },
    "952": {
        "file_id": 88,
        "content": "/applications/EIVideo/resources/cmd",
        "type": "filepath"
    },
    "953": {
        "file_id": 88,
        "content": "Updating PaddleVideo's EIVideo on GitHub: pushing and pulling development branches, splitting and rejoining code.",
        "type": "summary"
    },
    "954": {
        "file_id": 88,
        "content": "# 更新PaddleVideo上的EIVideo\ngit subtree push --prefix=applications/EIVideo/ https://github.com/QPT-Family/EIVideo 开发分支\ngit subtree pull --prefix=applications/EIVideo/ https://github.com/QPT-Family/EIVideo 开发分支 --squash\ngit subtree split --rejoin --prefix=applications/EIVideo/  --branch 开发分支",
        "type": "code",
        "location": "/applications/EIVideo/resources/cmd:1-4"
    },
    "955": {
        "file_id": 88,
        "content": "Updating PaddleVideo's EIVideo on GitHub: pushing and pulling development branches, splitting and rejoining code.",
        "type": "comment"
    },
    "956": {
        "file_id": 89,
        "content": "/applications/FightRecognition/README.md",
        "type": "filepath"
    },
    "957": {
        "file_id": 89,
        "content": "The README guides using PaddleVideo's Fight Recognition model for detecting fight and non-fight videos across four datasets. It includes data preparation, training, evaluation, exporting, quickstart guidance, and GPU usage control.",
        "type": "summary"
    },
    "958": {
        "file_id": 89,
        "content": "# 打架识别模型\n## 内容\n- [1 快速开始](#快速开始)\n- [2 数据准备](#数据准备)\n    - [2.1 数据集下载](#数据集下载)\n    - [2.2 视频抽帧](#视频抽帧)\n    - [2.3 训练集和验证集划分](#训练集和验证集划分)\n    - [2.4 视频裁剪](#视频裁剪)\n- [3 模型训练](#模型训练)\n- [4 模型评估](#模型评估)\n- [5 模型导出](#模型导出)\n实时行人分析工具[PP-Human](https://github.com/PaddlePaddle/PaddleDetection/tree/release/2.4/deploy/pphuman)中集成了视频分类的打架识别模块。本文档介绍如何基于[PaddleVideo](https://github.com/PaddlePaddle/PaddleVideo/)，完成打架识别模型的训练流程。\n目前打架识别模型使用的是[PP-TSM](https://github.com/PaddlePaddle/PaddleVideo/blob/63c88a435e98c6fcaf353429d2df6cc24b8113ba/docs/zh-CN/model_zoo/recognition/pp-tsm.md)，并在PP-TSM视频分类模型训练流程的基础上修改适配，完成模型训练。\n请先参考[使用说明](https://github.com/XYZ-916/PaddleVideo/blob/develop/docs/zh-CN/usage.md)了解PaddleVideo模型库的使用。\n<a name=\"快速开始\"></a>\n## 1 快速开始\n打架识别静态图模型获取[https://videotag.bj.bcebos.com/PaddleVideo-release2.3/ppTSM_fight.zip](https://videotag.bj.bcebos.com/PaddleVideo-release2.3/ppTSM_fight.zip)。\n打架识别[demo](https://videotag.bj.bcebos.com/PaddleVideo-release2.3/fight_demo.mp4)。\n首先需要将下载好的静态图模型解压并放到`inference`目录下，然后执行下面的命令即可直接判断一个给定的视频中是否存在打架行为：",
        "type": "code",
        "location": "/applications/FightRecognition/README.md:1-29"
    },
    "959": {
        "file_id": 89,
        "content": "This README provides an overview of the Fight Recognition model using PaddleVideo, including sections on quick start, data preparation, model training, evaluation, and model export. The PP-TSM model is used for fight recognition and can be adapted from the existing PP-TSM video classification model training process. Quickstart instructions and download links are provided, along with information on where to find additional usage guidance.",
        "type": "comment"
    },
    "960": {
        "file_id": 89,
        "content": "```\ncd ${PaddleVideo_root}\npython tools/predict.py --input_file fight.avi \\\n                           --config pptsm_fight_frames_dense.yaml \\\n                           --model_file inference/ppTSM/ppTSM.pdmodel \\\n                           --params_file inference/ppTSM/ppTSM.pdiparams \\\n                           --use_gpu=True \\\n                           --use_tensorrt=False\n```\n<a name=\"数据准备\"></a>\n## 2 数据准备\nPP-TSM是一个基于视频片段进行预测的模型。在PaddleVideo中，训练数据为`.mp4`、`.avi`等格式视频或者是抽帧后的视频帧序列，标签则可以是`.txt`格式存储的文件。\n<a name=\"数据集下载\"></a>\n### 2.1 数据集下载\n本项目基于6个公开的打架、暴力行为相关数据集合并后的数据进行模型训练。公开数据集具体信息如下：\n| 数据集 | 下载连接 | 简介 | 标注 | 数量 | 时长 |\n| ---- | ---- | ---------- | ---- | ---- | ---------- |\n|  Surveillance Camera Fight Dataset| https://github.com/sayibet/fight-detection-surv-dataset | 裁剪视频，监控视角 | 视频级别 | 打架：150；非打架：150 | 2s |\n| A Dataset for Automatic Violence Detection in Videos | https://github.com/airtlab/A-Dataset-for-Automatic-Violence-Detection-in-Videos | 裁剪视频，室内自行录制 | 视频级别 | 暴力行为：115个场景，2个机位，共230 ；非暴力行为：60个场景，2个机位，共120 | 几秒钟 |",
        "type": "code",
        "location": "/applications/FightRecognition/README.md:31-55"
    },
    "961": {
        "file_id": 89,
        "content": "This code is executing a Python script named \"predict.py\" in PaddleVideo's root directory, to predict fight events from a video file named 'fight.avi'. It uses the pre-trained pptsm_fight_frames_dense model and sets GPU usage and TensorRT as False.",
        "type": "comment"
    },
    "962": {
        "file_id": 89,
        "content": "| Hockey Fight Detection Dataset | https://www.kaggle.com/datasets/yassershrief/hockey-fight-vidoes?resource=download | 裁剪视频，非真实场景 | 视频级别 | 打架：500；非打架：500 | 2s |\n| Video Fight Detection Dataset | https://www.kaggle.com/datasets/naveenk903/movies-fight-detection-dataset | 裁剪视频，非真实场景 | 视频级别 | 打架：100；非打架：101 | 2s |\n| Real Life Violence Situations Dataset | https://www.kaggle.com/datasets/mohamedmustafa/real-life-violence-situations-dataset | 裁剪视频，非真实场景 | 视频级别 | 暴力行为：1000；非暴力行为：1000 | 几秒钟 |\n| UBI Abnormal Event Detection Dataset| http://socia-lab.di.ubi.pt/EventDetection/ | 未裁剪视频，监控视角 | 帧级别 | 打架：216；非打架：784；裁剪后二次标注：打架1976，非打架1630 | 原视频几秒到几分钟不等，裁剪后2s |\n打架（暴力行为）视频3956个，非打架（非暴力行为）视频3501个，共7457个视频，每个视频几秒钟。\n<a name=\"视频抽帧\"></a>\n### 2.2 视频抽帧\n为了加快训练速度，将视频进行抽帧。\n```bash\ncd ${PaddleVideo_root}\npython data/ucf101/extract_rawframes.py dataset/ rawframes/ --level 2 --ext mp4\n```\n其中，视频存放在`dataset`目录下，打架（暴力）视频存放在`dataset/fight`中；非打架（非暴力）视频存放在`dataset/nofight`中。`rawframes`目录存放抽取的视频帧。\n<a name=\"训练集和验证集划分\"></a>\n### 2.3 训练集和验证集划分",
        "type": "code",
        "location": "/applications/FightRecognition/README.md:56-75"
    },
    "963": {
        "file_id": 89,
        "content": "Code comments:\n- Hockey Fight Detection Dataset: URL, clipped videos, non-realistic scenarios, video level, 500 fight and 500 non-fight videos, 2s duration.\n- Video Fight Detection Dataset: URL, clipped videos, non-realistic scenarios, video level, 100 fights and 101 non-fights, 2s duration.\n- Real Life Violence Situations Dataset: URL, clipped videos, non-realistic scenarios, video level, 1000 fights and 1000 non-fights, a few seconds duration.\n- UBI Abnormal Event Detection Dataset: URL, unclipped videos, surveillance angle, frame level, 216 fights, 784 non-fights, 7,840 frames total, original video durations varying from a few seconds to a few minutes.\n- Extracting rawframes for faster training by running a script in PaddleVideo_root.\n- Split dataset into fight and non-fight videos stored in fight and nofight directories respectively.",
        "type": "comment"
    },
    "964": {
        "file_id": 89,
        "content": "本项目验证集1500条，来自Surveillance Camera Fight Dataset、A Dataset for Automatic Violence Detection in Videos、UBI Abnormal Event Detection Dataset三个数据集。\n也可根据下面的代码将数据按照0.8:0.2的比例划分成训练集和测试集：\n```python\nimport os\nimport glob\nimport random\nimport fnmatch\nimport re\nclass_id = {\n    \"nofight\":0,\n    \"fight\":1\n}\ndef get_list(path,key_func=lambda x: x[-11:], rgb_prefix='img_', level=1):\n    if level == 1:\n        frame_folders = glob.glob(os.path.join(path, '*'))\n    elif level == 2:\n        frame_folders = glob.glob(os.path.join(path, '*', '*'))\n    else:\n        raise ValueError('level can be only 1 or 2')\n    def count_files(directory):\n        lst = os.listdir(directory)\n        cnt = len(fnmatch.filter(lst, rgb_prefix + '*'))\n        return cnt\n    # check RGB\n    video_dict = {}\n    for f in frame_folders:\n        cnt = count_files(f)\n        k = key_func(f)\n        if level==2:\n            k = k.split(\"/\")[0]\n        video_dict[f]=str(cnt)+\" \"+str(class_id[k])\n    return video_dict\ndef fight_splits(video_dict, train_percent=0.8):",
        "type": "code",
        "location": "/applications/FightRecognition/README.md:77-118"
    },
    "965": {
        "file_id": 89,
        "content": "The code reads data from three datasets: Surveillance Camera Fight Dataset, A Dataset for Automatic Violence Detection in Videos, and UBI Abnormal Event Detection Dataset. It also allows for splitting the data into training and testing sets with an 80:20 ratio. The 'get_list' function retrieves the list of files and counts them, while the 'fight_splits' function takes the video dictionary and train percent as inputs to split the data into training and testing sets.",
        "type": "comment"
    },
    "966": {
        "file_id": 89,
        "content": "    videos = list(video_dict.keys())\n    train_num = int(len(videos)*train_percent)\n    train_list = []\n    val_list = []\n    random.shuffle(videos)\n    for i in range(train_num):\n        train_list.append(videos[i]+\" \"+str(video_dict[videos[i]]))\n    for i in range(train_num,len(videos)):\n        val_list.append(videos[i]+\" \"+str(video_dict[videos[i]]))\n    print(\"train:\",len(train_list),\",val:\",len(val_list))\n    with open(\"fight_train_list.txt\",\"w\") as f:\n        for item in train_list:\n            f.write(item+\"\\n\")\n    with open(\"fight_val_list.txt\",\"w\") as f:\n        for item in val_list:\n            f.write(item+\"\\n\")\nframe_dir = \"rawframes\"\nlevel = 2\ntrain_percent = 0.8\nif level == 2:\n    def key_func(x):\n        return '/'.join(x.split('/')[-2:])\nelse:\n    def key_func(x):\n        return x.split('/')[-1]\nvideo_dict = get_list(frame_dir, key_func=key_func, level=level)  \nprint(\"number:\",len(video_dict))\nfight_splits(video_dict, train_percent)\n```\n最终生成fight_train_list.txt和fight_val_list.txt两个文件。打架的标签为1，非打架的标签为0。",
        "type": "code",
        "location": "/applications/FightRecognition/README.md:119-160"
    },
    "967": {
        "file_id": 89,
        "content": "This code generates two lists, one for training and one for validation, based on a provided video dictionary. It then shuffles the list of videos and splits them into train and val lists. The code also defines a key function depending on the level parameter. Finally, it prints the lengths of both lists, writes them to separate files \"fight_train_list.txt\" and \"fight_val_list.txt\", and calls the fight_splits() function with the video dictionary and train percentage as parameters. These two files will contain the labels for training and validation sets, where fight (label 1) and non-fight (label 0) videos are listed separately.",
        "type": "comment"
    },
    "968": {
        "file_id": 89,
        "content": "<a name=\"视频裁剪\"></a>\n### 2.4 视频裁剪\n对于未裁剪的视频，需要先进行裁剪才能用于模型训练，这个给出视频裁剪的函数`cut_video`，输入为视频路径，裁剪的起始帧和结束帧以及裁剪后的视频保存路径。\n```python\nimport cv2\ndef cut_video(video_path, frameToStart, frametoStop, saved_video_path):\n    cap = cv2.VideoCapture(video_path)\n    FPS = cap.get(cv2.CAP_PROP_FPS)\n    #print(\"FPS:\",FPS)\n    TOTAL_FRAME = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))  # 获取视频总帧数\n    #print(\"TOTAL_FRAME:\",TOTAL_FRAME)\n    size = (cap.get(cv2.CAP_PROP_FRAME_WIDTH), cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    #print(\"size:\",size)\n    videoWriter =cv2.VideoWriter(saved_video_path,apiPreference = 0,fourcc = cv2.VideoWriter_fourcc(*'mp4v'),fps=FPS,\n            frameSize=(int(size[0]),int(size[1])))\n    COUNT = 0\n    while True:\n            success, frame = cap.read()\n            if success:\n                COUNT += 1\n                if COUNT <= frametoStop and COUNT > frameToStart:  # 选取起始帧\n                    videoWriter.write(frame)\n            else:\n                print(\"cap.read failed!\")\n                break\n            if COUNT > frametoStop:",
        "type": "code",
        "location": "/applications/FightRecognition/README.md:162-192"
    },
    "969": {
        "file_id": 89,
        "content": "The code defines a function `cut_video` which takes a video path, start and stop frame numbers, and a saved video path. It uses OpenCV to read the input video, determine its FPS, total frames, and size. The function then creates a new VideoWriter object with the specified output file name, fourcc codec, and same FPS as the input video. It writes only the frames between the start and stop frame numbers to the new video file.",
        "type": "comment"
    },
    "970": {
        "file_id": 89,
        "content": "                break\n    cap.release()\n    videoWriter.release()\n    print(saved_video_path)\n```\n<a name=\"模型训练\"></a>\n## 3 模型训练\n下载预训练模型：\n```bash\nwget https://videotag.bj.bcebos.com/PaddleVideo/PretrainModel/ResNet50_vd_ssld_v2_pretrained.pdparams\n```\n模型训练：\n```bash\n# 单卡训练\ncd ${PaddleVideo_root}\npython main.py --validate -c pptsm_fight_frames_dense.yaml\n```\n```bash\ncd ${PaddleVideo_root}\n# 多卡训练\nexport CUDA_VISIBLE_DEVICES=0,1,2,3\npython -B -m paddle.distributed.launch --gpus=“0,1,2,3” \\\n   --log_dir=log_pptsm_dense  main.py  --validate \\\n   -c pptsm_fight_frames_dense.yaml\n```\n<a name=\"模型评估\"></a>\n## 4 模型评估\n训练好的模型下载：[https://videotag.bj.bcebos.com/PaddleVideo-release2.3/ppTSM_fight.pdparams](https://videotag.bj.bcebos.com/PaddleVideo-release2.3/ppTSM_fight.pdparams)\n模型评估：\n```bash\ncd ${PaddleVideo_root}\npython main.py --test -c pptsm_fight_frames_dense.yaml \\\n   -w ppTSM_fight_best.pdparams\n```\n其中`ppTSM_fight_best.pdparams`为训练好的模型。\n<a name=\"模型导出\"></a>\n## 5 模型导出\n导出inference模型：\n```bash\ncd ${PaddleVideo_root}\npython tools/export_model.py -c pptsm_fight_frames_dense.yaml \\",
        "type": "code",
        "location": "/applications/FightRecognition/README.md:193-245"
    },
    "971": {
        "file_id": 89,
        "content": "This code represents the final part of the model training process. The first line is a break statement which implies the end of a loop or condition block. Following that, it releases the `cap` and `videoWriter` objects, suggesting they were used for capturing and writing video data respectively. The last line prints out the saved video path.",
        "type": "comment"
    },
    "972": {
        "file_id": 89,
        "content": "                                -p ppTSM_fight_best.pdparams \\\n                                -o inference/ppTSM\n```",
        "type": "code",
        "location": "/applications/FightRecognition/README.md:246-248"
    },
    "973": {
        "file_id": 89,
        "content": "This code is loading a pre-trained model, \"ppTSM_fight_best.pdparams\", and saving the inference output to the \"inference/ppTSM\" directory.",
        "type": "comment"
    },
    "974": {
        "file_id": 90,
        "content": "/applications/FigureSkating/README.md",
        "type": "filepath"
    },
    "975": {
        "file_id": 90,
        "content": "This code provides a guide for using OpenPose to process figure skating action data by converting videos into bone point data, suitable for model training and prediction in the PaddleVideo framework.",
        "type": "summary"
    },
    "976": {
        "file_id": 90,
        "content": "# 花样滑冰动作识别\n---\n## 内容\n- [视频数据处理方法](#视频数据处理方法)\n- [模型训练预测方法](#模型训练预测方法)\n<div align=\"center\">\n  <img src=\"Alex.gif\" width=250/></div>\n### 视频数据处理方法\n - 提供从视频中提取骨骼点数据的方法，方便用户自行提取数据进行测试。\n 花样滑冰数据提取采用了openpose，通过其提供的demo或是相应的api来实现数据的提取，因此需要用户配置openpose环境。\n 如下是通过花样滑冰数据集构建项目[Skeleton Scripts](https://github.com/HaxiSnake/skeleton_scripts)提取骨骼点数据方法的具体介绍。\n #### step1 安装openpose\n - 参考：https://github.com/CMU-Perceptual-Computing-Lab/openpose  \n #### step2 测试openpose提供demo\n - 这里通过测试openpose的demo程序来验证是否安装成功。\n demo1：检测视频中身体骨骼点（以linux系统为例）：\n ```bash\n ./build/examples/openpose/openpose.bin --video examples_video.avi --write_json output/ --display 0 --render_pose 0\n ```\n 执行成功之后会在output/路径下生成视频每一帧骨骼点数据的json文件。\n demo2：检测视频中身体+面部+手部骨骼点（以linux系统为例）：\n ```bash\n ./build/examples/openpose/openpose.bin --video examples_video.avi --write_json output/ --display 0 --render_pose 0 --face --hand\n ```\n 执行成功之后会在output/路径下生成视频每一帧身体+面部+手部骨骼点数据的json文件。\n #### step3 视频及相关信息处理\n - 由于[Skeleton Scripts](https://github.com/HaxiSnake/skeleton_scripts)为制作花样滑冰数据集所用，因此此处步骤可能存在不同程度误差，实际请用户自行调试代码。",
        "type": "code",
        "location": "/applications/FigureSkating/README.md:1-46"
    },
    "977": {
        "file_id": 90,
        "content": "This code is a guide for processing figure skating action data using openpose, a tool for detecting body skeletons from videos. It includes instructions on how to install and test openpose, as well as specific steps for processing video data with the Skeleton Scripts project.",
        "type": "comment"
    },
    "978": {
        "file_id": 90,
        "content": " 将要转化的花样滑冰视频储存到[Skeleton Scripts](https://github.com/HaxiSnake/skeleton_scripts)的指定路径（可自行创建）：\n ```bash\n ./skating2.0/skating63/\n ```\n 同时需要用户自行完成对视频信息的提取，保存为label_skating63.csv文件，储存到如下路径中（可自行创建）：\n ```bash\n ./skating2.0/skating63/\n ./skating2.0/skating63_openpose_result/\n ```\n label_skating63.csv中格式如下：\n | 动作分类 | 视频文件名 | 视频帧数 | 动作标签 |\n | :----: | :----: | :----: | :---- |\n 此处用户只需要输入视频文件名（无需后缀，默认后缀名为.mp4，其他格式需自行更改代码)，其他三项定义为空字符串即可，不同表项之间通过 ',' 分割。\n #### step4 执行skating_convert.py:\n - 注意，这一步需要根据用户对openpose的配置进行代码的更改，主要修改项为openpose路径、openpose-demo路径等，具体详见代码。\n 本脚步原理是调用openpose提供的demo提取视频中的骨骼点，并进行数据格式清洗，最后将每个视频的提取结果结果打包成json文件，json文件储存在如下路径：\n ```bash\n ./skating2.0/skating63_openpose_result/label_skating63_data/\n ```\n #### step5 执行skating_gendata.py:\n 将json文件整理为npy文件并保存，多个视频文件将保存为一个npy文件，保存路径为：\n ```bash\n ./skating2.0/skating63_openpose_result/skeleton_file/\n ```\n - 通过上述步骤就可以将视频数据转化为无标签的骨骼点数据。\n - 最后用户只需将npy数据输入送入网络开始模型测试，亦可通过预测引擎推理。\n ### 模型训练预测方法\n 模型使用方法参考[ST-GCN模型文档](../../docs/zh-CN/model_zoo/recognition/stgcn.md)",
        "type": "code",
        "location": "/applications/FigureSkating/README.md:48-92"
    },
    "979": {
        "file_id": 90,
        "content": "The code is outlining the steps to convert figure skating videos into bone point data, which can then be used for model training and prediction. This involves specifying the video storage paths, extracting video information, using OpenPose to process the videos, and saving the results as npy files. Finally, users can input these npy files into a model or prediction engine. The code is specifically for FigureSkating application in PaddleVideo codebase.",
        "type": "comment"
    },
    "980": {
        "file_id": 91,
        "content": "/applications/FootballAction/README.md",
        "type": "filepath"
    },
    "981": {
        "file_id": 91,
        "content": "The FootballAction model in PaddleVideo employs PP-TSM, BMN, and Attention LSTM for feature extraction and classification/regression. The code updates the recognizer2d.py file, exports PP-TSM inference models, creates datasets, and predicts BMN proposal information. The Attention LSTM model is trained with improvements, resulting in accuracy and F1-score enhancements.",
        "type": "summary"
    },
    "982": {
        "file_id": 91,
        "content": "# 足球动作检测模型\n## 内容\n- [1. 模型简介](#1-模型简介)\n- [2. 环境准备](#2-环境准备)\n- [3. 数据准备](#3-数据准备)\n    - [3.1 数据集简介](#31-数据集简介)\n    - [3.2 数据集下载](#32-数据集下载)\n    - [3.3 数据预处理](#33-数据预处理)\n- [4. 快速体验](#4-快速体验)\n- [5. 进阶使用](#5-进阶使用)\n    - [5.1 模型训练](#51-模型训练)\n    - [5.2 模型推理](#52-模型推理)\n    - [5.3 模型评估](#53-模型评估)\n    - [5.4 模型优化](#54-模型优化)\n    - [5.5 模型部署](#55-模型部署)\n- [6. 参考论文](#6-参考论文)\n<a name=\"模型简介\"></a>\n## 1. 模型简介\nFootballAction是基于PaddleVideo实现的足球动作检测算法，用于从足球比赛视频中定位出精彩动作片段发生的起止时间和对应的动作类别。可以定位的足球动作类型包括8种，分别为：\n```txt\n背景、进球、角球、任意球、黄牌、红牌、换人、界外球\n```\n我们提出的方案结合PP-TSM、BMN和AttentionLSTM三个模型，图像和音频两种模态进行动作检测，算法整体流程共分为以下三步：\n - 特征抽取\n    - 图像特性：PP-TSM\n    - 音频特征：VGGish\n - proposal提取：BMN\n - 动作分类 + 回归：AttentionLSTM\nAIStudio项目： [基于PP-TSM+BMN+AttentionLSTM实现足球精彩时刻剪辑](https://aistudio.baidu.com/aistudio/projectdetail/3473391?channelType=0&channel=0)\n<a name=\"环境准备\"></a>\n## 2. 环境准备\n- PaddleVideo模型库依赖安装请参考 [安装说明](../../docs/zh-CN/install.md)\n<a name=\"数据准备\"></a>\n## 3. 数据准备\n<a name=\"数据集简介\"></a>\n### 3.1 数据集简介\n数据集来自欧洲杯2016，共49个足球视频，其中训练集44个，验证集5个。\n- 数据集label格式\n```\n{\n    \"0\": \"背景\",",
        "type": "code",
        "location": "/applications/FootballAction/README.md:1-54"
    },
    "983": {
        "file_id": 91,
        "content": "This is a README for the FootballAction model in PaddleVideo, introducing a soccer action detection algorithm. It consists of sections on model introduction, environment preparation, data preparation (including dataset details), quick experience, advanced usage, references, and installation instructions. The model uses PP-TSM, BMN, and AttentionLSTM for feature extraction, proposal extraction, and action classification/regression from image and audio modalities. The dataset is derived from the 2016 European Cup, with 49 videos in total (44 training, 5 validation).",
        "type": "comment"
    },
    "984": {
        "file_id": 91,
        "content": "    \"1\": \"进球\",\n    \"2\": \"角球\",\n    \"3\": \"任意球\",\n    \"4\": \"黄牌\",\n    \"5\": \"红牌\",\n    \"6\": \"换人\",\n    \"7\": \"界外球\",\n}\n```\n- 数据集标注文件:\n```txt\ndatasets/EuroCup2016/label_cls8_train.json\ndatasets/EuroCup2016/label_cls8_val.json\n```\n- 数据集gts处理, 将原始标注数据处理成如下json格式\n```\n{\n    'fps': 5,\n    'gts': [\n        {\n            'url': 'xxx.mp4',\n            'total_frames': 6341,\n            'actions': [\n                {\n                    \"label_ids\": [7],\n                    \"label_names\": [\"界外球\"],\n                    \"start_id\": 395,\n                    \"end_id\": 399\n                },\n                ...\n            ]\n        },\n        ...\n    ]\n}\n```\n<a name=\"数据集下载\"></a>\n### 3.2 数据集下载\n数据集下载链接: [dataset_url.list](./datasets/EuroCup2016/dataset_url.list)\n可使用如下脚本下载：\n```\ncd datasets/EuroCup2016 && sh download_dataset.sh\n```\n<a name=\"数据预处理\"></a>\n### 3.3 数据预处理\n- 数据集抽帧, 由mp4, 得到frames和pcm, 这里需要添加ffmpeg环境\n```\ncd datasets/script && python get_frames_pcm.py\n```\n经过以上步骤，得到的代码结构如下所示：\n```\n|-- FootballAction\n   |--  checkpoints                # 模型存放路径\n   |--  datasets                   # 数据集和数据处理脚本",
        "type": "code",
        "location": "/applications/FootballAction/README.md:55-118"
    },
    "985": {
        "file_id": 91,
        "content": "This code defines a dictionary where each key corresponds to an action in the football game, such as \"进球\" or \"角球\". The dataset file contains these labeled examples of actions for training and validation purposes. The data preprocessing step involves handling these labels, creating a JSON format file containing frames per second (fps) and ground truth (gts) data with respective video URLs, total frames, and action information including label IDs, names, start and end frame indices. It also mentions that the dataset can be downloaded using a provided script, and that the code structure is organized in a specific way.",
        "type": "comment"
    },
    "986": {
        "file_id": 91,
        "content": "        |--  EuroCup2016           # 数据存放路径\n            |--  feature_bmn       # bmn提取到的proposal\n            |--  features          # image和audio特征, image fps=5, audio 每秒(1024)\n            |--  input_for_bmn     # bmn训练的输入数据，widows=40\n            |--  input_for_lstm    # lstm训练的输入数据\n            |--  input_for_pptsm    # pptsm训练的数据数据\n            |--  mp4               # 原始视频.mp4\n            |--  frames            # 图像帧, fps=5, '.jpg'格式\n            |--  pcm               # 音频pcm, 音频采样率16000，采用通道数1\n            |--  url.list          # 视频列表\n            |--  url_val.list          # 视频列表\n            |--  label_cls8_train.json  # 训练集原始gts\n            |--  label_cls8_val.json    # 验证集原始gts\n            |--  label.json        # 动作label\n        |--  script                # 数据集处理脚本\n    |--  predict                   # 模型预测代码\n    |--  extractor                 # 特征提取脚本\n    |--  train_lstm                # lstm训练代码\n    |--  train_proposal            # pptsm、bmn训练代码\n        |--  configs               # pptsm、bmn配置文件\n```\n<a name=\"快速体验\"></a>",
        "type": "code",
        "location": "/applications/FootballAction/README.md:119-141"
    },
    "987": {
        "file_id": 91,
        "content": "This directory contains data and scripts related to the FootballAction dataset. It includes original MP4 videos, their image frames, audio PCM files, URL lists, and JSON files for ground truth labels and classifications. There are also separate folders for scripting data processing, feature extraction, model training (LSTM), and proposal-based object detection (PPTSM and BDN). The configs folder contains the configuration files needed for these training scripts.",
        "type": "comment"
    },
    "988": {
        "file_id": 91,
        "content": "## 4. 快速体验\n首先，通过以下命令，下载训练好的模型文件：\n```bash\ncd checkpoints\nsh  download.sh\n```\n运行预测代码：\n```\ncd ${FootballAction_root}/predict && python predict.py\n```\n产出文件：results.json\n<a name=\"进阶使用\"></a>\n## 5. 进阶使用\n<a name=\"模型训练\"></a>\n### 5.1 模型训练\n采样方式：\n- image 采样频率fps=5，如果有些动作时间较短，可以适当提高采样频率\n- BMN windows=200，即40s，所以测试自己的数据时，视频时长需大于40s\n请先参考[使用说明](../../docs/zh-CN/usage.md)了解PaddleVideo模型库的使用。\n#### step1 PP-TSM训练\nPP-TSM模型使用文档参考[PP-TSM](../../docs/zh-CN/model_zoo/recognition/pp-tsm.md)\n##### step1.1  PP-TSM 训练数据处理\n使用如下命令结合frames和gts生成训练所需要的正负样本:\n```bash\ncd datasets/script && python get_instance_for_pptsm.py\n```\n完成该步骤后，数据存储位置\n```\n   |--  datasets                   # 数据集和数据处理脚本\n        |--  EuroCup2016           # 数据存放路径\n            |--  input_for_pptsm   # pptsm训练的数据\n```\n文件按照如下格式命名：\n```\n'{}_{}_{}_{}'.format(video_basename, start_id, end_id, label)\n```\n##### step1.2 PP-TSM模型训练\n训练启动命令如下：\n```bash\ncd ${FootballAction_root}\ncd ../..  #进入PaddleVideo目录下\npython -B -m paddle.distributed.launch \\\n    --gpus=\"0,1,2,3\" \\\n    --log_dir=./football/logs_pptsm \\\n    main.py  \\",
        "type": "code",
        "location": "/applications/FootballAction/README.md:142-201"
    },
    "989": {
        "file_id": 91,
        "content": "This code explains the steps to download a pre-trained model, run prediction, and perform advanced usage including training PP-TSM for the FootballAction application in PaddleVideo. It requires following specific commands and using provided scripts.",
        "type": "comment"
    },
    "990": {
        "file_id": 91,
        "content": "    --validate \\\n    -c applications/FootballAction/train_proposal/configs/pptsm_football_v2.0.yaml  \\\n    -o output_dir=./football/pptsm\n```\n我们也提供了训练好的PP-TSM模型，下载链接已在快速体验章节中给出。\n##### step1.3 导出PP-TSM推理模型\n在转为预测模式前，需要修改 `PaddleVideo/paddlevideo/modeling/framework/recognizers/recognizer2d.py` 文件，将 init 和 infer_step 函数分别更新为如下代码：\n```python\n    def __init__(self, backbone=None, head=None):\n        super().__init__(backbone=backbone, head=head)\n        self.avgpool2d = paddle.nn.AdaptiveAvgPool2D((1, 1), data_format='NCHW')\n    def infer_step(self, data_batch):\n        \"\"\"Define how the model is going to test, from input to output.\"\"\"\n        imgs = data_batch[0]\n        imgs = paddle.reshape_(imgs, [-1] + list(imgs.shape[2:]))\n        feature = self.backbone(imgs)\n        feat = self.avgpool2d(feature)\n        return feat\n```\n再执行如下命令：\n```bash\ncd ${PaddleVideo_root}\npython tools/export_model.py -c applications/FootballAction/train_proposal/configs/pptsm_football_v2.0.yaml \\\n                             -p ./football/pptsm/ppTSM_best.pdparams \\",
        "type": "code",
        "location": "/applications/FootballAction/README.md:202-230"
    },
    "991": {
        "file_id": 91,
        "content": "This code is updating the `recognizer2d.py` file in PaddleVideo, modifying the `__init__` and `infer_step` functions. Then it executes a command to export the PP-TSM inference model using the provided configuration file and best model parameters from previous training.",
        "type": "comment"
    },
    "992": {
        "file_id": 91,
        "content": "                             -o ./football/inference_model\n```\n#####  step1.4  基于PP-TSM的视频特征提取\n将 `PaddleVideo/applications/FootballAction/predict/action_detect/models/pptsm_infer.py` 文件中41行的\n```python\nself.output_tensor = self.predictor.get_output_handle(output_names[1])\n```\n替换为\n```python\nself.output_tensor = self.predictor.get_output_handle(output_names[0])\n```\n使用如下命令进行image和audio特征的提取，默认使用下载的模型进行特征提取，如果使用自己数据训练的模型，请注意修改配置文件中模型的文件路径:\n```bash\ncd ${FootballAcation}\ncd extractor && python extract_feat.py\n```\n完成该步骤后，数据存储位置\n```\n   |--  datasets                   # 训练数据集和处理脚本\n        |--  EuroCup2016            # 数据集\n            |--  features          # 视频的图像+音频特征\n```\n推理特征以pkl文件保存，格式如下：\n```txt\n# 特征维度, image(2048) + audio(1024)\nvideo_features = {'image_feature': np_image_features,\n                  'audio_feature': np_audio_features}\n```\n此特征接下来会用于BMN模型的训练。\n#### step2 BMN训练\nBMN模型使用文档参考[BMN](../../docs/zh-CN/model_zoo/localization/bmn.md)\n##### step2.1 BMN训练数据处理\n使用如下命令得到BMN训练所需要的数据集，默认使用windows=40，根据gts和特征得到训练所需的proposal：\n```bash",
        "type": "code",
        "location": "/applications/FootballAction/README.md:231-275"
    },
    "993": {
        "file_id": 91,
        "content": "In this code snippet, we are replacing a line of code in `pptsm_infer.py` to change the output tensor from the second to the first output name. This is followed by commands to extract image and audio features using the modified code, which are stored in the \"features\" folder within the respective dataset. These features will be used in the training of a BMN model.",
        "type": "comment"
    },
    "994": {
        "file_id": 91,
        "content": "cd FootballAction/datasets/script && python get_instance_for_bmn.py\n```\n完成该步骤后，数据存储位置\n```\n   |--  datasets                   # 训练数据集和处理脚本\n        |--  EuroCup2016            # 数据集\n            |--  input_for_bmn     # bmn训练的proposal\n                |--  feature\n                |--  label.json  \n```\n特征文件保存在`label.json`文件中，数据格式如下：\n```txt\n{\n    \"719b0a4bcb1f461eabb152298406b861_753_793\": {\n        \"duration_second\": 40.0,\n        \"duration_frame\": 200,\n        \"feature_frame\": 200,\n        \"subset\": \"train\",\n        \"annotations\": [\n            {\n                \"segment\": [\n                    15.0,\n                    22.0\n                ],\n                \"label\": \"3.0\",\n                \"label_name\": \"任意球\"\n            }\n        ]\n    },\n    ...\n}\n```\n##### step2.2  BMN模型训练\n训练启动命令如下：\n```bash\npython -B -m paddle.distributed.launch \\\n     --gpus=\"0,1\" \\\n     --log_dir=./football/logs_bmn \\\n     main.py  \\\n     --validate \\\n     -c applications/FootballAction/train_proposal/configs/bmn_football_v2.0.yaml \\\n     -o output_dir=./football/bmn",
        "type": "code",
        "location": "/applications/FootballAction/README.md:276-320"
    },
    "995": {
        "file_id": 91,
        "content": "This code changes the directory to \"FootballAction/datasets/script\" and runs a Python script named get_instance_for_bmn.py, which creates a dataset for BMN (Bounding Box Regression) model training. The resulting data is stored in the datasets folder with the instance information saved as JSON files within the input_for_bmn directory.",
        "type": "comment"
    },
    "996": {
        "file_id": 91,
        "content": "```\n我们也提供了训练好的BMN模型，下载链接已在快速体验章节中给出。\n##### step2.3 导出BMN推理模型\n模型导出命令如下:\n```bash\npython tools/export_model.py -c applications/FootballAction/train_proposal/configs/bmn_football_v2.0.yaml \\\n                              -p ./football/bmn/BMN_epoch_00016.pdparams \\\n                               -o ./football/inference_model\n```\n##### step2.4  BMN模型预测\n使用如下命令进行预测，得到动作proposal信息： start_id, end_id, score。如果使用自己数据训练的模型，请注意修改配置文件中模型的文件路径:\n```\ncd extractor && python extract_bmn.py\n```\n完成该步骤后，数据存储位置\n```\n   |--  datasets                   # 训练数据集和处理脚本\n        |--  EuroCup2016            # 数据集\n            |--  feature_bmn\n                 |--  prop.json    # bmn 预测结果\n```\n预测结果数据格式如下：\n```txt\n[\n    {\n        \"video_name\": \"c9516c903de3416c97dae91a59e968d7\",\n        \"num_proposal\": 5534,\n        \"bmn_results\": [\n            {\n                \"start\": 7850.0,\n                \"end\": 7873.0,\n                \"score\": 0.77194699622342\n            },\n            {\n                \"start\": 4400.0,\n                \"end\": 4443.0,\n                \"score\": 0.7663803287641536",
        "type": "code",
        "location": "/applications/FootballAction/README.md:321-362"
    },
    "997": {
        "file_id": 91,
        "content": "Step 2.3: Export BMN inference model with command `python tools/export_model.py -c applications/FootballAction/train_proposal/configs/bmn_football_v2.0.yaml -p ./football/bmn/BMN_epoch_00016.pdparams -o ./football/inference_model`.\nStep 2.4: Use command `cd extractor && python extract_bmn.py` to predict BMN proposal information.",
        "type": "comment"
    },
    "998": {
        "file_id": 91,
        "content": "            },\n            ...\n        ]\n    },\n    ...\n]\n```\n#### step3 LSTM训练\nAttentionLSTM模型使用文档参考[AttentionLSTM](../../docs/zh-CN/model_zoo/localization/bmn.md)，此处我们对原始对AttentionLSTM模型进行了改进，包括：\n1. 不同模态特征在LSTM中使用不同的hiddne_size\n2. 加入了一个回归分支用于回归iou\n3. 模型中加入了BN层抑制过拟合\n##### step3.1  LSTM训练数据处理\n将BMN得到的proposal截断并处理成LSTM训练所需数据集。同理，注意数据集文件修改路径。\n```\ncd datasets/script && python get_instance_for_lstm.py\n```\n完成该步骤后，数据存储位置\n```\n   |--  datasets                    # 训练数据集和处理脚本\n        |--  EuroCup2016            # 数据集\n            |--  input_for_lstm     # lstm训练的proposal\n                ├── feature         # 特征\n                ├── label_info.json # 标签信息\n                ├── train.txt       # 训练文件列表\n                └── val.txt         # 测试文件列表\n```\n- `label_info.json`数据格式如下：\n```\n{\n    \"fps\": 5,\n    \"results\": [\n        {\n            \"url\": \"https://xxx.mp4\",\n            \"mode\": \"train\",        # train or validation\n            \"total_frames\": 6128,\n            \"num_gts\": 93,\n            \"num_proposals\": 5043,\n            \"proposal_actions\": [",
        "type": "code",
        "location": "/applications/FootballAction/README.md:363-408"
    },
    "999": {
        "file_id": 91,
        "content": "This code is part of the Attention LSTM model training process in the FootballAction application. It mentions a few improvements made to the original AttentionLSTM model, such as using different hidden sizes for different modal features and adding a regression branch for IOU. The code also discusses processing training data for LSTM training and provides an example of the label_info.json format.",
        "type": "comment"
    }
}