{
    "8600": {
        "file_id": 634,
        "content": "        popd\n    elif [ ${model_name} == \"SlowFast\" ]; then\n        # pretrain lite train data\n        pushd ./data/k400\n        wget -nc https://videotag.bj.bcebos.com/Data/k400_videos_small.tar\n        tar -xf k400_videos_small.tar\n        popd\n    elif [ ${model_name} == \"BMN\" ]; then\n        # pretrain lite train data\n        pushd ./data\n        mkdir bmn_data\n        cd bmn_data\n        wget -nc https://paddlemodels.bj.bcebos.com/video_detection/bmn_feat.tar.gz\n        tar -xf bmn_feat.tar.gz\n        wget -nc https://paddlemodels.bj.bcebos.com/video_detection/activitynet_1.3_annotations.json\n        wget -nc https://paddlemodels.bj.bcebos.com/video_detection/activity_net_1_3_new.json\n        popd\n    else\n        echo \"Not added into TIPC yet.\"\n    fi\nelif [ ${MODE} = \"whole_infer\" ];then\n    if [ ${model_name} = \"PP-TSM\" ]; then\n        # download pretrained weights\n        wget -nc -P ./data https://videotag.bj.bcebos.com/PaddleVideo-release2.1/PPTSM/ppTSM_k400_uniform.pdparams --no-check-certificate\n    elif [ ${model_name} = \"PP-TSN\" ]; then",
        "type": "code",
        "location": "/test_tipc/prepare.sh:346-370"
    },
    "8601": {
        "file_id": 634,
        "content": "This code is checking the model_name and performing specific actions based on its value. If model_name is \"SlowFast\", it downloads pretrain lite train data for that model. If model_name is \"BMN\", it downloads required datasets for that model. For other model names, it prints a message indicating they are not added to TIPC yet. In the case of MODE being \"whole_infer\", it performs specific actions based on model_name such as downloading pretrained weights for PP-TSM and PP-TSN models.",
        "type": "comment"
    },
    "8602": {
        "file_id": 634,
        "content": "        # download pretrained weights\n        wget -nc -P ./data https://videotag.bj.bcebos.com/PaddleVideo-release2.2/ppTSN_k400.pdparams --no-check-certificate\n    elif [ ${model_name} == \"AGCN\" ]; then\n        # download pretrained weights\n        wget -nc -P ./data https://videotag.bj.bcebos.com/PaddleVideo-release2.2/AGCN_fsd.pdparams --no-check-certificate\n    elif [ ${model_name} == \"STGCN\" ]; then\n        # download pretrained weights\n        wget -nc -P ./data https://videotag.bj.bcebos.com/PaddleVideo-release2.2/STGCN_fsd.pdparams --no-check-certificate\n    elif [ ${model_name} == \"TSM\" ]; then\n        # download pretrained weights\n        wget -nc -P ./data https://videotag.bj.bcebos.com/PaddleVideo-release2.1/TSM/TSM_k400.pdparams --no-check-certificate\n    elif [ ${model_name} == \"TSN\" ]; then\n        # download pretrained weights\n        wget -nc -P ./data https://videotag.bj.bcebos.com/PaddleVideo-release2.2/TSN_k400.pdparams --no-check-certificate\n    elif [ ${model_name} == \"TimeSformer\" ]; then",
        "type": "code",
        "location": "/test_tipc/prepare.sh:371-385"
    },
    "8603": {
        "file_id": 634,
        "content": "This code is checking the value of 'model_name' variable and downloading the corresponding pretrained weights for different models using 'wget' command. If model name matches, it retrieves the respective model's file from a specific URL and saves it in the './data' directory without certificate checks.",
        "type": "comment"
    },
    "8604": {
        "file_id": 634,
        "content": "        # download pretrained weights\n        wget -nc -P ./data https://videotag.bj.bcebos.com/PaddleVideo-release2.2/TimeSformer_k400.pdparams --no-check-certificate\n    elif [ ${model_name} == \"AttentionLSTM\" ]; then\n        # download pretrained weights\n        wget -nc -P ./data https://videotag.bj.bcebos.com/PaddleVideo-release2.2/AttentionLSTM_yt8.pdparams --no-check-certificate\n    elif [ ${model_name} == \"SlowFast\" ]; then\n        # download pretrained weights\n        wget -nc -P ./data https://videotag.bj.bcebos.com/PaddleVideo/SlowFast/SlowFast.pdparams --no-check-certificate\n    elif [ ${model_name} == \"BMN\" ]; then\n        # download pretrained weights\n        wget -nc -P ./data https://videotag.bj.bcebos.com/PaddleVideo/BMN/BMN.pdparams --no-check-certificate\n    else\n        echo \"Not added into TIPC yet.\"\n    fi\nfi\nif [ ${MODE} = \"benchmark_train\" ];then\n    ${python} -m pip install -r requirements.txt\n    if [ ${model_name} == \"PP-TSM\" ]; then\n        # pretrain lite train data\n        pushd ./data/k400",
        "type": "code",
        "location": "/test_tipc/prepare.sh:386-406"
    },
    "8605": {
        "file_id": 634,
        "content": "This code downloads pre-trained model weights depending on the specified model name. It uses wget to retrieve the files from specific URLs and saves them in the \"./data\" directory. The code also checks if the MODE is \"benchmark_train\" and installs necessary packages using pip if so. Additionally, it changes the current directory to \"./data/k400\" to prepare for pre-training the Lite train data of PP-TSM model.",
        "type": "comment"
    },
    "8606": {
        "file_id": 634,
        "content": "        wget -nc https://videotag.bj.bcebos.com/Data/k400_rawframes_small.tar\n        tar -xf k400_rawframes_small.tar\n        popd\n        # download pretrained weights\n        wget -nc -P ./data https://videotag.bj.bcebos.com/PaddleVideo/PretrainModel/ResNet50_vd_ssld_v2_pretrained.pdparams --no-check-certificate\n    elif [ ${model_name} == \"PP-TSN\" ]; then\n        # pretrain lite train data\n        pushd ./data/k400\n        wget -nc https://videotag.bj.bcebos.com/Data/k400_videos_small.tar\n        tar -xf k400_videos_small.tar\n        popd\n        # download pretrained weights\n        wget -nc -P ./data https://videotag.bj.bcebos.com/PaddleVideo/PretrainModel/ResNet50_vd_ssld_v2_pretrained.pdparams --no-check-certificate\n    elif [ ${model_name} == \"AGCN\" ]; then\n        echo \"Not added into TIPC yet.\"\n    elif [ ${model_name} == \"STGCN\" ]; then\n        echo \"Not added into TIPC yet.\"\n    elif [ ${model_name} == \"TSM\" ]; then\n        # pretrain lite train data\n        pushd ./data/k400\n        wget -nc https://videotag.bj.bcebos.com/Data/k400_rawframes_small.tar",
        "type": "code",
        "location": "/test_tipc/prepare.sh:407-427"
    },
    "8607": {
        "file_id": 634,
        "content": "Code snippet checks the value of `model_name` and performs specific actions based on its value. For example, if it is \"PaddleVideo/ResNet50\", it downloads pretrained weights for that model. If `model_name` is not recognized, it prints a message saying it's not added to TIPC yet.",
        "type": "comment"
    },
    "8608": {
        "file_id": 634,
        "content": "        tar -xf k400_rawframes_small.tar\n        # download datalist for fleet benchmark\n        wget -nc https://videotag.bj.bcebos.com/PaddleVideo-release2.3/train_fleet_frames.list\n        wget -nc https://videotag.bj.bcebos.com/PaddleVideo-release2.3/val_fleet_frames.list\n        popd\n        # download pretrained weights\n        wget -nc -P ./data https://videotag.bj.bcebos.com/PaddleVideo/PretrainModel/ResNet50_pretrain.pdparams --no-check-certificate\n    elif [ ${model_name} == \"TSN\" ]; then\n        # pretrain lite train data\n        pushd ./data/k400\n        wget -nc https://videotag.bj.bcebos.com/Data/k400_rawframes_small.tar\n        tar -xf k400_rawframes_small.tar\n        popd\n        # download pretrained weights\n        wget -nc -P ./data https://videotag.bj.bcebos.com/PaddleVideo/PretrainModel/ResNet50_pretrain.pdparams --no-check-certificate\n    elif [ ${model_name} == \"TimeSformer\" ]; then\n        # pretrain lite train data\n        pushd ./data/k400\n        wget -nc https://videotag.bj.bcebos.com/Data/k400_videos_small.tar",
        "type": "code",
        "location": "/test_tipc/prepare.sh:428-446"
    },
    "8609": {
        "file_id": 634,
        "content": "This code is downloading pre-trained weights and data for PaddleVideo models, such as ResNet50, TSN, and TimeSformer. It checks the model_name and performs specific tasks accordingly: unzipping tar files, downloading lists of frames, and retrieving pretrained weights from specified URLs.",
        "type": "comment"
    },
    "8610": {
        "file_id": 634,
        "content": "        tar -xf k400_videos_small.tar\n        popd\n        # download pretrained weights\n        wget -nc -P ./data https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/ViT_base_patch16_224_pretrained.pdparams --no-check-certificate\n    elif [ ${model_name} == \"AttentionLSTM\" ]; then\n        echo \"Not added into TIPC yet.\"\n    elif [ ${model_name} == \"SlowFast\" ]; then\n        # pretrain lite train data\n        pushd ./data/k400\n        wget -nc https://videotag.bj.bcebos.com/Data/k400_videos_small.tar\n        tar -xf k400_videos_small.tar\n        popd\n    elif [ ${model_name} == \"BMN\" ]; then\n        # pretrain lite train data\n        pushd ./data\n        mkdir bmn_data\n        cd bmn_data\n        wget -nc https://paddlemodels.bj.bcebos.com/video_detection/bmn_feat.tar.gz\n        tar -xf bmn_feat.tar.gz\n        wget -nc https://paddlemodels.bj.bcebos.com/video_detection/activitynet_1.3_annotations.json\n        wget -nc https://paddlemodels.bj.bcebos.com/video_detection/activity_net_1_3_new.json\n        popd",
        "type": "code",
        "location": "/test_tipc/prepare.sh:447-468"
    },
    "8611": {
        "file_id": 634,
        "content": "Code snippet checks the model name and performs specific actions for each. If model is \"k400_videos_small\", it downloads pre-trained weights. If model is \"SlowFast\", it downloads lite train data. For \"BMN\", it downloads BMN training data, including annotations and JSON files. No action is taken for \"AttentionLSTM\" as it's not added to TIPC yet.",
        "type": "comment"
    },
    "8612": {
        "file_id": 634,
        "content": "    elif [ ${model_name} == \"VideoSwin\" ]; then\n        # pretrain lite train data\n        pushd ./data/k400\n        wget -nc https://videotag.bj.bcebos.com/Data/k400_videos_small.tar\n        tar -xf k400_videos_small.tar\n        popd\n        # download pretrained weights\n        wget -nc -P ./data https://videotag.bj.bcebos.com/PaddleVideo-release2.2/swin_small_patch4_window7_224.pdparams --no-check-certificate\n    else\n        echo \"Not added into TIPC yet.\"\n    fi\nfi\nif [ ${MODE} = \"klquant_whole_infer\" ]; then\n    if [ ${model_name} = \"PP-TSM\" ]; then\n        # download lite data\n        pushd ./data/k400\n        wget -nc https://videotag.bj.bcebos.com/Data/k400_rawframes_small.tar\n        tar -xf k400_rawframes_small.tar\n        popd\n        # download inference model\n        mkdir ./inference\n        pushd ./inference\n        wget -nc https://videotag.bj.bcebos.com/PaddleVideo-release2.3/ppTSM.zip --no-check-certificate\n        unzip ppTSM.zip\n        popd\n    else\n        echo \"Not added into TIPC yet.\"\n    fi",
        "type": "code",
        "location": "/test_tipc/prepare.sh:469-497"
    },
    "8613": {
        "file_id": 634,
        "content": "This code checks if the model is VideoSwin or PP-TSM. If VideoSwin, it downloads pretrain lite train data and pretrained weights. If PP-TSM, it downloads lite data and inference model. Other models are not added to TIPC yet.",
        "type": "comment"
    },
    "8614": {
        "file_id": 634,
        "content": "fi\nif [ ${MODE} = \"cpp_infer\" ];then\n    # install required packages\n    apt-get update\n    apt install libavformat-dev\n    apt install libavcodec-dev\n    apt install libswresample-dev\n    apt install libswscale-dev\n    apt install libavutil-dev\n    apt install libsdl1.2-dev\n    apt-get install ffmpeg\n    if [ ${model_name} = \"PP-TSM\" ]; then\n        # download pretrained weights\n        wget -nc -P data/ https://videotag.bj.bcebos.com/PaddleVideo-release2.1/PPTSM/ppTSM_k400_uniform.pdparams --no-check-certificate\n        # export inference model\n        ${python} tools/export_model.py -c configs/recognition/pptsm/pptsm_k400_frames_uniform.yaml -p data/ppTSM_k400_uniform.pdparams -o ./inference/ppTSM\n    elif [ ${model_name} = \"PP-TSN\" ]; then\n        # download pretrained weights\n        wget -nc -P data/ https://videotag.bj.bcebos.com/PaddleVideo-release2.2/ppTSN_k400.pdparams --no-check-certificate\n        # export inference model\n        ${python} tools/export_model.py -c configs/recognition/pptsn/pptsn_k400_videos.yaml -p data/ppTSN_k400.pdparams -o ./inference/ppTSN",
        "type": "code",
        "location": "/test_tipc/prepare.sh:498-520"
    },
    "8615": {
        "file_id": 634,
        "content": "This code installs necessary packages, downloads pre-trained model weights for either PP-TSM or PP-TSN, and exports the inference models for these two models. This is typically done before running inference on new data.",
        "type": "comment"
    },
    "8616": {
        "file_id": 634,
        "content": "    else\n        echo \"Not added into TIPC now.\"\n    fi\nfi\nif [ ${MODE} = \"serving_infer_python\" ];then\n    if [[ ${model_name} == \"PP-TSM\" ]];then\n        # prepare lite infer data for serving\n        pushd ./data\n        mkdir python_serving_infer_video_dir\n        cp ./example.avi python_serving_infer_video_dir/\n        popd\n        # prepare inference model\n        mkdir ./inference\n        pushd ./inference\n        wget -nc https://videotag.bj.bcebos.com/PaddleVideo-release2.3/ppTSM.zip --no-check-certificate\n        unzip ppTSM.zip\n        popd\n    elif [[ ${model_name} == \"PP-TSN\" ]];then\n        # prepare lite infer data for serving\n        pushd ./data\n        mkdir python_serving_infer_video_dir\n        cp ./example.avi python_serving_infer_video_dir/\n        popd\n        # prepare inference model\n        mkdir ./inference\n        pushd ./inference\n        wget -nc https://videotag.bj.bcebos.com/PaddleVideo-release2.3/ppTSN.zip --no-check-certificate\n        unzip ppTSN.zip\n        popd\n    else\n        echo \"Not added into TIPC now.\"",
        "type": "code",
        "location": "/test_tipc/prepare.sh:521-552"
    },
    "8617": {
        "file_id": 634,
        "content": "This code checks if the model_name is either \"PP-TSM\" or \"PP-TSN\". If it matches, it prepares lite infer data and downloads the corresponding inference model for serving. If not, it displays a message indicating that the model is not added into TIPC now.",
        "type": "comment"
    },
    "8618": {
        "file_id": 634,
        "content": "    fi\nfi\nif [ ${MODE} = \"paddle2onnx_infer\" ];then\n    # install paddle2onnx\n    python_name_list=$(func_parser_value \"${lines[2]}\")\n    IFS='|'\n    array=(${python_name_list})\n    python_name=${array[0]}\n    ${python_name} -m pip install paddle2onnx\n    ${python_name} -m pip install onnxruntime==1.9.0\n    if [ ${model_name} = \"PP-TSM\" ]; then\n        echo \"Not added into TIPC now.\"\n    elif [ ${model_name} = \"PP-TSN\" ]; then\n        mkdir -p ./inference\n        wget -P ./inference/ https://videotag.bj.bcebos.com/PaddleVideo-release2.3/ppTSN.zip\n        # unzip inference model\n        pushd ./inference\n        unzip ppTSN.zip\n        popd\n    else\n        echo \"Not added into TIPC now.\"\n    fi\nfi",
        "type": "code",
        "location": "/test_tipc/prepare.sh:553-577"
    },
    "8619": {
        "file_id": 634,
        "content": "This code snippet checks the current mode and performs specific actions accordingly. If the mode is \"paddle2onnx_infer\", it installs paddle2onnx and onnxruntime with a specified Python interpreter. For the \"PP-TSM\" model, it displays a message indicating that it's not added to TIPC. For the \"PP-TSN\" model, it downloads and unzips the inference model from a specific URL. If the mode is not recognized, it indicates that the corresponding action is not available in TIPC.",
        "type": "comment"
    },
    "8620": {
        "file_id": 635,
        "content": "/test_tipc/test_inference_cpp.sh",
        "type": "filepath"
    },
    "8621": {
        "file_id": 635,
        "content": "This code sets up PaddleVideo model inference, performs tests with MKLDNN or float point precision, iterates through thread settings and precisions, logs results, configures and builds PaddleVideo, sets OpenCV, CUDA, CUDNN directories, checks GPUID, runs inference tests on a list of model directories.",
        "type": "summary"
    },
    "8622": {
        "file_id": 635,
        "content": "#!/bin/bash\nsource test_tipc/common_func.sh\nFILENAME=$1\nMODE=$2\ndataline=$(awk 'NR==1, NR==18{print}'  $FILENAME)\n# parser params\nIFS=$'\\n'\nlines=(${dataline})\n# parser cpp inference model\nmodel_name=$(func_parser_value \"${lines[1]}\")\nuse_opencv=$(func_parser_value \"${lines[2]}\")\ncpp_infer_model_dir_list=$(func_parser_value \"${lines[3]}\")\ncpp_infer_is_quant=$(func_parser_value \"${lines[4]}\")\n# parser cpp inference\ninference_cmd=$(func_parser_value \"${lines[5]}\")\ncpp_use_gpu_key=$(func_parser_key \"${lines[6]}\")\ncpp_use_gpu_list=$(func_parser_value \"${lines[6]}\")\ncpp_use_mkldnn_key=$(func_parser_key \"${lines[7]}\")\ncpp_use_mkldnn_list=$(func_parser_value \"${lines[7]}\")\ncpp_cpu_threads_key=$(func_parser_key \"${lines[8]}\")\ncpp_cpu_threads_list=$(func_parser_value \"${lines[8]}\")\ncpp_batch_size_key=$(func_parser_key \"${lines[9]}\")\ncpp_batch_size_list=$(func_parser_value \"${lines[9]}\")\ncpp_use_trt_key=$(func_parser_key \"${lines[10]}\")\ncpp_use_trt_list=$(func_parser_value \"${lines[10]}\")\ncpp_precision_key=$(func_parser_key \"${lines[11]}\")",
        "type": "code",
        "location": "/test_tipc/test_inference_cpp.sh:1-29"
    },
    "8623": {
        "file_id": 635,
        "content": "This script uses Bash to parse input file lines, extracting model information and inference parameters for C++ models. It sources a common function script and then proceeds to parse each line of the input file into various variables like model name, OpenCV usage, C++ inference model directory list, inference command, GPU/MKLDNN/CPU thread settings, etc. These parsed values are stored in different variables for further use.",
        "type": "comment"
    },
    "8624": {
        "file_id": 635,
        "content": "cpp_precision_list=$(func_parser_value \"${lines[11]}\")\ncpp_infer_model_key=$(func_parser_key \"${lines[12]}\")\ncpp_image_dir_key=$(func_parser_key \"${lines[13]}\")\ncpp_infer_img_dir=$(func_parser_value \"${lines[13]}\")\ncpp_infer_key1=$(func_parser_key \"${lines[14]}\")\ncpp_infer_value1=$(func_parser_value \"${lines[14]}\")\ncpp_benchmark_key=$(func_parser_key \"${lines[15]}\")\ncpp_benchmark_value=$(func_parser_value \"${lines[15]}\")\ncpp_infer_key2=$(func_parser_key \"${lines[16]}\")\ncpp_infer_value2=$(func_parser_value \"${lines[16]}\")\ncpp_infer_key3=$(func_parser_key \"${lines[17]}\")\ncpp_infer_value3=$(func_parser_value \"${lines[17]}\")\nLOG_PATH=\"./test_tipc/output/${model_name}/${MODE}\"\nmkdir -p ${LOG_PATH}\nstatus_log=\"${LOG_PATH}/results_cpp.log\"\nfunction func_cpp_inference(){\n    IFS='|'\n    _script=$1\n    _model_dir=$2\n    _log_path=$3\n    _img_dir=$4\n    _flag_quant=$5\n    # inference\n    for use_gpu in ${cpp_use_gpu_list[*]}; do\n        if [ ${use_gpu} = \"False\" ] || [ ${use_gpu} = \"cpu\" ]; then\n            for use_mkldnn in ${cpp_use_mkldnn_list[*]}; do",
        "type": "code",
        "location": "/test_tipc/test_inference_cpp.sh:30-58"
    },
    "8625": {
        "file_id": 635,
        "content": "This code is setting up variables for running a PaddleVideo model inference using C++. It sets the precision list, infer model key, image directory key and value, and other keys and values required for the benchmarking process. The code also creates a log path for storing results of the C++ inference and prepares to loop through possible GPU usage and MKLDNN configurations.",
        "type": "comment"
    },
    "8626": {
        "file_id": 635,
        "content": "                if [ ${use_mkldnn} = \"False\" ] && [ ${_flag_quant} = \"True\" ]; then\n                    continue\n                fi\n                for threads in ${cpp_cpu_threads_list[*]}; do\n                    for batch_size in ${cpp_batch_size_list[*]}; do\n                        precision=\"fp32\"\n                        if [ ${use_mkldnn} = \"False\" ] && [ ${_flag_quant} = \"True\" ]; then\n                            precison=\"int8\"\n                        fi\n                        _save_log_path=\"${_log_path}/cpp_infer_cpu_usemkldnn_${use_mkldnn}_threads_${threads}_precision_${precision}_batchsize_${batch_size}.log\"\n                        set_infer_data=$(func_set_params \"${cpp_image_dir_key}\" \"${_img_dir}\")\n                        set_benchmark=$(func_set_params \"${cpp_benchmark_key}\" \"${cpp_benchmark_value}\")\n                        set_batchsize=$(func_set_params \"${cpp_batch_size_key}\" \"${batch_size}\")\n                        set_cpu_threads=$(func_set_params \"${cpp_cpu_threads_key}\" \"${threads}\")",
        "type": "code",
        "location": "/test_tipc/test_inference_cpp.sh:59-72"
    },
    "8627": {
        "file_id": 635,
        "content": "This code checks if MKLDNN is not being used and quantized precision is true. If so, it continues without executing the loop. Otherwise, it iterates through different thread settings, batch sizes, and precisions to execute inference tests on CPU using MKLDNN (if enabled) or float point precision (default). Logs are saved with details of parameters used for each run.",
        "type": "comment"
    },
    "8628": {
        "file_id": 635,
        "content": "                        set_model_dir=$(func_set_params \"${cpp_infer_model_key}\" \"${_model_dir}\")\n                        set_infer_params1=$(func_set_params \"${cpp_infer_key1}\" \"${cpp_infer_value1}\")\n                        set_infer_params2=$(func_set_params \"${cpp_infer_key2}\" \"${cpp_infer_value2}\")\n                        set_infer_params3=$(func_set_params \"${cpp_infer_key3}\" \"${cpp_infer_value3}\")\n                        command=\"${_script} ${cpp_use_gpu_key}=${use_gpu} ${cpp_use_mkldnn_key}=${use_mkldnn} ${set_cpu_threads} ${set_model_dir} ${set_batchsize} ${set_infer_data} ${set_benchmark} ${set_infer_params1} ${set_infer_params2} ${set_infer_params3} > ${_save_log_path} 2>&1 \"\n                        eval $command\n                        last_status=${PIPESTATUS[0]}\n                        eval \"cat ${_save_log_path}\"\n                        status_check $last_status \"${command}\" \"${status_log}\" \"${model_name}\"\n                    done\n                done\n            done\n        elif [ ${use_gpu} = \"True\" ] || [ ${use_gpu} = \"gpu\" ]; then",
        "type": "code",
        "location": "/test_tipc/test_inference_cpp.sh:73-85"
    },
    "8629": {
        "file_id": 635,
        "content": "This code is iterating over different model names and configurations, setting various parameters such as GPU usage and thread count. It then executes a command to run inference on the model and saves the log file. The script checks the status of the execution and logs any errors or warnings for debugging purposes.",
        "type": "comment"
    },
    "8630": {
        "file_id": 635,
        "content": "            for use_trt in ${cpp_use_trt_list[*]}; do\n                for precision in ${cpp_precision_list[*]}; do\n                    if [[ ${_flag_quant} = \"False\" ]] && [[ ${precision} =~ \"int8\" ]]; then\n                        continue\n                    fi\n                    if [[ ${precision} =~ \"fp16\" || ${precision} =~ \"int8\" ]] && [ ${use_trt} = \"False\" ]; then\n                        continue\n                    fi\n                    if [[ ${use_trt} = \"False\" || ${precision} =~ \"int8\" ]] && [ ${_flag_quant} = \"True\" ]; then\n                        continue\n                    fi\n                    for batch_size in ${cpp_batch_size_list[*]}; do\n                        _save_log_path=\"${_log_path}/cpp_infer_gpu_usetrt_${use_trt}_precision_${precision}_batchsize_${batch_size}.log\"\n                        set_infer_data=$(func_set_params \"${cpp_image_dir_key}\" \"${_img_dir}\")\n                        set_benchmark=$(func_set_params \"${cpp_benchmark_key}\" \"${cpp_benchmark_value}\")\n                        set_batchsize=$(func_set_params \"${cpp_batch_size_key}\" \"${batch_size}\")",
        "type": "code",
        "location": "/test_tipc/test_inference_cpp.sh:86-101"
    },
    "8631": {
        "file_id": 635,
        "content": "The code snippet is performing nested loops to iterate over different combinations of TensorRT (TRT) usage and precision options. It checks specific conditions using if statements, such as avoiding quantized precision with non-quantized flag set or excluding certain combinations based on TRT and precision values. Finally, it sets variables for the log path, input data parameters, benchmark value, and batch size before potentially executing further code within these loops.",
        "type": "comment"
    },
    "8632": {
        "file_id": 635,
        "content": "                        set_tensorrt=$(func_set_params \"${cpp_use_trt_key}\" \"${use_trt}\")\n                        set_precision=$(func_set_params \"${cpp_precision_key}\" \"${precision}\")\n                        set_model_dir=$(func_set_params \"${cpp_infer_model_key}\" \"${_model_dir}\")\n                        set_infer_params1=$(func_set_params \"${cpp_infer_key1}\" \"${cpp_infer_value1}\")\n                        set_infer_params2=$(func_set_params \"${cpp_infer_key2}\" \"${cpp_infer_value2}\")\n                        set_infer_params3=$(func_set_params \"${cpp_infer_key3}\" \"${cpp_infer_value3}\")\n                        command=\"${_script} ${cpp_use_gpu_key}=${use_gpu} ${set_tensorrt} ${set_precision} ${set_model_dir} ${set_batchsize} ${set_infer_data} ${set_benchmark} ${set_infer_params1} ${set_infer_params2} ${set_infer_params3} > ${_save_log_path} 2>&1 \"\n                        eval $command\n                        last_status=${PIPESTATUS[0]}\n                        eval \"cat ${_save_log_path}\"\n                        status_check $last_status \"${command}\" \"${status_log}\" \"${model_name}\"",
        "type": "code",
        "location": "/test_tipc/test_inference_cpp.sh:102-112"
    },
    "8633": {
        "file_id": 635,
        "content": "The code is setting parameters for a TensorRT inference script. It assigns values to various keys and directories before executing the script and saving the output log file. The last status of the command execution is checked, and the log file is displayed if no issues occurred.",
        "type": "comment"
    },
    "8634": {
        "file_id": 635,
        "content": "                    done\n                done\n            done\n        else\n            echo \"Does not support hardware other than CPU and GPU Currently!\"\n        fi\n    done\n}\ncd deploy/cpp_infer\nif [ ${use_opencv} = \"True\" ]; then\n    if [ -d \"opencv-3.4.7/opencv3/\" ] && [ $(md5sum opencv-3.4.7.tar.gz | awk -F ' ' '{print $1}') = \"faa2b5950f8bee3f03118e600c74746a\" ];then\n        echo \"################### build opencv skipped ###################\"\n    else\n        echo \"################### building opencv ###################\"\n        rm -rf opencv-3.4.7.tar.gz opencv-3.4.7/\n        wget https://paddleocr.bj.bcebos.com/dygraph_v2.0/test/opencv-3.4.7.tar.gz\n        tar -xf opencv-3.4.7.tar.gz\n        cd opencv-3.4.7/\n        install_path=$(pwd)/opencv3\n        rm -rf build\n        mkdir build\n        cd build\n        cmake .. \\\n            -DCMAKE_INSTALL_PREFIX=${install_path} \\\n            -DCMAKE_BUILD_TYPE=Release \\\n            -DBUILD_SHARED_LIBS=OFF \\\n            -DWITH_IPP=OFF \\\n            -DBUILD_IPP_IW=OFF \\",
        "type": "code",
        "location": "/test_tipc/test_inference_cpp.sh:114-146"
    },
    "8635": {
        "file_id": 635,
        "content": "The code checks if the current hardware supports CPU and GPU, and if not, it prints a message. If the OpenCV library is missing or outdated, it downloads the latest version and builds it. It then sets up the installation path for the built OpenCV library.",
        "type": "comment"
    },
    "8636": {
        "file_id": 635,
        "content": "            -DWITH_LAPACK=OFF \\\n            -DWITH_EIGEN=OFF \\\n            -DCMAKE_INSTALL_LIBDIR=lib64 \\\n            -DWITH_ZLIB=ON \\\n            -DBUILD_ZLIB=ON \\\n            -DWITH_JPEG=ON \\\n            -DBUILD_JPEG=ON \\\n            -DWITH_PNG=ON \\\n            -DBUILD_PNG=ON \\\n            -DWITH_TIFF=ON \\\n            -DBUILD_TIFF=ON \\\n            -DWITH_FFMPEG=ON\n        make -j\n        make install\n        cd ../\n        echo \"################### building opencv finished ###################\"\n    fi\nfi\nif [ !-d \"paddle_inference\" ]; then\n    echo \"################### download inference lib skipped ###################\"\nelse\n    echo \"################### downloading inference lib ###################\"\n    wget -nc https://paddle-inference-lib.bj.bcebos.com/2.1.1-gpu-cuda10.1-cudnn7-mkl-gcc8.2/paddle_inference.tgz\n    tar -xf paddle_inference.tgz\n    echo \"################### downloading inference lib finished ###################\"\nfi\necho \"################### building PaddleVideo demo ####################\"\nif [ ${use_opencv} = \"True\" ]; then",
        "type": "code",
        "location": "/test_tipc/test_inference_cpp.sh:147-178"
    },
    "8637": {
        "file_id": 635,
        "content": "This code sets various CMake flags to configure the build process, then proceeds with making and installing the required libraries. It checks if a directory exists, downloads necessary files if needed, and finally starts building the PaddleVideo demo.",
        "type": "comment"
    },
    "8638": {
        "file_id": 635,
        "content": "    OPENCV_DIR=$(pwd)/opencv-3.4.7/opencv3\nelse\n    OPENCV_DIR=''\nfi\nLIB_DIR=$(pwd)/paddle_inference\nCUDA_LIB_DIR=$(dirname `find /usr -name libcudart.so`)\nCUDNN_LIB_DIR=$(dirname `find /usr -name libcudnn.so`)\nBUILD_DIR=build\nrm -rf ${BUILD_DIR}\nmkdir ${BUILD_DIR}\ncd ${BUILD_DIR}\ncmake .. \\\n    -DPADDLE_LIB=${LIB_DIR} \\\n    -DWITH_MKL=ON \\\n    -DWITH_GPU=OFF \\\n    -DWITH_STATIC_LIB=OFF \\\n    -DWITH_TENSORRT=OFF \\\n    -DOPENCV_DIR=${OPENCV_DIR} \\\n    -DCUDNN_LIB=${CUDNN_LIB_DIR} \\\n    -DCUDA_LIB=${CUDA_LIB_DIR} \\\n    -DTENSORRT_DIR=${TENSORRT_DIR} \\\nmake -j\ncd ../../../\necho \"################### building PaddleVideo demo finished ###################\"\n# set cuda device\nGPUID=$2\nif [ ${#GPUID} -le 0 ];then\n    env=\" \"\nelse\n    env=\"export CUDA_VISIBLE_DEVICES=${GPUID}\"\nfi\nset CUDA_VISIBLE_DEVICES\neval $env\necho \"################### running test ###################\"\nexport Count=0\nIFS=\"|\"\ninfer_quant_flag=(${cpp_infer_is_quant})\nfor infer_model in ${cpp_infer_model_dir_list[*]}; do\n    #run inference\n    is_quant=${infer_quant_flag[Count]}",
        "type": "code",
        "location": "/test_tipc/test_inference_cpp.sh:179-225"
    },
    "8639": {
        "file_id": 635,
        "content": "This code is configuring and building PaddleVideo, setting up OpenCV, CUDA, and CUDNN directories, and preparing for running test inference. It also checks if GPUID is set and sets the CUDA_VISIBLE_DEVICES environment variable accordingly. Finally, it loops through a list of model directories to run inference tests.",
        "type": "comment"
    },
    "8640": {
        "file_id": 635,
        "content": "    func_cpp_inference \"${inference_cmd}\" \"${infer_model}\" \"${LOG_PATH}\" \"${cpp_infer_img_dir}\" ${is_quant}\n    Count=$(($Count + 1))\ndone",
        "type": "code",
        "location": "/test_tipc/test_inference_cpp.sh:226-228"
    },
    "8641": {
        "file_id": 635,
        "content": "This code snippet is calling a function \"func_cpp_inference\" to execute inference commands, incrementing the Count variable on each iteration of a loop. The function is called with input parameters for the command, model path, log path, image directory, and a quantization flag.",
        "type": "comment"
    },
    "8642": {
        "file_id": 636,
        "content": "/test_tipc/test_paddle2onnx.sh",
        "type": "filepath"
    },
    "8643": {
        "file_id": 636,
        "content": "The code reads a file for model details, extracts lines with common functions, sets up inference directories and parameters, enables ONNX checker, converts using paddle2onnx, saves logs, runs inference, and checks status for the \"func_paddle2onnx\" function.",
        "type": "summary"
    },
    "8644": {
        "file_id": 636,
        "content": "#!/bin/bash\nsource test_tipc/common_func.sh\nFILENAME=$1\nMODE=$2\ndataline=$(cat ${FILENAME})\nlines=(${dataline})\n# common params\nmodel_name=$(func_parser_value \"${lines[1]}\")\npython=$(func_parser_value \"${lines[2]}\")\n# parser params\ndataline=$(awk 'NR==1, NR==14{print}'  $FILENAME)\nIFS=$'\\n'\nlines=(${dataline})\n# parser paddle2onnx\nmodel_name=$(func_parser_value \"${lines[1]}\")\npython=$(func_parser_value \"${lines[2]}\")\npadlle2onnx_cmd=$(func_parser_value \"${lines[3]}\")\ninfer_model_dir_key=$(func_parser_key \"${lines[4]}\")\ninfer_model_dir_value=$(func_parser_value \"${lines[4]}\")\nmodel_filename_key=$(func_parser_key \"${lines[5]}\")\nmodel_filename_value=$(func_parser_value \"${lines[5]}\")\nparams_filename_key=$(func_parser_key \"${lines[6]}\")\nparams_filename_value=$(func_parser_value \"${lines[6]}\")\nsave_file_key=$(func_parser_key \"${lines[7]}\")\nsave_file_value=$(func_parser_value \"${lines[7]}\")\nopset_version_key=$(func_parser_key \"${lines[8]}\")\nopset_version_value=$(func_parser_value \"${lines[8]}\")\nenable_onnx_checker_key=$(func_parser_key \"${lines[9]}\")",
        "type": "code",
        "location": "/test_tipc/test_paddle2onnx.sh:1-32"
    },
    "8645": {
        "file_id": 636,
        "content": "Code is reading a file, parsing specific lines to extract model name, python path, and other parameters for paddle2onnx conversion. It's using common functions from \"common_func.sh\" and \"awk\" command for line extraction.",
        "type": "comment"
    },
    "8646": {
        "file_id": 636,
        "content": "enable_onnx_checker_value=$(func_parser_value \"${lines[9]}\")\n# parser onnx inference\ninference_py=$(func_parser_value \"${lines[10]}\")\nconfig_key=$(func_parser_key \"${lines[11]}\")\nconfig_value=$(func_parser_value \"${lines[11]}\")\nmodel_key=$(func_parser_key \"${lines[12]}\")\ninput_file_key=$(func_parser_key \"${lines[13]}\")\ninput_file_value=$(func_parser_value \"${lines[13]}\")\nLOG_PATH=\"./log/${model_name}/${MODE}\"\nmkdir -p ${LOG_PATH}\nstatus_log=\"${LOG_PATH}/results_paddle2onnx.log\"\nfunction func_paddle2onnx(){\n    IFS='|'\n    _script=$1\n    # paddle2onnx\n    _save_log_path=\"${LOG_PATH}/paddle2onnx_infer_cpu.log\"\n    set_dirname=$(func_set_params \"${infer_model_dir_key}\" \"${infer_model_dir_value}\")\n    set_model_filename=$(func_set_params \"${model_filename_key}\" \"${model_filename_value}\")\n    set_params_filename=$(func_set_params \"${params_filename_key}\" \"${params_filename_value}\")\n    set_save_model=$(func_set_params \"${save_file_key}\" \"${save_file_value}\")\n    set_opset_version=$(func_set_params \"${opset_version_key}\" \"${opset_version_value}\")",
        "type": "code",
        "location": "/test_tipc/test_paddle2onnx.sh:33-58"
    },
    "8647": {
        "file_id": 636,
        "content": "Creating function \"func_paddle2onnx\" with arguments _script, setting up log path and directories for paddle2onnx inference. It then sets parameters such as infer_model_dir_key, model_filename_key, params_filename_key, save_file_key, and opset_version_key.",
        "type": "comment"
    },
    "8648": {
        "file_id": 636,
        "content": "    set_enable_onnx_checker=$(func_set_params \"${enable_onnx_checker_key}\" \"${enable_onnx_checker_value}\")\n    trans_log=\"${LOG_PATH}/trans_model.log\"\n    trans_model_cmd=\"${padlle2onnx_cmd} ${set_dirname} ${set_model_filename} ${set_params_filename} ${set_save_model} ${set_opset_version} ${set_enable_onnx_checker} > ${trans_log} 2>&1 \"\n    eval $trans_model_cmd\n    last_status=${PIPESTATUS[0]}\n    status_check $last_status \"${trans_model_cmd}\" \"${status_log}\" \"${model_name}\"\n    # python inference\n    set_gpu=$(func_set_params \"${use_gpu_key}\" \"${use_gpu_value}\")\n    set_model_dir=$(func_set_params \"${model_key}\" \"${save_file_value}\")\n    set_input_file=$(func_set_params \"${input_file_key}\" \"${input_file_value}\")\n    set_config=$(func_set_params \"${config_key}\" \"${config_value}\")\n    infer_model_cmd=\"${python} ${inference_py} ${set_config} ${set_input_file} ${set_model_dir} > ${_save_log_path} 2>&1 \"\n    eval $infer_model_cmd\n    last_status=${PIPESTATUS[0]}\n    status_check $last_status \"${infer_model_cmd}\" \"${status_log}\" \"${model_name}\"",
        "type": "code",
        "location": "/test_tipc/test_paddle2onnx.sh:59-73"
    },
    "8649": {
        "file_id": 636,
        "content": "The code sets enable_onnx_checker and uses it to execute paddle2onnx conversion, saves the log. Then, it runs inference using Python and saves the status check log.",
        "type": "comment"
    },
    "8650": {
        "file_id": 636,
        "content": "}\necho \"################### run test ###################\"\nexport Count=0\nIFS=\"|\"\nfunc_paddle2onnx",
        "type": "code",
        "location": "/test_tipc/test_paddle2onnx.sh:74-81"
    },
    "8651": {
        "file_id": 636,
        "content": "This code segment is running a test for the function \"func_paddle2onnx\" by exporting Count variable, setting IFS to \"|\", and echoing a message.",
        "type": "comment"
    },
    "8652": {
        "file_id": 637,
        "content": "/test_tipc/test_ptq_inference_python.sh",
        "type": "filepath"
    },
    "8653": {
        "file_id": 637,
        "content": "The code reads parameters, separates configurations, and executes inference tests on different GPUs/CPUs for batch sizes. It sets up a loop for PaddleVideo model inference, handles hardware configurations, prepares settings for exporting models, logs results, and calls the \"func_inference\" function.",
        "type": "summary"
    },
    "8654": {
        "file_id": 637,
        "content": "#!/bin/bash\nsource test_tipc/common_func.sh\nFILENAME=$1\n# MODE be one of ['lite_train_lite_infer' 'lite_train_whole_infer' 'whole_train_whole_infer', 'whole_infer']\nMODE=$2\ndataline=$(awk 'NR==1, NR==32{print}'  $FILENAME)\n# parser params\nIFS=$'\\n'\nlines=(${dataline})\n# The training params\nmodel_name=$(func_parser_value \"${lines[1]}\")\npython=$(func_parser_value \"${lines[2]}\")\nuse_gpu_key=$(func_parser_key \"${lines[3]}\")\nuse_gpu_value=$(func_parser_value \"${lines[3]}\")\nquant_config_file_key=$(func_parser_key \"${lines[4]}\")\nquant_config_file_value=$(func_parser_value \"${lines[4]}\")\nmodel_path_key=$(func_parser_key \"${lines[5]}\")\nmodel_path_value=$(func_parser_value \"${lines[5]}\")\noutput_dir_key=$(func_parser_key \"${lines[6]}\")\noutput_dir_value=$(func_parser_value \"${lines[6]}\")\ndata_dir_key=$(func_parser_key \"${lines[7]}\")\ndata_dir_value=$(func_parser_value \"${lines[7]}\")\ndata_anno_key=$(func_parser_key \"${lines[8]}\")\ndata_anno_value=$(func_parser_value \"${lines[8]}\")\nbatch_num_key=$(func_parser_key \"${lines[9]}\")",
        "type": "code",
        "location": "/test_tipc/test_ptq_inference_python.sh:1-29"
    },
    "8655": {
        "file_id": 637,
        "content": "The code reads a file, parses parameters for model name, Python version, GPU usage, quantization configuration file, model path, output directory, data directory, data annotation file, and batch numbers. It uses awk to extract specific lines from the file and functions defined in common_func.sh for parameter extraction. The MODE variable can have values to determine the type of task being performed.",
        "type": "comment"
    },
    "8656": {
        "file_id": 637,
        "content": "batch_num_value=$(func_parser_value \"${lines[9]}\")\nquant_batch_size_key=$(func_parser_key \"${lines[10]}\")\nquant_batch_size_value=$(func_parser_value \"${lines[10]}\")\n# parser trainer\ntrain_py=$(func_parser_value \"${lines[13]}\")\n# parser inference\ninference_py=$(func_parser_value \"${lines[16]}\")\nuse_gpu_key=$(func_parser_key \"${lines[17]}\")\nuse_gpu_list=$(func_parser_value \"${lines[17]}\")\ninfer_config_file_key=$(func_parser_key \"${lines[18]}\")\ninfer_config_file_value=$(func_parser_value \"${lines[18]}\")\ninfer_batch_size_key=$(func_parser_key \"${lines[19]}\")\ninfer_batch_size_list=$(func_parser_value \"${lines[19]}\")\ninfer_model_key=$(func_parser_key \"${lines[20]}\")\ninfer_model_value=$(func_parser_value \"${lines[20]}\")\ninfer_params_key=$(func_parser_key \"${lines[21]}\")\ninfer_params_value=$(func_parser_value \"${lines[21]}\")\ninfer_video_key=$(func_parser_key \"${lines[22]}\")\ninfer_video_dir=$(func_parser_value \"${lines[22]}\")\nbenchmark_key=$(func_parser_key \"${lines[23]}\")\nbenchmark_value=$(func_parser_value \"${lines[23]}\")",
        "type": "code",
        "location": "/test_tipc/test_ptq_inference_python.sh:30-52"
    },
    "8657": {
        "file_id": 637,
        "content": "The code retrieves values and keys from a configuration file, storing them in variables for later use. It separates trainer and inference configurations, including GPU usage, inferential model parameters, video directory path, and benchmark options.",
        "type": "comment"
    },
    "8658": {
        "file_id": 637,
        "content": "function func_inference(){\n    IFS='|'\n    _python=$1\n    _script=$2\n    _model_dir=$3\n    _log_path=$4\n    _img_dir=$5\n    # inference\n    for use_gpu in ${use_gpu_list[*]}; do\n        # cpu\n        if [ ${use_gpu} = \"False\" ] || [ ${use_gpu} = \"cpu\" ]; then\n            for batch_size in ${infer_batch_size_list[*]}; do\n                _save_log_path=\"${_log_path}/python_infer_cpu_batchsize_${batch_size}.log\"\n                set_infer_data=$(func_set_params \"${infer_video_key}\" \"${_img_dir}\")\n                set_benchmark=$(func_set_params \"${benchmark_key}\" \"${benchmark_value}\")\n                set_batchsize=$(func_set_params \"${infer_batch_size_key}\" \"${batch_size}\")\n                set_model_file_path=$(func_set_params \"${infer_model_key}\" \"${infer_model_value}\")\n                set_params_file_path=$(func_set_params \"${infer_params_key}\" \"${infer_params_value}\")\n                set_config_file_path=$(func_set_params \"${infer_config_file_key}\" \"${infer_config_file_value}\")\n                command=\"${_",
        "type": "code",
        "location": "/test_tipc/test_ptq_inference_python.sh:55-74"
    },
    "8659": {
        "file_id": 637,
        "content": "This function executes inference on different GPUs and CPUs for various batch sizes. It sets log paths, parameters, model file path, params file path, and config file path using helper functions. The script performs inference using Python and logs the results.",
        "type": "comment"
    },
    "8660": {
        "file_id": 637,
        "content": "python} ${_script} ${use_gpu_key}=${use_gpu} ${set_config_file_path} ${set_model_file_path} ${set_params_file_path} ${set_batchsize} ${set_infer_data} ${set_benchmark} > ${_save_log_path} 2>&1 \"\n                # echo $command\n                eval $command\n                last_status=${PIPESTATUS[0]}\n                eval \"cat ${_save_log_path}\"\n                status_check $last_status \"${command}\" \"${status_log}\" \"${model_name}\"\n            done\n        # gpu\n        elif [ ${use_gpu} = \"True\" ] || [ ${use_gpu} = \"gpu\" ]; then\n            for batch_size in ${infer_batch_size_list[*]}; do\n                _save_log_path=\"${_log_path}/python_infer_gpu_batchsize_${batch_size}.log\"\n                set_infer_data=$(func_set_params \"${infer_video_key}\" \"${_img_dir}\")\n                set_benchmark=$(func_set_params \"${benchmark_key}\" \"${benchmark_value}\")\n                set_batchsize=$(func_set_params \"${infer_batch_size_key}\" \"${batch_size}\")\n                set_model_file_path=$(func_set_params \"${infer_model_key}\" \"${infer_model_value}\")",
        "type": "code",
        "location": "/test_tipc/test_ptq_inference_python.sh:74-88"
    },
    "8661": {
        "file_id": 637,
        "content": "This code is running a loop to execute inference tests on different GPU configurations. It sets variables for batch size, input data path, and model file path. The output logs are saved into specific files for later analysis.",
        "type": "comment"
    },
    "8662": {
        "file_id": 637,
        "content": "                set_params_file_path=$(func_set_params \"${infer_params_key}\" \"${infer_params_value}\")\n                set_config_file_path=$(func_set_params \"${infer_config_file_key}\" \"${infer_config_file_value}\")\n                command=\"${_python} ${_script} ${use_gpu_key}=${use_gpu} ${set_config_file_path} ${set_model_file_path} ${set_params_file_path} ${set_batchsize} ${set_infer_data} ${set_benchmark} > ${_save_log_path} 2>&1 \"\n                echo $command\n                eval $command\n                last_status=${PIPESTATUS[0]}\n                eval \"cat ${_save_log_path}\"\n                status_check $last_status \"${command}\" \"${status_log}\" \"${model_name}\"\n            done\n        else\n            echo \"Does not support hardware other than CPU and GPU Currently!\"\n        fi\n    done\n}\n# log\nLOG_PATH=\"./log/${model_name}/${MODE}\"\nmkdir -p ${LOG_PATH}\nstatus_log=\"${LOG_PATH}/results_python.log\"\nif [ ${MODE} = \"whole_infer\" ]; then\n    IFS=\"|\"\n    # run export\n    set_output_dir=$(func_set_params \"${output_dir_key}\" \"${output_dir_value}\")",
        "type": "code",
        "location": "/test_tipc/test_ptq_inference_python.sh:89-112"
    },
    "8663": {
        "file_id": 637,
        "content": "This code is setting up a loop to run inference on the PaddleVideo model for different hardware configurations and modes. It sets the necessary parameters, files, and batch size, then executes the command and checks the status of the execution. The output is logged in a specified directory. If the mode is \"whole_infer\", it uses IFS to separate the export settings.",
        "type": "comment"
    },
    "8664": {
        "file_id": 637,
        "content": "    set_data_dir=$(func_set_params \"${data_dir_key}\" \"${data_dir_value}\")\n    set_data_anno=$(func_set_params \"${data_anno_key}\" \"${data_anno_value}\")\n    set_batch_size=$(func_set_params \"${quant_batch_size_key}\" \"${quant_batch_size_value}\")\n    set_batch_num=$(func_set_params \"${batch_num_key}\" \"${batch_num_value}\")\n    set_model_path=$(func_set_params \"${model_path_key}\" \"${model_path_value}\")\n    set_config_file=$(func_set_params \"${quant_config_file_key}\" \"${quant_config_file_value}\")\n    set_use_gpu=$(func_set_params \"${use_gpu_key}\" \"${use_gpu_value}\")\n    export_log_path=\"${LOG_PATH}/${MODE}_export_${Count}.log\"\n    export_cmd=\"${python} ${train_py} ${set_use_gpu} ${set_config_file} ${set_model_path} ${set_batch_num} ${set_batch_size} ${set_data_dir} ${set_data_anno} ${set_output_dir} > ${export_log_path} 2>&1 \"\n    echo $export_cmd\n    eval $export_cmd\n    status_export=$?\n    status_check $status_export \"${export_cmd}\" \"${status_log}\" \"${model_name}\"\n    save_infer_dir=${output_dir_value}\n    #run inference",
        "type": "code",
        "location": "/test_tipc/test_ptq_inference_python.sh:113-129"
    },
    "8665": {
        "file_id": 637,
        "content": "This code is preparing various settings for executing a command to export a model. It sets values from input variables, exports the model with specified parameters, logs the results, and then checks the status of the export. Finally, it prepares a directory for running inference.",
        "type": "comment"
    },
    "8666": {
        "file_id": 637,
        "content": "    func_inference \"${python}\" \"${inference_py}\" \"${save_infer_dir}\" \"${LOG_PATH}\" \"${infer_video_dir}\"\nfi",
        "type": "code",
        "location": "/test_tipc/test_ptq_inference_python.sh:130-132"
    },
    "8667": {
        "file_id": 637,
        "content": "The code snippet is calling a function named \"func_inference\" with arguments such as python, inference_py (likely the path of the Python script), save_infer_dir, LOG_PATH and infer_video_dir. This could be a part of an if condition block, possibly initializing or running an inference process.",
        "type": "comment"
    },
    "8668": {
        "file_id": 638,
        "content": "/test_tipc/test_serving_infer_cpp.sh",
        "type": "filepath"
    },
    "8669": {
        "file_id": 638,
        "content": "This Bash script, using a configuration file and mode inputs, initializes a model, serves it via Python/C++, prepares the environment, logs execution, runs a GPU server, and tests a web service function with incrementing \"Count\" variable and IFS separation.",
        "type": "summary"
    },
    "8670": {
        "file_id": 638,
        "content": "#!/bin/bash\nsource test_tipc/common_func.sh\nFILENAME=$1\nMODE=$2\ndataline=$(awk 'NR==1, NR==18{print}'  $FILENAME)\n# parser params\nIFS=$'\\n'\nlines=(${dataline})\n# parser serving\nmodel_name=$(func_parser_value \"${lines[1]}\")\npython_list=$(func_parser_value \"${lines[2]}\")\ntrans_model_py=$(func_parser_value \"${lines[3]}\")\ninfer_model_dir_key=$(func_parser_key \"${lines[4]}\")\ninfer_model_dir_value=$(func_parser_value \"${lines[4]}\")\nmodel_filename_key=$(func_parser_key \"${lines[5]}\")\nmodel_filename_value=$(func_parser_value \"${lines[5]}\")\nparams_filename_key=$(func_parser_key \"${lines[6]}\")\nparams_filename_value=$(func_parser_value \"${lines[6]}\")\nserving_server_key=$(func_parser_key \"${lines[7]}\")\nserving_server_value=$(func_parser_value \"${lines[7]}\")\nserving_client_key=$(func_parser_key \"${lines[8]}\")\nserving_client_value=$(func_parser_value \"${lines[8]}\")\nserving_dir_value=$(func_parser_value \"${lines[9]}\")\nrun_model_path_key=$(func_parser_key \"${lines[10]}\")\nrun_model_path_value=$(func_parser_value \"${lines[10]}\")",
        "type": "code",
        "location": "/test_tipc/test_serving_infer_cpp.sh:1-28"
    },
    "8671": {
        "file_id": 638,
        "content": "This code is a Bash script that takes in two arguments: the filename of a configuration file and the mode to operate in. It uses `awk` to extract a specific section from the configuration file, parses this data into variables using custom functions, and then sets up various parameters for running an image classification model.",
        "type": "comment"
    },
    "8672": {
        "file_id": 638,
        "content": "port_key=$(func_parser_key \"${lines[11]}\")\nport_value=$(func_parser_value \"${lines[11]}\")\ncpp_client_value=$(func_parser_value \"${lines[12]}\")\ninput_video_key=$(func_parser_key \"${lines[13]}\")\ninput_video_value=$(func_parser_value \"${lines[13]}\")\nLOG_PATH=\"./test_tipc/output/log/${model_name}/${MODE}\"\nmkdir -p ${LOG_PATH}\nstatus_log=\"${LOG_PATH}/results_serving.log\"\nfunction func_serving(){\n    IFS='|'\n    _python=$1\n    _script=$2\n    _model_dir=$3\n    # phase 1: save model\n    set_dirname=$(func_set_params \"${infer_model_dir_key}\" \"${infer_model_dir_value}\")\n    set_model_filename=$(func_set_params \"${model_filename_key}\" \"${model_filename_value}\")\n    set_params_filename=$(func_set_params \"${params_filename_key}\" \"${params_filename_value}\")\n    set_serving_server=$(func_set_params \"${serving_server_key}\" \"${serving_server_value}\")\n    set_serving_client=$(func_set_params \"${serving_client_key}\" \"${serving_client_value}\")\n    python_list=(${python_list})\n    python=${python_list[0]}\n    trans_log=\"${LOG_PATH}/cpp_trans_model.log\"",
        "type": "code",
        "location": "/test_tipc/test_serving_infer_cpp.sh:29-54"
    },
    "8673": {
        "file_id": 638,
        "content": "This code parses keys and values from a configuration file, sets directory names and filenames for saving the model, and initializes variables for later use. It uses Python and potentially C++ for model serving. The code creates log files to store results of the model inference and transfer process, and prepares for the next steps involving Python scripts and possibly C++ client or server execution.",
        "type": "comment"
    },
    "8674": {
        "file_id": 638,
        "content": "    trans_model_cmd=\"${python} ${trans_model_py} ${set_dirname} ${set_model_filename} ${set_params_filename} ${set_serving_server} ${set_serving_client} > ${trans_log} 2>&1 \"\n    eval ${trans_model_cmd}\n    last_status=${PIPESTATUS[0]}\n    status_check $last_status \"${trans_model_cmd}\" \"${status_log}\" \"${model_name}\"\n    # modify the alias name of fetch_var to \"outputs\"\n    server_fetch_var_line_cmd=\"sed -i '/fetch_var/,/is_lod_tensor/s/alias_name: .*/alias_name: \\\"outputs\\\"/' $serving_server_value/serving_server_conf.prototxt\"\n    eval ${server_fetch_var_line_cmd}\n    client_fetch_var_line_cmd=\"sed -i '/fetch_var/,/is_lod_tensor/s/alias_name: .*/alias_name: \\\"outputs\\\"/' $serving_client_value/serving_client_conf.prototxt\"\n    eval ${client_fetch_var_line_cmd}\n    cd ${serving_dir_value}\n    echo $PWD\n    unset https_proxy\n    unset http_proxy\n    _save_log_path=\"${LOG_PATH}/cpp_client_infer_gpu_batchsize_1.log\"\n    # phase 2: run server\n    server_log_path=\"${LOG_PATH}/cpp_server_gpu.log\"\n    cpp_ser",
        "type": "code",
        "location": "/test_tipc/test_serving_infer_cpp.sh:55-73"
    },
    "8675": {
        "file_id": 638,
        "content": "This code snippet modifies a serving configuration file, sets up the environment for running a C++ server and client, logs their execution, and finally runs the C++ server on GPU.",
        "type": "comment"
    },
    "8676": {
        "file_id": 638,
        "content": "ver_cmd=\"${python} -m paddle_serving_server.serve ${run_model_path_key} ${run_model_path_value} ${port_key} ${port_value} > ${server_log_path} 2>&1 &\"\n    eval ${cpp_server_cmd}\n    sleep 20s\n    # phase 3: run client\n    real_model_name=${model_name/PP-/PP}\n    serving_client_conf_path=\"${serving_client_value/deploy\\/cpp_serving\\/}\"\n    serving_client_conf_path=\"${serving_client_conf_path/\\/\\//}serving_client_conf.prototxt\"\n    cpp_client_cmd=\"${python} ${cpp_client_value} -n ${real_model_name} -c ${serving_client_conf_path} ${input_video_key} ${input_video_value} > ${_save_log_path} 2>&1 \"\n    eval ${cpp_client_cmd}\n    last_status=${PIPESTATUS[0]}\n    eval \"cat ${_save_log_path}\"\n    cd ../../\n    status_check $last_status \"${cpp_server_cmd}\" \"${status_log}\" \"${model_name}\"\n    ps ux | grep -i 'paddle_serving_server' | awk '{print $2}' | xargs kill -s 9\n}\n# set cuda device\nGPUID=$3\nif [ ${#GPUID} -le 0 ];then\n    env=\" \"\nelse\n    env=\"export CUDA_VISIBLE_DEVICES=${GPUID}\"\nfi\nset CUDA_VISIBLE_DEVICES\neval $env",
        "type": "code",
        "location": "/test_tipc/test_serving_infer_cpp.sh:73-100"
    },
    "8677": {
        "file_id": 638,
        "content": "The script starts a PaddlePaddle serving server, runs a client against it, and performs status checks. The CUDA device can be set using the GPUID parameter.",
        "type": "comment"
    },
    "8678": {
        "file_id": 638,
        "content": "echo \"################### run test ###################\"\nexport Count=0\nIFS=\"|\"\nfunc_serving \"${web_service_cmd}\"",
        "type": "code",
        "location": "/test_tipc/test_serving_infer_cpp.sh:103-107"
    },
    "8679": {
        "file_id": 638,
        "content": "This code is executing a test function for serving a web service, incrementing the \"Count\" variable and using IFS to separate the function arguments with \"|\".",
        "type": "comment"
    },
    "8680": {
        "file_id": 639,
        "content": "/test_tipc/test_serving_infer_python.sh",
        "type": "filepath"
    },
    "8681": {
        "file_id": 639,
        "content": "The Bash script configures a model serving environment, sets up an API server, transfers the model using provided Python code, and handles cleanup tasks. It also adjusts alias names, logs paths, and CUDA visible devices while running video processing pipeline tests.",
        "type": "summary"
    },
    "8682": {
        "file_id": 639,
        "content": "#!/bin/bash\nsource test_tipc/common_func.sh\nFILENAME=$1\ndataline=$(awk 'NR==1, NR==18{print}'  $FILENAME)\nMODE=$2\n# parser params\nIFS=$'\\n'\nlines=(${dataline})\n# parser serving\nmodel_name=$(func_parser_value \"${lines[1]}\")\npython_list=$(func_parser_value \"${lines[2]}\")\ntrans_model_py=$(func_parser_value \"${lines[3]}\")\ninfer_model_dir_key=$(func_parser_key \"${lines[4]}\")\ninfer_model_dir_value=$(func_parser_value \"${lines[4]}\")\nmodel_filename_key=$(func_parser_key \"${lines[5]}\")\nmodel_filename_value=$(func_parser_value \"${lines[5]}\")\nparams_filename_key=$(func_parser_key \"${lines[6]}\")\nparams_filename_value=$(func_parser_value \"${lines[6]}\")\nserving_server_key=$(func_parser_key \"${lines[7]}\")\nserving_server_value=$(func_parser_value \"${lines[7]}\")\nserving_client_key=$(func_parser_key \"${lines[8]}\")\nserving_client_value=$(func_parser_value \"${lines[8]}\")\nserving_dir_value=$(func_parser_value \"${lines[9]}\")\nweb_service_py=$(func_parser_value \"${lines[10]}\")\npipeline_py=$(func_parser_value \"${lines[11]}\")\nvideo_dir_key=$(func_parser_key \"${lines[12]}\")",
        "type": "code",
        "location": "/test_tipc/test_serving_infer_python.sh:1-29"
    },
    "8683": {
        "file_id": 639,
        "content": "This Bash script is parsing a configuration file and extracting various parameters for running model inference. It assigns values to variables such as 'model_name', 'python_list', and others that will be used later in the code. The purpose is to set up an environment for serving the model and potentially run inferences on videos.",
        "type": "comment"
    },
    "8684": {
        "file_id": 639,
        "content": "video_dir_value=$(func_parser_value \"${lines[12]}\")\nLOG_PATH=\"./test_tipc/output/log/${model_name}/${MODE}\"\nmkdir -p ${LOG_PATH}\nstatus_log=\"${LOG_PATH}/results_serving.log\"\nfunction func_serving(){\n    IFS='|'\n    _python=$1\n    _script=$2\n    _model_dir=$3\n    # python serving code\n    set_dirname=$(func_set_params \"${infer_model_dir_key}\" \"${infer_model_dir_value}\")\n    set_model_filename=$(func_set_params \"${model_filename_key}\" \"${model_filename_value}\")\n    set_params_filename=$(func_set_params \"${params_filename_key}\" \"${params_filename_value}\")\n    set_serving_server=$(func_set_params \"${serving_server_key}\" \"${serving_server_value}\")\n    set_serving_client=$(func_set_params \"${serving_client_key}\" \"${serving_client_value}\")\n    python_list=(${python_list})\n    python=${python_list[0]}\n    trans_log=\"${LOG_PATH}/python_trans_model.log\"\n    trans_model_cmd=\"${python} ${trans_model_py} ${set_dirname} ${set_model_filename} ${set_params_filename} ${set_serving_server} ${set_serving_client} > ${trans_log} 2>&1 \"",
        "type": "code",
        "location": "/test_tipc/test_serving_infer_python.sh:30-54"
    },
    "8685": {
        "file_id": 639,
        "content": "The code defines a function `func_serving` that takes Python executable path, script, model directory, and sets various parameters for serving. It then executes a command to transfer the model to the specified server or client using a provided Python script. The output is logged in the `trans_log` file.",
        "type": "comment"
    },
    "8686": {
        "file_id": 639,
        "content": "    eval ${trans_model_cmd}\n    # modify the alias name of fetch_var to \"outputs\"\n    server_fetch_var_line_cmd=\"sed -i '/fetch_var/,/is_lod_tensor/s/alias_name: .*/alias_name: \\\"outputs\\\"/' $serving_server_value/serving_server_conf.prototxt\"\n    eval ${server_fetch_var_line_cmd}\n    client_fetch_var_line_cmd=\"sed -i '/fetch_var/,/is_lod_tensor/s/alias_name: .*/alias_name: \\\"outputs\\\"/' $serving_client_value/serving_client_conf.prototxt\"\n    eval ${client_fetch_var_line_cmd}\n    cd ${serving_dir_value}\n    echo 'PWD= '$PWD\n    unset https_proxy\n    unset http_proxy\n    server_log_path=\"${LOG_PATH}/python_server_gpu.log\"\n    web_service_cmd=\"${python} ${web_service_py} > ${server_log_path} 2>&1 &\"\n    eval $web_service_cmd\n    last_status=${PIPESTATUS[0]}\n    status_check $last_status \"${web_service_cmd}\" \"${status_log}\" \"${model_name}\"\n    sleep 30s # not too short is ok\n    _save_log_path=\"../../${LOG_PATH}/python_server_infer_gpu_batchsize_1.log\"\n    set_video_dir=$(func_set_params \"${video_dir_key}\" \"${video_dir_value}\")",
        "type": "code",
        "location": "/test_tipc/test_serving_infer_python.sh:56-77"
    },
    "8687": {
        "file_id": 639,
        "content": "This code modifies alias names in configuration files, sets log paths and starts a web service using Python. It also checks the status of the service, sleeps for 30 seconds, and saves logs into a specific path. The code is executed within a specific directory and sets environment variables before running the commands.",
        "type": "comment"
    },
    "8688": {
        "file_id": 639,
        "content": "    pipeline_cmd=\"${python} ${pipeline_py} ${set_video_dir} > ${_save_log_path} 2>&1 \"\n    eval $pipeline_cmd\n    last_status=${PIPESTATUS[0]}\n    eval \"cat ${_save_log_path}\"\n    cd ../../\n    status_check $last_status \"${pipeline_cmd}\" \"${status_log}\" \"${model_name}\"\n    ps ux | grep -E 'web_service|pipeline' | awk '{print $2}' | xargs kill -s 9\n}\n# set cuda device\nGPUID=$3\nif [ ${#GPUID} -le 0 ];then\n    env=\" \"\nelse\n    env=\"export CUDA_VISIBLE_DEVICES=${GPUID}\"\nfi\nset CUDA_VISIBLE_DEVICES\neval $env\necho \"################### run test ###################\"\nexport Count=0\nIFS=\"|\"\nfunc_serving \"${web_service_cmd}\"",
        "type": "code",
        "location": "/test_tipc/test_serving_infer_python.sh:78-105"
    },
    "8689": {
        "file_id": 639,
        "content": "This code is setting up the environment and running a test for a video processing pipeline. It sets the CUDA visible devices, runs the test using specified command, and performs clean-up by killing related processes after the test.",
        "type": "comment"
    },
    "8690": {
        "file_id": 640,
        "content": "/test_tipc/test_train_dy2static_python.sh",
        "type": "filepath"
    },
    "8691": {
        "file_id": 640,
        "content": "The code trains and analyzes two models (dygraph and dy2static), compares their losses, logs the differences, and prints the results.",
        "type": "summary"
    },
    "8692": {
        "file_id": 640,
        "content": "source test_tipc/common_func.sh\nIFS=$'\\n'\nBASE_CONFIG_FILE=$1\n# always use the lite_train_lite_infer mode to speed. Modify the config file.\nMODE=lite_train_lite_infer\nBASEDIR=$(dirname \"$0\")\n# get the log path.\ndataline=$(cat ${BASE_CONFIG_FILE})\nlines=(${dataline})\nmodel_name=$(func_parser_value \"${lines[1]}\")\nLOG_PATH=\"./test_tipc/output/${model_name}/${MODE}\"\nrm -rf $LOG_PATH\nmkdir -p ${LOG_PATH}\nstatus_log=\"${LOG_PATH}/results_python.log\"\n# make cudnn algorithm deterministic, such as conv.\nexport FLAGS_cudnn_deterministic=True\n# read the base config and parse and run the sub commands\nconfig_line_numbers=`cat ${BASE_CONFIG_FILE} | grep -n \"============\" | cut -d':' -f1`\nfor cln in $config_line_numbers\ndo\n    # change IFS to prevent \\n is parsed as delimiter.\n    IFS=\"\"\n    config_lines=$(cat ${BASE_CONFIG_FILE} | sed -n \"${cln},\\$p\" | head -n 22)\n    config_name=`echo ${config_lines} | grep '=====' | cut -d' ' -f2`\n    FILENAME=$LOG_PATH/dy2static_$config_name.txt\n    echo \"[Start dy2static]\" \"${config_name} : ${FILENAME}\"",
        "type": "code",
        "location": "/test_tipc/test_train_dy2static_python.sh:1-30"
    },
    "8693": {
        "file_id": 640,
        "content": "Source common functions and set IFS to handle line breaks. Read the BASE_CONFIG_FILE, identify MODE, get log path, delete existing directory if it exists, create a new one, set CUDNN deterministic for stable results, read base config, parse sub commands, and output relevant information.",
        "type": "comment"
    },
    "8694": {
        "file_id": 640,
        "content": "    echo ${config_lines} > $FILENAME\n    sed -i 's/gpu_list.*$/gpu_list:0/g' $FILENAME\n    # execute the last line command\n    custom_cmd=$(echo $config_lines | tail -n 1)\n    echo \"CustomCmd is: \" $custom_cmd\n    eval $custom_cmd\n    IFS=$'\\n'\n    # start dygraph train\n    dygraph_output=$LOG_PATH/${config_name}_python_train_infer_dygraph_output.txt\n    dygraph_loss=$LOG_PATH/${config_name}_dygraph_loss.txt\n    cmd=\"bash test_tipc/test_train_inference_python.sh ${FILENAME} $MODE >$dygraph_output 2>&1\"\n    echo $cmd\n    eval $cmd\n    # start dy2static train\n    dy2static_output=$LOG_PATH/${config_name}_python_train_infer_dy2static_output.txt\n    dy2static_loss=$LOG_PATH/${config_name}_dy2static_loss.txt\n    sed -i '16s/$/ -o to_static=True/' ${FILENAME}\n    cmd=\"bash test_tipc/test_train_inference_python.sh ${FILENAME} $MODE >$dy2static_output 2>&1\"\n    echo $cmd\n    eval $cmd\n    # analysis and compare the losses.\n    dyout=`cat $dy2static_output | python test_tipc/extract_loss.py -v 'train step' -e 'loss: {%f} ' | head -n 3`",
        "type": "code",
        "location": "/test_tipc/test_train_dy2static_python.sh:31-57"
    },
    "8695": {
        "file_id": 640,
        "content": "This code is configuring, running and analyzing two different training models. It first sets the necessary environment, then runs a dygraph training model and a dy2static one, saving their results in separate logs. Finally, it extracts the losses from the dy2static log for comparison with the dygraph's loss.",
        "type": "comment"
    },
    "8696": {
        "file_id": 640,
        "content": "    stout=`cat $dygraph_output   | python test_tipc/extract_loss.py -v 'train step' -e 'loss: {%f} ' | head -n 3`\n    echo $dyout > $dygraph_loss\n    echo $stout > $dy2static_loss\n    diff_log=$LOG_PATH/${config_name}_diff_log.txt\n    diff_cmd=\"diff -w $dygraph_loss $dy2static_loss > $diff_log\"\n    eval $diff_cmd\n    last_status=$?\n    cat $diff_log\n    if [ \"$dyout\" = \"\" ]; then\n        status_check 1 $diff_cmd $status_log $model_name $diff_log\n    elif [ \"$stout\" = \"\" ]; then\n        status_check 2 $diff_cmd $status_log $model_name $diff_log\n    else\n        status_check $last_status $diff_cmd $status_log $model_name $diff_log\n    fi\ndone",
        "type": "code",
        "location": "/test_tipc/test_train_dy2static_python.sh:58-73"
    },
    "8697": {
        "file_id": 640,
        "content": "This code compares the outputs of two models (dygraph_loss and dy2static_loss), checks for differences using a diff command, and logs the result to diff_log. If either dyout or stout is empty, it runs status_check with different codes. Finally, it prints the diff_log.",
        "type": "comment"
    },
    "8698": {
        "file_id": 641,
        "content": "/test_tipc/test_train_inference_python.sh",
        "type": "filepath"
    },
    "8699": {
        "file_id": 641,
        "content": "This code optimizes PaddleVideo model performance by configuring environment variables for efficient training or export tasks, evaluates models, saves trained models, logs, and runs evaluation scripts.",
        "type": "summary"
    }
}