{
    "2600": {
        "file_id": 201,
        "content": "        for idx, sub_action in enumerate(sub_actions):\n            if sub_action['end_id'] - sub_action['start_id'] > bmn_window:\n                sub_actions.pop(idx)\n        # 【滑动窗口，把每一个视频里的动作片段提取出来】\n        root_actions = [sub_actions[0]]\n        # before_id, 前一动作的最后一帧\n        # after_id, 后一动作的第一帧\n        before_id = 0\n        for idx in range(1, len(sub_actions)):\n            cur_action = sub_actions[idx]\n            duration = (cur_action['end_id'] - root_actions[0]['start_id'])\n            if duration > bmn_window:  # windows只能包住一个动作就包，包不住就包多个\n                after_id = cur_action['start_id']\n                gts_bmn['gts'][-1]['root_actions'].append({\n                    'before_id':\n                    before_id,\n                    'after_id':\n                    after_id,\n                    'actions':\n                    root_actions\n                })\n                before_id = root_actions[-1]['end_id']  #更新滑窗\n                root_actions = [cur_action]\n            else:\n                root_actions.append(cur_action)",
        "type": "code",
        "location": "/applications/TableTennis/get_instance_for_bmn.py:49-74"
    },
    "2601": {
        "file_id": 201,
        "content": "This code extracts video action segments using a sliding window and stores them in the \"root_actions\" list. If the duration of an action is too long, it splits it into multiple actions and appends them to the \"gts_bmn['gts'][-1]['root_actions']\". The \"before_id\" and \"after_id\" keep track of the first and last frame of each extracted action, while the \"bmn_window\" determines the maximum duration for a single action.",
        "type": "comment"
    },
    "2602": {
        "file_id": 201,
        "content": "            if idx == len(sub_actions) - 1:\n                after_id = max_length\n                gts_bmn['gts'][-1]['root_actions'].append({\n                    'before_id':\n                    before_id,\n                    'after_id':\n                    after_id,\n                    'actions':\n                    root_actions\n                })\n    return gts_bmn\ndef combile_gts(gts_bmn, gts_process, mode):\n    \"\"\"\n    1、bmn_window 范围内只有一个动作，只取一个目标框\n    2、bmn_window 范围内有多个动作，取三个目标框(第一个动作、最后一个动作、所有动作)\n    \"\"\"\n    global fps\n    fps = gts_process['fps']\n    duration_second = bmn_window * 1.0\n    duration_frame = bmn_window * fps\n    feature_frame = duration_frame\n    for item in gts_process['gts']:\n        url = item['url']\n        basename = os.path.basename(url).split('.')[0]\n        root_actions = item['root_actions']\n        # 把每一个视频里的动作片段提取出来\n        for root_action in root_actions:\n            segments = []\n            # all actions\n            segments.append({\n                'actions': root_action['actions'],",
        "type": "code",
        "location": "/applications/TableTennis/get_instance_for_bmn.py:75-108"
    },
    "2603": {
        "file_id": 201,
        "content": "The code is defining a function `combile_gts` that takes in `gts_bmn`, `gts_process`, and `mode` as parameters. It sets `fps` based on the `gts_process` data, calculates `duration_frame` and `feature_frame`. Then it iterates over the `gts_process['gts']` list to extract action segments from each item's root actions, appending them to the `segments` list. The function returns these segments.",
        "type": "comment"
    },
    "2604": {
        "file_id": 201,
        "content": "                'before_id': root_action['before_id'],\n                'after_id': root_action['after_id']\n            })\n            if len(root_action['actions']) > 1:  #如果有多个动作，则第一个动作和最后一个动作，额外添加一次\n                # first action\n                segments.append({\n                    'actions': [root_action['actions'][0]],\n                    'before_id':\n                    root_action['before_id'],\n                    'after_id':\n                    root_action['actions'][1]['start_id']\n                })\n                # last action\n                segments.append({\n                    'actions': [root_action['actions'][-1]],\n                    'before_id':\n                    root_action['actions'][-2]['end_id'],\n                    'after_id':\n                    root_action['after_id']\n                })\n            # 把动作片段处理成window size大小，以适配BMN输入\n            for segment in segments:\n                before_id = segment['before_id']\n                after_id = segment['after_id']\n                actions = segment['actions']",
        "type": "code",
        "location": "/applications/TableTennis/get_instance_for_bmn.py:109-134"
    },
    "2605": {
        "file_id": 201,
        "content": "This code processes a list of actions and splits them into segments based on the number of elements. It adds extra segments for the first and last actions if there are more than one. Then, it processes each segment to fit a window size for compatibility with BMN input.",
        "type": "comment"
    },
    "2606": {
        "file_id": 201,
        "content": "                # before_id到after_id太长了，从里面取window_size帧，要先确定一个起始点，然后动作都要包住\n                box0 = max(actions[-1]['end_id'] - bmn_window,\n                           before_id)  #确定起始点\n                box1 = min(actions[0]['start_id'],\n                           after_id - bmn_window)  #确实起始点\n                if box0 <= box1:  # 一次检查\n                    if int(box0) - int(box1) == 0:\n                        cur_start = box0\n                    else:\n                        box0 = math.ceil(box0)\n                        box1 = int(box1)\n                        cur_start = random.randint(box0, box1)\n                    cur_end = cur_start + bmn_window\n                    cur_start = round(cur_start, 2)\n                    cur_end = round(cur_end, 2)\n                    name = '{}_{}_{}'.format(basename, cur_start, cur_end)\n                    annotations = []\n                    for action in actions:\n                        label = str(1.0 * action['label_ids'][0])\n                        label_name = action['label_names'][0]",
        "type": "code",
        "location": "/applications/TableTennis/get_instance_for_bmn.py:135-154"
    },
    "2607": {
        "file_id": 201,
        "content": "This code snippet is determining the start and end points for a segment of video data based on action IDs. It ensures that the segment contains the entire sequence of actions, with some randomness in selecting the starting point within the specified range. The selected segment will be used to create an instance of the TableTennis application.",
        "type": "comment"
    },
    "2608": {
        "file_id": 201,
        "content": "                        seg0 = 1.0 * round((action['start_id'] - cur_start),\n                                           2)  #存储的是到开始位置(时间: s)的距离\n                        seg1 = 1.0 * round((action['end_id'] - cur_start), 2)\n                        annotations.append({\n                            'segment': [seg0, seg1],\n                            'label': label,\n                            'label_name': label_name\n                        })\n                    gts_bmn[name] = {\n                        'duration_second': duration_second,\n                        'duration_frame': duration_frame,\n                        'feature_frame': feature_frame,\n                        'subset': mode,\n                        'annotations': annotations\n                    }\n    return gts_bmn\ndef save_feature_to_numpy(gts_bmn, folder):\n    global fps\n    print('save feature for bmn ...')\n    if not os.path.exists(folder):\n        os.mkdir(folder)\n    process_gts_bmn = {}\n    miss = 0\n    for item, value in gts_bmn.items():\n        # split to rsplit 针对文件命名修改",
        "type": "code",
        "location": "/applications/TableTennis/get_instance_for_bmn.py:155-182"
    },
    "2609": {
        "file_id": 201,
        "content": "The code segment defines a function that calculates segments of video data based on start and end IDs. It then appends the calculated segments, along with their corresponding labels and label names, to an 'annotations' list. The function returns a dictionary containing information about the duration, frame rate, feature frames, subset type, and annotations for a given dataset or model (in this case, named 'bmn'). Additionally, the code defines another function that saves the calculated features to a specified folder if it doesn't exist, and handles any missing files.",
        "type": "comment"
    },
    "2610": {
        "file_id": 201,
        "content": "        basename, start_id, end_id = item.rsplit('_', 2)\n        if not basename in process_gts_bmn:\n            process_gts_bmn[basename] = []\n        process_gts_bmn[basename].append({\n            'name': item,\n            'start': float(start_id),\n            'end': float(end_id)\n        })\n    for item, values in process_gts_bmn.items():\n        feat_path = os.path.join(feat_dir, item + '.pkl')\n        feature_video = pickle.load(open(feat_path, 'rb'))['image_feature']\n        for value in values:\n            save_cut_name = os.path.join(folder, value['name'])\n            a, b, c = save_cut_name.rsplit('_', 2)\n            if float(b) > 360:\n                print(b)\n            start_frame = round(value['start'] * fps)\n            end_frame = round(value['end'] * fps)\n            if end_frame > len(feature_video):\n                miss += 1\n                continue\n            feature_cut = [\n                feature_video[i] for i in range(start_frame, end_frame)\n            ]\n            np_feature_cut = np.array(feature_cut, dtype=np.float32)",
        "type": "code",
        "location": "/applications/TableTennis/get_instance_for_bmn.py:183-207"
    },
    "2611": {
        "file_id": 201,
        "content": "The code is parsing video file names and extracting features from them. It then stores these features in a dictionary with corresponding start and end timestamps, and checks if any segments exceed the video length before saving the feature cut.",
        "type": "comment"
    },
    "2612": {
        "file_id": 201,
        "content": "            np.save(save_cut_name, np_feature_cut)\n    print('miss number (broken sample):', miss)\nif __name__ == \"__main__\":\n    if not os.path.exists(out_dir):\n        os.mkdir(out_dir)\n    gts_bmn = {}\n    for item, value in label_files.items():\n        label_file = os.path.join(dataset, value)\n        gts_data = json.load(open(label_file, 'rb'))\n        gts_process = gen_gts_for_bmn(gts_data)\n        gts_bmn = combile_gts(gts_bmn, gts_process, item)\n    with open(out_dir + '/label.json', 'w', encoding='utf-8') as f:\n        data = json.dumps(gts_bmn, indent=4, ensure_ascii=False)\n        f.write(data)\n    save_feature_to_numpy(gts_bmn, out_dir + '/feature')",
        "type": "code",
        "location": "/applications/TableTennis/get_instance_for_bmn.py:208-227"
    },
    "2613": {
        "file_id": 201,
        "content": "The code is saving processed data for a table tennis dataset. It creates a dictionary 'gts_bmn' from json files, processes it using 'gen_gts_for_bmn', combines it with existing data in 'gts_bmn', and then saves it as 'label.json' and 'feature'. It also handles creating the output directory if necessary.",
        "type": "comment"
    },
    "2614": {
        "file_id": 202,
        "content": "/applications/TableTennis/gts_format_transfer.py",
        "type": "filepath"
    },
    "2615": {
        "file_id": 202,
        "content": "Code reads a JSON file, modifies its format, and writes it back as a new JSON file.",
        "type": "summary"
    },
    "2616": {
        "file_id": 202,
        "content": "import json\nwith open('/home/aistudio/work/BMN/Input_for_bmn/label_fixed.json') as f:\n    data = json.load(f)\nf.close()\ntarget_format = {'taxonomy': None, 'database': data, 'version': None}\njsonString = json.dumps(target_format, indent=4, ensure_ascii=False)\njsonFile = open('/home/aistudio/work/BMN/Input_for_bmn/label_gts.json', 'w')\njsonFile.write(jsonString)\njsonFile.close()",
        "type": "code",
        "location": "/applications/TableTennis/gts_format_transfer.py:1-12"
    },
    "2617": {
        "file_id": 202,
        "content": "Code reads a JSON file, modifies its format, and writes it back as a new JSON file.",
        "type": "comment"
    },
    "2618": {
        "file_id": 203,
        "content": "/applications/TableTennis/predict/action_detect/action.py",
        "type": "filepath"
    },
    "2619": {
        "file_id": 203,
        "content": "The Python script uses Baidu Cloud for action detection and includes audio, image processing functions. It has classes like ActionDetection and ModelPredict to initialize models, extract features from video input, retrieve proposals using BMN, classify actions based on extracted features and proposals, and log debugging information. Results are stored in a JSON file.",
        "type": "summary"
    },
    "2620": {
        "file_id": 203,
        "content": "#!./python27-gcc482/bin/python\n# coding: utf-8\n\"\"\"\nBAIDU CLOUD action\n\"\"\"\nimport os\nimport sys\nimport pickle\nimport json\nimport time\nimport functools\nimport numpy as np\nfrom utils.preprocess import get_images\nfrom utils.config_utils import parse_config, print_configs\nimport mfcc.feature_extractor as mfcc_extractor\nimport models.pptsm_infer as image_model\nimport models.audio_infer as audio_model\nimport models.bmn_infer as prop_model\nimport models.lstm_infer as classify_model\nimport logger\nlogger = logger.Logger()\ndef record_time_info(func):\n    \"\"\"decorator func to log cost time for func\n    \"\"\"\n    @functools.wraps(func)\n    def timer(*args):\n        \"\"\"log cost time for func\n        \"\"\"\n        logger.info(\"function [{}] processing ...\".format(func.__name__))\n        start_time = time.time()\n        retval = func(*args)\n        cost_time = round(time.time() - start_time, 5)\n        logger.info(\"function [{}] run time: {:.2f} min\".format(\n            func.__name__, cost_time / 60))\n        return retval\n    return timer\nclass ActionDetection(object):",
        "type": "code",
        "location": "/applications/TableTennis/predict/action_detect/action.py:1-48"
    },
    "2621": {
        "file_id": 203,
        "content": "This code is a Python script for action detection using Baidu Cloud, which includes functions for processing audio and image data to predict actions. It utilizes various models such as mfcc_extractor, image_model, audio_model, prop_model, and classify_model. The ActionDetection class is defined, which likely contains the main logic of the action detection algorithm. The record_time_info function is a decorator used to log the time taken for executing specific functions.",
        "type": "comment"
    },
    "2622": {
        "file_id": 203,
        "content": "    \"\"\"ModelPredict\"\"\"\n    def __init__(self, cfg_file=\"configs/configs.yaml\"):\n        cfg = parse_config(cfg_file)\n        self.configs = cfg\n        print_configs(self.configs, \"Infer\")\n        name = 'COMMON'\n        self.DEBUG = cfg[name]['DEBUG']\n        self.BMN_ONLY = cfg[name]['BMN_ONLY']\n        self.LSTM_ONLY = cfg[name]['LSTM_ONLY']\n        self.PCM_ONLY = cfg[name]['PCM_ONLY']\n        if self.LSTM_ONLY:\n            self.prop_dict = {}\n            for dataset in ['EuroCup2016']:\n                prop_json = '/home/work/datasets/{}/feature_bmn/prop.json'.format(\n                    dataset)\n                json_data = json.load(open(prop_json, 'r'))\n                for item in json_data:\n                    basename = prop_json.replace('feature_bmn/prop.json', 'mp4')\n                    basename = basename + '/' + item['video_name'] + '.mp4'\n                    self.prop_dict[basename] = item['bmn_results']\n    @record_time_info\n    def load_model(self):\n        \"\"\"\n        load_model\n        \"\"\"\n        if not self.DEBUG:",
        "type": "code",
        "location": "/applications/TableTennis/predict/action_detect/action.py:49-76"
    },
    "2623": {
        "file_id": 203,
        "content": "This code defines a ModelPredict class with an initializer that reads configs from a specified file and prints them. It also checks certain conditions related to LSTM_ONLY, sets properties based on those conditions, and loads a model if not in DEBUG mode.",
        "type": "comment"
    },
    "2624": {
        "file_id": 203,
        "content": "            self.image_model = image_model.InferModel(self.configs)\n            if not self.PCM_ONLY:\n                self.audio_model = audio_model.InferModel(self.configs)\n        if not self.LSTM_ONLY:\n            self.prop_model = prop_model.InferModel(self.configs)\n        if not self.BMN_ONLY:\n            self.classify_model = classify_model.InferModel(self.configs)\n        logger.info(\"==> Action Detection prepared.\")\n    @record_time_info\n    def infer(self, imgs_path, pcm_path, fps=5):\n        \"\"\"\n        extract_feature\n        \"\"\"\n        self.imgs_path = imgs_path\n        self.pcm_path = pcm_path\n        self.configs['COMMON']['fps'] = fps\n        logger.info(\"==> input video {}\".format(os.path.basename(\n            self.imgs_path)))\n        # step 1: extract feature\n        video_features = self.extract_feature()\n        # step2: get proposal\n        bmn_results = self.extract_proposal(video_features)\n        # step3: classify\n        material = {'feature': video_features, 'proposal': bmn_results}",
        "type": "code",
        "location": "/applications/TableTennis/predict/action_detect/action.py:77-108"
    },
    "2625": {
        "file_id": 203,
        "content": "The code initializes different models for image, audio, and proposal extraction, and a classifier. It then extracts features from the input video, retrieves proposals using BMN (Bidirectional Motion Model), and finally classifies the action based on these extracted features and proposals.",
        "type": "comment"
    },
    "2626": {
        "file_id": 203,
        "content": "        action_results = self.video_classify(material)\n        return bmn_results, action_results\n    @record_time_info\n    def video_classify(self, material):\n        \"\"\"video classify\"\"\"\n        if self.BMN_ONLY:\n            return []\n        action_results = self.classify_model.predict(self.configs,\n                                                     material=material)\n        logger.info('action shape {}'.format(np.array(action_results).shape))\n        return action_results\n    @record_time_info\n    def extract_proposal(self, video_features):\n        \"\"\"extract proposal\"\"\"\n        if self.LSTM_ONLY:\n            basename = self.imgs_path.replace('frames', 'mp4') + '.mp4'\n            bmn_results = self.prop_dict[basename]\n            return bmn_results\n        bmn_results = self.prop_model.predict(self.configs,\n                                              material=video_features)\n        logger.info('proposal shape {}'.format(np.array(bmn_results).shape))\n        return bmn_results\n    @record_time_info\n    def extract_feature(self):",
        "type": "code",
        "location": "/applications/TableTennis/predict/action_detect/action.py:109-136"
    },
    "2627": {
        "file_id": 203,
        "content": "This code defines several methods for video classification and feature extraction. It uses a model called \"classify_model\" to predict actions based on input material, and another model called \"prop_model\" to extract proposals. The BMN_ONLY and LSTM_ONLY flags determine if certain models are used or not. The code also includes logging for debugging purposes.",
        "type": "comment"
    },
    "2628": {
        "file_id": 203,
        "content": "        \"\"\"extract feature\"\"\"\n        if not self.DEBUG:\n            image_path_list = get_images(self.imgs_path)\n            self.configs['PPTSM']['frame_list'] = image_path_list\n            self.configs['AUDIO']['pcm_file'] = self.pcm_path\n            image_features = self.image_model.predict(self.configs)\n            if self.PCM_ONLY:\n                sample_rate = self.configs['AUDIO']['sample_rate']\n                pcm_features = mfcc_extractor.extract_pcm(\n                    self.pcm_path, sample_rate)\n                audio_features = []\n            else:\n                audio_features, pcm_features = self.audio_model.predict(\n                    self.configs)\n            np_image_features = np.array(image_features, dtype=np.float32)\n            np_audio_features = np.array(audio_features, dtype=np.float32)\n            np_pcm_features = np.array(pcm_features, dtype=np.float32)\n            video_features = {\n                'image_feature': np_image_features,\n                'audio_feature': np_audio_features,",
        "type": "code",
        "location": "/applications/TableTennis/predict/action_detect/action.py:137-158"
    },
    "2629": {
        "file_id": 203,
        "content": "Extracts features from images and audio in a video file for further processing. If PCM_ONLY is True, extracts only MFCC features from audio using mfcc_extractor.",
        "type": "comment"
    },
    "2630": {
        "file_id": 203,
        "content": "                'pcm_feature': np_pcm_features\n            }\n        else:\n            feature_path = self.imgs_path.replace(\"frames\", \"features\") + '.pkl'\n            video_features = pickle.load(open(feature_path, 'rb'))\n        logger.info(\"feature shape {} {} {}\".format(\n            video_features['image_feature'].shape,\n            video_features['audio_feature'].shape,\n            video_features['pcm_feature'].shape))\n        return video_features\nif __name__ == '__main__':\n    model_predict = ActionDetection(cfg_file=\"../configs/configs.yaml\")\n    model_predict.load_model()\n    imgs_path = \"/home/work/datasets/EuroCup2016/frames/1be705a8f67648da8ec4b4296fa80895\"\n    pcm_path = \"/home/work/datasets/EuroCup2016/pcm/1be705a8f67648da8ec4b4296fa80895.pcm\"\n    bmn_results, action_results = model_predict.infer(imgs_path, pcm_path)\n    results = {'bmn_results': bmn_results, 'action_results': action_results}\n    with open('results.json', 'w', encoding='utf-8') as f:\n        data = json.dumps(results, indent=4, ensure_ascii=False)",
        "type": "code",
        "location": "/applications/TableTennis/predict/action_detect/action.py:159-185"
    },
    "2631": {
        "file_id": 203,
        "content": "The code loads video features from frames or pcm file and returns the features in the form of a dictionary. It then proceeds to initialize an instance of the ActionDetection class, load the model, define image and audio paths, and finally calls the infer function to generate bmn_results and action_results which are stored in the results dictionary and saved into a json file.",
        "type": "comment"
    },
    "2632": {
        "file_id": 203,
        "content": "        f.write(data)",
        "type": "code",
        "location": "/applications/TableTennis/predict/action_detect/action.py:186-186"
    },
    "2633": {
        "file_id": 203,
        "content": "Writes data to file.",
        "type": "comment"
    },
    "2634": {
        "file_id": 204,
        "content": "/applications/TableTennis/predict/action_detect/logger.py",
        "type": "filepath"
    },
    "2635": {
        "file_id": 204,
        "content": "This code defines a custom logger class for the news stripper application. It checks if the 'logs' directory exists, creates it if not, and sets up a file handler to log action detection information into \"action_detect.log\" in the 'logs' directory. The logging level is set to INFO, which will log informational messages and above (DEBUG, WARNING, ERROR, CRITICAL).",
        "type": "summary"
    },
    "2636": {
        "file_id": 204,
        "content": "\"\"\"\nlogger\n\"\"\"\nimport os\nimport logging\nclass Logger(logging.Logger):\n    \"\"\"Customized logger for news stripper\n    \"\"\"\n    def __init__(self):\n        super(Logger, self).__init__(self)\n        if not os.path.exists('logs'):\n            os.mkdir('logs')\n        handler = logging.FileHandler(\"logs/action_detect.log\")\n        # handler.setLevel(logging.DEBUG)\n        handler.setLevel(logging.INFO)\n        format = \"%(levelname)s: %(asctime)s: %(filename)s:%(lineno)d %(message)s\"\n        datefmt = \"%y-%m-%d %H:%M:%S\"\n        formatter = logging.Formatter(format, datefmt)\n        handler.setFormatter(formatter)\n        self.addHandler(handler)",
        "type": "code",
        "location": "/applications/TableTennis/predict/action_detect/logger.py:1-24"
    },
    "2637": {
        "file_id": 204,
        "content": "This code defines a custom logger class for the news stripper application. It checks if the 'logs' directory exists, creates it if not, and sets up a file handler to log action detection information into \"action_detect.log\" in the 'logs' directory. The logging level is set to INFO, which will log informational messages and above (DEBUG, WARNING, ERROR, CRITICAL).",
        "type": "comment"
    },
    "2638": {
        "file_id": 205,
        "content": "/applications/TableTennis/predict/action_detect/mfcc/feature_extractor.py",
        "type": "filepath"
    },
    "2639": {
        "file_id": 205,
        "content": "The code extracts audio features for Table Tennis prediction, using spectrogram and Mel scale transformation, and reads WAV files with VGG-16 model for MFCC and STFT feature extraction.",
        "type": "summary"
    },
    "2640": {
        "file_id": 205,
        "content": "\"\"\"\naudio feature extract\n\"\"\"\n# coding: utf-8\nimport os\nimport numpy as np\nimport pickle\nimport mfcc.vgg_params as vgg_params\nimport sys\ndef frame(data, window_length, hop_length):\n    \"\"\"\n    frame\n    \"\"\"\n    num_samples = data.shape[0]\n    #print(\"window_length , hop_length\", window_length, hop_length)\n    #print(\"num_sample = \", num_samples)\n    num_frames = 1 + int(np.floor((num_samples - window_length) / hop_length))\n    #print(\" num_frames = \", num_frames)\n    shape = (num_frames, window_length) + data.shape[1:]\n    #print(\" shape = \", shape)\n    strides = (data.strides[0] * hop_length, ) + data.strides\n    #print(\"data.strides = \", data.strides)\n    #print(\"strides = \", strides)\n    return np.lib.stride_tricks.as_strided(data, shape=shape, strides=strides)\ndef periodic_hann(window_length):\n    \"\"\"\n    periodic_hann\n    \"\"\"\n    return 0.5 - (0.5 *\n                  np.cos(2 * np.pi / window_length * np.arange(window_length)))\ndef stft_magnitude(signal, fft_length, hop_length=None, window_length=None):\n    \"\"\"\n    stft_magnitude",
        "type": "code",
        "location": "/applications/TableTennis/predict/action_detect/mfcc/feature_extractor.py:1-39"
    },
    "2641": {
        "file_id": 205,
        "content": "This code is for audio feature extraction in TableTennis application. It defines functions `frame`, `periodic_hann` and `stft_magnitude`. The `frame` function resizes the data array into frames with specified window length and hop length. The `periodic_hann` function generates a periodic Hann window for the STFT operation. Finally, `stft_magnitude` calculates the magnitude of the Short-Time Fourier Transform (STFT) of an audio signal.",
        "type": "comment"
    },
    "2642": {
        "file_id": 205,
        "content": "    \"\"\"\n    frames = frame(signal, window_length, hop_length)\n    window = periodic_hann(window_length)\n    windowed_frames = frames * window\n    return np.abs(np.fft.rfft(windowed_frames, int(fft_length)))\n_MEL_BREAK_FREQUENCY_HERTZ = 700.0\n_MEL_HIGH_FREQUENCY_Q = 1127.0\ndef hertz_to_mel(frequencies_hertz):\n    \"\"\"\n    hertz_to_mel\n    \"\"\"\n    return _MEL_HIGH_FREQUENCY_Q * np.log(1.0 + (frequencies_hertz /\n                                                 _MEL_BREAK_FREQUENCY_HERTZ))\ndef spectrogram_to_mel_matrix(num_mel_bins=20,\n                              num_spectrogram_bins=129,\n                              audio_sample_rate=8000,\n                              lower_edge_hertz=125.0,\n                              upper_edge_hertz=3800.0):\n    \"\"\"\n    spectrogram_to_mel_matrix\n    \"\"\"\n    nyquist_hertz = audio_sample_rate / 2.\n    if lower_edge_hertz >= upper_edge_hertz:\n        raise ValueError(\"lower_edge_hertz %.1f >= upper_edge_hertz %.1f\" %\n                         (lower_edge_hertz, upper_edge_hertz))",
        "type": "code",
        "location": "/applications/TableTennis/predict/action_detect/mfcc/feature_extractor.py:40-70"
    },
    "2643": {
        "file_id": 205,
        "content": "The code defines functions for feature extraction and conversion of audio signals. The \"hertz_to_mel\" function converts frequencies from Hertz to Mel scale, which is used in psychoacoustics. The \"spectrogram_to_mel_matrix\" function creates a mel-frequency cepstral coefficients (MFCC) matrix for audio spectrograms. It checks for lower and upper frequency edge validity and calculates Mel frequencies based on the provided parameters.",
        "type": "comment"
    },
    "2644": {
        "file_id": 205,
        "content": "    spectrogram_bins_hertz = np.linspace(0.0, nyquist_hertz,\n                                         num_spectrogram_bins)\n    spectrogram_bins_mel = hertz_to_mel(spectrogram_bins_hertz)\n    band_edges_mel = np.linspace(hertz_to_mel(lower_edge_hertz),\n                                 hertz_to_mel(upper_edge_hertz),\n                                 num_mel_bins + 2)\n    mel_weights_matrix = np.empty((num_spectrogram_bins, num_mel_bins))\n    for i in range(num_mel_bins):\n        lower_edge_mel, center_mel, upper_edge_mel = band_edges_mel[i:i + 3]\n        lower_slope = ((spectrogram_bins_mel - lower_edge_mel) /\n                       (center_mel - lower_edge_mel))\n        upper_slope = ((upper_edge_mel - spectrogram_bins_mel) /\n                       (upper_edge_mel - center_mel))\n        mel_weights_matrix[:,\n                           i] = np.maximum(0.0,\n                                           np.minimum(lower_slope, upper_slope))\n    mel_weights_matrix[0, :] = 0.0\n    return mel_weights_matrix\ndef log_mel_spectrogram(data,",
        "type": "code",
        "location": "/applications/TableTennis/predict/action_detect/mfcc/feature_extractor.py:71-91"
    },
    "2645": {
        "file_id": 205,
        "content": "This code is performing Mel frequency cepstral coefficients (MFCC) feature extraction on audio data. It creates spectrogram bins in Hz, converts them to the mel scale, defines mel band edges, and computes the corresponding mel weights matrix. The function returns this matrix after setting the first row to zero. This process is commonly used for speech processing and analysis.",
        "type": "comment"
    },
    "2646": {
        "file_id": 205,
        "content": "                        audio_sample_rate=8000,\n                        log_offset=0.0,\n                        window_length_secs=0.025,\n                        hop_length_secs=0.010,\n                        **kwargs):\n    \"\"\"\n    log_mel_spectrogram\n    \"\"\"\n    window_length_samples = int(round(audio_sample_rate * window_length_secs))\n    #print(\"audio_sample_rate = \", audio_sample_rate)\n    #print(\"window_length_secs = \", window_length_secs)\n    #print(\"window_length_sample \", window_length_samples)\n    hop_length_samples = int(round(audio_sample_rate * hop_length_secs))\n    #print(\"hop_length_samples \", hop_length_samples)\n    fft_length = 2**int(np.ceil(np.log(window_length_samples) / np.log(2.0)))\n    #print(\" fft_lengt = \", fft_length)\n    spectrogram = stft_magnitude(data,\n                                 fft_length=fft_length,\n                                 hop_length=hop_length_samples,\n                                 window_length=window_length_samples)\n    #print(\" spectrogram.shape = \", spectrogram.shape)",
        "type": "code",
        "location": "/applications/TableTennis/predict/action_detect/mfcc/feature_extractor.py:92-112"
    },
    "2647": {
        "file_id": 205,
        "content": "This code defines a function called `log_mel_spectrogram` which takes audio data, sample rate, and optional keyword arguments. It calculates window length in samples, hop length in samples, FFT length, and then uses the Short-Time Fourier Transform (STFT) to generate a spectrogram from the input audio data. The resulting spectrogram is stored in the `spectrogram` variable and its shape is printed for debugging or reference purposes.",
        "type": "comment"
    },
    "2648": {
        "file_id": 205,
        "content": "    mel_spectrogram = np.dot(\n        spectrogram,\n        spectrogram_to_mel_matrix(num_spectrogram_bins=spectrogram.shape[1],\n                                  audio_sample_rate=audio_sample_rate,\n                                  **kwargs))\n    return np.log(mel_spectrogram + log_offset)\ndef wav_to_example(wav_data, sample_rate):\n    \"\"\"\n    wav_to_example\n    \"\"\"\n    #sample_rate, wav_data = wavfile.read(wav_file)\n    assert wav_data.dtype == np.int16, 'Bad sample type: %r' % wav_data.dtype\n    #wav_data = wav_data[:16000*30]\n    #print(\" wav_data \", wav_data.shape)\n    #print(\" wav_data \", wav_data.shape)\n    pad_zero_num = int(sample_rate * (vgg_params.STFT_WINDOW_LENGTH_SECONDS -\n                                      vgg_params.STFT_HOP_LENGTH_SECONDS))\n    wav_data_extend = np.hstack((wav_data, np.zeros(pad_zero_num)))\n    wav_data = wav_data_extend\n    #print(\" wav_data \", wav_data.shape)\n    wav_data = wav_data / 32768.0  # Convert to [-1.0, +1.0]\n    #print(\" wav_data after convert to -1 1\", wav_data)",
        "type": "code",
        "location": "/applications/TableTennis/predict/action_detect/mfcc/feature_extractor.py:113-137"
    },
    "2649": {
        "file_id": 205,
        "content": "Function `spectrogram_to_mel_matrix` converts the spectrogram to Mel scale. Code calculates Mel spectrogram by taking dot product of spectrogram with `spectrogram_to_mel_matrix`. The result is then log transformed to avoid numerical underflow and returned.\nThe function `wav_to_example` takes wav file data, validates sample type, pads zeros to achieve desired window length, scales the wav data to range -1 to 1 by dividing by 32768.0. It is used for audio feature extraction in TableTennis application of PaddleVideo.",
        "type": "comment"
    },
    "2650": {
        "file_id": 205,
        "content": "    #if wav_data.shape[0] > max_second * sample_rate:\n    #    wav_data = wav_data[:max_second * sample_rate, :]\n    if len(wav_data.shape) > 1:\n        wav_data = np.mean(wav_data, axis=1)\n    #print(\" wav_data after mean\", wav_data.shape, len(wav_data.shape), wav_data)\n    # Resample to the rate assumed by vgg.\n    #if sample_rate != vgg_params.SAMPLE_RATE:\n    #    wav_data = resampy.resample(wav_data, sample_rate, vgg_params.SAMPLE_RATE)\n    log_mel = log_mel_spectrogram(\n        wav_data,\n        audio_sample_rate=vgg_params.SAMPLE_RATE,\n        log_offset=vgg_params.LOG_OFFSET,\n        window_length_secs=vgg_params.STFT_WINDOW_LENGTH_SECONDS,\n        hop_length_secs=vgg_params.STFT_HOP_LENGTH_SECONDS,\n        num_mel_bins=vgg_params.NUM_MEL_BINS,\n        lower_edge_hertz=vgg_params.MEL_MIN_HZ,\n        upper_edge_hertz=vgg_params.MEL_MAX_HZ)\n    # Frame features into examples.\n    features_sample_rate = 1.0 / vgg_params.STFT_HOP_LENGTH_SECONDS\n    example_window_length = int(\n        round(vgg_params.EXAMPLE_WINDOW_SECONDS * features_sample_rate))",
        "type": "code",
        "location": "/applications/TableTennis/predict/action_detect/mfcc/feature_extractor.py:138-158"
    },
    "2651": {
        "file_id": 205,
        "content": "This code extracts audio features for Table Tennis prediction. It first reshapes and resamples the input wav_data if necessary, then calculates log mel spectrogram from wav_data using given parameters. Finally, it frames these features into examples with a specific window length.",
        "type": "comment"
    },
    "2652": {
        "file_id": 205,
        "content": "    example_hop_length = int(\n        round(vgg_params.EXAMPLE_HOP_SECONDS * features_sample_rate))\n    log_mel_examples = frame(log_mel,\n                             window_length=example_window_length,\n                             hop_length=example_hop_length)\n    return log_mel_examples\ndef extract_pcm(pcm_file, sample_rate):\n    with open(pcm_file, \"rb\") as f:\n        pcm_data = f.read()\n    audio_data = np.fromstring(pcm_data, dtype=np.int16)\n    examples = wav_to_example(audio_data, sample_rate)\n    return examples\nif __name__ == \"__main__\":\n    wav_file = sys.argv[1]\n    print(\"wav_file = \", wav_file)\n    with open(wav_file, \"rb\") as f:\n        pcm_data = f.read()\n    audio_data = np.fromstring(pcm_data, dtype=np.int16)\n    examples_batch = wav_to_example(audio_data, 16000)\n    print(\"examples_batch.shape\", examples_batch.shape)",
        "type": "code",
        "location": "/applications/TableTennis/predict/action_detect/mfcc/feature_extractor.py:160-183"
    },
    "2653": {
        "file_id": 205,
        "content": "This code extracts audio features from a WAV file using the VGG-16 model, specifically focusing on MFCC (Mel Frequency Cepstral Coefficients) and STFT (Short-Time Fourier Transform). The code also defines a function to convert PCM data into examples and another to extract MFCC features. Lastly, it demonstrates how to use the code by reading a WAV file and printing its shape.",
        "type": "comment"
    },
    "2654": {
        "file_id": 206,
        "content": "/applications/TableTennis/predict/action_detect/mfcc/model_config.py",
        "type": "filepath"
    },
    "2655": {
        "file_id": 206,
        "content": "The ModelAudio class extracts audio features using wav_to_example and slices the data into parts, calculating features for each part. The predict method appends these features to a list and returns the audio feature list after dividing by sample rate.",
        "type": "summary"
    },
    "2656": {
        "file_id": 206,
        "content": "\"\"\"\naudio model config\n\"\"\"\nimport numpy as np\nimport mfcc.feature_extractor as feature_extractor\nclass ModelAudio(object):\n    \"\"\"\n    modelAudio\n    \"\"\"\n    def __init__(self, configs, use_gpu=1):\n        self.use_gpu = use_gpu\n        self.audio_fps = configs.COMMON.fps\n        self.audio_feat_scale = configs.TSN.audio_scale\n        self.sample_rate = 16000\n    def predict_slice(self, wav_data, sample_rate):\n        \"\"\"\n        audio predict\n        \"\"\"\n        examples_batch = feature_extractor.wav_to_example(\n            wav_data, sample_rate)[0]\n        return examples_batch\n    def predict_audio(self, audio_file):\n        \"\"\"\n        predict_audio\n        \"\"\"\n        audio_feature_list = []\n        # read pcm\n        sample_rate = self.sample_rate\n        try:\n            with open(audio_file, \"rb\") as f:\n                pcm_data = f.read()\n            audio_data = np.fromstring(pcm_data, dtype=np.int16)\n            audio_status = \"audio load success\"\n        except Exception as e:\n            audio_data = []\n            audio_status = \"audio load failed\"",
        "type": "code",
        "location": "/applications/BasketballAction/predict/action_detect/mfcc/model_config.py:1-42"
    },
    "2657": {
        "file_id": 206,
        "content": "The code defines a ModelAudio class which takes in audio-related configurations and performs audio feature extraction using the feature_extractor module's wav_to_example function. The class also predicts audio by converting PCM data to numpy array and handles audio file reading exceptions.",
        "type": "comment"
    },
    "2658": {
        "file_id": 206,
        "content": "        step = 1\n        len_video = int(len(audio_data) / sample_rate)\n        print(len_video)\n        for i in range(0, len_video, step):\n            audio_data_part = audio_data[i * sample_rate:(i + step) *\n                                         sample_rate]\n            feature_audio = self.predict_slice(audio_data_part, sample_rate)\n            audio_feature_list.append(feature_audio)\n        return audio_feature_list",
        "type": "code",
        "location": "/applications/BasketballAction/predict/action_detect/mfcc/model_config.py:43-51"
    },
    "2659": {
        "file_id": 206,
        "content": "The code slices the audio data into parts of size 'step' and calculates features for each part using a predict method, then appends the features to a list. The length of the entire audio data is divided by the sample rate to determine how many steps can fit in it. This function returns the audio feature list.",
        "type": "comment"
    },
    "2660": {
        "file_id": 207,
        "content": "/applications/TableTennis/predict/action_detect/mfcc/vgg_params.py",
        "type": "filepath"
    },
    "2661": {
        "file_id": 207,
        "content": "The code defines global parameters for the VGGish model, including architectural constants, hyperparameters, and optimizer settings. It extracts audio features from spectrogram patches using PCA quantization and embedding processing, with options to adjust STFT window and hop lengths, mel frequency bins, and learning rate.",
        "type": "summary"
    },
    "2662": {
        "file_id": 207,
        "content": "\"\"\"Global parameters for the VGGish model.\nSee vggish_slim.py for more information.\n\"\"\"\n# Architectural constants.\nNUM_FRAMES = 50  # Frames in input mel-spectrogram patch.\nNUM_BANDS = 64  # Frequency bands in input mel-spectrogram patch.\nEMBEDDING_SIZE = 128  # Size of embedding layer.\n# Hyperparameters used in feature and example generation.\nSAMPLE_RATE = 16000\nSTFT_WINDOW_LENGTH_SECONDS = 0.040\nSTFT_HOP_LENGTH_SECONDS = 0.020\nNUM_MEL_BINS = NUM_BANDS\nMEL_MIN_HZ = 125\nMEL_MAX_HZ = 7500\nLOG_OFFSET = 0.01  # Offset used for stabilized log of input mel-spectrogram.\nEXAMPLE_WINDOW_SECONDS = 1.00  # Each example contains 96 10ms frames\nEXAMPLE_HOP_SECONDS = 1.00  # with zero overlap.\n# Parameters used for embedding postprocessing.\nPCA_EIGEN_VECTORS_NAME = 'pca_eigen_vectors'\nPCA_MEANS_NAME = 'pca_means'\nQUANTIZE_MIN_VAL = -2.0\nQUANTIZE_MAX_VAL = +2.0\n# Hyperparameters used in training.\nINIT_STDDEV = 0.01  # Standard deviation used to initialize weights.\nLEARNING_RATE = 1e-4  # Learning rate for the Adam optimizer.",
        "type": "code",
        "location": "/applications/BasketballAction/predict/action_detect/mfcc/vgg_params.py:1-29"
    },
    "2663": {
        "file_id": 207,
        "content": "This code sets global parameters for the VGGish model. It defines architectural constants, hyperparameters for feature and example generation, embedding postprocessing, and training. The VGGish model is used to extract audio features from spectrogram patches, with options for PCA-based quantization and embedding processing. Hyperparameters control the STFT window and hop lengths, mel frequency bins, and learning rate for Adam optimizer.",
        "type": "comment"
    },
    "2664": {
        "file_id": 207,
        "content": "ADAM_EPSILON = 1e-8  # Epsilon for the Adam optimizer.\n# Names of ops, tensors, and features.\nINPUT_OP_NAME = 'vggish/input_features'\nINPUT_TENSOR_NAME = INPUT_OP_NAME + ':0'\nOUTPUT_OP_NAME = 'vggish/embedding'\nOUTPUT_TENSOR_NAME = OUTPUT_OP_NAME + ':0'\nAUDIO_EMBEDDING_FEATURE_NAME = 'audio_embedding'",
        "type": "code",
        "location": "/applications/BasketballAction/predict/action_detect/mfcc/vgg_params.py:30-37"
    },
    "2665": {
        "file_id": 207,
        "content": "This code sets the Adam optimizer's epsilon value to 1e-8, defines names for input and output operations, tensors, and features. It also assigns the name \"audio_embedding\" to a feature.",
        "type": "comment"
    },
    "2666": {
        "file_id": 208,
        "content": "/applications/TableTennis/predict/action_detect/models/audio_infer.py",
        "type": "filepath"
    },
    "2667": {
        "file_id": 208,
        "content": "The code defines a \"InferModel\" class for audio inference using PaddleVideo, performs prediction on an audio file, and outputs shape, first value, and time taken. The model's predict function is called with 32-sample data as placeholders.",
        "type": "summary"
    },
    "2668": {
        "file_id": 208,
        "content": "\"\"\"\nppTSM InferModel\n\"\"\"\nimport sys\nimport numpy as np\nimport time\nsys.path.append('../')\nfrom utils.preprocess import get_images\nfrom utils.config_utils import parse_config\nimport reader\nfrom paddle.inference import Config\nfrom paddle.inference import create_predictor\nclass InferModel(object):\n    \"\"\"audio infer\"\"\"\n    def __init__(self, cfg, name='AUDIO'):\n        name = name.upper()\n        self.name = name\n        model_file = cfg[name]['model_file']\n        params_file = cfg[name]['params_file']\n        gpu_mem = cfg[name]['gpu_mem']\n        device_id = cfg[name]['device_id']\n        # model init\n        config = Config(model_file, params_file)\n        config.enable_use_gpu(gpu_mem, device_id)\n        config.switch_ir_optim(True)  # default true\n        config.enable_memory_optim()\n        # use zero copy\n        config.switch_use_feed_fetch_ops(False)\n        self.predictor = create_predictor(config)\n        input_names = self.predictor.get_input_names()\n        self.input_tensor = self.predictor.get_input_handle(input_names[0])",
        "type": "code",
        "location": "/applications/TableTennis/predict/action_detect/models/audio_infer.py:1-37"
    },
    "2669": {
        "file_id": 208,
        "content": "The code above defines a class \"InferModel\" for audio inference. It initializes the model by setting the model file, parameters file, GPU memory, and device ID from a configuration file. The code enables IR optimization, memory optimization, and disables zero copy. Finally, it creates a predictor object and retrieves the input handle for the first input name.",
        "type": "comment"
    },
    "2670": {
        "file_id": 208,
        "content": "        output_names = self.predictor.get_output_names()\n        self.output_tensor = self.predictor.get_output_handle(output_names[0])\n    def infer(self, input):\n        \"\"\"infer\"\"\"\n        self.input_tensor.copy_from_cpu(input)\n        self.predictor.run()\n        output = self.output_tensor.copy_to_cpu()\n        return output\n    def predict(self, infer_config):\n        \"\"\"predict\"\"\"\n        infer_reader = reader.get_reader(self.name, 'infer', infer_config)\n        feature_list = []\n        pcm_list = []\n        for infer_iter, data in enumerate(infer_reader()):\n            inputs = np.array(data, dtype='float32')\n            output = self.infer(inputs)\n            feature_list.append(np.squeeze(output))\n            pcm_list.append(inputs)\n        feature_values = np.vstack(feature_list)\n        pcm_values = np.vstack(pcm_list)\n        return feature_values, pcm_values\nif __name__ == \"__main__\":\n    cfg_file = '/home/work/inference/configs/configs.yaml'\n    cfg = parse_config(cfg_file)\n    model = InferModel(cfg)",
        "type": "code",
        "location": "/applications/TableTennis/predict/action_detect/models/audio_infer.py:39-67"
    },
    "2671": {
        "file_id": 208,
        "content": "This code defines a class for audio inference using the PaddleVideo library. It has an `infer` method that takes input data and returns output data after running the inference, and a `predict` method that loops through inferencer data, performs inference for each data point, and returns feature values and pcm values as arrays. The main part of the code initializes an instance of this class using a configuration file (configs.yaml), which is then used for further processing.",
        "type": "comment"
    },
    "2672": {
        "file_id": 208,
        "content": "    pcm_path = '/home/work/datasets/WorldCup2018/pcm/6e577252c4004961ac7caa738a52c238.pcm'\n    t0 = time.time()\n    cfg['AUDIO']['pcm_file'] = pcm_path\n    outputs = model.predict(cfg)\n    # outputs = model.infer(np.random.rand(32, 8, 3, 224, 224).astype(np.float32))\n    t1 = time.time()\n    print(outputs.shape)\n    print(outputs[0])\n    print('cost time = {} min'.format((t1 - t0) / 60.0))",
        "type": "code",
        "location": "/applications/TableTennis/predict/action_detect/models/audio_infer.py:69-78"
    },
    "2673": {
        "file_id": 208,
        "content": "This code loads an audio file, configures the model with its path, performs prediction, and prints the output shape, first output value, and time taken for the process. The model's predict function is called with a random 32-sample data array as placeholders.",
        "type": "comment"
    },
    "2674": {
        "file_id": 209,
        "content": "/applications/TableTennis/predict/action_detect/models/bmn_infer.py",
        "type": "filepath"
    },
    "2675": {
        "file_id": 209,
        "content": "The \"InferModel\" class is a GPU-optimized inference function for generating action boundaries in videos using propositions and scoring functions. It calculates running averages of predictions, predicts video features, and saves results as proposals in 'results.json'.",
        "type": "summary"
    },
    "2676": {
        "file_id": 209,
        "content": "\"\"\"\nppTSM InferModel\n\"\"\"\nimport sys\nimport numpy as np\nimport json\nimport pickle\nimport time\nsys.path.append('../')\nfrom utils.preprocess import get_images\nfrom utils.config_utils import parse_config\nfrom utils.process_result import process_proposal\nimport reader\nfrom paddle.inference import Config\nfrom paddle.inference import create_predictor\nclass InferModel(object):\n    \"\"\"bmn infer\"\"\"\n    def __init__(self, cfg, name='BMN'):\n        name = name.upper()\n        self.name = name\n        model_file = cfg[name]['model_file']\n        params_file = cfg[name]['params_file']\n        gpu_mem = cfg[name]['gpu_mem']\n        device_id = cfg[name]['device_id']\n        self.nms_thread = cfg[name]['nms_thread']\n        self.min_pred_score = cfg[name]['score_thread']\n        self.min_frame_thread = cfg['COMMON']['fps']\n        # model init\n        config = Config(model_file, params_file)\n        config.enable_use_gpu(gpu_mem, device_id)\n        config.switch_ir_optim(True)  # default true\n        config.enable_memory_optim()\n        # use zero copy",
        "type": "code",
        "location": "/applications/TableTennis/predict/action_detect/models/bmn_infer.py:1-39"
    },
    "2677": {
        "file_id": 209,
        "content": "The code defines a class called \"InferModel\" which implements the bmn infer function. It initializes the model with specified configuration and enables GPU usage if available. The class takes in a config file that specifies model, parameters files, GPU memory, device ID, thread count for nms, minimum prediction score threshold, and frame thread count. The code also switches on IR optimizations and enables memory optimization for efficient execution of the model.",
        "type": "comment"
    },
    "2678": {
        "file_id": 209,
        "content": "        config.switch_use_feed_fetch_ops(False)\n        self.predictor = create_predictor(config)\n        input_names = self.predictor.get_input_names()\n        self.input_tensor = self.predictor.get_input_handle(input_names[0])\n        output_names = self.predictor.get_output_names()\n        self.output1_tensor = self.predictor.get_output_handle(output_names[0])\n        self.output2_tensor = self.predictor.get_output_handle(output_names[1])\n        self.output3_tensor = self.predictor.get_output_handle(output_names[2])\n    def infer(self, input):\n        \"\"\"infer\"\"\"\n        self.input_tensor.copy_from_cpu(input)\n        self.predictor.run()\n        output1 = self.output1_tensor.copy_to_cpu()\n        output2 = self.output2_tensor.copy_to_cpu()\n        output3 = self.output3_tensor.copy_to_cpu()\n        return output1, output2, output3\n    def generate_props(self,\n                       pred_bmn,\n                       pred_start,\n                       pred_end,\n                       max_window=200,\n                       min_window=5):",
        "type": "code",
        "location": "/applications/TableTennis/predict/action_detect/models/bmn_infer.py:40-65"
    },
    "2679": {
        "file_id": 209,
        "content": "The code initializes a predictor, sets input and output tensors for inference, and defines an \"infer\" method to perform inference. The \"generate_props\" function takes predictions, start and end timestamps, and generates properties based on the given parameters.",
        "type": "comment"
    },
    "2680": {
        "file_id": 209,
        "content": "        \"\"\"generate_props\"\"\"\n        video_len = min(pred_bmn.shape[-1],\n                        min(pred_start.shape[-1], pred_end.shape[-1]))\n        pred_bmn = pred_bmn[0, :, :] * pred_bmn[1, :, :]\n        start_mask = self.boundary_choose(pred_start)\n        start_mask[0] = 1.\n        end_mask = self.boundary_choose(pred_end)\n        end_mask[-1] = 1.\n        score_results = []\n        for idx in range(min_window, max_window):\n            for jdx in range(video_len):\n                start_index = jdx\n                end_index = start_index + idx\n                if end_index < video_len and start_mask[\n                        start_index] == 1 and end_mask[end_index] == 1:\n                    xmin = start_index\n                    xmax = end_index\n                    xmin_score = pred_start[start_index]\n                    xmax_score = pred_end[end_index]\n                    bmn_score = pred_bmn[idx, jdx]\n                    conf_score = xmin_score * xmax_score * bmn_score\n                    score_results.append([xmin, xmax, conf_score])",
        "type": "code",
        "location": "/applications/TableTennis/predict/action_detect/models/bmn_infer.py:66-87"
    },
    "2681": {
        "file_id": 209,
        "content": "This code generates propositions for action boundaries in a video. It iterates through possible window sizes to find valid start and end indices, checks if start and end masks match, then computes the confidence score based on boundary scores and BNM score. The results are stored as a list of [xmin, xmax, confidence] values.",
        "type": "comment"
    },
    "2682": {
        "file_id": 209,
        "content": "        return score_results\n    def boundary_choose(self, score_list):\n        \"\"\"boundary_choose\"\"\"\n        max_score = max(score_list)\n        mask_high = (score_list > max_score * 0.5)\n        score_list = list(score_list)\n        score_middle = np.array([0.0] + score_list + [0.0])\n        score_front = np.array([0.0, 0.0] + score_list)\n        score_back = np.array(score_list + [0.0, 0.0])\n        mask_peak = ((score_middle > score_front) & (score_middle > score_back))\n        mask_peak = mask_peak[1:-1]\n        mask = (mask_high | mask_peak).astype('float32')\n        return mask\n    def predict(self, infer_config, material):\n        \"\"\"predict\"\"\"\n        infer_reader = reader.get_reader(self.name,\n                                         'infer',\n                                         infer_config,\n                                         material=material)\n        feature_list = []\n        for infer_iter, data in enumerate(infer_reader()):\n            inputs = [items[0] for items in data]\n            winds = [items[1] for items in data]",
        "type": "code",
        "location": "/applications/TableTennis/predict/action_detect/models/bmn_infer.py:88-112"
    },
    "2683": {
        "file_id": 209,
        "content": "The code defines three functions: \"action_detect.models.bmn_infer.py\" contains the \"score_results\", \"boundary_choose\", and \"predict\" functions. The \"score_results\" function returns a list of scores for each action. The \"boundary_choose\" function determines boundary scores based on peak, front, and back scores. It uses masks to identify relevant positions in the score list. Finally, the \"predict\" function initializes an infer reader, iterates through data, and gathers input data for prediction.",
        "type": "comment"
    },
    "2684": {
        "file_id": 209,
        "content": "            feat_info = [items[2] for items in data]\n            feature_T = feat_info[0][0]\n            feature_N = feat_info[0][1]\n            inputs = np.array(inputs)\n            pred_bmn, pred_sta, pred_end = self.infer(inputs)\n            if infer_iter == 0:\n                sum_pred_bmn = np.zeros((2, feature_N, feature_T))\n                sum_pred_sta = np.zeros((feature_T, ))\n                sum_pred_end = np.zeros((feature_T, ))\n                sum_pred_cnt = np.zeros((feature_T, ))\n            for idx, sub_wind in enumerate(winds):\n                sum_pred_bmn[:, :, sub_wind[0]:sub_wind[1]] += pred_bmn[idx]\n                sum_pred_sta[sub_wind[0]:sub_wind[1]] += pred_sta[idx]\n                sum_pred_end[sub_wind[0]:sub_wind[1]] += pred_end[idx]\n                sum_pred_cnt[sub_wind[0]:sub_wind[1]] += np.ones(\n                    (sub_wind[1] - sub_wind[0], ))\n        pred_bmn = sum_pred_bmn / sum_pred_cnt\n        pred_sta = sum_pred_sta / sum_pred_cnt\n        pred_end = sum_pred_end / sum_pred_cnt",
        "type": "code",
        "location": "/applications/TableTennis/predict/action_detect/models/bmn_infer.py:113-135"
    },
    "2685": {
        "file_id": 209,
        "content": "This code performs a running average of predictions from a series of windows. It calculates the sum of each prediction for each window, divides it by the count of non-zero frames in that window, and stores the results in `sum_pred_bmn`, `sum_pred_sta`, and `sum_pred_end`. Finally, it divides these sums by the corresponding counts to get the final predictions.",
        "type": "comment"
    },
    "2686": {
        "file_id": 209,
        "content": "        score_result = self.generate_props(pred_bmn, pred_sta, pred_end)\n        results = process_proposal(score_result, self.min_frame_thread,\n                                   self.nms_thread, self.min_pred_score)\n        return results\nif __name__ == \"__main__\":\n    cfg_file = '/home/work/inference/configs/configs.yaml'\n    cfg = parse_config(cfg_file)\n    model = InferModel(cfg)\n    imgs_path = '/home/work/datasets/WorldCup2018/frames/6e577252c4004961ac7caa738a52c238'\n    # feature\n    feature_path = imgs_path.replace(\"frames\", \"features\") + '.pkl'\n    video_features = pickle.load(open(feature_path, 'rb'))\n    t0 = time.time()\n    outputs = model.predict(cfg, video_features)\n    # outputs = model.infer(np.random.rand(32, 8, 3, 224, 224).astype(np.float32))\n    t1 = time.time()\n    results = {'proposal': outputs}\n    with open('results.json', 'w', encoding='utf-8') as f:\n        data = json.dumps(results, indent=4, ensure_ascii=False)\n        f.write(data)\n    print('cost time = {} min'.format((t1 - t0) / 60.0))",
        "type": "code",
        "location": "/applications/TableTennis/predict/action_detect/models/bmn_infer.py:137-164"
    },
    "2687": {
        "file_id": 209,
        "content": "The code initializes an instance of the InferModel class from the given configuration file. It then predicts the video features by calling the model's predict method, passing the video features as input and returns the results in the form of proposals. The output is then saved to a JSON file named 'results.json'.",
        "type": "comment"
    },
    "2688": {
        "file_id": 210,
        "content": "/applications/TableTennis/predict/action_detect/models/lstm_infer.py",
        "type": "filepath"
    },
    "2689": {
        "file_id": 210,
        "content": "This code initializes a Table Tennis action detection model using LSTM, loads configurations, and processes proposals for multiple datasets. It applies inference, predicts actions on video features, sorts predictions, and saves results in JSON format.",
        "type": "summary"
    },
    "2690": {
        "file_id": 210,
        "content": "\"\"\"\nppTSM InferModel\n\"\"\"\nimport sys\nimport numpy as np\nimport json\nimport pickle\nimport time\nsys.path.append('../')\nfrom utils.preprocess import get_images\nfrom utils.config_utils import parse_config\nfrom utils.process_result import get_action_result\nimport reader\nfrom paddle.inference import Config\nfrom paddle.inference import create_predictor\nclass InferModel(object):\n    \"\"\"lstm infer\"\"\"\n    def __init__(self, cfg, name='ACTION'):\n        name = name.upper()\n        self.name = name\n        model_file = cfg[name]['model_file']\n        params_file = cfg[name]['params_file']\n        gpu_mem = cfg[name]['gpu_mem']\n        device_id = cfg[name]['device_id']\n        self.topk = cfg[name]['topk']\n        self.frame_offset = cfg[name]['nms_offset']\n        self.nms_thread = cfg[name]['nms_thread']\n        self.cls_thread = cfg[name]['classify_score_thread']\n        self.iou_thread = cfg[name]['iou_score_thread']\n        self.label_map_file = cfg['COMMON']['label_dic']\n        self.fps = cfg['COMMON']['fps']\n        self.nms_id = 5",
        "type": "code",
        "location": "/applications/TableTennis/predict/action_detect/models/lstm_infer.py:1-38"
    },
    "2691": {
        "file_id": 210,
        "content": "This code defines a class named InferModel that implements an LSTM model for action detection. The model is initialized with configuration parameters, including the path to the model and parameter files, GPU memory usage, and device ID. Additional configuration settings include topk, frame_offset, nms_thread, classify_score_thread, iou_score_thread, label_dic, fps, and nms_id. These parameters control various aspects of the action detection process. The code imports necessary libraries and modules for preprocessing, config utilities, result processing, and model loading from PaddlePaddle.",
        "type": "comment"
    },
    "2692": {
        "file_id": 210,
        "content": "        # model init\n        config = Config(model_file, params_file)\n        config.enable_use_gpu(gpu_mem, device_id)\n        config.switch_ir_optim(True)  # default true\n        config.enable_memory_optim()\n        # use zero copy\n        config.switch_use_feed_fetch_ops(False)\n        self.predictor = create_predictor(config)\n        input_names = self.predictor.get_input_names()\n        self.input1_tensor = self.predictor.get_input_handle(input_names[0])\n        self.input2_tensor = self.predictor.get_input_handle(input_names[1])\n        output_names = self.predictor.get_output_names()\n        self.output1_tensor = self.predictor.get_output_handle(output_names[0])\n        self.output2_tensor = self.predictor.get_output_handle(output_names[1])\n    def infer(self, input1_arr, input1_lod, input2_arr=None, input2_lod=None):\n        \"\"\"infer\"\"\"\n        self.input1_tensor.copy_from_cpu(input1_arr)\n        self.input1_tensor.set_lod(input1_lod)\n        if not input2_arr is None:\n            self.input2_tensor.copy_from_cpu(input2_arr)",
        "type": "code",
        "location": "/applications/TableTennis/predict/action_detect/models/lstm_infer.py:40-62"
    },
    "2693": {
        "file_id": 210,
        "content": "Initializes model and sets up input/output tensors for inferencing.",
        "type": "comment"
    },
    "2694": {
        "file_id": 210,
        "content": "            self.input2_tensor.set_lod(input2_lod)\n        self.predictor.run()\n        output1 = self.output1_tensor.copy_to_cpu()\n        output2 = self.output2_tensor.copy_to_cpu()\n        # print(output.shape)\n        return output1, output2\n    def pre_process(self, input):\n        \"\"\"pre process\"\"\"\n        input_arr = []\n        input_lod = [0]\n        start_lod = 0\n        end_lod = 0\n        for sub_item in input:\n            end_lod = start_lod + len(sub_item)\n            input_lod.append(end_lod)\n            input_arr.extend(sub_item)\n            start_lod = end_lod\n        input_arr = np.array(input_arr)\n        # print(input_arr.shape)\n        # print([input_lod])\n        return input_arr, [input_lod]\n    def predict(self, infer_config, material):\n        \"\"\"predict\"\"\"\n        infer_reader = reader.get_reader(self.name,\n                                         'infer',\n                                         infer_config,\n                                         material=material)\n        results = []",
        "type": "code",
        "location": "/applications/TableTennis/predict/action_detect/models/lstm_infer.py:63-92"
    },
    "2695": {
        "file_id": 210,
        "content": "The code is part of a model for action detection in Table Tennis. It sets the input's layout of dimension (LOD) and performs preprocessing, prediction, and returns output results. The LOD defines the shape of data along the spatial dimensions.",
        "type": "comment"
    },
    "2696": {
        "file_id": 210,
        "content": "        for infer_iter, data in enumerate(infer_reader()):\n            video_id = [[items[-2], items[-1]] for items in data]\n            input1 = [items[0] for items in data]\n            input2 = [items[1] for items in data]\n            input1_arr, input1_lod = self.pre_process(input1)\n            input2_arr, input2_lod = self.pre_process(input2)\n            output1, output2 = self.infer(input1_arr, input1_lod, input2_arr,\n                                          input2_lod)\n            # output1, output2 = self.infer(input1_arr, input1_lod)\n            predictions_id = output1\n            predictions_iou = output2\n            for i in range(len(predictions_id)):\n                topk_inds = predictions_id[i].argsort()[0 - self.topk:]\n                topk_inds = topk_inds[::-1]\n                preds_id = predictions_id[i][topk_inds]\n                preds_iou = predictions_iou[i][0]\n                results.append((video_id[i], preds_id.tolist(),\n                                topk_inds.tolist(), preds_iou.tolist()))",
        "type": "code",
        "location": "/applications/TableTennis/predict/action_detect/models/lstm_infer.py:93-111"
    },
    "2697": {
        "file_id": 210,
        "content": "This code iterates through a data source, preprocesses the input, and performs inference on it using a model. The resulting outputs are then sorted to obtain the top k predictions for each input. The video ID, predicted action IDs, sorted indices, and IOU scores are stored in a results list.",
        "type": "comment"
    },
    "2698": {
        "file_id": 210,
        "content": "        predict_result = get_action_result(results, self.label_map_file,\n                                           self.fps, self.cls_thread,\n                                           self.iou_thread, self.nms_id,\n                                           self.nms_thread, self.frame_offset)\n        return predict_result\nif __name__ == \"__main__\":\n    cfg_file = '/home/work/inference/configs/configs.yaml'\n    cfg = parse_config(cfg_file)\n    model = InferModel(cfg)\n    # proposal total\n    prop_dict = {}\n    for dataset in ['EuroCup2016', 'WorldCup2018']:\n        prop_json = '/home/work/datasets/{}/feature_bmn/prop.json'.format(\n            dataset)\n        json_data = json.load(open(prop_json, 'r'))\n        for item in json_data:\n            basename = prop_json.replace('feature_bmn/prop.json', 'mp4')\n            basename = basename + '/' + item['video_name'] + '.mp4'\n            prop_dict[basename] = item['bmn_results']\n    imgs_path = '/home/work/datasets/WorldCup2018/frames/6e577252c4004961ac7caa738a52c238'",
        "type": "code",
        "location": "/applications/TableTennis/predict/action_detect/models/lstm_infer.py:113-136"
    },
    "2699": {
        "file_id": 210,
        "content": "The code is a part of a Table Tennis action detection model implemented using LSTM (Long Short-Term Memory). It loads configurations from a YAML file, initializes the model, and processes proposals for multiple datasets. The model takes results from previous processing steps, applies inference based on labels, frame rate, and other parameters, and returns the final prediction result.",
        "type": "comment"
    }
}